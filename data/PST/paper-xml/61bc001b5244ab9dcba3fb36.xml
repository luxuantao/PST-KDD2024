<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Structure Learning with Variational Information Bottleneck</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-16">16 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingyun</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<email>penghao@act.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
							<email>jia.wu@mq.edu.au</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingcheng</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Cheng</surname></persName>
							<email>jicheng@act.buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shenyuan Honors College</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Structure Learning with Variational Information Bottleneck</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-16">16 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.08903v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have shown promising results on a broad spectrum of applications. Most empirical studies of GNNs directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, graphs in the real-world are inevitably noisy or incomplete, which could even exacerbate the quality of graph representations. In this work, we propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the perspective of information theory. VIB-GSL advances the Information Bottleneck (IB) principle for graph structure learning, providing a more elegant and universal framework for mining underlying task-relevant relations. VIB-GSL learns an informative and compressive graph structure to distill the actionable information for specific downstream tasks. VIB-GSL deduces a variational approximation for irregular graph data to form a tractable IB objective function, which facilitates training stability. Extensive experimental results demonstrate that the superior effectiveness and robustness of VIB-GSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen a significant growing amount of interest in graph representation learning <ref type="bibr" target="#b32">(Zhang et al. 2018;</ref><ref type="bibr" target="#b25">Tong et al. 2021)</ref>, especially in efforts devoted to developing more effective graph neural networks (GNNs) <ref type="bibr" target="#b35">(Zhou et al. 2020)</ref>. Despite GNNs' powerful ability in learning graph representations, most of them directly take the observed graph as input, assuming the observed structure perfectly depicts the accurate and complete relations between nodes. However, these raw graphs are naturally admitted from network-structure data (e.g., social network) or constructed from the original feature space by some pre-defined rules, which are usually independent of the downstream tasks and lead to the gap between the raw graph and the optimal graph for specific tasks. Moreover, most of graphs in the real-word are noisy or incomplete due to the error-prone data collection <ref type="bibr" target="#b4">(Chen, Wu, and Zaki 2020)</ref>, which could even exacerbate the quality of representations produced by GNNs <ref type="bibr">(Zügner, Akbarnejad, and Günnemann 2018;</ref><ref type="bibr" target="#b21">Sun et al. 2018</ref>). It's also found that the properties of a graph are mainly determined by some critical structures rather than the whole graph <ref type="bibr" target="#b22">(Sun et al. 2021;</ref><ref type="bibr" target="#b15">Peng et al. 2021)</ref>. Furthermore, many graph enhanced applications (e.g., text classification <ref type="bibr">(Li et al. 2020</ref>) and vision navigation <ref type="bibr">(Gao et al. 2021</ref>)) may only have data without graph-structure and require additional graph construction to perform representation learning. The above issues pose a great challenge for applying GNNs to real-world applications, especially in some risk-critical scenarios. Therefore, learning a task-relevant graph structure is a fundamental problem for graph representation learning.</p><p>To adaptively learn graph structures for GNNs, many graph structure learning methods <ref type="bibr" target="#b36">(Zhu et al. 2021;</ref><ref type="bibr" target="#b7">Franceschi et al. 2019;</ref><ref type="bibr" target="#b4">Chen, Wu, and Zaki 2020)</ref> are proposed, most of which optimize the adjacency matrix along with the GNN parameters toward downstream tasks with assumptions (e.g., community) or certain constraints (e.g., sparsity, low-rank, and smoothness) on the graphs. However, these assumptions or explicit certain constraints may not be applicable to all datasets and tasks. There is still a lack of a general framework that can mine underlying relations from the essence of representation learning.</p><p>Recalling the above problems, the key of structure learning problem is learning the underlying relations invariant to task-irrelevant information. Information Bottleneck (IB) principle <ref type="bibr" target="#b24">(Tishby, Pereira, and Bialek 2000)</ref> provides a framework for constraining such task-irrelevant information retained at the output by trading off between prediction and compression. Specifically, the IB principle seeks for a representation Z that is maximally informative about target Y (i.e., maximize mutual information I(Y ; Z)) while being minimally informative about input data X (i.e., minimize mutual information I(X; Z)). Based on the IB principle, the learned representation is naturally more robust to data noise. IB has been applied to representation learning <ref type="bibr" target="#b12">(Kim et al. 2021;</ref><ref type="bibr">Jeon et al. 2021;</ref><ref type="bibr">Pan et al. 2020;</ref><ref type="bibr" target="#b2">Bao 2021;</ref><ref type="bibr" target="#b6">Dubois et al. 2020</ref>) and numerous deep learning tasks such as model ensemble <ref type="bibr">(Sinha et al. 2020), fine-tuning (Mahabadi, Belinkov, and</ref><ref type="bibr" target="#b14">Henderson 2021)</ref>, salient region discovery <ref type="bibr" target="#b34">(Zhmoginov, Fischer, and Sandler 2020)</ref>.</p><p>In this paper, we advance the IB principle for graph to solve the graph structure learning problem. We propose a novel Variational Information Bottleneck guided Graph Structure Learning framework, namely VIB-GSL. VIB-GSL employs the irrelevant feature masking and structure learning method to generate a new IB-Graph G IB as a bottleneck to distill the actionable information for the downstream task. VIB-GSL consists of three steps: (1) the IB-Graph generator module learns the IB-graph G IB by masking irrelevant node features and learning a new graph structure based on the masked feature; (2) the GNN module takes the IBgraph G IB as input and learns the distribution of graph representations; (3) the graph representation is sampled from the learned distribution with a reparameterization trick and then used for classification. The overall framework can be trained efficiently with the supervised classification loss and the distribution KL-divergence loss for the IB objective. The main contributions are summarized as follows:</p><p>• VIB-GSL advances the Information Bottleneck principle for graph structure learning, providing an elegant and universal framework in the perspective of information theory. 2 Background and Problem Formulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Structure Learning</head><p>Graph structure learning <ref type="bibr" target="#b36">(Zhu et al. 2021)</ref> targets jointly learning an optimized graph structure and corresponding representations to improving the robustness of GNN models. In this work, we focus on graph structure learning for graph-level tasks.</p><p>Let G ∈ G be a graph with label Y ∈ Y. Given a graph G = (X, A) with node set V , node feature matrix X ∈ R |V |×d , and adjacency matrix A ∈ R |V |×|V | , or only given a feature matrix X, the graph structure learning problem we consider in this paper can be formulated as producing an optimized graph G * = (X * , A * ) and its corresponding node/graph representations Z * = f (G * ), with respect to the downstream graph-level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Bottleneck</head><p>The Information Bottleneck <ref type="bibr" target="#b24">(Tishby, Pereira, and Bialek 2000)</ref> seeks the balance between data fit and generalization using the mutual information as both cost function and regularizer. We will use the following standard quantities in the information theory <ref type="bibr" target="#b5">(Cover 1999)</ref> frequently: Shannon entropy H(X) = E X∼p(X) [− log p(X)], cross entropy H(p(X), q(X)) = E X∼p(X) [− log q(X)], Shannon mutual information I(X; Y ) = H(X) − H(X|Y ), and Kullback Leiber divergence D KL (p(X)||q(X) = E X∼p(X) log p(X) q(X) . Following standard practice in the IB literature <ref type="bibr" target="#b24">(Tishby, Pereira, and Bialek 2000)</ref>, given data X, representation Z of X and target Y , (X, Y, Z) are following the Markov Chain &lt; Y → X → Z &gt;.</p><p>Definition 1 (Information Bottleneck). For the input data X and its label Y , the Information Bottleneck principle aims to learn the minimal sufficient representation Z:</p><formula xml:id="formula_0">Z = arg min Z −I(Z; Y ) + βI(Z; X), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where β is the Lagrangian multiplier trading off sufficiency and minimality.</p><p>Deep VIB <ref type="bibr" target="#b1">(Alemi et al. 2016</ref>) proposed a variational approximation to the IB objective by parameterizing the distribution via a neural network:</p><formula xml:id="formula_2">L = 1 N N i=1 dZp(Z|X i ) log q(Y i |Z) + βD KL (p(Z|X i ), r(Z)) ,<label>(2)</label></formula><p>where q(Y i |Z) is the variational approximation to p(Y i |Z) and r(Z) is the variational approximation of p(Z).</p><p>The IB framework has received significant attention in machine learning and deep learning <ref type="bibr" target="#b1">(Alemi et al. 2016;</ref><ref type="bibr" target="#b17">Saxe et al. 2019)</ref>. As for irregular graph data, there are some recent works <ref type="bibr" target="#b27">(Wu et al. 2020;</ref><ref type="bibr" target="#b30">Yu et al. 2020;</ref><ref type="bibr" target="#b29">Yang et al. 2021;</ref><ref type="bibr" target="#b31">Yu et al. 2021)</ref> introducing the IB principle to graph learning. GIB <ref type="bibr" target="#b27">(Wu et al. 2020</ref>) extends the general IB to graph data with regularization of the structure and feature information for robust node representations. SIB <ref type="bibr" target="#b30">(Yu et al. 2020</ref><ref type="bibr" target="#b31">(Yu et al. , 2021) )</ref> was proposed for the subgraph recognition problem. HGIB <ref type="bibr" target="#b29">(Yang et al. 2021)</ref> was proposed to implement the consensus hypothesis of heterogeneous information networks in an unsupervised manner. We illustrate the difference between related graph IB methods and our method in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variational Information Bottleneck Guided</head><p>Graph Structure learning</p><p>In this section, we elaborate the proposed VIB-GSL, a novel variational information bottleneck principle guided graph structure learning framework. First, we formally define the IB-Graph and introduce a tractable upper bound for IB objective. Then, we introduce the graph generator to learn the optimal IB-Graph as a bottleneck and give the overall framework of VIB-GSL. Lastly, we compare VIB-GSL with two graph IB methods to illustrate its difference and properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Information Bottleneck</head><p>In this work, we focus on learning an optimal graph G IB = (X IB , A IB ) named IB-Graph for G, which is compressed with minimum information loss in terms of G's properties.</p><p>Definition 2 (IB-Graph). For a graph G = (X, A) and its label Y , the optimal graph G IB = (X IB , A IB ) found by Information Bottleneck is denoted as IB-Graph:</p><formula xml:id="formula_3">G IB = arg min GIB −I(G IB ; Y ) + βI(G IB ; G),<label>(3)</label></formula><p>where X IB is the task-relevant feature set and A IB is the learned task-relevant graph adjacency matrix.</p><p>Intuitively, the first term −I(G IB ; Y ) is the prediction term, which encourages that essential information to the graph property is preserved. The second term I(G IB ; G) is the compression term, which encourages that labelirrelevant information in G is dropped. And the Lagrangian multiplier β indicates the degree of information compression, where larger β indicates more information in G was retained to</p><formula xml:id="formula_4">G IB . Suppose G n ∈ G is a task-irrelevant nuisance in G, the learning procedure of G IB follows the Markov Chain &lt; (Y, G n ) → G → G IB &gt;.</formula><p>IB-Graph only preserves the task-relevant information in the observed graph G and is invariant to nuisances in data.</p><formula xml:id="formula_5">Lemma 1 (Nuisance Invariance). Given a graph G ∈ G with label Y ∈ Y, let G n ∈ G be a task-irrelevant nuisance for Y . Denote G IB</formula><p>as the IB-Graph learned from G, then the following inequality holds:</p><formula xml:id="formula_6">I(G IB ; G n ) ≤ I(G IB ; G) − I(G IB ; Y ) (4)</formula><p>Please refer to the Technical Appendix for the detailed proof. Lemma 1 indicates that optimizing the IB objective in Eq. ( <ref type="formula" target="#formula_3">3</ref>) is equivalent to encourage G IB to be less related to task-irrelevant information in G, leading to the nuisanceinvariant property of IB-Graph.</p><p>Due to the non-Euclidean nature of graph data and the intractability of mutual information, the IB objective in Eq. ( <ref type="formula" target="#formula_3">3</ref>) is hard to optimize directly. Therefore, we introduce two tractable variational upper bounds of −I(G IB ; Y ) and I(G IB ; G), respectively. First, we examine the prediction term −I(G IB ; Y ) in Eq. ( <ref type="formula" target="#formula_3">3</ref>), which encourages G IB is informative of Y . Please refer to Technical Appendix for the detailed proof of Proposition 1.</p><formula xml:id="formula_7">Proposition 1 (Upper bound of −I(G IB ; Y )). For graph G ∈ G with label Y ∈ Y and IB-Graph G IB learned from G, we have −I(Y ; G IB ) ≤ − p(Y, G IB ) log q θ (Y |G IB )dY dG IB + H(Y ),<label>(5)</label></formula><p>where q θ (Y |G IB ) is the variational approximation of the true posterior p(Y |G IB ).</p><p>Then we examine the compression term I(G IB ; G) in Eq. ( <ref type="formula" target="#formula_3">3</ref>), which constrains the information that G IB receives from G. Please refer to the Technical Appendix for the detailed proof of Proposition 2.</p><p>Proposition 2 (Upper bound of I(G IB ; G) ). For graph G ∈ G and IB-Graph G IB learned from G, we have</p><formula xml:id="formula_8">I(G IB ; G) ≤ p(G IB , G) log p(G IB |G) r(G IB ) dG IB dG,<label>(6)</label></formula><p>where r(G IB ) is the variational approximation to the prior distribution p(G IB ) of G IB .</p><p>Finally, plug Eq. ( <ref type="formula" target="#formula_7">5</ref>) and Eq. ( <ref type="formula" target="#formula_8">6</ref>) into Eq. ( <ref type="formula" target="#formula_3">3</ref>) to derive the following objective function, which we try to minimize:</p><formula xml:id="formula_9">−I(G IB ; Y ) + βI(G IB ; G) ≤ − p(Y, G IB ) log q θ (Y |G IB )dY dG IB + β p(G IB , G) log p(G IB |G) r(G IB ) dG IB dG.</formula><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instantiating the VIB-GSL Framework</head><p>Following the theory discussed in Section 3.1, we first obtain the graph representation Z IB of G IB to optimize the IB objective in Eq. ( <ref type="formula">7</ref>). We assume that there is no information loss during this process, which is the general practice of mutual information estimation <ref type="bibr" target="#b23">(Tian et al. 2020)</ref>. Therefore, we have</p><formula xml:id="formula_10">I(G IB ; Y ) ≈ I(Z IB ; Y ) and I(G IB ; G) ≈ I(Z IB ; G).</formula><p>In practice, the integral over G IB and G can be approximated by Monte Carlo sampling <ref type="bibr" target="#b19">(Shapiro 2003</ref>) on all training samples</p><formula xml:id="formula_11">{G i ∈ G, Y i ∈ Y, i = 1, . . . , N }. − I(G IB ; Y ) + βI(G IB ; G) ≈ −I(Z IB ; Y ) + βI(Z IB ; G) ≤ 1 N N i=1 − log q θ (Y i |Z IBi )+βp(Z IBi |G i )log p(Z IBi |G i ) r(Z IB ) .</formula><p>(8) As shown in Figure <ref type="figure" target="#fig_0">1</ref>, VIB-GSL consists of three steps:</p><p>Step-1: Generate IB-Graph G IB . We introduce an IB-Graph generator to generate the IBgraph G IB for the input graph G. Following the assumption that nuisance information exists in both irrelevant feature and structure, the generation procedure consists of feature masking and structure learning. Feature Masking. We first use a feature masking scheme to discretely drop features that are irrelevant to the downstream task, which is formulated as:</p><formula xml:id="formula_12">X IB = {X i M, i = 1, 2, • • • , |V |},<label>(9)</label></formula><p>where M ∈ R d is a learnable binary feature mask and is the element-wise product. Intuitively, if a particular feature is not relevant to task, the corresponding weight in M takes value close to zero. We can reparameterize X IB using the reparameterization trick <ref type="bibr" target="#b13">(Kingma and Welling 2013)</ref> to backpropagate through a d-dimensional random variable:</p><formula xml:id="formula_13">X IB = X r + (X − X r ) M,<label>(10)</label></formula><p>where X r is a random variable sampled from the empirical distribution of X.</p><p>Structure Learning. We model all possible edges as a set of mutually independent Bernoulli random variables parameterized by the learned attention weights π:</p><formula xml:id="formula_14">A IB = u,v∈V {a u,v ∼ Ber (π u,v )} . (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>For each pair of nodes, we optimized the edge sampling probability π jointly with the graph representation learning. π u,v describes the task-specific quality of edge (u, v) and smaller π u,v indicates that the edge (u, v) is more likely to be noise and should be assigned small weight or even be removed. For a pair of nodes (u, v), the edge sampling probability π u,v is calculated by:</p><formula xml:id="formula_16">Z(u) = NN (X IB (u)) , π u,v = sigmoid Z(u)Z(v) T ,<label>(12)</label></formula><p>where NN(•) denotes a neural network and we use a twolayer perceptron in this work. One issue is that A IB is not differentiable with respect to π as Bernoulli distribution. We thus use the concrete relaxation (Jang, Gu, and Poole 2017) of the Bernoulli distribution to update π:</p><formula xml:id="formula_17">Ber(π u,v ) ≈ sigmoid 1 t log π u,v 1 − π u,v + log 1 − ,<label>(13)</label></formula><p>where ∼ Uniform(0, 1) and t ∈ R + is the temperature for the concrete distribution. After concrete relaxation, the binary entries a u,v from a Bernoulli distribution are transformed into a deterministic function of π u,v and .</p><p>The graph structure after the concrete relaxation is a weighted fully connected graph, which is computationally expensive. We hence extract a symmetric sparse adjacency matrix by masking off those elements which are smaller than a non-negative threshold a 0 .</p><p>Step-2: Learn Distribution of IB-Graph Representation. For the compression term I(Z IB ; G) in Eq. ( <ref type="formula">8</ref>), we consider a parametric Gaussian distribution as prior r(Z IB ) and p(Z IB |G) to allow an analytic computation of Kullback Leibler (KL) divergence <ref type="bibr" target="#b10">(Hershey and Olsen 2007)</ref>:</p><formula xml:id="formula_18">r (Z IB ) = N (µ 0 , Σ 0 ) , p (Z IB |G) = N f µ φ (G IB ) , f Σ φ (G IB ) ,<label>(14)</label></formula><p>where µ ∈ R K and Σ ∈ R K×K is the mean vector and the diagonal co-variance matrix of Z IB encoded by f φ (G IB ).</p><p>The dimensionality of Z IB is denoted as K, which specifies the bottleneck size. We model the f φ (G IB ) as a graph neural network (GNN) with weights φ, where</p><formula xml:id="formula_19">f µ φ (G IB ) and f Σ φ (G IB ) are the 2K-dimensional output value of the GNN: ∀u ∈ V, Z IB (u) = GNN (X IB , A IB ) , f µ φ (G IB ) , f Σ φ (G IB ) = Pooling ({Z IB (u) , ∀u ∈ V }) ,<label>(15)</label></formula><p>where the first K-dimension outputs encode µ and the remaining K-dimension outputs encode Σ (we use a softplus transform for f Σ φ (G IB ) to ensure the non-negativity). We treat r(Z IB ) as a fixed d-dimensional spherical Gaussian r(Z IB ) = N (Z IB |0, I) as in <ref type="bibr" target="#b1">(Alemi et al. 2016)</ref>.</p><p>Step-3: Sample IB-Graph Representation.</p><p>To obtain Z IB , we can use the reparameterization trick <ref type="bibr" target="#b13">(Kingma and Welling 2013)</ref> for gradients estimation:</p><formula xml:id="formula_20">Z IB = f µ φ (G IB ) + f Σ φ (G IB ) ε,<label>(16)</label></formula><p>where ε ∈ N (0, I) is an independent Gaussian noise and denotes the element-wise product. By using the reparameterization trick, randomness is transferred to ε, which does not affect the back-propagation. For the first term I(Z IB , Y ) in Eq. ( <ref type="formula">8</ref>), q θ (Y |Z IB ) outputs the label distribution of learned graph G IB and we model it as a multi-layer perceptron classifier with parameters θ. The multi-layer perceptron classifier takes Z IB as input and outputs the predicted label.</p><p>Training Objective. We can efficiently compute the upper bounds in Eq. ( <ref type="formula">8</ref>) on the training data samples using the gradient descent based backpropagation techniques, as illustrated in Algorithm 1. The overall loss is:  Property of VIB-GSL Different with traditional GNNs and graph structure learning methods (e.g., IDGL (Chen, Wu, and Zaki 2020), NeuralSparse <ref type="bibr" target="#b33">(Zheng et al. 2020</ref>)), VIB-GSL is independent of the original graph structure since it learns a new graph structure. This property renders VIB-GSL extremely robust to noisy information and structure perturbations, which is verified in Section 4.2.</p><formula xml:id="formula_21">L = L CE (Z IB , Y ) + βD KL (p (Z IB |G) ||r (Z IB )) ,<label>(</label></formula><formula xml:id="formula_22">for e = 1, 2, • • • , E do // Learn IB-Graph 3 X IB ← {X i M, i ∈ |V |}; 4 A IB ← u,v∈V {a u,v ∼ Ber(π u,v )}; 5 G IB ← (X IB , A IB ); // Learn distribution 6 Encode (f µ φ (G IB ), f Σ φ (G IB )) by a GNN; // Sample graph representation 7 Reparameterize Z IB = f µ φ (G IB ) + f Σ φ (G IB ) ε; // Optimize 8 L = L CE (Z IB , Y )+βD KL (p (Z IB |G) ||r (Z IB ));</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with multiple related methods.</head><p>In this subsection, we discuss the relationship between the proposed VIB-GSL and two related works using the IB principle for graph representation learning, i.e., GIB <ref type="bibr" target="#b27">(Wu et al. 2020)</ref> and SIB <ref type="bibr" target="#b30">(Yu et al. 2020)</ref></p><formula xml:id="formula_23">. Remark that VIB-GSL fol- lows the Markov Chain &lt; (Y, G n ) → G → G IB &gt;.</formula><p>VIB-GSL vs. GIB GIB <ref type="bibr" target="#b27">(Wu et al. 2020</ref>) aims to learn robust node representations Z by the IB principle following the Markov Chain &lt; (Y, G n ) → G → Z &gt;. Specifically, GIB regularizes and controls the structure and feature information in the computation flow of latent representations layer by layer. Our VIB-GSL differs in that we aim to learn an optimal graph explicitly, which is more interpretable than denoising in the latent space. Besides, our VIB-GSL focuses on graph-level tasks while GIB focuses on node-level ones.</p><p>VIB-GSL vs. SIB SIB <ref type="bibr" target="#b30">(Yu et al. 2020</ref>) aims to recognise the critical subgraph G sub for input graph following the Markov Chain &lt; (Y, G n ) → G → G sub &gt;. Our VIB-GSL aims to learn a new graph structure and can be applied for non-graph structured data. Moreover, SIB directly estimates the mutual information between subgraph and graph by MINE <ref type="bibr" target="#b3">(Belghazi et al. 2018</ref>) and uses a bi-level optimization scheme for the IB objective, leading to an unstable and inefficient training process. Our VIB-GSL is more stable to train with the tractable variational approximation, which is demonstrated by experiments in Figure <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate VIB-GSL<ref type="foot" target="#foot_0">1</ref> on two tasks: graph classification and graph denoising, to verify whether VIB-GSL can improve the effectiveness and robustness of graph representation learning. Then we analyze the impact of information compression quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Datasets. We empirically perform experiments on VIB-GSL on four widely-used social datasets including IMDB-B, IMDB-M, REDDIT-B, and COLLAB <ref type="bibr" target="#b16">(Rossi and Ahmed 2015)</ref>. We choose the social datasets for evaluation because much noisy information may exist in social interactions.</p><p>Baselines. We compare the proposed VIB-GSL with a number of graph-level structure learning baselines, including NeuralSparse <ref type="bibr" target="#b33">(Zheng et al. 2020)</ref>, SIB <ref type="bibr" target="#b30">(Yu et al. 2020)</ref> and IDGL <ref type="bibr" target="#b4">(Chen, Wu, and Zaki 2020)</ref>, to demonstrate the effectiveness and robustness of VIB-GSL. We do not include GIB in our baselines since it focuses on node-level representation learning. Similar with SIB <ref type="bibr" target="#b30">(Yu et al. 2020)</ref>, we plug various GNN backbones<ref type="foot" target="#foot_1">2</ref> into VIB-GSL including GCN <ref type="bibr" target="#b14">(Kipf and Welling 2016)</ref>, GAT <ref type="bibr" target="#b26">(Veličković et al. 2017)</ref>, GIN <ref type="bibr" target="#b28">(Xu et al. 2019)</ref> to see whether the VIB-GSL can boost the performance of graph classification or not. For a fair comparison, we use the mean pooling operation to obtain the graph representation and use a 2-layer perceptron as the graph classifier for all baselines. Parameter Settings. We set both the information bottleneck size K and the embedding dimension of baseline methods as 16. For VIB-GSL, we set t = 0.1 in Eq. ( <ref type="formula" target="#formula_17">13</ref>), a 0 = 0.1 and perform hyperparameter search of β ∈ {10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 , 10 −6 } for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>Graph Classification. We first examine VIB-GSL's capability of improving graph classification. We perform 10fold cross-validation and report the average accuracy and the standard deviation across the 10 folds in Table <ref type="table" target="#tab_2">1</ref>, where ∆ denotes the performance improvement for specific backbone and "-" indicates that there is no performance improvement for backbones without structure learner. The best results in each backbone group are underlined and the best results of each dataset are shown in bold. As shown in Table <ref type="table" target="#tab_2">1</ref>, the proposed VIB-GSL consistently outperforms all baselines on all datasets by a large margin. Generally, the graph sparsification models (i.e., NeuralSparse and SIB) show only a small improvement in accuracy and even have a negative impact on performance (e.g., on COLLAB), which is because they are constrained by the observed structures without mining underlying relations. The performance superiority of VIB-GSL over different GNN backbones implies that  Graph Denoise. To evaluate the robustness of VIB-GSL, we generate a synthetics dataset by deleting or adding edges on REDDIT-B. Specifically, for each graph in the dataset, we randomly remove (if edges exist) or add (if no such edges) 25%, 50%, 75% edges. The reported results are the mean accuracy (solid lines) and standard deviation (shaded region) over 5 runs. As shown in Figure <ref type="figure">3</ref>, the classification accuracy of GCN dropped by 5% with 25% missing edges and dropped by 10% with 25% noisy edges, indicating that GNNs are indeed sensitive to structure noise. Since the proposed VIB-GSL does not depend on the original graph structure, it achieves better results without performance degradation. IDGL is still sensitive to structure noise since it iteratively updates graph structure based on node embeddings, which is tightly dependent on the observed structure.</p><p>Parameter Sensitivity: Trade Off between Prediction and Compression. We explore the influence of the Lagrangian multiplier β trading off prediction and compression in Eq. ( <ref type="formula" target="#formula_3">3</ref>) and Eq. ( <ref type="formula">8</ref>). Note that there is a relationship between increasing β and decreasing K (Shamir, Sabato, and 0LVVLQJHGJHV</p><p>$FFXUDF\ nal graph and IB-Graphs learned by VIB-GSL in Figure <ref type="figure">4</ref>, where |E| indicates the number of edges. To further analyze the impact of information compression degree, we visualize the learned IB-Graph with different β when VIB-GSL achieves the same testing performance. Note that VIB-GSL does not set sparsity constraint as in most structure learning methods. As shown in Figure <ref type="figure">4</ref>, we make the following observations: (1)VIB-GSL tends to generate edges that connect nodes playing the same structure roles, which is consistent with the homophily assumption.</p><p>(2)When achieving the same testing performance, VIB-GSL with larger β will generate a more dense graph structure. It is because with the degree of information compression increasing, the nodes need more neighbors to obtain enough information.</p><p>Training Stability. As mentioned in Section 3.3, VIB-GSL deduces a tractable variational approximation for the IB objective, which facilitates the training stability. In this subsection, we analyze the convergence of VIB-GSL and SIB <ref type="bibr" target="#b30">(Yu et al. 2020</ref>) on REDDIT-B with a learning rate of 0.001. The IB objective in <ref type="bibr" target="#b30">(Yu et al. 2020</ref>) is L = L CE + βL MI + αL con , where L CE is the cross-entropy loss, L MI is the MINE loss of estimating mutual information between original graph and learned subgraph and L con is a connectivity regularizer. Figure <ref type="figure" target="#fig_5">5</ref>(a) depicts the losses of VIB-GSL (i.e., overall loss L, cross-entropy loss L CE for classification, and the KL-divergence loss D KL ) with β = 10 −3 , where the dash lines indicates the mean value in the last 10 epochs when VIB-GSL converges. As mentioned in Section 3.3, SIB adopted a bi-level optimization scheme for IB objective.  very difficult to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we advance the Information Bottleneck principle for graph structure learning and propose a framework named VIB-GSL, which jointly optimizes the graph structure and graph representations. VIB-GSL deduces a variational approximation to form a tractable IB objective function that facilitates training stability and efficiency. We evaluate the proposed VIB-GSL in graph classification and graph denoising. Experimental results verify the superior effectiveness and robustness of VIB-GSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technical Appendix A. Proofs Proof of Lemma 1</head><p>We first provide the proof of Lemma 1 in Section 3.1.</p><p>Proof. We prove the Lemma 1 following the same strategy of Proposition 3.1 in <ref type="bibr" target="#b0">(Achille and Soatto 2018)</ref>. Suppose G is defined by Y and G n , and G IB depends on G n only through G. We can define the Markov Chain &lt;</p><formula xml:id="formula_24">(Y, G n ) → G → G IB &gt;.</formula><p>According to the data processing inequality (DPI), we have: </p><formula xml:id="formula_25">I(G IB ; G) ≥ I(G IB ; Y, G n ) = I(G IB ; G n ) + I(G IB ; Y |G n ) = I(G IB ; G n ) + H(Y |G n ) − H(Y |G n ; G IB ). (18) Since G n is be a task-irrelevant nuisance, it is independent with Y , we have H(Y |G n ) = H(Y ) and H(Y |G n ; G IB ) ≤ H(Y |G IB ). Then I(G IB ; G) ≥ I(G IB ; G n ) + H(Y |G n ) − H(Y |G n ; G IB ) ≥ I(G IB ; G n ) + H(Y )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 1</head><p>Then we provide the proof of Proposition 1 in Section 3.1.  We compare the training efficiency of VIB-GSL with other baselines and show the mean training time of one epoch in seconds (10 runs) in Figure <ref type="figure" target="#fig_9">6</ref>. For Subgraph-IB, we set the inner loop iterations as 10. For IDGL, we set the maximal number of iterations in the dynamic stopping strategy to 10 as suggested in its source code. As shown in Figure <ref type="figure" target="#fig_9">6</ref>, VIB-GSL shows comparable efficiency with other methods when achieving the best performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of VIB-GSL. Given G as input, VIB-GSL consists of the following three steps: (1) Generate IB-Graph: the IB-Graph generator learns an IB-Graph G IB by masking irrelevant features and learning a new structure; (2) Learn distribution of IB-Graph representation: the GNN module learns the distribution of IB-Graph representation Z IB ; (3) Sample IB-Graph representation: Z IB is sampled from the learned distribution by a reparameterization trick for classification.</figDesc><graphic url="image-5.png" coords="4,153.55,174.12,180.95,52.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>9</head><label></label><figDesc>Update model parameters to minimize L. 10 end facilitates the training stability effectively, as shown in Section 4.2. We also analyze the impact of compression coefficient β on performance and learned structure in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of β on IMDB-B and REDDIT-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Test accuracy (± standard deviation) in percent for the edge attack scenarios on REDDIT-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5(b) depicts the losses of SIB (i.e., overall loss L, classification loss L CE , the MI estimation loss L MI , and the connectivity loss L con ) with β = 0.2 and α = 5 as suggested in its source code. As shown in Figure 5(a), VIB-GSL converge steadily, showing the effectiveness of the variational approximation. As shown in Figure 5(b), the MI estimation loss L MI is very unstable because of the bi-level optimization scheme, making SIB is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training dynamics of VIB-GSL and SIB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>− H(Y |G IB ) = I(G IB ; G n ) + I(G IB ; Y ). (19) Thus we obtain I(G IB ; G n ) ≤ I(G IB ; G) − I(G IB ; Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Proof.</head><label></label><figDesc>According to the definition of mutual information,−I(Y, G IB ) = − p(Y, G IB ) log p(Y, G IB ) p(Y )p(G IB ) dY dG IB = − p(Y, G IB ) log p(Y |G IB ) p(Y ) dY dG IB ,(20) where p(Y |G IB ) can be fully defined by the Markov Chain&lt; (Y, G n ) → G → G IB &gt; as p(Y |G IB ) = p(Y |G IB )p(G IB |G)dG. Since p(Y |G IB ) is intractable, let q θ (Y |G IB )be the variational approximation of the true posterior p(Y |G IB ). According to the non-negativity of Kullback Leiber divergence:D KL (p(Y |G IB )||q θ (Y |G IB )) ≥ 0 =⇒ p(Y |G IB ) log p(Y |G IB )dY ≥ p(Y |G IB ) log q θ (Y |G IB )dY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>20) into Eq. (21), then we have−I(Y, G IB ) ≤ − p(Y, G IB ) log q θ (Y |G IB ) p(Y ) dY dG IB = − p(Y, G IB ) log q θ (Y |G IB )dY dG IB + H(Y ),(22)where H(Y ) is the entropy of label Y , which can be ignored in optimization procedure.Proof of Proposition 2We next provide the proof of Proposition 2 in Section 3.1.Proof. According to the definition of mutual information,I(G IB , G) = p(G IB , G) log p(G IB |G) p(G IB ) dG IB dG. (23)In general, computing the distribution p(G IB ) = p(G IB |G)p(G)dG is very difficult, so we use r(G IB ) as the variational approximation to p(G IB ). Since the Kullback Leiber divergence D KL (p(Z)||r(Z)) ≥ 0,D KL (p(Z)||r(Z)) ≥ 0 =⇒ p(z) log p(z)dz ≥ p(z) log r(z)dz. (24)Plug Eq. (23) into Eq. (24), then we haveI(G IB , G) ≤ p(G IB , G) log p(G IB |G) r(G IB ) dG IB dG.(25)B. Training EfficiencyFor VIB-GSL, the cost of learning an IB-Graph is O(nd + n 2 d) for a graph with n nodes in R d , while computing graph representation costs O(n 2 d + ndK), where d is the node feature dimension and K is the bottleneck size. If we assume that d ≈ K and d n, the overall time complexity is O(Kn 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Training time of one epoch on various datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• VIB-GSL is model-agnostic and has a tractable variational optimization upper bound that is easy and stable to optimize. It is sufficient to plug existing GNNs into the VIB-GSL framework to enhance their performances.• Extensive experiment results in graph classification and graph denoising demonstrate that the proposed VIB-GSL enjoys superior effectiveness and robustness compared to other strong baselines.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Output: IB-graph G IB , predicted label Ŷ</figDesc><table><row><cell>1 Parameter initialization;</cell></row><row><cell>2</cell></row></table><note>17) where L CE is the cross-entropy loss and D KL (•||•) is the KL divergence. The variational approximation proposed above Algorithm 1: The overall process of VIB-GSL Input: Graph G = (X, A) with label Y ; Number of training epochs E;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of graph classification results: "average accuracy ± standard deviation" and "improvements" (%). Underlined: best performance of specific backbones, bold: best results of each dataset.</figDesc><table><row><cell cols="2">Structure Learner Backbone</cell><cell>IMDB-B Accuracy</cell><cell>∆</cell><cell>IMDB-M Accuracy</cell><cell>∆</cell><cell cols="2">REDDIT-B Accuracy</cell><cell>∆</cell><cell>COLLAB Accuracy</cell><cell>∆</cell></row><row><cell></cell><cell>GCN</cell><cell>70.7±3.7</cell><cell>-</cell><cell>49.7±2.1</cell><cell>-</cell><cell>73.6±4.5</cell><cell></cell><cell>-</cell><cell>77.6±2.6</cell><cell>-</cell></row><row><cell>N/A</cell><cell>GAT</cell><cell>71.3±3.5</cell><cell>-</cell><cell>50.9±2.7</cell><cell>-</cell><cell>73.1±2.6</cell><cell></cell><cell>-</cell><cell>75.4±2.4</cell><cell>-</cell></row><row><cell></cell><cell>GIN</cell><cell>72.1±3.8</cell><cell>-</cell><cell>49.7±0.4</cell><cell>-</cell><cell>85.4±3.0</cell><cell></cell><cell>-</cell><cell>78.8±1.4</cell><cell>-</cell></row><row><cell></cell><cell>GCN</cell><cell>72.0±2.6</cell><cell>↑1.3</cell><cell>50.1±3.1</cell><cell>↑0.4</cell><cell>72.1±5.2</cell><cell cols="2">↓1.5</cell><cell>76.0±2.0</cell><cell>↓1.6</cell></row><row><cell>NeuralSparse</cell><cell>GAT</cell><cell>73.4±2.2</cell><cell>↑2.1</cell><cell>53.7±3.1</cell><cell>↑2.8</cell><cell>74.3±3.1</cell><cell cols="2">↑1.2</cell><cell>75.4±5.8</cell><cell>0.0</cell></row><row><cell></cell><cell>GIN</cell><cell>73.8±1.6</cell><cell>↑1.7</cell><cell>54.2±5.4</cell><cell>↑4.5</cell><cell>86.2±2.7</cell><cell cols="2">↑0.8</cell><cell>76.6±2.1</cell><cell>↓2.2</cell></row><row><cell></cell><cell>GCN</cell><cell>72.2±3.9</cell><cell>↑1.5</cell><cell>51.8±3.9</cell><cell>↑2.1</cell><cell>76.7±3.0</cell><cell cols="2">↑3.1</cell><cell>76.3±2.3</cell><cell>↓1.3</cell></row><row><cell>SIB</cell><cell>GAT</cell><cell>72.9±4.6</cell><cell>↑1.6</cell><cell>51.3±2.4</cell><cell>↑0.4</cell><cell>75.3±4.7</cell><cell cols="2">↑2.2</cell><cell>77.3±1.9</cell><cell>↑1.9</cell></row><row><cell></cell><cell>GIN</cell><cell>73.7±7.0</cell><cell>↑1.6</cell><cell>51.6±4.8</cell><cell>↑1.9</cell><cell>85.7±3.5</cell><cell cols="2">↑0.3</cell><cell>77.2±2.3</cell><cell>↓1.6</cell></row><row><cell></cell><cell>GCN</cell><cell>72.2±4.2</cell><cell>↑1.5</cell><cell>52.1±2.4</cell><cell>↑2.4</cell><cell>75.1±1.4</cell><cell cols="2">↑1.5</cell><cell>78.1±2.1</cell><cell>↑0.5</cell></row><row><cell>IDGL</cell><cell>GAT</cell><cell>71.5±4.6</cell><cell>↑0.2</cell><cell>51.8±2.4</cell><cell>↑0.9</cell><cell>76.2±2.5</cell><cell cols="2">↑3.1</cell><cell>76.8±4.4</cell><cell>↑1.4</cell></row><row><cell></cell><cell>GIN</cell><cell>74.1±3.2</cell><cell>↑2.0</cell><cell>51.1±2.1</cell><cell>↑1.4</cell><cell>85.7±3.5</cell><cell cols="2">↑0.3</cell><cell>76.7±3.8</cell><cell>↓2.1</cell></row><row><cell></cell><cell>GCN</cell><cell>74.1±3.3</cell><cell>↑3.4</cell><cell>54.3±1.7</cell><cell>↑4.6</cell><cell>77.5±2.4</cell><cell cols="2">↑3.9</cell><cell>78.3±1.4</cell><cell>↑0.7</cell></row><row><cell>VIB-GSL</cell><cell>GAT</cell><cell>75.2±2.7</cell><cell>↑3.9</cell><cell>54.1±2.7</cell><cell>↑3.2</cell><cell>78.1±2.5</cell><cell cols="2">↑5.0</cell><cell>79.1±1.2</cell><cell>↑3.7</cell></row><row><cell></cell><cell>GIN</cell><cell>77.1±1.4</cell><cell>↑5.0</cell><cell>55.6±2.0</cell><cell>↑5.9</cell><cell>88.5±1.8</cell><cell cols="2">↑3.1</cell><cell>79.3±2.1</cell><cell>↑0.5</cell></row><row><cell>,0'%%</cell><cell></cell><cell>5('',7%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>$FFXUDF\</cell><cell>$FFXUDF\</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">10 6 10 5 10 4 10 3 10 2 10 1</cell><cell cols="2">10 6 10 5 10 4 10 3 10 2 10 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code is available at https://github.com/RingBDStack/VIB-GSL.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"> 2  We follow the protocol in https://github.com/rusty1s/pytorch geometric/tree/master/benchmark/kernel.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The corresponding author is Jianxin Li. The authors of this paper are supported by the (No.U20B2053 and 61872022), State Key Laboratory of Software Development Environment (SKLSDE-2020ZX-12), Outstanding Research Project of Shen Yuan Honors College, BUAA, ( 230121208), the ARC DECRA Project (No. DE200100964), and in part by NSF under grants III-1763325, III-1909323, III-2106758,  and SaTC-1930941.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Disentangled Variational Information Bottleneck for Multiview Representation Learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07599</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning optimal representations with the decodable information bottleneck</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Room-and-object aware knowledge reasoning for remote embodied referring expression</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<biblScope unit="page" from="3064" to="3073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Approximating the Kullback Leibler divergence between Gaussian mixture models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<idno>ICASSP, IV-317</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">IB-GAN: Disengangled Representation Learning with Information Bottleneck Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Iclr. Jeon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Pyeon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7926" to="7934" />
		</imprint>
	</monogr>
	<note>Categorical reparameterization with gumbel-softmax</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational Information Bottleneck for Effective Low-Resource Fine-Tuning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Iclr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07372</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016. 2020. 2021. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Disentangled Information Bottleneck. arXiv preprint</note>
	<note>Pan,</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM Transactions on Information Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The network data repository with interactive graph analytics and visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4292" to="4293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the information bottleneck theory of deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">124020</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning and generalization with the information bottleneck</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="page" from="2696" to="2711" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Handbooks in operations research and management science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="353" to="425" />
		</imprint>
	</monogr>
	<note>Monte Carlo sampling methods</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diversity inducing Information Bottleneck in Model Ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bharadhwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shkurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9666" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10528</idno>
		<title level="m">Adversarial attack and defense on graph data: A survey</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SUGAR: Subgraph neural network with reinforcement pooling and self-supervised mutual information mechanism</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2081" to="2091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Directed Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph information bottleneck</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Information Bottleneck</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1638" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph Information Bottleneck for Subgraph Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing Predictive Substructures with Subgraph Information Bottleneck</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Information-bottleneck approach to salient region discovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="531" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications. AI Open</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03036</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Graph Structure Learning for Robust Representations: A Survey</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2021. 2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ACM SIGKDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
