<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Shape and the Computability of Emotions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<email>xinlu@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Poonam</forename><surname>Suryanarayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reginald</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
							<email>regadams@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<email>jiali@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michelle</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
							<email>jwang@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<country>Pennsylvania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Shape and the Computability of Emotions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2167F1AFEA7135A6EA43A0329439E399</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content analysis and indexing; I.4.7 [Image Processing and Computer Vision]: Feature measurement Algorithms</term>
					<term>Experimentation</term>
					<term>Human Factors Human Emotion</term>
					<term>Psychology</term>
					<term>Shape Features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigated how shape features in natural images influence emotions aroused in human beings. Shapes and their characteristics such as roundness, angularity, simplicity, and complexity have been postulated to affect the emotional responses of human beings in the field of visual arts and psychology.</p><p>However, no prior research has modeled the dimensionality of emotions aroused by roundness and angularity.</p><p>Our contributions include an in-depth statistical analysis to understand the relationship between shapes and emotions. Through experimental results on the International Affective Picture System (IAPS) dataset we provide evidence for the significance of roundness-angularity and simplicitycomplexity on predicting emotional content in images. We combine our shape features with other state-of-theart features to show a gain in prediction and classification accuracy.</p><p>We model emotions from a dimensional perspective in order to predict valence and arousal ratings which have advantages over modeling the traditional discrete emotional categories. Finally, we distinguish images with strong emotional content from emotionally neutral images with high accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The study of human visual preferences and the emotions imparted by various works of art and natural images has long been an active topic of research in the field of visual arts and psychology. A computational perspective to this problem has interested many researchers and resulted in articles on modeling the emotional and aesthetic content in images <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13]</ref>. However, there is a wide gap between what humans can perceive and feel and what can be explained using current computational image features. Bridging this gap is considered the "holy grail" of computer vision and the multimedia community. There have been many psychological theories suggesting a link between human affective responses and the low-level features in images apart from the semantic content. In this work, we try to extend our understanding of some of the low-level features which have not been explored in the study of visual affect through extensive statistical analyses.</p><p>In contrast to prior studies on image aesthetics, which intended to estimate the level of visual appeal <ref type="bibr" target="#b10">[10]</ref>, we try to leverage some of the psychological studies on characteristics of shapes and their effect on human emotions. These studies indicate that roundness and complexity of shapes are fundamental to understanding emotions.</p><p>• Roundness -Studies <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b21">21]</ref> indicate that geometric properties of visual displays convey emotions like anger and happiness. Bar et al. <ref type="bibr" target="#b5">[5]</ref> confirm the hypothesis that curved contours lead to positive feelings and that sharp transitions in contours trigger a negative bias. • Complexity of shapes -As enumerated in various works of art, humans visually prefer simplicity. Any stimulus pattern is always perceived in the most simplistic structural setting. Though the perception of simplicity is partially subjective to individual experiences, it can also be highly affected by two objective factors, parsimony and orderliness. Parsimony refers to the minimalistic structures that are used in a given representation, whereas orderliness refers to the simplest way of organizing these structures <ref type="bibr" target="#b3">[3]</ref>.</p><p>These findings provide an intuitive understanding of the low-level image features that motivate the affective response, but the small scale of studies from which the inferences have been drawn makes the results less convincing. In order Example images from IAPS (The International Affective Picture System) dataset <ref type="bibr" target="#b15">[15]</ref>. Images with positive affect from left to right, and high arousal from bottom to top.</p><note type="other">Valance Arousal</note><p>to make a fair comparison of observations, psychologists created the standard International Affective Picture System (IAPS) <ref type="bibr" target="#b15">[15]</ref> dataset by obtaining user ratings on three basic dimensions of affect, namely valence, arousal, and dominance (Figure <ref type="figure" target="#fig_0">1</ref>). However, the computational work on the IAPS dataset to understand the visual factors that affect emotions has been preliminary. Researchers <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> investigated factors such as color, texture, composition, and simple semantics to understand emotions, but have not quantitatively addressed the effect of perceptual shapes. The study that did explore shapes by Zhang et al. <ref type="bibr" target="#b27">[27]</ref> predicted emotions evoked by viewing abstract art images through low-level features like color, shape, and texture. However, this work only handles abstract images, and focused on the representation of textures with little accountability of shape.</p><p>The current work is an attempt to systematically investigate how perceptual shapes contribute to emotions aroused from images through modeling the visual properties of roundness, angularity and simplicity using shapes. Unlike edges or boundaries, shapes are influenced by the context and the surrounding shapes influence the perception of any individual shape <ref type="bibr" target="#b3">[3]</ref>. To model these shapes in the images, the proposed framework statistically analyzes the line segments and curves extracted from strong continuous contours.</p><p>Investigating the quantitative relationship between perceptual shapes and emotions aroused from images is non-trivial. First, emotions aroused by images are subjective. Thus, individuals may not have the same response to a given image, making the representation of shapes in complex images highly challenging. Second, images are not composed of simple and regular shapes, making it difficult to model the complexity existing in natural images <ref type="bibr" target="#b3">[3]</ref>.</p><p>Leveraging the proposed shape features, the current work attempts to automatically distinguish the images with strong emotional content from emotionally neutral images. In psychology, emotionally neutral images refer to images which evoke very weak or no emotions in humans.</p><p>Also, the current study models emotions from a noncategorical or discrete emotional perspective. In previous work, emotions were distinctly classified into categories like anger, fear, disgust, amusement, awe, and contentment, among others. This paper is, to our knowledge, the first to predict emotions aroused from images by adopting a dimensional representation (Figure <ref type="figure" target="#fig_1">2</ref>). Valence represents the positive or negative aspect of human emotions, where common emotions, like joy and happiness, are positive, whereas anger and fear are negative. Arousal describes the human physiological state of being reactive to stimuli. A higher value of arousal indicates higher excitation. Dominance represents the controlling nature of the emotion. For instance, anger can be more controlling than fear.</p><p>Researchers <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b28">28]</ref> have investigated the emotional content of videos through the dimensional approach. Their emphasis was on the accommodation of the change in features over time rather than low-level feature improvement. However, static images, with less information, are often more challenging to interpret. Low-level features need to be punctuated.</p><p>This work adopts the dimensional approaches of emotion motivated by recent studies in psychology, which argued for the strengths of dimensional approaches. According to Bradley and Lang <ref type="bibr" target="#b6">[6]</ref>, categorized emotions do not provide a one-to-one relationship between the content and emotion of an image since participants perceive different emotions in the same image. This highlights the utility of a dimensional approach, which controls for the intercorrelated nature of human emotions aroused by images. From the perspective of neuroscience studies, it has been demonstrated that the dimensional approach is more consistent with how the brain is organized to process emotions at their most basic level <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b17">17]</ref>. Dimensional approaches also allow the separation of images with strong emotional content from images with weak emotional content.</p><p>In summary, our main contributions are:</p><p>• We systematically investigate the correlation between visual shapes and emotions aroused from images.</p><p>• We quantitatively model the concepts of roundnessangularity and simplicity-complexity from the perspective of shapes using a dimensional approach.</p><p>• We distinguish images with strong emotional content from those with weak emotional content.</p><p>The rest of the paper is organized as follows, Section 2 provides a summary of previous work. Section 3 introduces some definitions and themes which recur throughout the paper. The overall framework followed by details of the perceptual shape descriptors are described in Section 4. Experimental results and in-depth analyses are presented in Section 5. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Previous work <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b18">18]</ref> predicted emotions aroused by images mainly through training classifiers on visual features to distinguish categorical emotions, such as happiness, anger, and sad.</p><p>Low-level stimuli such as color and composition have been widely used in computational modeling of emotions. Affective concepts were modeled using color palettes, which showed that the bag of colors and Fisher vectors (i.e., higher order statistics about the distribution of local descriptors) were effective <ref type="bibr" target="#b9">[9]</ref>. Zhang et al. <ref type="bibr" target="#b27">[27]</ref> characterized shape through Zernike features, edge statistics features, object statistics, and Gabor filters. Emotion-histogram and bag-of-emotion features were used to classify emotions by Solli et al. <ref type="bibr" target="#b24">[24]</ref>. These emotion metrics were extracted based on the findings from psychophysiological experiments indicating that emotions can be represented through homogeneous emotion regions and transitions among them.</p><p>The first work that comprehensively modeled categorical emotions, Machajdik and Hanbury <ref type="bibr" target="#b18">[18]</ref> used color, texture, composition, content, and semantic level features such as number of faces to model eight discrete emotional categories. Besides the eight basic emotions, to model categorized emotions, adjectives or word pairs were used to represent human emotions. The earliest work based on the Kansei system employs 23 word pairs (e.g., like-dislike, warmcool, cheerful-gloomy) to establish the emotional space <ref type="bibr" target="#b23">[23]</ref>. Along the same lines, researchers enumerated more word pairs to reach a universal, distinctive, and comprehensive representation of emotions in Wang et al. <ref type="bibr" target="#b25">[25]</ref>. Yet, the aforementioned approaches of emotion representation ignore the interrelationship among types of emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CONCEPT INTERPRETATION</head><p>This work captures emotions evoked by images by leveraging shape descriptors.</p><p>Shapes in images are difficult to capture, mainly due to the perceptual and merging boundaries of objects which are often not easy to differentiate using even state-of-the-art segmentation or contour extraction algorithms.</p><p>In contemporary computer vision literature <ref type="bibr">[7,</ref><ref type="bibr" target="#b20">20]</ref>, there are a number of statistical representations of shape through characteristics like the straightness, sinuosity, linearity, circularity, elongation, orientation, symmetry, and the mass of a curve. We chose roundness-angularity and simplicity-complexity characteristics because they have been found previously by psychologists to influence the affect of human beings through controlled human subject studies. Symmetry is also known to effect emotion and aesthetics of images <ref type="bibr" target="#b22">[22]</ref>. However, quantifying symmetry in natural images is challenging.</p><p>To make it more convenient to introduce the shape features proposed, this section defines the four terms used: line segments, angles, continuous lines, and curves. The framework for extracting perceptual shapes through lines and curves is derived from <ref type="bibr" target="#b8">[8]</ref>. The contours are extracted using the algorithm in <ref type="bibr" target="#b1">[1]</ref>, which used color, texture, and brightness of each image for contour extraction. The extracted contours are of different intensities and indicate the algorithm's confidence on the presence of edges.</p><p>Considering the temporal resolution of our vision system, we adopted a threshold of 40%. Example results are presented in Figures <ref type="figure" target="#fig_2">3,</ref><ref type="figure" target="#fig_3">4</ref>, 5, and 6. Pixels with an intensity higher than 40% are treated equally, which results in the binary contour map presented in the second column. The last three columns show the line segments, continuous lines, and curves.</p><p>Line segments -Line segments refer to short straight lines generated by fitting nearby pixels. We generated line segments from each image to capture its structure. From the structure of the image, we propose to interpret the simplicity-complexity. We extracted locally optimized line segments by connecting neighboring pixels from the contours extracted from the image <ref type="bibr" target="#b16">[16]</ref>.</p><p>Angles -Angles in the image are obtained by calculating angles between each of any two intersecting line segments extracted previously.</p><p>According to Julian Hochberg's theory <ref type="bibr" target="#b3">[3]</ref>, the number of angles and the number of different angles in an image can be effectively used to describe its simiplicity-complexity. The distribution of angles also indicates the degree of angularity of the image. A high number of acute angles makes an image more angular.</p><p>Continuous lines -Continuous lines are generated by connecting intersecting line segments having the same orientations with a small margin of error. Line segments of inconsistent orientations can be categorized as either corner points or points of inflexion. Corner points, shown in Figure <ref type="figure">7</ref>(a), refer to angles that are lower than 90 degrees. Inflexion points, shown in Figure <ref type="figure">7</ref>(b), refer to the midpoint of two angles with opposite orientations. Continuous lines and the degree of curving can be used to interpret the complexity of the image.</p><p>Curves -Curves are a subset of continuous lines, the collection of which are employed to measure the roundness of an image. To achieve this, we consider each curve as a section of an ellipse, thus we use ellipses to fit continuous lines. Fitted curves are represented by parameters of its corresponding ellipses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CAPTURING EMOTION FROM SHAPE</head><p>For decades, numerous theories have been promoted that are focused on the relationship between emotions and the visual characteristics of simplicity, complexity, roundness, and angularity. Despite these theories, researchers have yet to resolve how to model these relationships quantitatively. In this section, we propose to use shape features to capture those visual characteristics. By identifying the link between shape features and emotions, we are able to determine the relationship between the aforementioned visual characteristics and emotions.</p><p>We now present the details of the proposed shape features: line segments, angles, continuous lines, and curves. A total of 219 shape features are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Line segments</head><p>Psychologists and artists have claimed that the simplicitycomplexity of an image is determined not only by lines or curves, but also by its overall structure and support <ref type="bibr" target="#b3">[3]</ref>. Based on this idea, we employed line segments extracted from images to capture their structure. Particularly, we used the orientation, length, and mass of line segments to determine the complexity of the images.</p><p>Orientation -To capture an overall orientation, we employed statistical measures of minimum (min), maximum   (max), 0.75 quantile, 0.25 quantile, the difference between 0.75 quantile and 0.25 quantile, the difference between max and min, sum, total number, median, mean, and standard deviation (we will later refer to these as {statistical measures}), and entropy. We experimented with both 6-and 18-bin histograms. The unique orientations were measured based on the two histograms to capture the simplicitycomplexity of the image.</p><p>Among all line segments, horizontal lines and vertical lines are known <ref type="bibr" target="#b3">[3]</ref> to be static and to represent the feelings of calm and stability within the image. Horizontal lines suggest peace and calm, whereas vertical lines indicate strength. To capture the emotions evoked by these characteristics, we counted the number of horizontal lines and vertical lines through an 18-bin histogram. The orientation θ, of horizontal lines fall within 0 • &lt; θ &lt; 10 • or 170 • &lt; θ &lt; 180 • , and 80 • &lt; θ &lt; 100 • for vertical lines.</p><p>Length -The length of line segments reflects the simplicity of images. Images with simple structure might use long lines to fit contours, whereas complex contours have shorter lines. We characterized the length distribution by calculating the {statistical measures} of lengths of line segments within the image.</p><p>Mass of the image -The centroid of line segments may indicate associated relationships among line segments within the visual design <ref type="bibr" target="#b3">[3]</ref>. Hence, we calculate the mean and standard deviation of the x and y coordinates of the line segments to find the mass of each image. Some of the example images and their features are presented in Figures <ref type="figure" target="#fig_6">8</ref> and<ref type="figure" target="#fig_8">9</ref>. Figure <ref type="figure" target="#fig_6">8</ref>   These two figures indicate that the length or the orientation cannot be examined separately to determine the simplicity-complexity of the image. Lower mean values of the length of line segments might refer to either simple images such as the first four images in Figure <ref type="figure" target="#fig_6">8</ref> or highly complex images such as the last four images in that figure . 
The histogram of the orientation of line segments helps us to distinguish the complex images from simple images by examining variation of values in each bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Angles</head><p>Angles are important elements in analyzing the simplicitycomplexity and the angularity of an image. We capture the visual characteristics from angles through two perspectives.</p><p>• Angle count -We first calculate the two quantitative features claimed by Julian Hochberg, who has attempted to define simplicity (he used the valueladen term "figural goodness") via information theory: "The smaller the amount of information needed to define a given organization as compared to the other alternatives, the more likely that the figure will be so perceived" <ref type="bibr" target="#b3">[3]</ref>. Hence this minimal information structure is captured using the number of angles and the percentage of unique angles in the image.</p><p>• Angular metrics -We use the {statistical measures} to extract angular metrics. We also calculate the 6and 18-bin histograms on angles and their entropies. Some of the example images and features are presented in Figures <ref type="figure" target="#fig_9">10</ref> and<ref type="figure" target="#fig_10">11</ref>. Images with lowest and highest number of angles are shown along with their corresponding contours in Figure <ref type="figure" target="#fig_9">10</ref>. These examples show promising relationships between angular features and simplicity-complexity of the image. Example results for the histogram of angles in the image are presented in Figure <ref type="figure" target="#fig_10">11</ref>. The 18 bins refer to the number of line segments with an orientation in [10(i-1), 10i) degrees where i ∈ {1, 2, ..., 18}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Continuous lines</head><p>We attempt to capture the degree of curvature from continuous lines, which has implications for the simplicitycomplexity of images. We also calculated the number of continuous lines, which is the third quantitative feature specified by Julian Hochberg <ref type="bibr" target="#b3">[3]</ref>. For continuous lines, open/closeness are factors affecting the simplicitycomplexity of an image. In the following, we focus on the   Length Span(l) = max</p><formula xml:id="formula_0">p i ∈l,p j ∈l EuclideanDist(pi, pj),<label>(1)</label></formula><p>where {p1, p2, ..., pN } are the points on continuous line l.</p><p>• Degree of curving -We calculated the degree of curving of each line as</p><formula xml:id="formula_1">Degree of Curving(l) = Length Span(l)/N, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where N is the number of points on continuous line l.</p><p>To capture the statistical characteristics of contiguous lines in the image, we calculated the {statistical measures}. We also generated a 5-bin histogram on the degree of curving of all continuous lines (Figures <ref type="figure" target="#fig_11">12</ref> and<ref type="figure" target="#fig_12">13</ref>).</p><p>• Length span -We used {statistical measures} for the length span of all continuous lines.</p><p>• Line count -We counted the total number of continuous lines, the total number of open lines, and the total number of closed lines in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Curves</head><p>We used the nature of curves to model the roundness of images. For each curve, we calculated the extent of fit to an ellipse as well as the parameters of the ellipse such as its area, circularity, and mass of curves. The curve features are explained in detail below.</p><p>• Fitness, area, circularity -The fitness of an ellipse refers to the overlap between the proposed ellipse and the curves in the image. The area of the fitted ellipse is also calculated. The circularity is represented by the ratio of the minor and major axes of the ellipses. The angular orientation of the ellipse is also measured.</p><p>For each of the measures, we used the {statistical measures} and entropies of the histograms as the features to depict the roundness of the image.</p><p>• Mass of curves -We used the mean value and standard deviation of (x, y) coordinates to describe the mass of curves.</p><p>• Top round curves -To make full use of the discovered curves and to depict roundness, we included the fitness, area, circularity, and mass of curves for each of the top three curves.</p><p>To examine the relationship between curves and positivenegative images, we calculated the average number of curves in terms of values of circularity and fitness on positive images (i.e., the value is higher than 6 in the dimension of valance) and negative images (i.e., the value is lower than 4.5 in the dimension of valance).</p><p>The results are shown in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref>. Positive images  have more curves with 60% -100% fitness to ellipses and higher average curve count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>To demonstrate the relationship between proposed shape features and the felt emotions, the shape features were utilized in three tasks. First, we distinguished images with strong emotional content from emotionally neutral images. Second, we fit valence and arousal dimensions using regression methods. We then performed classification on discrete emotional categories. The proposed features were compared with the features discussed in Machajdik et al. <ref type="bibr" target="#b18">[18]</ref>, and overall accuracy was quantified by combining those features. Forward selection and Principal Component Analysis (PCA) strategies were employed for feature selection and to find the best combination of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We used two subsets of the IAPS <ref type="bibr" target="#b15">[15]</ref> dataset, which were developed by examining human affective responses to color photographs with varying degrees of emotional content. The IAPS dataset contains 1, 182 images, wherein each image is associated with an empirically derived mean and standard deviation of valance, arousal, and dominance ratings.</p><p>Subset A of the IAPS dataset includes many images with faces and human bodies. Facial expressions and body language strongly affect emotions aroused by images, slight changes of which might lead to an opposite emotion. The proposed shape features are sensitive to faces hence we removed all images with faces and human bodies from the scope of this study. In experiments, we only considered the remaining 484 images, which we labeled as Subset A. To provide a better understanding of the ratings of the dataset, we analyzed the distribution of ratings within valence and arousal, as shown in Figure <ref type="figure" target="#fig_14">14</ref>. We also calculated average variations of ratings in each rating unit (i.e., 1-2, 2-3, . . . , <ref type="bibr">[7]</ref><ref type="bibr" target="#b8">[8]</ref>. Valence ratings between 3 and 4, and 6 and 7, have the maximum variance for single images. Similarly, arousal ratings between 4 and 5 varied the most.</p><p>Subset B are images with category labels (with discrete emotions), generated by Mikels <ref type="bibr" target="#b19">[19]</ref>. Subset B includes eight categories namely, anger, disgust, fear, sadness, amusement, awe, contentment, and excitement, with 394 images in total. Subset B is a commonly used dataset, hence we used it to benchmark our classification accuracy with the results mentioned in Machajdik et al. <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Identifying Strong Emotional Content</head><p>Images with strong emotional content have very high or very low valance and arousal ratings. Images with values around the mean values of valance and arousal lack emotions and wered used as samples for emotionally neutral images.</p><p>Based on dimensions of valance and arousal respectively, we generated two sample sets from Subset A. In Set 1, images with valence values higher than 6 or lower than 3.5 were considered images with strong emotional content and the rest to represent emotionally neutral images. This resulted in 247 emotional images and 237 neutral images. Similarly, images with arousal values higher than 5.5 or lower than 3.7 were defined as emotional images, and others as neutral images. With similar thresholds, we obtained 239 emotional images and 245 neutral images in Set 2.</p><p>We used the traditional Support Vector Machines (SVM) with radial basis function (RBF) kernel to perform the classification task. We trained SVM models using the proposed shape features, Machajdik's features, and combined (Machajdik's and shape) features. Training and testing were performed by dividing the dataset uniformly into training and testing sets. As we removed all images with faces and human bodies, we did not consider facial and skin features discussed in <ref type="bibr" target="#b18">[18]</ref>. We used both forward selection and PCA methods to perform feature selection. In the forward selection method, we used the greedy strategy and accumulated one feature at a time to obtain the subset of features that maximized the classification accuracy. The seed features were also chosen at random over     multiple iterations to obtain better results. Our analyses showed that the forward selection strategy achieved greater accuracy for Set 2, whereas PCA performed better for Set 1 (Figure <ref type="figure" target="#fig_15">15</ref>). The feature comparison showed that the combined (Machajdik's and shape) features achieved the highest classification accuracy, whereas individually the shape features alone were much stronger than the features from <ref type="bibr" target="#b18">[18]</ref> (Machajdik's features). This result is intuitive since emotions evoked by images cannot be well represented by shapes alone and can definitely be bolstered by other image features including their color composition and texture. By analyzing valence and arousal ratings of the correctly classified images, we observed that very complex/simple, round and angular images had strong emotional content and high valence values. Simple structured images with very low degrees of curving also tends to portray strong emotional content as well as to have high arousal values. By analyzing the individual features for classification accuracy we found that line count, fitness, length span, degree of curving, and the number of horizontal lines achieved the best classification accuracy in Set 1. Fitness and line orientation were more dominant in Set 2. We present a few example images, which were wrongly classified based on the proposed shape features in Figures <ref type="figure" target="#fig_5">16</ref> and<ref type="figure" target="#fig_0">17</ref>.</p><p>The misclassification can be explained as a shortcoming of the shape features in understanding the semantics. Some of the images generated extreme emotions based on image content irrespective of the low-level features. Besides the semantics, our performance was also limited by the performance of the contour extraction algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Fitting the Dimensionality of Emotion</head><p>Emotions can be represented by word pairs, as previously done in <ref type="bibr" target="#b23">[23]</ref>. However, some emotions are difficult to label. Modeling basic emotional dimensions helps in alleviating this problem. We represented emotion as a tuple consisting of valence and arousal values. The values of valence and arousal were in the range of <ref type="bibr" target="#b1">(1,</ref><ref type="bibr" target="#b9">9)</ref>. In order to predict the values of valence and arousal we proposed to learn a regression model for either dimension separately.</p><p>We used SVM regression with RBF kernel to model the valance and arousal values using shape, Machajdik's features, as well as the combination of features. The mean squared error (MSE) was computed for each of the individual features as well as combined for both valence and arousal values separately. The MSE values are shown in Figure <ref type="figure" target="#fig_18">18(a)</ref>. These figures show that the valance values were modeled more accurately by Machajdik's features than our shape features. Arousal was well modeled by shape features with a mean squared error of 0.9. However, the combined feature performance did not show any improvements. The results indicated that visual shapes provide a stronger cue in understanding the valence as opposed to the combination of color, texture, and composition in images.</p><p>We also computed the correlation between quantified individual shape features and valence-arousal ratings. The higher the correlation, the more relevant the features were. Through this process we found that angular count, fitness, circularity, and orientation of line segments showed higher correlations with valance, whereas angle count, angle metrics, straightness, length span, and orientation of curves had higher correlations with arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Classifying Categorized Emotions</head><p>To evaluate the relationship between shape features and emotions on discrete emotions, we classified images into one of the eight categories, anger, disgust, fear, sadness,   amusement, awe, contentment, and excitement. We followed Machajdik et al. <ref type="bibr" target="#b18">[18]</ref> and performed one-vs-all classification to compare and benchmark our classification accuracy. The classification results are reported in Figure <ref type="figure" target="#fig_18">18</ref>(b). We used SVM to assign the images to one of the eight classes. The highest accuracy was obtained by combining Machajdik's with shape features. We also observed a considerable increase in the classification accuracy by using the shape features alone, which proves that shape features indeed capture emotions in images more effectively.</p><p>In this experiment, we also built classifiers for each of the shape features. Each of the shape features listed in Table <ref type="table" target="#tab_3">4</ref> achieved a classification accuracy of 30% or higher. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>We investigated the computability of emotion through shape modeling. To achieve this goal, we first extracted contours from complex images, and then represented contours using lines and curves extracted from images. Statistical analyses were conducted on locally meaningful lines and curves to represent the concept of roundness, angularity, and simplicity, which have been postulated as playing a key role in evoked emotion for years. Leveraging the computational representation of these physical stimulus properties, we evaluated the proposed shape features through three tasks: distinguishing emotional images from neutral images; classifying images according to categorized emotions; and fitting the dimensionality of emotion based on proposed shape features.</p><p>We have achieved an improvement over the state-of-the-art solution <ref type="bibr" target="#b18">[18]</ref>. We also attacked the problem of modeling the presence or absence of strong emotional content in images, which has long been overlooked.</p><p>Separating images with strong emotional content from emotionally neutral ones can aid in many applications including improving the performance of keyword based image retrieval systems. We empirically verified that our proposed shape features indeed captured emotions in the images. The area of understanding emotions in images is still in its infancy and modeling emotions using low-level features is the first step toward solving this problem. We believe our contribution takes us closer to understanding emotions in images. In the future, we hope to expand our experimental dataset and provide stronger evidence of established relationships between shape features and emotions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:Example images from IAPS (The International Affective Picture System) dataset<ref type="bibr" target="#b15">[15]</ref>. Images with positive affect from left to right, and high arousal from bottom to top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dimensional representation of emotions and the location of categorical emotions in these dimensions (Valance, Arousal, and Dominance).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perceptual shapes of images with high valance.</figDesc><graphic coords="4,160.32,296.96,89.63,67.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perceptual shapes of images with low valance.</figDesc><graphic coords="4,160.32,368.16,89.63,67.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Perceptual shapes of images with high arousal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Perceptual shapes of images with low arousal. line segments extracted from these images and the third row shows the 18-bin histogram for line segments in the images. The 18 bins refer to the number of line segments with an orientation of [-90 + 10(i -1), -90 + 10i) degrees where i ∈ {1, 2, ..., 18}. Similarly, Figure 9 presents the ten highest mean values of the length of line segments.</figDesc><graphic coords="5,59.18,303.99,90.38,67.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Images with low mean value of the length of line segments and their associated orientation histograms. The first row is the original images; the second row shows the line segments; and the third row shows the 18-bin histogram for line segments in the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Images with high mean value of the length of line segments and their associated orientation histograms. The first row is the original images; the second row shows the line segments; and the third row shows the 18-bin histogram for line segments in the images. calculation of the degree of curving, the length span value, and the number of open lines and closed lines. The length span refers to the highest Euclidean distance among all pairs of points on the continuous lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Images with highest and lowest number of angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The distribution of angles in images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Images with highest degree of curving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Images with lowest degree of curving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Distribution of ratings in IAPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Classification accuracy (%) for emotional images and neutral images (Set 1 and Set 2 are defined in Section 5.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(a) Images with strong emotional content (b) Emotionally neutral images Figure 16: Examples of misclassification in Set 1. The four rows are original images, image contours, line segments, and continuous lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Images with strong emotional content (b) Emotionally neutral images Figure 17: Examples of misclassification in Set 2. The four rows are original images, image contours, line segments, and continuous lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Experimental results. (a) Mean squared error for the dimensions of valance and arousal. (b) Accuracy for the classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of shape features.</figDesc><table><row><cell>Category</cell><cell>Short Name</cell><cell>#</cell></row><row><cell>Line Segments</cell><cell>Orientation</cell><cell>60</cell></row><row><cell></cell><cell>Length</cell><cell>11</cell></row><row><cell></cell><cell>Mass of the image</cell><cell>4</cell></row><row><cell>Continuous Lines</cell><cell>Degree of curving</cell><cell>14</cell></row><row><cell></cell><cell>Length span</cell><cell>9</cell></row><row><cell></cell><cell>Line count</cell><cell>4</cell></row><row><cell></cell><cell>Mass of continuous lines</cell><cell>4</cell></row><row><cell>Angles</cell><cell>Angle count</cell><cell>3</cell></row><row><cell></cell><cell>Angular metrics</cell><cell>35</cell></row><row><cell>Curves</cell><cell>Fitness</cell><cell>14</cell></row><row><cell></cell><cell>Circularity</cell><cell>17</cell></row><row><cell></cell><cell>Area</cell><cell>8</cell></row><row><cell></cell><cell>Orientation</cell><cell>14</cell></row><row><cell></cell><cell>Mass of curves</cell><cell>4</cell></row><row><cell></cell><cell>Top round curves</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average number of curves in terms of the value of fitness in positive and negative images.</figDesc><table><row><cell></cell><cell cols="4">(0.8, 1] (0.6, 0.8] (0.4, 0.6] (0.2, 0.4]</cell></row><row><cell>Positive imgs</cell><cell>2.12</cell><cell>9.33</cell><cell>5.7</cell><cell>2.68</cell></row><row><cell>Negative imgs</cell><cell>1.42</cell><cell>7.5</cell><cell>5.02</cell><cell>2.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average number of curves in terms of the value of circularity in positive and negative images.</figDesc><table><row><cell></cell><cell cols="4">(0.8, 1] (0.6, 0.8] (0.4, 0.6] (0.2, 0.4]</cell></row><row><cell>Positive imgs</cell><cell>0.96</cell><cell>2.56</cell><cell>5.1</cell><cell>11.2</cell></row><row><cell>Negative imgs</cell><cell>0.73</cell><cell>2.19</cell><cell>4</cell><cell>9.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Significant features to emotions.</figDesc><table><row><cell>Emotion</cell><cell>Features</cell></row><row><cell>Angry</cell><cell>Circularity</cell></row><row><cell>Disgust</cell><cell>Length of line segments</cell></row><row><cell>Fear</cell><cell>Orientation of line segments</cell></row><row><cell></cell><cell>and angle count</cell></row><row><cell>Sadness</cell><cell>Fitness, mass of curves, circularity,</cell></row><row><cell></cell><cell>and orientation of line segments</cell></row><row><cell>Amusement</cell><cell>Mass of curves</cell></row><row><cell></cell><cell>and orientation of line segments</cell></row><row><cell>Awe</cell><cell>Orientation of line segments</cell></row><row><cell>Excitement</cell><cell>Orientation of line segments</cell></row><row><cell>Contentment</cell><cell>Mass of lines, angle count,</cell></row><row><cell></cell><cell>and orientation of line segments</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wang are also affiliated with the National Science Foundation. This material is based upon work supported by the Foundation. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computation method for video segmentation utilizing the pleasure-arousaldominance emotional information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arifin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y K</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Art and visual perception: A psychology of the creative eye</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arnheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How we recognize angry and happy emotion in people, places, and things</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aronoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cross-Cultural Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Humans prefer curved visual objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="645" to="648" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The international affective picture system(IAPS) in the study of emotion and attention</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Emotion Elicitation and Assessment</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="29" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical shape features in content-based image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1062" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object recognition by discriminative combinations of line segments, ellipses and appearance features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1758" to="1772" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building look &amp; feel concept models from color combinations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1039" to="1053" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Algorithmic inferencing of aesthetics and emotion in natural image: An exposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Affective video content representation and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotion, motivation, and anxiety: Brain mechanisms and psychophysiology</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Cuthbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychiatry</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1248" to="1263" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">International affective picture system: Affective ratings of pictures and instruction manual</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Cuthbert</surname></persName>
		</author>
		<idno>A-8</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Gainesville, FL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic two-strip algorithm in curve fitting</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The brain basis of emotion: A meta-analytic review</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bliss-Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="86" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Emotional category data on images from the international affective picture system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Fredrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Reuter-Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of shape feature extraction techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mingqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kidiyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="43" to="90" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Processing fluency and aesthetic pleasure: Is beauty in the perceiver&apos;s processing experience?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Review</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="382" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sense and Perception: An Integrated Approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Schiffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kansei image retrieval system for street landscape-discrimination and graphical parameters based on correlation of two image systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="274" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color based bags-of-emotions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Solli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">5702</biblScope>
			<biblScope unit="page" from="573" to="580" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Affective understanding in film</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analyzing emotional semantics of abstract art using low-level image features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Augilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Data Analysis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="413" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Utilizing affective analysis for efficient movie browsing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1853" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
