<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
							<email>cywang@mail.nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
							<email>wu.yu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Aisa</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Aisa</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
							<email>yangzl@nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Aisa</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model leads to significant improvements in En-De and En-Fr translation irrespective of the backbones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language <ref type="bibr" target="#b24">(Munro 2010)</ref>; or in online courses, where audiences and speakers use different languages <ref type="bibr" target="#b12">(Jan et al. 2018)</ref>. To tackle this problem, existing approaches can be categorized into cascaded method <ref type="bibr" target="#b25">(Ney 1999;</ref><ref type="bibr" target="#b21">Ma et al. 2019)</ref>, where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method <ref type="bibr" target="#b8">(Duong et al. 2016;</ref><ref type="bibr" target="#b32">Weiss et al. 2017)</ref>, where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest. Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory.</p><p>Since it is hard to obtain a large-scale ST dataset, multitask learning <ref type="bibr" target="#b32">(Weiss et al. 2017;</ref><ref type="bibr" target="#b6">Bérard et al. 2018</ref>) and pretraining techniques <ref type="bibr" target="#b3">(Bansal et al. 2019)</ref> have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT. A common practice is to pre-train two encoderdecoder models for ASR and MT respectively, and then initialize the ST model with the encoder of the ASR model and the decoder of the MT model. Subsequently, the ST model is optimized with the multi-task learning by weighing the losses of ASR, MT, and ST. This approach, however, causes a huge gap between pre-training and fine-tuning, which are summarized into three folds:</p><p>• Subnet Waste: The ST system just reuses the ASR encoder and the MT decoder, while discards other pretrained subnets, such as the MT encoder. Consequently, valuable semantic information captured by the MT encoder cannot be inherited by the final ST system.</p><p>• Role Mismatch: The speech encoder plays different roles in pre-training and fine-tuning. The encoder is a pure acoustic model in pre-training, while it has to extract semantic and linguistic features additionally in fine-tuning, which significantly increases the learning difficulty.</p><p>• Non-pre-trained Attention Module: Previous work <ref type="bibr" target="#b6">(Bérard et al. 2018)</ref> trains attention modules for ASR, MT and ST respectively, hence, the attention module of ST does not benefit from the pre-training.</p><p>To address these issues, we propose a Tandem Connectionist Encoding Network <ref type="bibr">(TCEN)</ref>, which is able to reuse all subnets in pre-training, keep the roles of subnets consistent, and pre-train the attention module. Concretely, the TCEN consists of three components, a speech encoder, a text encoder, and a target text decoder. Different from the previous work that pre-trains an encoder-decoder based ASR model, we only pre-train an ASR encoder by optimizing the Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b9">(Graves et al. 2006)</ref> objective function. In this way, the additional decoder of ASR is not required while keeping the ability to read acoustic features into the source language space by the speech encoder. Besides, the text encoder and decoder can be pre-trained on a large MT dataset. After that, we employ common used multi-task learning method to jointly learn ASR, MT and ST tasks.</p><p>Compared to prior works, the encoder of TCEN is a concatenation of an ASR encoder and an MT encoder and our model does not have an ASR decoder, so the subnet waste Since the text encoder consumes word embeddings of plausible texts in MT task but uses speech encoder outputs in ST task, another question is how one guarantees the speech encoder outputs are consistent with the word embeddings. We further modify our model to achieve semantic consistency and length consistency. Specifically, (1) the projection matrix at the CTC classification layer for ASR is shared with the word embedding matrix, ensuring that they are mapped to the same latent space, and (2) the length of the speech encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence. To bridge the length gap, source sentences in MT are lengthened by adding word repetitions and blank tokens to mimic the CTC output sequences.</p><p>We conduct comprehensive experiments on the IWSLT18 speech translation benchmark <ref type="bibr" target="#b12">(Jan et al. 2018)</ref>, demonstrating the effectiveness of each component. Our model can lead to significant improvements for both LSTM and Transformer backbone.</p><p>Our contributions are three-folds: 1) we shed light on why previous ST models cannot sufficiently utilize the knowledge learned from the pre-training process; 2) we propose a new ST model, which alleviates shortcomings in existing methods; and 3) we empirically evaluate the proposed model on a large-scale public dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>End-to-end speech translation aims to translate a piece of audio into a target-language translation in one step. The raw speech signals are usually converted to sequences of acoustic features. Here, we define the speech feature sequence as</p><formula xml:id="formula_0">x = (x 1 , • • • , x Tx ).</formula><p>The transcription and translation sequences are denoted as y s = (y s 1 , • • • , y s Ts ), and y t = (y t 1 , • • • , y t Tt ) repec-tively. Each symbol in y s or y t is an integer index of the symbol in a vocabulary V src or V trg respectively (e.g.</p><formula xml:id="formula_1">y s i = k, k ∈ [0, |V src | − 1]).</formula><p>In this work, we suppose that an ASR dataset, an MT dataset, and a ST dataset are available, denoted as A = {(x i , y s i )} I i=0 , M = {(y s j , y t j )} J j=0 and S = {(x l , y t l )} L l=0 respectively. Given a new piece of audio x, our goal is to learn an end to end model to generate a translation sentence y t without generating an intermediate result y s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Task Learning and Pre-training for ST</head><p>To leverage large scale ASR and MT data, multi-task learning and pre-training techniques are widely employed to improve the ST system. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, there are three popular multi-task strategies for ST, including 1) one-tomany setting, in which a speech encoder is shared between ASR and ST tasks; 2) many-to-one setting in which a decoder is shared between MT and ST tasks; and 3) many-tomany setting where both the encoder and decoder are shared.</p><p>A many-to-many multi-task model contains two encoders as well as two decoders. It can be jointly trained on ASR, MT, and ST tasks. As the attention module is task-specific, three attentions are defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our method</head><p>In this section, we first introduce the architecture of TCEN, which consists of two encoders connected in tandem, and one decoder with an attention module. Then we give the pre-training and fine-tuning strategy for TCEN. Finally, we propose our solutions for semantic and length inconsistency problems, which are caused by multi-task learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unified formulation for TCEN Architecture</head><formula xml:id="formula_2">𝑦 2 𝑦 1 … 𝜋 𝑦 1 𝑦 𝑇 𝑠 𝑦 𝑇 𝑠 𝑦 1 𝑦 2 𝑦 𝑇 𝑠 … 𝑦 𝑠 - 𝑦 2 𝑦 𝑇 𝑠 𝑦 2 - … 𝑦 1 - 𝑦 𝑇𝑠 𝑦 1 𝑦 2 𝑦 𝑇 𝑠 … 𝑦 𝑠 𝑊 𝑐𝑡𝑐 ≔ 𝑊 𝐸 𝑠 𝑊 𝐸 𝑠 ≔ 𝑊 𝑐𝑡𝑐 𝑦 𝑠′ 𝑒 𝑠 ASR MT ST CTC layer … … Figure 2:</formula><p>The architecture of our model. The linear projection matrix in ASR is shared with the word embedding matrix in MT.</p><p>word or subword representations h s :</p><formula xml:id="formula_3">x = EncPre(x) (1) h s = EncBody(x)<label>(2)</label></formula><p>Then enc t learns high-level linguistic knowledge into hidden representations h t :</p><formula xml:id="formula_4">h t = enct(h s )<label>(3)</label></formula><p>Finally, the dec defines a distribution probability over target words through attention mechanism:</p><formula xml:id="formula_5">c k = att(z k−1 , h t )<label>(4)</label></formula><formula xml:id="formula_6">z k = dec(z k−1 , y t k−1 , c k )<label>(5)</label></formula><formula xml:id="formula_7">P (y t k |y t &lt;k , x) = softmax(W • z k ) (6)</formula><p>Here, z k is the the hidden state of the deocder at k step and c k is a time-dependent context vector computed by the attention att.</p><p>The advantage of our architecture is that two encoders disentangle acoustic feature extraction and linguistic feature extraction, making sure that valuable knowledge learned from ASR and MT tasks can be effectively leveraged for ST training. However, there exists another problem: In ST task, enc t accepts speech encoder output h s as input. While in MT, enc t consumes the word embedding representation e s derived from y s , where each element e s i is computed by choosing the y s i -th vector from the source embedding matrix W E s . Since h s and e s belong to different latent space and have different lengths, there remain semantic and length inconsistency problems. We will provide our solutions in Section 3.3. To verify the generalization of our framework, we test on LSTM based setting and Transformer <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref> based setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Procedure</head><p>Following previous work, we split the training procedure to pre-training and fine-tuning stages. In pre-training stage, the speech encoder enc s is trained towards CTC objective using Transcript y s we were not v @en @ge @ful at all CTC path π 1 -(11) we we -(3) were -(3) not -(4) v @en @en @ge -@ful -(8) at at -(3) all -(10)</p><p>CTC path π 2 -(9) we -(3) were were -(4) not not -(3) v v @en @en @en @ge -@ful -(7) at -(3) all all -(10) Table <ref type="table">1</ref>: An example of the comparison between the golden transcript and the predicted CTC paths given the corresponding speech. '-' denotes the blank token and the following number represents repeat times.</p><p>dataset A, while the text encoder enc t and the decoder dec are trained on MT dataset M. In fine-tuning stage, we jointly train the model on ASR, MT, and ST tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>To sufficiently utilize the large dataset A and M, the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.</p><p>For ASR task, in order to get rid of the requirement for decoder and enable the enc s to generate subword representation, we leverage connectionist temporal classification (CTC) <ref type="bibr" target="#b9">(Graves et al. 2006</ref>) loss to train the speech encoder.</p><p>Given an input x, enc s emits a sequence of hidden vectors h s , then a softmax classification layer predicts a CTC path π, where π t ∈ V src ∪ {'-'} is the observing label at particular RNN step t, and '-' is the blank token representing no observed labels:</p><formula xml:id="formula_8">P (π|x) ≈ T t=1 P (πt|x) = T t=1 softmax(Wctc • h s t )<label>(7)</label></formula><p>where W ctc ∈ R d×(|Vsrc|+1) is the weight matrix in the classification layer and T is the total length of encoder RNN. A legal CTC path π is a variation of the source transcription y s by allowing occurrences of blank tokens and repetitions, as shown in Table <ref type="table">1</ref>. For each transcription y, there exist many legal CTC paths in length T . The CTC objective trains the model to maximize the probability of observing the golden sequence y s , which is calculated by summing the probabilities of all possible legal paths:</p><formula xml:id="formula_9">P (y|x) = π∈Φ T (y) P (π|x) (8) LCT C (θ) = − (x,y s )∈A logP (y s |x; θenc s , θW ctc ) (9)</formula><p>where Φ T (y) is the set of all legal CTC paths for sequence y with length T . The loss can be easily computed using forward-backward algorithm. More details about CTC are provided in supplementary material.</p><p>For MT task, we use the cross-entropy loss as the training objective. During training, y s is converted to embedding vectors e s through embedding layer W E s , then enc t consumes e s and pass the output h t to decoder. The objective function is defined as:</p><formula xml:id="formula_10">LMT (θ) = − (y s ,y t )∈M logP (y t |y s ; θenc t , θ dec , θW E s ) (10)</formula><p>Fine-tune In fine-tune stage, we jointly update the model on ASR, MT, and ST tasks. The training for ASR and MT follows the same process as it was in pre-training stage.</p><p>For ST task, the enc s reads the input x and generates h s , then enc t learns high-level linguistic knowledge into h t . Finally, the dec predicts the target sentence. The ST loss function is defined as:</p><formula xml:id="formula_11">LST (θ) = − (x,y t )∈S logP (y t |x; θenc s , θenc t , θ dec )<label>(11)</label></formula><p>Following the update strategy proposed by Luong et al. <ref type="bibr">(2016)</ref>, we allocate a different training ratio α i for each task. When switching between tasks, we select randomly a new task i with probability αi j αj .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subnet-Consistency</head><p>Our model keeps role consistency between pre-training and fine-tuning by connecting two encoders for ST task. However, this leads to some new problems: 1) The text encoder consumes e s during MT training, while it accepts h s during ST training. However, e s and h s may not follow the same distribution, resulting in the semantic inconsistency. 2) Besides, the length of h s is not the same order of magnitude with the length of e s , resulting in the length inconsistency.</p><p>In response to the above two challenges, we propose two countermeasures: 1) We share weights between CTC classification layer and source-end word embedding layer during training of ASR and MT, encouraging e s and h s in the same space. 2)We feed the text encoder source sentences in the format of CTC path, which are generated from a seq2seq model, making it more robust toward long inputs.</p><p>Semantic Consistency As shown in Figure <ref type="figure" target="#fig_1">2</ref>, during multi-task training, two different hidden features will be fed into the text encoder enc t : the embedding representation e s in MT task, and the enc s output h s in ST task. Without any regularization, they may belong to different latent spaces. Due to the space gap, the enc t has to compromise between two tasks, limiting its performance on individual tasks. To bridge the space gap, our idea is to pull h s into the latent space where e s belong. Specifically, we share the weight W ctc in CTC classification layer with the source embedding weights W E s , which means W ctc = W E s . In this way, when predicting the CTC path π, the probability of observing the particular label w i ∈ V src ∪{'-'} at time step t, p(π t = w i |x), is computed by normalizing the product of hidden vector h s t and the i-th vector in W E s :</p><formula xml:id="formula_12">P (πt = wi|x) = exp(W (i) E s • h s t ) |Vsrc|+1 j exp(W (j) E s • h s t )<label>(12)</label></formula><p>The loss function closes the distance between h s t and golden embedding vector, encouraging h s have the same distribution with e s .</p><p>Length Consistency Another existing problem is length inconsistency. The length of the sequence h s is proportional to the length of the input frame x, which is much longer than the length of e s . To solve this problem, we train an RNNbased seq2seq model to transform normal source sentences to noisy sentences in CTC path format, and replace standard MT with denoising MT for multi-tasking.</p><p>Specifically, we first train a CTC ASR model based on dataset A = {(x i , y s i )} I i=0 , and generate a CTC-path π i for each audio x i by greedy decoding. Then we define an operation S(•), which converts a CTC path π to a sequence of the unique tokens u and a sequence of repetition times for each token l, denoted as S(π) = (u, l). Notably, the operation is reversible, meaning that S −1 (u, l) = π. We use the example π 1 in Table <ref type="table">1</ref> and show the corresponding u and l in Table <ref type="table">2</ref>.</p><p>Then we build a dataset P = {(y s i , u i , l i )} I i=0 by decoding all the audio pieces in A and transform the resulting path by the operation S(•). After that, we train a seq2seq model, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, which takes y s i as input and decodes u i , l i as outputs. With the seq2seq model, a noisy MT dataset M = {(π l , y t l )} L l=0 is obtained by converting every source sentence y s i ∈ M to π i , where π i = S −1 (u i , l i ). We did not use the standard seq2seq model which takes y s as input and generates π directly, since there are too many blank tokens '-' in π and the model tends to generate a long sequence with only blank tokens. During MT training, we randomly sample text pairs from M and M according to a CTC path π 1 -(11) we we -(3) were -(3) not -(4) v @en @en @ge -@ful -(8) at at -(3) all -(10) u we -were -not -v @en @ge -@ful -at -all -</p><formula xml:id="formula_13">l 11 2 3 1 3 1 4 1 2 1 1 1 8 2 3 1 10</formula><p>Table <ref type="table">2</ref>: The CTC path π 1 and corresponding unique tokens u and repetition times l, where S(π) = (u, l).</p><p>hyper-parameter k. After tuning on the validation set, about 30% pairs are sampled from M . In this way, the enc t is more robust toward the longer inputs given by the enc s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We Data preprocessing Our acoustic features are 80dimensional log-Mel filterbanks and 3-dimensional pitch features extracted with a step size of 10ms and window size of 25ms and extended with mean subtraction and variance normalization. The utterances with more than 3000 frames are discarded. All the sentences are in lower-case and the punctuation is removed. To increase the amount of training data, we perform speed perturbation on the raw signals with speed factors 0.9 and 1.1.</p><p>For the MT pre-training data, sentences longer than 80 words or shorter than 10 words are removed. Besides, we discard pairs whose length ratio between source and target sentence is smaller than 0.5 or larger than 2.0. Word 1 https://wit3.fbk.eu/mt.php?release=2017-01-trnted tokenization is performed using the Moses scripts<ref type="foot" target="#foot_0">2</ref> and all words are in lower-case.</p><p>For ST-TED experiments, we apply both subword-level decoding and character-level decoding. For the subword setting, both English and German vocabularies are generated using sentencepiece<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b18">(Kudo 2018)</ref> with a fixed size of 5k tokens. For Librispeech En-Fr experiments, we only apply character-level decoding.</p><p>Since there are no human annotated alignments provided in ST-TED test sets, we segment each audio with the LIUM SpkDiarization tool <ref type="bibr" target="#b23">(Meignier and Merlin 2010)</ref> and then perform MWER segmentation with RWTH toolkit <ref type="bibr" target="#b4">(Bender et al. 2004</ref>). Case-insensitive BLEU is used as evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setups</head><p>Model architecture For LSTM based models, we follow the model structure in Inaguma et al. <ref type="bibr">(2018)</ref>. The EncPre corresponds to 2-layers of VGG-like max-pooling, resulting 4-fold downsampling of input feature. The EncBody is five bidirectional LSTM layers with cell size of 1024. The decoder is defined as two unidirectional LSTM layers with an additive attention. The decoder has the same dimension as the encoder RNNs.</p><p>For Transformer based models, we use two-layer CNN with 256 channels, stride size 2 and kernel size 3 as EncPre. Baselines We compare our method with the following baselines.</p><p>• Vanilla ST baseline: The vanilla ST has only a speech encoder and a decoder, which is trained from scratch with only the speech-translation data.</p><p>• Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, 2) decoder pre-training, and 3) encoder-decoder pre-training. The pre-trained ASR model has the same architecture with vanilla ST model. The MT model has a enc t and dec with the same architecture of which in TCEN.</p><p>• Pre-training + MTL: In this setting, we train a many-tomany multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models.</p><p>Implementation All our models are implemented based on ESPnet <ref type="bibr" target="#b31">(Watanabe et al. 2018)</ref>. For LSTM based models, we use a dropout of 0.3 for embedding and encoders. The </p><formula xml:id="formula_14">= k • d −0.5 model • min(n −0.5 , n • warmup n −1.5 )</formula><p>We set k = 10 and warmup n = 25000 in our experiments. All the models are trained on 4 Tesla P40 GPU for a maximum of 20 epochs.</p><p>For training of TCEN, we set α asr = 0.2 and α mt = 0.8 in the pre-training stage, since the MT dataset is much larger than ASR dataset. For fine-tune, we use α st = 0.6, α asr = 0.2 and α mt = 0.2. At inference time, we use a beam size of 10 and a length normalization weight of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>Reults on ST TED Table <ref type="table" target="#tab_1">3</ref> shows the LSTM-based results on four test sets as well as the average performance. In this setting, we also re-implement the triangle multi-task strategy (Anastasopoulos and Chiang 2018) as our baseline, denoted as 'triangle+pretrain'. They concatenate a ST decoder to an ASR encoder-decoder model.</p><p>From the table, we can see that our method significantly outperforms the strong 'pretrain+MTL' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective. Besides, both pre-training and multi-task learning can improve translation quality. We observe a performance degradation in the 'triangle+pretrain' baseline. Compared to our method, where the decoder receives higher-level knowledge extracted from text encoder, their ASR decoder can only provide lower word-level linguistic information. Besides, their model cannot utilize the large-scale MT data in all the training stages. Table <ref type="table" target="#tab_2">4</ref> shows the comparison between our best model with the cascaded systems, which combines the ASR model and MT model. In addition to a simple combination system, we also re-segment the ASR outputs before feeding to the MT system, denoted as 'cascaded+re-seg'. Specifically, we train a seq2seq model <ref type="bibr" target="#b2">(Bahdanau, Cho, and Bengio 2015)</ref> on the MT dataset, where the source side is a no punctuation sentence and the target side is a natural sentence. After that, we use the seq2seq model to add sentence boundaries and punctuation on ASR outputs. It can be seen that our end-toend model outperforms the simple cascaded model over 2 BLEU scores, and achieves a comparable performance with the 'cascaded+re-seg' system.</p><p>We list Transformer-based results on tst2013 in Table <ref type="table">5</ref>. In this setting, we use character-level decoding strategy due to its better performance. Only in-domain MT data is used during pre-training. It can be seen that our TCEN framework works well on Transformer-based architecture and it outperforms the 'pretrain+MTL' baseline by 2.1 BLEU scores.</p><p>Results on Librispeech For this dataset, we only perform LSTM-based experiments and report results in Table <ref type="table">6</ref>. Even without utilizing large-scale ASR data or MT data, our method can outperform the pre-training baselines and achieve the same performance with Park et al. <ref type="bibr">(2019)</ref>, which uses a MT model as a teacher model to guide the ST model.  Learning Curve It is interesting to investigate why our method is superior to baselines. We find that TCEN achieves a higher final result owing to a better start-point in finetuning. Figure <ref type="figure">4</ref>  cause its networks are not pre-trained on the ASR and MT data. The trends of our model and 'many-to-many+pretrain' are similar, but our model outperforms it about five points in the whole fine-tuning process. It indicates that the gain comes from bridging the gap between pre-training and finetuning rather than a better fine-tuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Early works conduct ST in a pipeline manner (Ney 1999; Matusov, Kanthak, and <ref type="bibr" target="#b22">Ney 2005)</ref>, where the ASR output are fed into an MT system to generate target sentences. HMM <ref type="bibr" target="#b15">(Juang and Rabiner 1991)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has investigated the end-to-end method for ST. We propose a method to reuse every sub-net and keep the role of sub-net consistent between pre-training and finetuning, alleviating the gap between pre-training and finetuning in previous methods. Empirical studies have demonstrated that our model significantly outperforms baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of multi-task learning for speech translation. Networks inherited from pre-trained models are labeled by rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 sketches the overall architecture of TCEN, including a speech encoder enc s , a text encoder enc t and a decoder dec with an attention module att. The enc s usually contains two modules: EncPre and EncBody. During training, the enc s acts like an acoustic model which reads the input x to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of seq2seq model. It predicts the next token and its number of repetition at the same time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The other modules are similar as in paper Dong, Xu, and Xu (2018) (e = 12, d = 6, d model = 256, d f f = 2048 and d head = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>source embedding layer. These two experiments prove that both weight sharing and using noisy MT input benefit to the final translation quality. Performance degrades more in '-weight sharing', indicating the semantic consistency contributes more to our model.In the '-pretrain' experiment, we remove the pre-training stage and directly update the model on three tasks, leading to a dramatic decrease on BLEU score, indicating the pretraining is an indispensable step for end-to-end ST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4: Model learning curves in fine-tuning.</figDesc><graphic url="image-12.png" coords="7,72.45,539.11,201.60,139.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, DenseNet (Huang et al. 2017), TDNN (Peddinti, Povey, and Khudanpur 2015) are commonly used ASR systems, while RNN with attention (Bahdanau, Cho, and Bengio 2015) and Transformer (Vaswani et al. 2017) are top choices for MT. To avoid error propagation and high latency issues, recent works propose translating the acoustic speech into text in target language without yielding the source transcription (Duong et al. 2016; Berard et al. 2016). Since ST data is scarce, pre-training (Bansal et al. 2019), multi-task learning (Duong et al. 2016; Bérard et al. 2018), curriculum learning (Kano, Sakti, and Nakamura 2018), attention-passing (Sperber et al. 2019), and knowledge distillation (Liu et al. 2019; Jia et al. 2019a) strategies have been explored to utilize ASR data and MT data. Specifically, Weiss et al. (2017) show improvements of performance by training the ST model jointly with the ASR and the MT model. Bérard et al. (2018) observe faster convergence and better results due to pretraining and multi-task learning on a larger dataset. Bansal et al. (2019) show that pre-training a speech encoder on one language can improve ST quality on a different source language. All of them follow the traditional multi-task training strategies. Kano, Sakti, and Nakamura (2018) propose to use curriculum learning to improve ST performance on syntactically distant language pairs. To effectively leverage transcriptions in ST data, Anastasopoulos and Chiang (2018) augment the multi-task model where the target decoder receives information from the source decoder and they show improvements on low-resource speech translation. Jia et al. (2019b) use pre-trained MT and text-to-speech (TTS) synthesis models to convert weakly supervised data into ST pairs and demonstrate that an end-to-end MT model can be trained using only synthesised data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Deléglise, and Esteve 2014) with 207h of speech data for ASR pre-training. For MT model, we use WMT2018 en-de data in pre-training stage and use sentence pairs in the ST-TED corpus as well as WIT3 1 in fine-tune stage. The pre-training data contains 41M sentence pairs and fine-tuning data contains 330k sentence paris in total. We split 2k segments from the ST-TED corpus as dev set and tst2010, tst2013, tst2014, tst2015 are used as test sets.</figDesc><table><row><cell>conduct experiments on the Speech Translation TED</cell></row><row><cell>(ST-TED) En-De corpus (Jan et al. 2018) and the aug-</cell></row><row><cell>mented Librispeech En-Fr corpus (Kocabiyikoglu, Besacier,</cell></row><row><cell>and Kraif 2018).</cell></row><row><cell>ST-TED En-De The corpus contains 272 hours of English</cell></row><row><cell>speech with 171k segments. Each example consists of raw</cell></row><row><cell>English wave, English transcription, and aligned German</cell></row><row><cell>translation. Aside from ST-TED, we use TED-LIUM2 cor-</cell></row><row><cell>pus (Rousseau, Librispeech En-Fr This corpus is colleted by aligning e-</cell></row><row><cell>books in French with English utterances, which contains</cell></row><row><cell>236 hours of speech in total. The English speech, English</cell></row><row><cell>transcription, French text translations from alignment and</cell></row><row><cell>Google Translate references are provided. Following previ-</cell></row><row><cell>ous work (Bérard et al. 2018), we only use the 100 hours</cell></row><row><cell>clean train set and double the training size by concatenat-</cell></row><row><cell>ing the aligned references with Google Translate references.</cell></row><row><cell>We use the speech-transcription pairs and transcription-</cell></row><row><cell>translation pairs for ASR and MT pre-training. No additional</cell></row><row><cell>data is used. The dev set is used as validation set and we re-</cell></row><row><cell>port results on the test set.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Results of LSTM-based models on ST TED. "Average" denotes it averages the results of all test sets. We copy the numbers of vanilla model from https://github.com/espnet/espnet/blob/master/egs/iwslt18/st1/RESULTS. Since pre-training data is different, we run ESPnet code to obtain the numbers of pre-training and multi-task learning method, which are slightly higher than numbers in their report.</figDesc><table><row><cell></cell><cell cols="4">tst2010 tst2013 tst2014 tst2015</cell></row><row><cell>cascaded</cell><cell>13.38</cell><cell>15.84</cell><cell>12.94</cell><cell>13.79</cell></row><row><cell cols="2">cascaded+re-seg 17.12</cell><cell>17.77</cell><cell>14.94</cell><cell>15.01</cell></row><row><cell>our model</cell><cell>17.61</cell><cell>17.67</cell><cell>15.73</cell><cell>14.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>BLEU comparison of cascaded results and our best end-to-end results. re-seg denotes the ASR outputs are resegmented before fed into the MT model. model is trained using Adadelta with initial learning rate of 1.0.For Transformer based model, we use a dropout rate of 0.1 and a gradient clip of 5.0. Following (), we use Adam optimizer according to the learning rate schedule formula:</figDesc><table><row><cell>lrate</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Ablation Study To better understand the contribution of each component, we perform an ablation study on subwordlevel experiments for ST TED corpus. The results are shown in Table7. In '-MT noise' setting, we do not add noise to source sentences for MT. In '-weight sharing' setting, we use different parameters in CTC classification layer and Ablation study for subword-level experiments.</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell cols="3">BLEU</cell></row><row><cell>MT</cell><cell cols="3">Bérard et al.(2018)</cell><cell>19.2</cell></row><row><cell></cell><cell>ESPnet*</cell><cell></cell><cell></cell><cell>18.3</cell></row><row><cell cols="4">Cascaded Bérard et al.(2018)</cell><cell>14.6</cell></row><row><cell></cell><cell>ESPnet*</cell><cell></cell><cell></cell><cell>15.8</cell></row><row><cell>E2E</cell><cell cols="3">Bérard et al.(2018)</cell><cell>12.9</cell></row><row><cell>ST</cell><cell cols="2">+Pretrain+MTL</cell><cell></cell><cell>13.4</cell></row><row><cell></cell><cell cols="2">Liu et al.(2019)</cell><cell></cell><cell>17.02</cell></row><row><cell></cell><cell>ESPnet*</cell><cell></cell><cell></cell><cell>15.71</cell></row><row><cell></cell><cell cols="2">+enc pretrain</cell><cell></cell><cell>16.30</cell></row><row><cell></cell><cell cols="4">+enc dec pretrain 16.78</cell></row><row><cell></cell><cell cols="2">TCEN-LSTM</cell><cell></cell><cell>17.05</cell></row><row><cell cols="6">Table 6: BLEU results of LSTM-based models on Lib-</cell></row><row><cell cols="6">rispeech En-Fr. *: The ESPnet baseline results are</cell></row><row><cell cols="6">copied from https://github.com/espnet/espnet/blob/master/</cell></row><row><cell>egs/libri trans</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>System</cell><cell cols="5">tst2010 tst2013 tst2014 tst2015</cell></row><row><cell>TCEN</cell><cell>15.49</cell><cell>15.50</cell><cell cols="2">13.21</cell><cell>13.02</cell></row><row><cell>-MT noise</cell><cell>15.01</cell><cell>14.95</cell><cell cols="2">13.34</cell><cell>12.80</cell></row><row><cell cols="2">-weight sharing 13.51</cell><cell>14.02</cell><cell cols="2">12.25</cell><cell>11.66</cell></row><row><cell>-pretrain</cell><cell>8.98</cell><cell>8.42</cell><cell>7.94</cell><cell></cell><cell>8.08</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://github.com/moses-smt/mosesdecoder/blob/master/ scripts/tokenizer/tokenizer.perl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/google/sentencepiece</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant No.U1636116, 11431006.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Subword Level Decoder Char Level Decoder tst2010 tst2013 tst2014 tst2015 Average tst2010 tst2013 tst2014 tst2015 Average Vanilla</title>
				<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tied multitask learning for neural speech translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-training on high-resource speech recognition improves low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Alignment templates: the RWTH SMT system</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT 2004</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on End-toend Learning for Speech and Audio Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end automatic speech translation of audiobooks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6224" to="6228" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speechtransformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An attentional model for speech translation without transcription</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2016</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="949" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The jhu/kyotou speech translation system for iwslt</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The iwslt 2018 evaluation campaign</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">IWSLT</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Leveraging weakly supervised data to improve endto-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laurenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>ICASSP 2019</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="7180" to="7184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leveraging weakly supervised data to improve end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laurenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>ICASSP 2019</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="7180" to="7184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hidden markov models for speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="272" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Structured-based curriculum learning for end-to-end english-japanese speech translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="page" from="2630" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kraif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">End-to-end speech translation with knowledge distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">2019. 2019. 1904.08075</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">STACL: simultaneous translation with implicit anticipation and controllable latency using prefix-toprefix framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3025" to="3036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the integration of speech recognition and statistical machine translation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2005</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3177" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lium spkdiarization: an open source toolkit for diarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meignier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merlin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CMU SPUD Workshot</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crowdsourced translation for emergency response in haiti: the global collaboration of local knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMTA Workshop on Collaborative Crowdsourcing for Translation</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech translation: coupling of recognition and translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 1999</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR abs/1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhancing the ted-lium corpus with selected data for language modeling and more ted talks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2014</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3935" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-passing models for robust and data-efficient end-to-end speech translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="313" to="325" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<title level="m">Espnet: End-to-end speech processing toolkit</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
