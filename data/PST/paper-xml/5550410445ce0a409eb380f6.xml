<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloud-Based Collaborative 3D Mapping in Real-Time With Low-Cost Robots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gajamohan</forename><surname>Mohanarajah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vladyslav</forename><surname>Usenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mayank</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Raffaello</forename><surname>D'andrea</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Waibel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country>Germany. M</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Singh is with Cisco Systems</orgName>
								<address>
									<postCode>560 001</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cloud-Based Collaborative 3D Mapping in Real-Time With Low-Cost Robots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AEA56E5C1DEF022F82ACE56BB5175280</idno>
					<idno type="DOI">10.1109/TASE.2015.2408456</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud robotics</term>
					<term>cloud-based mapping</term>
					<term>dense visual odometry</term>
					<term>platform-as-a-Service</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an architecture, protocol, and parallel algorithms for collaborative 3D mapping in the cloud with low-cost robots. The robots run a dense visual odometry algorithm on a smartphone-class processor. Key-frames from the visual odometry are sent to the cloud for parallel optimization and merging with maps produced by other robots. After optimization the cloud pushes the updated poses of the local key-frames back to the robots. All processes are managed by Rapyuta, a cloud robotics framework that runs in a commercial data center. This paper includes qualitative visualization of collaboratively built maps, as well as quantitative evaluation of localization accuracy, bandwidth usage, processing speeds, and map storage. Note to Practitioners-This paper presents an architecture for cloud-based collaborative 3D mapping with low-cost robots. The low-cost robots used in this work consist mainly of a mobile base, a smart phone class processor, an RGB-D sensor, and a wireless interface. Each robot runs its own visual odometry algorithm, which estimates the pose of the robot using the color and the depth frames (images) from the RGB-D sensor. The dense visual odometry algorithm presented herein uses no image features and requires no specialized hardware. In addition to pose estimation, the visual odometry algorithm also produces key-frames, which is a subset of frames that in a way summarizes the motion of the robot. These key-frames are sent to the cloud for further optimization and merging with the key-frames produced by other robots. By sending only the key-frames (instead of all the frames produced by the sensor), bandwidth requirements are significantly reduced. Each robot is connected to the cloud infrastructure using a WebSocket-based bidirectional full duplex communication channel. The cloud infrastructure is provided using Rapyuta, a Platform-as-a-Service framework for building scalable cloud robotics applications. The key-frame pose optimization and the merging processes are parallelized in order to make them scalable. The updated key-frame poses are eventually sent back to the robot to improve its localization accuracy. In addition to describing the architecture and the design choices, the paper provides qualitative and quantitative evaluations of the integrated system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE PAST decade has seen the first successful, large-scale use of mobile robots. However, a large proportion of these robots continue to either use simple control strategies (e.g., robot vacuum cleaners) or be remotely operated by humans (e.g., drones, telepresence robots). A primary reason for the lack of more complex algorithms in such systems is the cost (both direct and indirect) of onboard computation and storage.</p><p>The rapid progress of wireless technologies and the availability of commercial data centers, with high-bandwidth connections and highly scalable computation, storage, and communication infrastructures ("the cloud" <ref type="bibr" target="#b0">[1]</ref>) may allow robots to overcome many of the current bottlenecks. Currently, several frameworks <ref type="bibr" target="#b2">[2]</ref>- <ref type="bibr" target="#b5">[5]</ref> and robotic applications <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> are being developed to exploit the cloud's potential for creating light, fast, and intelligent low-cost robots.</p><p>In this paper, we focus on using the cloud for mapping and localization-two of the most important tasks for any mobile robot. The process of simultaneously building a map and localizing a robot, also known as Simultaneous Localization and Mapping (SLAM), has been a research topic for many years and many SLAM algorithms have been proposed. Although the algorithms are increasing in precision, they require substantial onboard computation and often become infeasible when used for making larger maps over a long period of time.</p><p>Furthermore, running everything locally also limits the potential for collaborative mapping.</p><p>A cloud-based parallel implementation of Fast-SLAM <ref type="bibr" target="#b8">[8]</ref> was presented in <ref type="bibr" target="#b4">[4]</ref> and showed a significant reduction in computation time. In this work, the authors presented a cloud infrastructure based on Hadoop <ref type="bibr" target="#b9">[9]</ref> and received data from the robot using a common Robot Operating System (ROS) <ref type="bibr" target="#b10">[10]</ref> master that managed all communications. Similar to <ref type="bibr" target="#b4">[4]</ref>, authors of <ref type="bibr" target="#b11">[11]</ref> proposed a collaborative mapping framework where they moved the computationally intensive bundle adjustment process of the Parallel Tracking and Mapping (PTAM) <ref type="bibr" target="#b12">[12]</ref> algorithm to a high-performance server connected to the client computer. In addition to the above robotic scenarios, the Kinect@Home project <ref type="bibr" target="#b13">[13]</ref> aims to develop a collection of RGB-D datasets through the use of crowdsourcing, by allowing any user with a Kinect and an appropriate web browser plugin to scan their environment. Once the dataset is uploaded, Kinect@Home performs a batch optimization and generates a 3D representation of the map for the user in the web browser. For a scalable real-time volumetric surface reconstruction approach see <ref type="bibr" target="#b14">[14]</ref>.</p><p>Matterport <ref type="bibr" target="#b15">[15]</ref> is now developing a commercial system with custom cameras (similar to Kinect@Home), with the goal of making it easy for anyone to create 3D images of real-world spaces and share them online. Several centralized collaborative approaches that have the potential to run in a decentralized manner also exist. A 2D mapping system using manifold representation was introduced in <ref type="bibr" target="#b16">[16]</ref>, where the problem of map optimization and merging maps from different robots has been discussed. However, loop closure and map merging were only possible when another robot was recognized visually. In <ref type="bibr" target="#b17">[17]</ref>, the authors present a collaborative visual SLAM system for dynamic environments that is capable of tracking camera pose over time and deciding if some of the cameras observe the same scene; information is combined into groups that run the tracking together. More recently, several visual-inertial odometry systems <ref type="bibr" target="#b18">[18]</ref>, including Google's Project Tango <ref type="bibr" target="#b19">[19]</ref> that runs on a custom cellphone with specialized hardware, has shown superior accuracy and consistency over the other approaches. But scalability, global optimization, and map merging remains open in the above mentioned visual-inertial systems.</p><p>This paper shows that low-cost robot platforms with a smartphone-class processor and a wireless connection are able to collaboratively map relatively large environments at quality levels comparable to the current SLAM methods. Furthermore, this paper shows a scalable approach to map optimization, storage, and merging of maps from different sources. Fig. <ref type="figure">2</ref>. The overview of the proposed architecture based on Rapyuta: Each robot has a corresponding clone in the cloud. The clone is a set of processes (light-gray circles) running under a secured computational environment (rectangular boxes). Every computational environment has its own ROS master (dashed circles) and Rapyuta acts as a multimaster connecting processes running in different environments. Map optimization and merging are parallelized using multiple computational environments (stacked rectangles). All processes running inside the computational environments have a high bandwidth access to the database (cylinders). Robots have a WebSocket-based bidirectional full duplex connection (dotted curved lines) to their corresponding clones in the cloud.</p><p>The main contributions of this paper are: • Open source parallel implementation of dense visual odometry on a smartphone-class ARM multicore CPU. • A novel cloud-based SLAM architecture and protocol, which significantly reduces the bandwidth usage. • Techniques for parallel map optimization and merging over multiple machines in a commercial data center. • An experimental demonstrator for quantitative and qualitative evaluation of the proposed methods. The remainder of this paper is organized as follows. We first give an overview of the system architecture in Section II. Onboard algorithms are presented in Section III. After presenting the data representation and communication protocol in Section IV, we introduce the optimization and merging algorithms in Section V. Finally, the evaluation results of our implementation are presented in Section VIand we conclude in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM ARCHITECTURE</head><p>Real-time constrains, data I/O, network bandwidth, and computational requirements played an important role in the design choices of the proposed architecture. Generally, processes that were sensitive to network delays or which connected high-bandwidth sensors were run on the robot, while computation-or memory-intensive processes without hard real-time constraints were run in the cloud.</p><p>Our architecture, see Fig. <ref type="figure">2</ref>, mainly consists of the following.</p><p>• Mobile robot: Low-cost robots, each with an RGB-D sensor, smartphone-class processor and a wireless connection to the data center, see Fig. <ref type="figure" target="#fig_1">3</ref>. • Robot clone: A set of processes for each robot connected to the cloud that manages key-frames and other data accumulation tasks, while updating the robot with optimized (or post-processed) maps. Currently, the robot clone sends the preprogrammed motion commands to the robot. This "cloud-based control" functionality can be extended in the future to do motion planning based on the map being built, see Fig. After each optimization cycle, the map optimizer updates the database and triggers the robot clone to update the robot with the new map. • Map merger: This process tries to match frames from different maps. Once a match is found, transformations between two maps are computed and the two maps are merged into a single map. All computational processes run on Rapyuta <ref type="bibr" target="#b2">[2]</ref>, a cloud Robotic platform that manages the computational processes and handles the robots' bidirectional communication and authentication. See Section II-B for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robot</head><p>Our robots, shown in Fig. <ref type="figure" target="#fig_1">3</ref>, consist mainly of off-the-shelf components. The differential drive base of the iRobot Create provides the serial interface for sending control commands and receiving sensor information. PrimeSense CARMIN 1.08 is used for the RGB-D sensing, and provides two registered depth and color images in VGA resolution at 30 frames per second. A mm embedded board with a smartphone-class multicore ARM processor is used for onboard computation. The embedded board runs a standard Linux operating system and connects to the cloud through a dual-band USB wireless device. In addition to running the RGB-D sensor driver and controlling the robot, the onboard processor also runs a dense visual odometry algorithm to estimate the current pose of the robot. The key-frames produced by the visual odometry are sent to the cloud processes through the wireless device. See Section III for more information on the visual odometry algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Cloud and Software</head><p>We use Rapyuta <ref type="bibr" target="#b2">[2]</ref>, a cloud robotics platform we developed previously, to run all our processes in the cloud. Since Rapyuta uses the WebSocket protocol to communicate with the robots, the robots and mapping processes need not be in the same network as they were in <ref type="bibr" target="#b4">[4]</ref> and <ref type="bibr" target="#b11">[11]</ref>. This allows us to seamlessly connect our robots in Zurich, Switzerland, to a commercial Amazon <ref type="bibr" target="#b20">[20]</ref> data center in Ireland. Furthermore, since WebSockets allow for persistent connection between processes, the processes running in the cloud can push data/updates to the robots without the robots having to periodically poll for updates.</p><p>Rapyuta can spawn multiple secure ROS-compatible computing environments, launch processes inside these computing environments, and facilitate the communication between these processes (even across different computing environments). This allowed graceful scaling of map optimizer and map merger processes in experiments. Moreover, Rapyuta enables custom message converters to be employed between the robot and the cloud. This flexibility enabled us to use optimal compression schemes, compared to the standard ROS message used in <ref type="bibr" target="#b11">[11]</ref>. Visit http:/ /rapyuta.org/ for more details on Rapyuta.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ONBOARD VISUAL ODOMETRY</head><p>In order to build a map of the environment, it is necessary to track the position of the robot over time. Although several methods (such as wheel odometry, visual odometry, and the use of Inertial Measurement Units) provide information on the relative motion of the robot, only a few of these (i.e., visual odometry) provide the option to remove the accumulated errors with global optimization. The dense visual odometry algorithm used on board the robots is largely inspired by <ref type="bibr" target="#b21">[21]</ref>- <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>This section defines some concepts and introduces the symbols used throughout the paper. Let represent the (color) intensity image and depth image of the camera, respectively. To represent the camera's rigid body motion we use the twist vector and define as</p><p>The over parametrized transformation matrix can now be expressed as</p><p>Using a pinhole camera model, the projection , and the inverse projection between the 3D point and its corresponding pixel representation, is given by where , denotes the focal lengths and , denotes the image center. Note that the second argument of inverse projection for our scenario comes from the corresponding depth pixel . Given a frame, a tuple consisting of , and some other information (see Section IV), the warp of its pixel to a frame with the relative pose is given by Finally, key-frames are a subset of frames that in a way summarizes the full set. The key-frames are also used as a base/reference to represent the pose of other frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dense Visual Odometry Algorithm</head><p>The dense visual odometry algorithm starts with an empty set of key-frames. When it receives the first pair of color and depth images, they are added to the map as an initial key-frame with the initial pose. A map in our scenario consists of key-frames and their corresponding poses.</p><p>After initialization, the dense visual odometry algorithm estimates the pose of the camera based on each incoming frame from the camera. This pose estimation is done by minimizing the photometric error between the intensity images of the current frame and the key-frame given by <ref type="bibr" target="#b0">(1)</ref> where is a set of all pixels that are available in both frames that were not occluded while warped, and is the relative pose of the key-frame with respect to the current frame. The key-frame that is closest to the last estimate of camera pose is used as the current key-frame.</p><p>To minimize the nonlinear cost function given in ( <ref type="formula">1</ref>) with respect to we use the Gauss-Newton method for solving nonlinear least-squares problems <ref type="bibr" target="#b24">[24]</ref>. Here, the iteration is given by where is the Jacobian of the residual and is initialized with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This iteration converges to</head><p>At every iteration the Jacobian can be calculated using the following chain rule:</p><p>Note that the first term in the right-hand side is the color gradient and the other terms can be calculated analytically. The implementation of this algorithm was optimized to run on a multi-core ARM processor. All operations, such as color conversion, subsampling, image gradient computation, and 3D point reprojection are parallelized. These operations involve independent per pixel operations, so they can be easily parallelized by splitting all pixels between several CPU cores. To achieve this we use a Threading Building Blocks library <ref type="bibr" target="#b25">[25]</ref>, which provides templates for easy code parallelization. In particular. the parallel_for and parallel_reduce templates are used heavily in our implementation. We also use the auto-vectorization tools of the GCC compiler, which automatically replaces the regular instructions with specialized vector instructions where possible.</p><p>Since 3D point positions and image gradients are needed only for key-frames, they are computed only when a new key-frame is added (0-2 FPS depending on the robot speed). All images are stored in fixed-point values format (8-bit for intensity images and 16-bit for depth images), which may decrease the accuracy due to the rounding errors, but significantly improves the computational efficiency compared to processing images represented with floating-point values. With our implementation, we were able to achieve a processing time of 15-20 ms for QVGA depth and color images with all 4 cores of CPU where loaded at approximately 60%. During this process, the visual odometry algorithm adds a new key-frame when the distance or the angle to the nearest key-frame in the map exceeds a predefined threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MAP REPRESENTATION AND COMMUNICATION PROTOCOL</head><p>Every map is a set of key-frames and a key-frame is a tuple represented as where is a global index of the key-frame, is the intrinsic parameters of the camera, the unit quaternion and the translation vector. Note that and together represent the pose of the key-frame in the coordinate system of the current robot map. In the current implementation, the global index is a 64-bit integer, where the first 32-bits are used to identify the robot and the rest are used to index the key-frames collected by that robot. This indexing scheme saves approximately 4 billion key-frames from 4 billion robots, which is far beyond current needs.</p><p>The map is synchronized using the protocol shown in Fig. <ref type="figure" target="#fig_4">5</ref>. When the visual odometry adds a new key-frame to the local map, it also sends one to the robot clone. All depth and color images are compressed with PNG for transmission. PNG is a lossless image compression that supports RGB and grayscale images with up to 16-bit per pixel.</p><p>Once the key-frame has reached the robot clone, it is added to the database; the map optimizer process includes this key-frame in its next cycle. The map optimizer triggers the robot clones after the end of each cycle in order to update the local map on the robot. Once triggered, the robot clone gets the key-frame IDs of the local map on the robot, retrieves the updated keyframe pose from the database, and sends it back to the robot. The bandwidth requirement of this map update protocol is relatively low, since the update does not include any images/key-frame transmissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MAP OPTIMIZATION AND MERGING</head><p>The visual odometry that runs on the robot accumulates errors over time and causes a drift in the key-frame pose. This section presents the optimization techniques used to reduce the accumulated errors; these techniques work by minimizing error measures that include all acquired key-frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Map Initialization</head><p>Although this map initialization step is optional, it is recommended since it allows for the calibration of the camera intrinsic parameters. Further, where map initialization was used in experiments, the highly optimized initial map resulted in increased tracking stability.</p><p>During initialization the robot makes a 360 in-place rotation. Assuming pure rotation allows to use well-established methods such as panorama optimization to be used. Our map-initialization is based on <ref type="bibr" target="#b26">[26]</ref> and it globally optimizes all key-frame poses and the intrinsic camera parameters. When pure rotation is assumed, pixels from th key-frame can be transformed to th key-frame by simply multiplying with the homography matrix where and are rotation matrices of the key-frames and with respect to a common reference, and is an intrinsic calibration matrix parametrized by , , , and (see Section III-A). In order to find the optimal key-frame orientations and the intrinsic parameters, one must find the parameter vector that minimizes the per-pixel error of each overlapping pair of frames , given by where and are intensity images of the overlapping keyframes and . The minimization of with respect to was performed using the Gauss-Newton method after parametrizing the updates using the Lie Algebra (see <ref type="bibr" target="#b26">[26]</ref> for details). After the optimization, the floor of the map is selected using RANSAC and the -plane of the (world) coordinate frame was aligned with the floor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Map Optimization</head><p>The global optimization of a map reduces errors accumulated during visual odometry, and consists of two main steps.</p><p>Step 1) Construct a graph where: 1) every key-frame of the map has a corresponding node and 2) an edge between two nodes exists if the corresponding keyframes overlap and a relative transformation can be determined from the image data.</p><p>Step 2) Solve the graph-based non-linear least squares problem given where is the pose vector of all keyframes, is the constraint due to the overlap of key-frames and (calculated in Step 1), and is an appropriate error measure that describes how well the pair , satisfy the constraint . In our case, we are using the error function that minimizes the translational error and the rotational error (magnitude of the real part of the unit quaternion that represents the rotational error) both equally weighted. Once the graph is constructed, several state-of-the-art open source frameworks such as g2o <ref type="bibr" target="#b27">[27]</ref> and Ceres <ref type="bibr" target="#b28">[28]</ref> can be used to solve Step 2. Our architecture uses g2o for Step 2. Since construction of the graph in Step 1 involves the matching of key-frames, which is a computationally expensive task, we parallelize this process over multiple machines, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>The graph optimization node retrieves pairs of key-frame indexes from the database, which don't have a transformation yet, and distributes these between worker nodes. Note that the graph optimization node only selects the key-frame pairs that are within a distance threshold in order to limit the exponential increase of the number of key-frame pairs.</p><p>The worker nodes try to compute the transformation between each pair of key-frames they receive. To compute the transformation, worker node loads the precomputed SURF keypoints for these key-frames and their respective 3D positions from the database and tries to find a transformation between them using RANSAC. If it succeeds, it saves the transformation to the database and proceeds to the next pair. Once all worker nodes have finished, the optimization node optimizes the error function, completing the optimization cycle. After every optimization cycle, key-frame poses are updated in the database and an update trigger is sent to the robot clones to update the local map on the robot. The graph structure of each map is stored as a table of pairs in a database and updated every time the new key-frames are added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Map Merging</head><p>During collaborative mapping the robots can enter areas that have already been already explored by other robots. Being aware of the overlaps significantly decreases the mapping time and increases the map accuracy.</p><p>For the collaborative mapping, no prior knowledge on the initial robot poses is assumed and robots starts out with a separate map. The map merging runs as background process, continuously selecting a random key-frame from a map in the database and trying to find a matching key-frame from the other map.  The process extracts SURF key-points from these key-frames and tries to match them using RANSAC. If a valid transformation is all key-frame poses of the smaller map are transformed into to the coordinate system of the other and the database entries are updated with the new values. Note that except for a minor difference in database update logic, the same worker nodes of the map optimization can be reused to parallelize map merging. Fig. <ref type="figure" target="#fig_0">1</ref> shows a map merged from two robots and the relocalized robots in the new map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>The experimental setup for evaluation consisted of two lowcost robots (Fig. <ref type="figure" target="#fig_1">3</ref>) in Zurich, Switzerland, and the cloud-based architecture (Fig. <ref type="figure">2</ref>) running in Amazon's data center in Ireland. In addition to qualitatively evaluating the building and merging of maps created in different environments, as shown in Figs. 1 (72 key-frames) and 7 (423 key-frames), we quantitatively evaluated network usage, localization accuracy, and global map optimization times.</p><p>Figs. <ref type="figure" target="#fig_7">8</ref> and<ref type="figure" target="#fig_8">9</ref> show the network usage of the robot executing a 360 in-place rotation and a 2 m straight motion with different speeds. It is clearly visible that bandwidth is proportional to the velocity of the robot, with the highest bandwidth about For purposes of comparison, note that our cloud-based KinectFusion <ref type="bibr" target="#b29">[29]</ref>, a dense mapping algorithm, uses around 3.5 MB/s since all frames must be sent to the cloud for processing. For more details on this demonstrator and the video compressions used visit https://github.com/ID-SCETHZurich/rapyuta-kinfu.   To evaluate the accuracy of visual odometry and influence of the global optimization, a high precision commercial motion tracking system was used. Figs. <ref type="figure" target="#fig_9">10</ref> and<ref type="figure" target="#fig_10">11</ref> show the translation and rotation (yaw) errors error of the visual odometry with and without the cloud-based global optimization during a 360 in-place rotation. Fig. <ref type="figure" target="#fig_11">12</ref> shows translation error for a 2 m straight line motion. The yaw error during the straight line motion was below 0.01 rad for both the optimized and the non-optimized visual odometry. In all cases (Figs. <ref type="figure" target="#fig_9">10</ref><ref type="figure" target="#fig_10">11</ref><ref type="figure" target="#fig_11">12</ref>), the cloud-based optimization was able to reduce the errors significantly, especially when there is a loop closure, such as in Figs. <ref type="figure" target="#fig_9">10</ref> and<ref type="figure" target="#fig_10">11</ref>. Note that, due to the relatively low visual features in the motion capture space, the maps of this space were of low quality compared to the ones given in Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_6">7</ref>. Finally, Fig. <ref type="figure" target="#fig_12">13</ref> shows the time taken for map optimization against the number of worker nodes. Although the processing time initially decreases with the number of worker nodes, this decrease later vanishes due to communication latencies. The measurements also show that the gain due to parallelization is significantly more for larger sets of key-frames. To reduce latencies due to database access during map optimization, we compared a relational and a non-relational database with respect to their I/O speeds. MySQL was used to represent relational databases, whereas MongoDB was used to represent nonrelational databases and the results are shown in Fig. <ref type="figure" target="#fig_12">13</ref>(a) and (b). Although both databases gave a similar performance with respect to speed, using the JOIN clause of MySQL (join clause combines records from two or more tables in a database), a significant amount of computation was offloaded from the graph optimization node to the database during the key-frame pair selection (see Section V-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We presented first steps towards a scalable cloud robotics service for mapping and localization using Rapyuta <ref type="bibr" target="#b2">[2]</ref>, an opensource cloud robotics framework we developed in our previous work.</p><p>First, we provided an open source implementation of a stateof-the art, dense visual odometry algorithm on a smartphoneclass ARM multicore CPU. <ref type="foot" target="#foot_0">1</ref> Second, we developed a data protocol that sends only compressed key-frames to the cloud, reducing the bandwidth requirements. <ref type="foot" target="#foot_1">2</ref> . In addition, the protocol allows the cloud processes to push key-frame pose updates back to the robots without the need for constant polling. Third, we presented techniques for parallelizing the computationally expensive operations of map optimization and map merging in a commercial data center, and provided a corresponding open source software implementation .</p><p>As illustrated by our demonstrator, this cloud-based architecture holds the potential to greatly increase the number . Finally, we showed both qualitative and first quantitative results achieved with the architecture. As shown in Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_6">7</ref>, as well as in the accompanying video, our implementation yields maps comparable to those obtained with more expensive robot hardware. First quantitative experiments confirmed that bandwidth requirements are well within those typically available in modern wireless networks ( 0.5 MB/s). They also confirmed that map optimization provided via the cloud significantly reduces uncertainty of the robot's visual odometry. Moreover, they confirmed the computational advantage of parallelization for map optimization in the cloud.</p><p>Possible future improvements include the incorporation of the depth error into visual odometry <ref type="bibr" target="#b22">[22]</ref>, substituting the current naive bag-of-words-based place recognition to a more probabilistic approach such as FAB-MAP <ref type="bibr" target="#b31">[31]</ref> for map merging, and the creation of larger maps using more robots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A point cloud map of a room at ETH Zurich built in real-time by the two robots shown in Fig. 3. The individual maps generated by the two robots are merged and optimized by processes running on a datacenter in Ireland. The robots are relocalized and the robot models are overlaid in the merged map [2]. (a) Top view. (b) Side view with a photo taken in a similar perspective.</figDesc><graphic coords="2,51.00,65.10,228.00,294.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The two low-cost ( 600$) robots used in our evaluations: Each robot consists mainly of a differential drive base (iRobot Create), an RGB-D sensor (PrimeSense), an ARM-based single board computer (ODROID-U2), and a dual band USB wireless device [2].</figDesc><graphic coords="3,49.98,64.14,228.00,181.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2. • Database: A database for storing maps. A relational (MySQL) database and a non-relational database (Mon-goDB) was used for comparison. • Map optimizer: Parallel optimization algorithm to find the optimal pose graph based on all accumulated key-frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The onboard processor Odroid-U2: 48 52 mm embedded board with a smartphone-class quad core ARM Cortex-A9 processor.</figDesc><graphic coords="3,314.04,64.14,227.94,126.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sequence diagram of the proposed map synchronization protocol: All key-frames from the robot are sent to the robot clone, processed and saved to the back-end database. After every cycle of back-end optimization, the robot clone gets the local key-frame IDs from the robot and updates the local key-frame poses.</figDesc><graphic coords="5,72.00,64.14,183.06,237.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Map optimization architecture: Pose graph construction is distributed among worker nodes and the constructed graph is optimized in the graph optimization node.</figDesc><graphic coords="6,63.00,64.14,205.02,151.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A point cloud map of a 40 m-long corridor. The map was collaboratively built by two robots, and consists of 423 key-frames. Different colors in the right image show parts of the map built by different robots.</figDesc><graphic coords="6,306.00,64.14,246.00,114.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Network usage in bytes per second for a single robot performing a 360 in-place rotation.</figDesc><graphic coords="6,336.00,233.16,186.00,130.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Network usage in bytes per second for a single robot performing a 2 m straight motion.</figDesc><graphic coords="7,70.98,64.14,186.00,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Translation error of key-frames extracted by visual odometry during a 360 in-place rotation with and without map optimization. The errors are based on the ground truth measurements from VICON, a high-precision motion capture system.</figDesc><graphic coords="7,70.98,235.14,186.00,118.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Rotation error of key-frames extracted by visual odometry during 360 in-place rotation with and without map optimization. The errors are based on the ground truth measurements from VICON, a high-precision motion capture system.</figDesc><graphic coords="7,70.98,408.12,186.00,118.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Translation error of key-frames extracted by visual odometry during a 2 m forward motion with and without map optimization. The errors are based on the ground truth measurements from VICON, a high-precision motion capture system.</figDesc><graphic coords="7,334.98,63.12,186.00,121.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Map optimization times against the number of worker nodes. The numbers in parenthesis in the legend denote the number of key-frames. loop0 and loop1 are the two loops of the corridor shown in Fig. 7. loop_merged is a combination of both. fr2_desk is a public data set obtained from [30]. (a) MySQL. (b) MongoDB.</figDesc><graphic coords="8,105.00,69.12,384.00,127.98" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://github.com/IDSCETHZurich/rapyuta-mapping</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that the bandwidth depends on many factors such as the speed of the robot and thresholds used for generating key-frames. Therefore, it is hard to do a rigorous comparison with other works such as<ref type="bibr" target="#b11">[11]</ref>. However, our advantage compared to<ref type="bibr" target="#b11">[11]</ref> include scalability, the nonrequirement for the robot and the server to be in the same network, and our PNG-based custom converters to achieve a better compression compared to the ROS standard messages.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank D. Pangercic for his continuous support and motivation, and their colleagues D. Hunziker and D. Sathe for their support with Rapyuta.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper was recommended for publication by Associate Editor J. Civera and Editor S. Sarma upon evaluation of the reviewers' comments. The work of G. Mohanarajah, R. D'Andrea, M. Waibel, V. Usenko, and M. Singh was done while at the Institute for Dynamic Systems and Control (IDSC), ETH Zurich, Zurich, Switzerland. This work was supported in part by the European Union Seventh Framework Program FP7/2007-2013 under Grant Agreement 248942 RoboEarth, and also in part from Amazon Web Services (AWS) in Education Grant Award. G. Mohanarajah, R. D'Andrea, and M. Waibel are with the Department of Mechanical and Process Engineering, ETH Zurich, Zurich 8092, Switzerland</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The NIST definition of cloud computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grance</surname></persName>
		</author>
		<imprint>
			<publisher>Special Pub</publisher>
			<biblScope unit="page" from="800" to="145" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Gaithersburg</surname></persName>
		</author>
		<author>
			<persName><surname>Usa</surname></persName>
		</author>
		<ptr target="http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rapyuta: A cloud robotics platform</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mohanarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hunziker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>D'andrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="481" to="493" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>D'andrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elfring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Haussermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perzylo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiessle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tenorth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zweigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Molengraft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Mag., IEEE</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="82" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
	<note>RoboEarth</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Davinci: A cloud computing framework for service robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arumugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Enti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="3084" to="3089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cloud Networked Robotics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hagita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2012-06">May-Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud-based robot grasping with the google object recognition engine</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kehoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Candido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward cloud-based grasping with uncertainty in shape: Estimating lower bounds on achieving force closure with zero-slip push grasps</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kehoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="576" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<title level="m">Probabilistic Robotics (Intelligent Robotics and Autonomous Agents)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Hadoop distributed file system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shvachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chansler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Sym. Mass Storage Syst. Technol. (MSST)</title>
		<meeting>IEEE Sym. Mass Storage Syst. Technol. (MSST)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ROS: An open-source Robot Operating System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Gerkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leibs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA Workshop on Open Source Softw</title>
		<meeting>ICRA Workshop on Open Source Softw</meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">C2TAM: A cloud framework for cooperative tracking and mapping</title>
		<author>
			<persName><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robo. Auton. Syst</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping on a camera phone</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Mixed and Augmented Reality</title>
		<meeting>IEEE Int. Symp. Mixed and Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="83" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kinect@Home: Crowdsourced RGB-D data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goransson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aydemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
		<ptr target="http://www.kinectathome.com/" />
	</analytic>
	<monogr>
		<title level="m">Proc. IROS Workshop on Cloud</title>
		<meeting>IROS Workshop on Cloud</meeting>
		<imprint/>
	</monogr>
	<note>Robot., 201.3 [Online</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable real-time volumetric surface reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bautembach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Matterport</surname></persName>
		</author>
		<ptr target="http://matterport.com/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-robot mapping using manifold representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gaurav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Maja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="4198" to="4203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CoSLAM: Collaborative visual slam in dynamic environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="366" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Camera-imu-based localization: Observability analysis and consistency improvement</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Kottas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Roumeliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="182" to="201" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Project Tango</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="http://www.google.com/atap/projecttango/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Amazon elastic compute cloud</title>
		<ptr target="http://aws.amazon.com/ec2/am" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Amazon.com Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time visual odometry from dense RGB-D images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Steinbrucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Comput. Vision Workshop</title>
		<meeting>ICCV Comput. Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust odometry estimation for RGB-D cameras</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="3748" to="3754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Direct iterative closest point for real-time visual odometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tykkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Audras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Comport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Comput. Vision Workshop</title>
		<meeting>ICCV Comput. Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="2050" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tingleff</surname></persName>
		</author>
		<ptr target="http://www2.imm.dtu.dk/pubdb/views/edocdownload.php/3215/pdf/imm3215.pdf" />
	</analytic>
	<monogr>
		<title level="m">Methods For Non-Linear Least Squares Problems</title>
		<meeting><address><addrLine>Lyngby, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intel Threading Building Blocks, 1st ed</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reinders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>O&apos;Reilly &amp; Assoc., Inc</publisher>
			<pubPlace>Sebastopol, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time spherical mosaicing using whole image alignment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Euro. Conf. Comput. Vision</title>
		<meeting>11th Euro. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A general framework for graph optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kümmerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="3607" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ceres solver</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mierle</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/ceres-solver/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Mixed Augmented Reality (ISMAR)</title>
		<meeting>IEEE Int. Symp. Mixed Augmented Reality (ISMAR)</meeting>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of RGB-D slam systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Intell. Robot Syst. (IROS)</title>
		<meeting>Int. Conf. Intell. Robot Syst. (IROS)</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
