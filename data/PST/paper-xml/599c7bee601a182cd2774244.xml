<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically Enhanced Software Traceability Using Deep Learning Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinghui</forename><surname>Cheng</surname></persName>
							<email>jinghuicheng@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Cleland-Huang</surname></persName>
							<email>janeclelandhuang@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantically Enhanced Software Traceability Using Deep Learning Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6B65125FF94ECDF81D45F973ABDD8753</idno>
					<idno type="DOI">10.1109/ICSE.2017.9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Traceability</term>
					<term>Deep Learning</term>
					<term>Recurrent Neural Network</term>
					<term>Semantic Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed stateof-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>Requirements traceability plays an essential role in the software development process. Defined as "the ability to describe and follow the life of a requirement in both a forwards and backwards direction through periods of ongoing refinement and iteration" <ref type="bibr" target="#b25">[26]</ref>, traceability supports a diverse set of software engineering activities including change impact analysis, regression test selection, cost prediction, and compliance verification <ref type="bibr" target="#b24">[25]</ref>. In high-dependability systems, regulatory standards, such as the US Federal Aviation Authority's (FAA) DO178b/c <ref type="bibr" target="#b21">[22]</ref>, prescribe the need for trace links to be established and maintained between hazards, faults, requirements, design, code, and test cases in order to demonstrate that a system is safe for use <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, the tracing task is arduous to perform and error-prone <ref type="bibr" target="#b45">[46]</ref>, even when industrial tools are used to manually create links or to capture them as a byproduct of the development process <ref type="bibr" target="#b13">[14]</ref>. In practice, trace links are often incomplete and inaccurate <ref type="bibr" target="#b15">[16]</ref>, even in safety-critical systems <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>To address these problems, researchers have proposed and developed solutions for automating the task of creating and maintaining trace links <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Solutions have included information retrieval approaches <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b4">[5]</ref>, machine learning <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b50">[51]</ref>, heuristic techniques <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and AI swarming algorithms <ref type="bibr" target="#b64">[65]</ref>. Other approaches, especially in the area of feature location <ref type="bibr" target="#b19">[20]</ref>, require additional information obtained from runtime execution traces. Results have been mixed, especially when applied to industrial-sized datasets, where acceptable recall levels above 90% can often only be achieved at extremely low levels of precision <ref type="bibr" target="#b42">[43]</ref>.</p><p>One of the primary reasons that automated approaches have underperformed is the term mismatch that often exists between pairs of related artifacts <ref type="bibr" target="#b9">[10]</ref>. To illustrate this we draw on an example from the Positive Train Control (PTC) domain. PTC is a communication-based train control system designed to ensure that trains follow directives in order to prevent accidents from occurring <ref type="bibr" target="#b67">[68]</ref>. The requirement stating that "The BOS Administrative Toolset shall allow the Authorized Administrator to view an On-board's last reported On-board Software Version, including the associated repository name, MD5, and whether the fileset is preferred or acceptable." is associated with the design artifact stating that "The Operational Data Panel is used to provide information about the current PTC operations in a subdivision". Recognizing and establishing this link requires non-trivial knowledge of domain conceptsfor example, understanding that BOS Administrative Toolset contains the Operational Data Panel, each locomotive contains an On-board unit for PTC operation, and that the Operational Data Panel displays the information of locomotives such as the On-board Software Version to the BOS Authorized Administrator. This link would likely be missed by popular trace retrieval algorithms such as the Vector Space Model (VSM), Latent Semantic Indexing (LSI), and Latent Direchlet Allocation (LDA), which all represent artifacts as bags of words and therefore lose the artifacts' embedded semantics. It would also be missed by techniques that incorporate phrasing without understanding their conceptual associations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In fact, most current techniques lack the sophistication needed to reason about semantic associations between artifacts and therefore fail to establish trace links when there is little meaningful overlap in use of terms.</p><p>In our prior work we developed Domain-Contextualized Intelligent Traceability (DoCIT) <ref type="bibr" target="#b28">[29]</ref> as a proof of concept solution to investigate the integration of domain knowledge into the tracing process. We demonstrated that for the domain of PTC systems DoCIT returned accurate trace links achieving mean average precision (MAP) of .822 in comparison to .590 achieved using VSM. However, the cost of setting up DoCIT for a domain was non-trivial, as it required carefully handcrafting a domain ontology and manually defining trace link heuristics capable of reasoning over the semantics of the artifacts and associated domain knowledge. Furthermore, DoCIT depended upon a conventional syntactic parser to analyze the artifacts in order to extract meaningful concepts. The approach was therefore sensitive to errors in the parser, terms missing from the ontology, and missing or inadequate heuristics. As such, DoCIT was effective but fragile, and would require significant effort to transfer into new project domains.</p><p>On the other hand, deep learning techniques have successfully been applied to solve many Natural Language Processing (NLP) tasks including parsing <ref type="bibr" target="#b62">[63]</ref>, sentiment analysis <ref type="bibr" target="#b65">[66]</ref>, question answering <ref type="bibr" target="#b34">[35]</ref>, and machine translation <ref type="bibr" target="#b5">[6]</ref>. Such techniques abstract problems into multiple layers of nonlinear processing nodes; they leverage either supervised or unsupervised learning techniques to automatically learn a representation of the language and then use this representation to perform complex NLP tasks. The goal of the work described in this paper is to utilize deep learning to deliver a scalable, portable, and fully automated solution for bridging the semantic gap that currently inhibits the success of trace link creation algorithms. Our solution is designed to automate the capture of domain knowledge and the artifacts' textual semantics with the explicit goal of improving accuracy of the trace link generation task.</p><p>The approach we propose includes two primary phases. First, we learn a set of word embeddings for the domain using an unsupervised learning approach trained over a large set of domain documents. The approach generates high dimensional word vectors that capture distributional semantics and cooccurrence statistics for each word <ref type="bibr" target="#b52">[53]</ref>. Second, we use an existing training set of validated trace links from the domain to train a Tracing Network to predict the likelihood of a trace link existing between two software artifacts. Within the tracing network, we adopt a Recurrent Neural Network architecture to learn the representation of artifact semantics. For each artifact (i.e. each regulation, requirement, or source code file etc.), each word is replaced by its associated vector representation learned in the word embedding training phase and then sequentially fed into the RNN. The final output of RNN is a vector that represents the semantic information of the artifact. The tracing network then compares the semantic vectors of two artifacts and outputs the probability that they are linked.</p><p>Given the need for an initial training set of trace links, our approach cannot be used in an entirely green field domain. However, based on requests from our industrial collaborators, we envision the following primary usage scenarios: (1) Train the tracing network on an initial set of manually constructed trace links for a project and then use it to automate the production of other links as the project proceeds; (2) Train the tracing network on the complete set of trace links for a project and then use it to find additional links that may have been missed during the manual link construction process; and finally (3) Train the tracing network on the trace links for one project, or for specific types of artifacts in one project, and then apply it to other projects and artifact types within the same domain. In this paper we focus on the first scenario.</p><p>We evaluate our approach on a large industrial dataset taken from the domain of PTC systems to address two research questions: RQ1: How should RNN be configured in order to generate the most accurate trace links? RQ2: Is RNN able to significantly improve trace link accuracy in comparison to standard baseline techniques?</p><p>The remainder of the paper is structured as follows. We first introduce deep learning techniques related to the tracing network in Section II. The architecture of the tracing network is described in Section III. Sections IV and V describe the process used to configure the tracing network, our experimental design, and the results achieved. Finally, in Sections VI to VIII we discuss related work, threats to validity, and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Deep Learning for Natural Language Processing</head><p>Many modern deep learning models and associated training methods originated from research in artificial neural networks (ANN). Inspired by advances in neuroscience, ANNs were designed to approximate complex functions of the human brain by connecting a large number of simple computational units in a multi-layered structure. Based on ANNs, Deep Learning models feature more complex network connections in a larger number of layers. A benefit gained from a more complex structure is the ability to represent data features with multiple levels of abstraction; this is usually preferable to more traditional machine learning techniques, in which human expertise is needed to select features of data for training. Backpropagation <ref type="bibr" target="#b57">[58]</ref> is widely recognized as an effective method for training deep neural networks; it indicates how the network should adapt its internal parameters to better compute the representation in each layer. Before presenting our approach, we describe fundamental concepts of deep learning techniques, especially as related to NLP tasks. Furthermore, as our interest lies in comparatively evaluating different models for purposes of trace link creation, we describe these various techniques in some depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Word Embedding</head><p>Conventional NLP and information retrieval techniques treat unique words as atomic symbols and therefore do not take associations among words into account. To address this limitation, word embedding learns the representation of each word from a corpus as a continuous high dimensional vector, such that similar words are close together in the vector space. In addition, the embedded word vectors encode syntactic and semantic relationships between words as linear relationships between word vectors <ref type="bibr" target="#b47">[48]</ref>. The use of learned word vectors is considered one of the primary reasons for the success of recent deep learning models for NLP tasks <ref type="bibr" target="#b20">[21]</ref>.</p><p>Skip-gram with negative sampling <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref> and GloVe <ref type="bibr" target="#b52">[53]</ref> are the most popular word embedding models due to the notable improvement they bring to word analogy tasks over more traditional approaches such as Latent Semantic Analysis <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Word embedding models are trained using unlabeled natural language text by utilizing co-occurrence statistics of words in the corpus. The Skip-gram model scans context windows across the entire training text to train prediction models <ref type="bibr" target="#b47">[48]</ref>. Given the center word in the window of size T , this model maximizes the probability that the targeted word appears around the center word, while minimizing the probability that a random word appears around the center word. The GloVe model uses matrix factorization; however we do not discuss it further because its performance is equivalent to the Skip-gram with negative sampling approach while it is less robust and utilizes more system resources <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Network Structures</head><p>Deep learning for NLP tasks are typically addressed using neural network techniques. Feedforward networks, also referred to as multi-layer perceptrons (MLPs), represent a traditional neural network structure and lay the foundation for many other structures <ref type="bibr" target="#b31">[32]</ref>. However, the number of parameters in a fully connected MLP can grow extremely large as the width and depth of the network increases. To address this limitation, researchers have proposed various neural network structures targeting different types of practical problems. For example, convolutional neural networks (CNNs) are especially well suited for image recognition and video analysis tasks <ref type="bibr" target="#b39">[40]</ref>. For NLP tasks, Recurrent neural networks (RNNs) are widely used and are recognized as a good fit to the unique needs of NLP <ref type="bibr" target="#b57">[58]</ref>. In particular, RNN and its variants have produced significant breakthroughs in many NLP tasks including language modeling <ref type="bibr" target="#b48">[49]</ref>, machine translation <ref type="bibr" target="#b5">[6]</ref>, and semantic entailment <ref type="bibr" target="#b65">[66]</ref>. In the following sections, we first introduce background about RNN and then discuss several RNN variants that were evaluated in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Standard Recurrent Neural Networks (RNN)</head><p>RNNs are particularly well suited for processing sequential data such as text and audio. They connect computational units of the network in a directed cycle such that at each time step t, a unit in the RNN not only takes input of the current step (i.e., the next word embedding), but also the hidden state of the same unit from the previous time step t -1. This feedback mechanism simulates a "memory", so that a RNN's output is determined by both its current and prior inputs. Furthermore, because RNNs use the same unit (with the same parameters) across all time steps, they are able to process sequential data of arbitrary length. This is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. At a given time step t with input vector x t and its previous hidden output vector h t-1 , a standard RNN unit calculates its output as</p><formula xml:id="formula_0">h t = tanh(W x t + Uh t-1 + b) (1)</formula><p>where W, U and b are the affine transformation parameters, and tanh is the hyperbolic tangent function: tanh(z) = (e ze -z )/(e z + e -z ). A prominent drawback of the standard RNN model is that the network degrades when long dependencies exist in the sequence due to the phenomenon of exploding or vanishing gradients during back-propagation <ref type="bibr" target="#b8">[9]</ref>. This makes a standard RNN model difficult to train. The exploding gradients problem can be effectively addressed by scaling down the gradient when its norm is bigger than a preset value (i.e. Gradient Clipping) <ref type="bibr" target="#b8">[9]</ref>. To address the vanishing gradients problem of the standard RNN model, researchers have proposed several variants with mechanics to preserve long-term dependencies; these variants included Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Long Short Term Memory (LSTM)</head><p>LSTM networks include a memory cell vector in the recurrent unit to preserve long term dependencies <ref type="bibr" target="#b32">[33]</ref>. LSTM also introduces a gating mechanism to control when and how to read or write information to the memory cell. A gate in LSTM usually uses a sigmoid function σ(z) = 1/(1 + e -z ) and controls information throughput using a point-wise multiplication operation . Specifically, when the sigmoid function outputs 0, the gate forbids any information from passing, while all information is allowed to pass when the sigmoid function output is 1. Each LSTM unit contains an input gate (i t ), a forget gate ( f t ), and an output gate (o t ). The state of each gate is decided by x t and h t-1 such that:</p><formula xml:id="formula_1">i t = σ(W i x t + U i h t-1 + b i ) f t = σ(W f x t + U f h t-1 + b f ) o t = σ(W o x t + U o h t-1 + b o )<label>(2)</label></formula><p>To update the information in the memory cell, a memory candidate vector ct is first calculated using the tanh function. This memory candidate passes through the input gate, which controls how much each dimension in the candidate vector should be "remembered". At the same time, the forget gate controls how much each dimension in the previous memory cell state c t-1 should be retained. The actual memory cell state c t is then updated using the sum of these two parts.</p><formula xml:id="formula_2">ct = tanh(W c x t + U c h t-1 + b c ) c t = i t ct + f t c t-1 (3)</formula><p>Finally, the LSTM unit calculates its output h t with an output gate as follows:</p><formula xml:id="formula_3">h t = o t tanh(c t )<label>(4)</label></formula><p>Figure <ref type="figure">2</ref> (a) illustrates a typical LSTM unit. Using retained memory cell state and the gating mechanism, the LSTM unit "remembers" information until it is erased by the forget gate; Fig. <ref type="figure">2</ref>: Comparison between single units from LSTM and GRU networks. In (2a), i, f and o are the input, forget and output gates respectively; c is the memory cell vector. In (2b), r is the reset gate and u is the update gate <ref type="bibr" target="#b12">[13]</ref>.</p><p>as such, LSTM handles long-term dependencies more effectively. LSTM has been repeatedly applied to solve semantic relatedness tasks and has achieved convincing performance <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b56">[57]</ref>. These advances motivated us to adopt LSTM for reasoning semantics in the tracing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Gated Recurrent Unit (GRU)</head><p>Finally, the recently proposed Gated Recurrent Unit (GRU) model also uses a gating mechanism to control the information flow within a unit; but it has a simplified unit structure and does not have a dedicated memory cell vector <ref type="bibr" target="#b11">[12]</ref>. It contains only a reset gate r t and an update gate u t :</p><formula xml:id="formula_4">r t = σ(W r x t + U r h t-1 + b r ) u t = σ(W u x t + U u h t-1 + b u )<label>(5)</label></formula><p>In GRU networks, the previous hidden output h t-1 goes through the reset gate r t and is sent back to the unit. An output candidate ht is then calculated using the gated h t-1 and the unit's current input x t as follows:</p><formula xml:id="formula_5">ht = tanh(W h x t + U h (r t h t-1 ) + b h ) (6)</formula><p>The actual output of a unit h t is a linear interpolation between the previous output h t-1 and the candidate output ht , controlled by the update gate u t . As such, the update gate balances how much of the current output is updated using ht and h t-1 .</p><formula xml:id="formula_6">h t = (1 -u t ) h t-1 + u t ht<label>(7)</label></formula><p>Consequently, a GRU unit embeds long-term information directly into the hidden output vectors. Figure <ref type="figure">2</ref> compares the unit structures of LSTM and GRU networks. Despite having a simpler structure, GRU has achieved competitive results with LSTM for many NLP tasks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and as such no decisive conclusion has been drawn about which model is better. In this work, we compare the performance of LSTM and GRU in order to identify the most suitable model for addressing the tracing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Other RNN Variables</head><p>In addition to modifying the structure within a single RNN unit, the structure of the overall RNN network can be varied. For instance, multi-layered RNNs <ref type="bibr" target="#b51">[52]</ref> stack more than one RNN unit at each time step <ref type="bibr" target="#b58">[59]</ref> with the aim of extracting more abstract features from the input sequence. In contrast, bidirectional RNNs <ref type="bibr" target="#b59">[60]</ref> process sequential data in both forward and backward directions at the same time; this enables the output to be influenced by both past and future data in the sequence. In this study, we also explored the use of twolayered RNNs and bidirectional RNNs for generating trace links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. The Tracing Network</head><p>The tracing process comprises several steps: first, the human analyst initiates a trace for a source artifact; second, the similarity between the source artifact and each of the potentially linked target artifacts is computed; third, a list of ranked candidate links are returned; and finally the human evaluates the links and accepts the ones deemed to be correct. The process is repeated for all source artifacts <ref type="bibr" target="#b33">[34]</ref>. Out study investigates the effectiveness of various deep learning models and methods for calculating similarities between source and target artifact pairs, with the goal of generating accurate trace links. This is essentially a textual comparison task in which the tracing network needs to leverage domain knowledge to understand the semantics of two individual artifacts and then to evaluate their semantic relatedness for tracing purposes. Valid associations need to be established between related artifacts even when no common words are present. Based on our initial analysis of the strengths and weaknesses of current techniques, we decided to adopt word embeddings and RNN techniques to achieve this goal. Therefore, we first need to learn word embeddings from a domain corpus in order to effectively encode word relations, and then utilize such word embeddings in the tracing network structure to extract and compare their semantics. In this section, we describe our approach for designing and training such a tracing network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>The design of the neural network architecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Given the textual content of a source artifact A s and a target artifact A t , each word in A s and A t is first mapped onto its vector representation through the Word Embedding layer. Such mappings are trained from the domain corpus using the Skip-gram model introduced in Section II-A. The vectors of words in source artifact s 1 , s 2 , . . . , s m are then sent to the RNN layers sequentially and output as a single vector v s representing its semantic information. In the case of the bidirectional-RNN, the word vectors are also sent in reverse order as s m , s m-1 , . . . , s 1 . The target semantic vector v t is generated in the same way using RNN layers. Finally, these two vectors are compared in the Semantic Relation Evaluation layers.</p><p>The Semantic Relation Evaluation layers in our tracing network adopt the structure proposed by Tai et al. <ref type="bibr" target="#b65">[66]</ref>, targeted to perform semantic entailment classification tasks for sentence pairs. The overall calculation of this part of the network can be represented as:</p><formula xml:id="formula_7">r pmul = v s v t r sub = |v s -v t | r = σ(W r r pmul + U r r sub + b r ) p tracelink = so f tmax(W p r + b p ) (8)</formula><p>where</p><formula xml:id="formula_8">so f tmax(z) j = e z j / K k=1 e zk , f or j = 1, . . . , K<label>(9)</label></formula><p>Here, is the point-wise multiplication operator used to compare the direction of source and target vectors on each dimension. The absolute vector subtraction result, r sub , represents the distance between the two vectors in each dimension. The network then uses a hidden sigmoid layer to integrate r pmul and r sub and output a single vector to represent their semantic similarity. Finally, the output softmax layer uses the result to produce the probability that a valid trace link exists; the result of a softmax function is a K-dimensional vector of real values in the range (0, 1) that add up to 1 (K=2 in this case).</p><p>A concrete tracing network is built upon this architecture and is further configured by a set of network settings. Those settings specify the type of RNN unit (i.e. GRU or LSTM), the number of hidden dimensions in RNN units and the Semantic Relation Evaluation layers, and other RNN variables such as the number of RNN layers and whether to use bidirectional RNN. To address our first research question (RQ1) we explored several different configurations. We describe how we optimized network settings in Section IV-B. The tracing network is implemented on the Torch framework (http://torch.ch), and the source code is available at https://github.com/jinguo/TraceNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training the Tracing Network</head><p>A powerful network is only useful when it can be properly trained using existing data and when it is generalizable to unseen data. To train the tracing network, we use the regularized negative log likelihood as our objective loss function to be minimized. This objective function is commonly used in categorical prediction models <ref type="bibr" target="#b6">[7]</ref> and can be written as:</p><formula xml:id="formula_9">J(θ) = - 1 N N i=1 logP(Y = y i |x i , θ) + λ 2 θ 2 2 (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where θ indicates the network parameters that need to be trained, N is the total number of examples in the training data, x i is the input of the ith training example, y i is the actual category of that example (i.e. link or non-link); as a result, P(Y = y i |x i , θ) represents the network's prediction on the correct category given the current input and parameters. The second part of the loss function represents a L 2 parameter regularization that prevents overfitting, where θ 2 is the Euclidean norm of θ, and λ controls the strength of the regularization.</p><p>Based on this loss function, we used a stochastic gradient descent method <ref type="bibr" target="#b66">[67]</ref> to update the network parameters. According to this method, a typical training process is comprised of a number of epochs. Each epoch iterates through all of the training data one time to train the network; as such, the overall training process uses all the training data several times until the objective loss is sufficiently small or fails to decrease further. In each epoch, the training data is further randomly divided into a number of "mini batches;" each contains one or more training datapoints. After each batch is processed, a gradient of parameters is calculated based on the loss function. The network then updates its parameters based on this gradient and a "learning rate" that specifies how fast the parameters move along the gradient direction. During training, we adopted an adaptive learning rate procedure <ref type="bibr" target="#b66">[67]</ref> to adjust the learning rate based on the current training performance. To help the network converge, we also decreased the learning rate after each epoch until epoch τ, such that the learning rate at epoch k is determined by:</p><formula xml:id="formula_11">k = (1 -α) 0 + α τ (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where 0 is initial learning rate, α = k/τ. In our experiment, τ is set to 0 /100, and τ set to 500.</p><p>Based on these general methods, a tracing network training process is further determined by a set of predefined hyperparameters that help steer the learning behavior. Common hyper-parameters include the initial learning rate, gradient clip value, regulation strength (λ), the number of datapoints in a mini batch (i.e. mini batch size), and the number of epochs included in the training process. The techniques for selecting hyper-parameters are described in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Experiment Setup</head><p>In this section, we explain the methods used to (1) prepare data, (2) systematically tune the configuration (i.e. network settings and hyper-parameters) of the tracing network, and (3) compare the performance of the best configuration against other popular trace evaluation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preparation</head><p>To train the word embeddings, we used a corpus from the PTC domain that is comprised of 52.7MB of clean text extracted from related domain documents and software artifacts. The original corpus of domain documents was collected from the Internet by our industrial collaborators as part of their initial domain analysis process. We also added the latest Wikipedia dump containing about 19.92GB of clean text to the corpus and used it for one variant of the word embedding configuration. All documents were preprocessed by transforming characters to lower-case and removing all nonalpha-numeric characters except for underscores and hyphens.</p><p>To train and evaluate other parts of the tracing network, we used PTC project data provided by our industrial collaborators. The dataset contains 1,651 Software Subsystem Requirements (SSRS) as source artifacts and 466 Software Subsystem Design Descriptions (SSDD) as target artifacts. Each source artifact contained an average of 33 tokens and described a functional requirement of the Back Office Server (BOS) subsystem. Each target artifact contained an average of 99 tokens and specified design details. There were 1,387 trace links between SSRS and SSDD artifacts, all of which were constructed and validated by our industrial collaborators. This dataset is considerably larger than those used in most previous studies on requirements traceability <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b33">[34]</ref> and the task of creating links across such a large dataset represents a challenging industrial-strength tracing problem. We randomly selected 45% of the 769,366 artifact pairs from the PTC project dataset (i.e. 1,651 × 466) for inclusion in a training set, 10% for a development set, and 45% as a testing set. Given a fixed tracing network configuration, the training set was used to update the network parameters (i.e. the weight and bias for affine transformation in each layer) in order to minimize the objective loss function. The development set was used to select the best general model during an initial training process to ensure that the model was not overtrained. The test data was set aside and only used for evaluating the performance of the final network model.</p><p>Software project data exhibits special characteristics that impact the training of a neural network <ref type="bibr" target="#b29">[30]</ref>. In particular, the number of actual trace links is usually very small for a given set of source and target artifacts compared to the total number of artifact pairs. In our dataset, among all 769,366 artifact pairs, only 0.18% are valid links. Training a neural network using such an unbalanced dataset is very challenging. A common and relatively simple approach for handling unbalanced datasets is to weight the minority class (i.e. the linked artifacts) higher than the majority class (i.e. the non-linked ones). However, in the gradient descent method, a larger loss weighting could improperly amplify the gradient update for the minority class making the training unstable and causing failure to converge. Another common way to handle unbalanced data is to downsample the majority case in order to produce a fixed and balanced training set. Based on initial experimentation we found that this approach did not yield good results because the examples of non-links used for training the network tended to randomly exclude artifact pairs that lay at the frontier at which links and non-links are differentiated. Furthermore, based on initial experimentation, we also ruled out the upsampling method because this considerably increased the size of the training set, excessively prolonging the training time.</p><p>Based on further experimentation we adopted a strategy that dynamically constructed balanced training sets using subdatasets. In each epoch, a balanced training set was constructed by including all valid links from the original training set as well as a randomly selected equal number of non-links form the training set. The selection of non-links was updated at the start of each epoch. This approach ensured that over time </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Selection and Hyper-Parameters Optimization</head><p>Finding suitable network settings and a good set of hyperparameters is crucial to the success of applying deep learning methods to practical problems <ref type="bibr" target="#b53">[54]</ref>. However, given the running time required for training, the search space of all the possible combinations of different configurations was too large to provide full coverage. We therefore first identified several configurations that were expected to produce good performance. This was accomplished by manually observing how training loss changed during early epochs and following heuristics suggested in <ref type="bibr" target="#b7">[8]</ref>. We then created a network configuration search space centered around these manually identified configurations; our search space is summarized in table I. We conducted a grid search and trained all the combinations of each configuration in Table I using the training set and then compared their performance on the development set to find the best configuration. We describe our search space below.</p><p>For learning the word embeddings, we used the Skip-gram model provided by the Word2vec tool <ref type="bibr" target="#b47">[48]</ref>. We trained the word vectors with two settings: 50-dimension vectors using the PTC corpus only and 300-dimension using both PTC and Wikipedia dump. The number of dimensions are set differently because the PTC corpus (38,771 tokens) contains considerably less tokens than the PTC + Wikipedia dump (8,025,288 tokens). While a smaller vector dimension would result in faster training, a larger dimension is needed to effectively represent the semantics of all tokens in the latter corpus.</p><p>To compare which variation of RNN best suits the tracing network, we evaluated GRU, LSTM, bi-directional GRU (BI-GRU), bi-directional LSTM (BI-LSTM) with both 1 and 2 layers introduced in Section II-C. The hidden dimensions in each RNN unit were set to either 30 or 60, while the hidden dimensions for the Integration layer were set to 10 or 20 correspondingly. As a baseline method, we also replaced the RNN layers with a bag-of-word method in which the semantic vector of an artifact is simply set to be the average of all word vectors contained in the artifact ("AveVect" in Table <ref type="table" target="#tab_0">I</ref>). We also summarize the search space for other hyper-parameters of the tracing network in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of Tracing Methods</head><p>In practical requirements tracing settings, a tracing method returns a list of candidate links between a source artifact, serving the role of user query, and a set of target artifacts. An effective algorithm would return all valid links close to the top of the list. The effectiveness of a tracing algorithm is therefore often measured using Mean Average Precision (MAP). To calculate MAP, we first calculate the Average Precision (AP) for each individual query as:</p><formula xml:id="formula_13">AP = |Retrieved| i=1 (Precision(i) × relevant(i)) |RelevantLinks| (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where |RetrievedLinks| is the number of retrieved links, i is the rank in the sequence of retrieved candidates links, relevant(i) is a binary function assigned 1 if the link is valid and 0 otherwise, and Precision(i) is the precision computed after truncating the list immediately below i. Then, Mean Average Precision (MAP) is computed as the mean AP across all queries. In typical information retrieval settings, MAP is computed for the top N returned links; however, for traceability purposes we compute it when returning all valid links as specified in the trace matrix. This means that our version of MAP is computed for recall of 100%. We computed MAP using the test dataset only, and compared the performance of our tracing network with other popular tracing methods, i.e. Vector Space Model (VSM) and Latent Semantic Indexing (LSI). To make a fair comparison, we also optimized the configurations for the VSM and the LSI methods using a Genetic Algorithm to search through an extensive configuration space of preprocessors and parameters <ref type="bibr" target="#b42">[43]</ref>. Finally, we configured VSM to use a local Inverse Document Frequency (IDF) weighting scheme when calculating the cosine similarity <ref type="bibr" target="#b33">[34]</ref>. LSI was reduced to 75% dimensions. For both VSM and LSI we preprocessed the text to remove non alpha-numeric characters, remove stop words, and to stem each word using Porter's stemming algorithm.</p><p>We also evaluated the results by plotting a precision vs. recall curve. The graph depicts recall and precision scores at different similarity or probability values. The Precision-Recall Curve thus shows trade-offs between precision and recall and provides insights into where each method performs best -for example, whether a technique improves precision at higher or lower levels of recall <ref type="bibr" target="#b10">[11]</ref>. A curve that is farther away from the origin indicates better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Results and Discussion</head><p>In this section, we report (1) the best configurations found in our network configuration search space, (2) the performance of the tracing network with the best configuration compared against VSM and LSI, and (3) the performance of the tracing network when trained with a larger training set of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. What is the best configuration for the tracing network?</head><p>This experiment aims to address the first research question (RQ1). When optimizing the network configuration of the tracing network, we first selected the best configuration for each RNN unit type; Table <ref type="table" target="#tab_1">II</ref> summarizes these results. We found that the best configurations for all four RNN unit types were very similar: one layer RNN model with 30 hidden dimensions and an Integration layer of 10 hidden dimensions, learning rate of 1e-02, gradient clip value of 10, and λ of 1e-04. Performance varies for different RNN unit types.  <ref type="figure" target="#fig_3">4</ref> illustrates the learning curves on the training dataset of the four configurations. All four RNN unit types outperformed the Average Vector method. This supports our hypothesis that word order plays an important role when comparing the semantics of sentences. Both GRU and BI-GRU achieved faster convergence and more desirable (smaller) loss than LSTM and BI-LSTM. Although quite similar, the bidirectional models performed slightly better than the unidirectional models for both GRU and LSTM on the training set; the bidirectional models also achieved better results on the development dataset compared to their unidirectional counterparts. As a result, the overall best performance was achieved using BI-GRU configured as shown in Table <ref type="table" target="#tab_1">II</ref>. We also found that in three of the four best configurations, the word embedding vectors were trained using the PTC corpus alone. We speculate that one reason the PTC-trained word vectors performed better than the PTC+Wiki-trained vectors is due to differences in content of the two corpora. The PTC+Wiki corpus contains significantly more words that are used in diverse contexts because the majority of articles in the Wiki corpus are not related to the PTC domain. In the case of words that appear commonly in Wiki articles but convey specific meanings in the PTC domain (e.g. message, administrator, field, etc.), their context in more general articles Fig. <ref type="figure">5</ref>: Precision-Recall Curve on test set -45% total data is likely to negatively affect the reasoning task on domain specific semantics when there is insufficient training data to disambiguate their usage. We also tested the tracing network performance using 300-dimension word embedding trained by the PTC corpus. The result is similar to the best configuration found in Table <ref type="table" target="#tab_1">II</ref>. These findings suggest that using only the domain corpus to train word vectors with a reasonable size is more computationally economical and can yield better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Does the tracing network outperform leading trace retrieval algorithms?</head><p>We now evaluate whether the best configuration of the tracing network (i.e. BI-GRU with configuration shown in Table <ref type="table" target="#tab_1">II</ref>) outperforms leading trace retrieval algorithms. Links were therefore generated for each source and target artifact pair in the test set (45% total data) and for each source artifact ranked by descending probability scores. Average Precision (AP) was calculated using Equation <ref type="formula" target="#formula_13">12</ref>. We then compared the APs for our tracing network against those generated using the best performing VSM and LSI configurations. Our tracing network was able to achieve a MAP of .598; this value is 41% higher than that achieved using VSM (MAP = .423) and 32% higher than LSI (MAP = .451). We conducted a Friedman test and found a statistically significant difference among the AP values associated with the three methods (X 2 (2) = 89.40, p &lt; .001). We then conducted three pairwise Wilcoxon signed ranks tests with Bonferroni p-value adjustments. Results indicated that the APs associated with our tracing network were significantly higher (M = .598, SD = .370) than those achieved using VSM (M = .423, SD = .391; p &lt; .001) and LSI (M = .451, SD = .400; p &lt; .001); in contrast, there was no significant difference when comparing APs for VSM versus those for LSI.</p><p>When comparing the Precision-Recall Curves for the three methods (Figure <ref type="figure">5</ref>), we observed that the tracing network outperformed VSM and LSI at higher levels of recall. Given the goal to achieve close to 100% recall when performing tracing tasks, this is an important achievement. Precision improved notably when recall was above 0.2. This improvement can be attributed to the fact that the tracing network is able to extract semantic information from artifacts and to reason over associations between them.</p><p>While it is almost impossible to completely decipher how GRU extracts a semantic vector from natural lan- guage <ref type="bibr" target="#b36">[37]</ref>, observing gate behavior when GRU processes a sentence can provide some insights into how GRU performs this task so well. As an example, we examine the gate behavior when our best performing GRU processes the following artifact text: "Exceptions: If the SA FileTransferConfiguration datapoint indicates that the BOS is not configured for file transfer, the File Transfer Manager sends an 01105 FileTransferUnavailable message to the Onboard.". The gate behavior varies among dimensions in the sentence semantic vector. In Figure <ref type="figure" target="#fig_4">6</ref>, we show the levels of the reset and the update gates and the change of the output values on two of the 30 dimensions after GRU takes each word from this artifact. Unlike LSTM, the GRU does not have an explicit memory cell; however, the unit output itself can be considered as a persisting memory, whereas the output candidate in Equation 6 acts as a temporary memory. Recall that the reset gate controls how much previous output adds to the current input when determining the output candidate (i.e. temporary memory); in contrast, the update gate balances the contributions of this temporary memory and the previous output (i.e. persisting memory) to the actual current output. From Figure <ref type="figure" target="#fig_4">6b</ref>, we observe that for the 24th dimension, the reset gate is constantly small. This means that the temporary memory is mostly decided by the current input word. The update gate for this dimension was also small for most of the input words until the words datapoint, transfer and to came. Those spikes indicate moments when the temporary memory was integrated into the persisting memory. As such, we can speculate that this semantic vector dimension functions to accumulate information from specific keywords across the entire sentence. Conversely, for the 12th dimension, the reset gate was constantly high, indicating that the information stored in the temporary memory was based on both previous output and current input. Therefore, the actual output is more sensitive to local context rather than a single keyword. This is confirmed by the fluctuating shape of the actual output shown in the figure. For example, the output value remained in the same range until the topic was changed from "datapoint indication" to "message transfer". We believe that this versatile behavior of the gating mechanism might enable GRU to encode complex semantic information from a sentence into a vector to support trace link generation and other challenging NLP tasks.</p><p>From Figure <ref type="figure">5</ref>, we also notice that the tracing network hits a glass ceiling for improving precision above 0.27. We consider this to be caused by its inability to rule out some false positive links that contain valid associations. For example, the tracing network assigns a 97.27% probability of a valid link between artifact "The BOS administrative toolset shall allow an authorized administrator to view internal errors generated by the BOS" and the artifact "The MessagesEvents panel provides the functionality to view message and event logs. The panel provides searching and filtering capabilities where the user can search by a number of parameters depending on the type of data the user wants to view". There are direct associations between these artifacts: MessagesEvents panel is part of the BOS administrative toolset for viewing message and event log, administrator is a system user and internal errors generated by the BOS is an event. But this is not a valid link because MessagesEvents panel only displays messages and events related to external Railroad Systems rather than to internal events. It is likely that the tracing network fails to exclude this link because it has not been exposed to sufficient similar negative examples in the training data. As we described in Section IV-A, every positive example is used while negative examples (i.e. non-links) are randomly selected during each epoch. We plan to explore more adequate methods for handling unbalanced data problems caused by characteristics of the tracing data in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. How does the tracing network react to more training data?</head><p>The number of trace links tends to increase as a software project evolves. To explore the potential impact of folding them into the training set, we increased the training dataset to 80%. We randomly selected part of the test data and moved it into the training set to reach 80%, while retaining the remaining data in the testing set. Using the same configuration as described in Section IV-B, we then retrained the tracing network. Because the size of the test set decreased to 10%, we could not make direct comparisons to our previous results. Instead we used both of the trained tracing networks (i.e.  <ref type="figure">7</ref>. With increased training data, the network can better differentiate links and non-links, and therefore improve both precision and recall. Improvements were observed especially at low levels of recall.</p><p>The performance of the tracing network trained using 80% of data was again compared against VSM and LSI for this larger training set. A Friedman test identified a statistically significant difference among the APs associated with the three methods on this new dataset division (X 2 (2) = 141.11, p &lt; .001). Using pairwise Wilcoxon signed ranks tests with Bonferroni p-value adjustments, we found that our tracing network performed significantly better (MAP = .834) than VSM (MAP = .625; p &lt; .001) and LSI (MAP = .637; p &lt; .001). As such, we address RQ2 and conclude that in general our tracing network improved trace link accuracy in comparison to both VSM and LSI, and that improvements were more marked as the size of the training set increased. We expect additional improvements by reconfiguring the tracing network for use with a larger training set <ref type="bibr" target="#b7">[8]</ref>. However, we also observed that when using the original training set (i.e. 45% of data), our tracing network only outperformed LSI at higher levels of recall as shown in Figure <ref type="figure">5</ref> and Figure <ref type="figure">7</ref>. In future work, we plan to explore the trade-offs between these two methods for specific data features, and to further improve the performance of the tracing network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Related Work</head><p>In this section we focus on prior work that has integrated ontology, semantics, or NLP into the tracing process. Researchers have attempted to improve bag-of-word approaches such as the Vector Space Model (VSM) <ref type="bibr" target="#b33">[34]</ref> by integrating matching terms, project glossaries, and other forms of thesauri <ref type="bibr" target="#b33">[34]</ref>.</p><p>Basic enhancements have included user feedback techniques such as Rocchio <ref type="bibr" target="#b33">[34]</ref> or Direct Query manipulation (DQM) <ref type="bibr" target="#b60">[61]</ref> to increase or decrease term weights. However, these approaches fail to leverage semantic information.</p><p>Other techniques identify terms for briding the term mismatch between source and target artifacts. Dietrich et al. utilized validated trace links to identify frequent item sets of terms occurring across pairs of source and target artifacts, and then used these to augment the text in the trace query <ref type="bibr" target="#b18">[19]</ref>. Gibiec et al. <ref type="bibr" target="#b23">[24]</ref> approached trace query augmentation by acquiring related documents from the Internet, and then extracting domain related terms. Researchers have also used phrase detection and chunking to search for requirements impacted by change requests <ref type="bibr" target="#b1">[2]</ref> or to improve the trace retrieval process <ref type="bibr" target="#b69">[70]</ref>. None of these techniques attempted to understand semantics of the artifacts.</p><p>Researchers have also explored the use of knowledge bases to create and utilize semantically aware associations in the trace creation process -where a knowledge base include basic domain terms and sentences that describe the relationships between those terms <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Data is typically represented as an ontology in which relationships are represented using AND, OR, implication, and negation operators <ref type="bibr" target="#b35">[36]</ref>. Traceability researchers have proposed the idea of using ontology to connect source and target artifacts <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Approaches have been proposed for weighting the evidence for a trace link according to distance between concepts in the ontology <ref type="bibr" target="#b41">[42]</ref>. Unfortunately, building domain-specific ontologies is time consuming and ontologies are generally not available for technical software engineering domains.</p><p>Finally, while researchers have proposed techniques that more closely mimic the way human analysts reason about trace links and perform tracing tasks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b45">[46]</ref>, there is very limited work in this area. Our prior work with DoCIT, described in Section I, is one exception <ref type="bibr" target="#b27">[28]</ref>. DoCIT utilizes both ontology and heuristics to reason over concepts in the domain in order to deliver accurate trace links. However, as previously explained, DoCIT requires non-trivial setup costs to build a customized ontology and heuristics for each domain, and is sensitive to flaws in syntactic parsing. In contrast, the RNN approach described in this paper requires only a corpus of domain documents and a training set of validated trace links.</p><p>On the other hand, deep learning has been successfully applied to many software engineering tasks. For example, Lam et al. combined deep neural networks with information retrieval techniques to identify buggy files in bug reports <ref type="bibr" target="#b38">[39]</ref>. Wang et al. utilized a deep belief network to extract semantic features from source code for the purpose of defect prediction <ref type="bibr" target="#b68">[69]</ref>. Raychev et al. adopted RNN and N-gram to build the language model for the task of synthesizing code completions <ref type="bibr" target="#b54">[55]</ref>. We were not able to find work that applied deep learning techniques to traceability tasks in our literature review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Threats to Validity</head><p>Two primary threats to validity potentially impact our work. First, due to the challenge of obtaining large industrial datasets including artifacts and trace links, and the time needed to experiment with different algorithms for learning word embeddings and generating trace links, our work focused on a single domain of Positive Train Control. As a result, we cannot claim generalizability. However, the PTC dataset included text taken from external regulations, and written by multiple requirements engineers, systems engineers, and developers. The threat to validity arises primarily from the possibility that characteristics of our specific dataset may have impacted results of our experiment. For example, the size of the overall dataset, the characteristics of the vocabulary used, and/or the nature of each individual artifact, may make our approach more or less effective. In the next phase of our work we will evaluate our approach on additional datasets.</p><p>Second, we cannot guarantee that the trace matrix used for evaluation is 100% correct. However, it was provided by our industrial collaborators and used throughout their project to demonstrate coverage or regulatory codes. The metrics we used (i.e. MAP, Recall, and Precision) are all accepted research standards for evaluating trace results <ref type="bibr" target="#b33">[34]</ref>. To avoid comparison against a weak baseline, we report comparisons against two standard baselines: VSM and LSI and configured them using a Genetic Algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. Conclusions</head><p>In this paper, we have proposed a neural network architecture that utilizes word embedding and RNN techniques to automatically generate trace links. The Bidirectional Recurrent Gated Unit effectively constructed semantic associations between artifacts, and delivered significantly higher MAP scores than either VSM or LSI when evaluated on our large industrial dataset. It also notably increased both precision and recall. Given an initial training set of trace links, our tracing network is fully automated and highly scalable. In future work, we will focus on improving precision of the tracing network by identifying and including more representative negative examples in the training set.</p><p>The tracing network is currently trained to process natural language text. In future work, we will investigate techniques for applying it to other types of artifacts such as source code or formatted data. Finally, given the difficulty and limitations of acquiring large corpora of data we will investigate hybrid approaches that combine human knowledge with the neural network. In summary, the findings we have presented in this paper have demonstrated that deep learning techniques can be effectively applied to the tracing process. We see this as a non-trivial advance in our goal of automating the creation of accurate trace links in industrial-strength datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Standard RNN model (left) and its unfolded architecture through time (right). The black square in the left figure indicates a one time step delay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The architecture of the tracing network. The software artifacts are first mapped into sequences of embedded word vectors and go through RNN layers to generate the semantic vectors, which are then fed into the Semantic Relation Evaluation layers to predict the probability that they are linked.</figDesc><graphic coords="4,409.60,421.96,124.59,92.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>!!Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of learning curves for RNN variants using their best configurations. GRU and BI-GRU (left) converged faster and achieved smaller loss than LSTM and BI-LSTM (right). They all outperformed the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: The reset gate and update gate behavior in GRU and their corresponding output on the 24th and 12th dimension of sentence semantic vector. The x-axis indicates the sequential input of words while the y-axis is the value of reset gate, update gate and output after taking each word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Tracing Network Configuration Search Space</figDesc><table><row><cell>Word Embedding Source</cell><cell>PTC docs -50 dim,</cell></row><row><cell></cell><cell>PTC docs + Wikipedia dump -300 dim</cell></row><row><cell>RNN Unit Type</cell><cell>GRU, LSTM, BI-GRU, BI-LSTM,</cell></row><row><cell></cell><cell>(AveVect as baseline)</cell></row><row><cell>RNN Layer</cell><cell>1, 2</cell></row><row><cell>Hidden Dimension</cell><cell>RNN30 + Intg10, RNN60 + Intg20</cell></row><row><cell>Init Learning Rate (lr)</cell><cell>1e-03, 1e-02, 1e-01</cell></row><row><cell>Gradient Clip Value (gc)</cell><cell>10, 100</cell></row><row><cell>Regularization Strength λ</cell><cell>1e-04, 1e-03</cell></row><row><cell>Mini Batch Size</cell><cell>1</cell></row><row><cell>Epoch</cell><cell>60</cell></row><row><cell cols="2">the sampled non-links used for training were representative</cell></row><row><cell cols="2">and preserved an equal contribution of links and non-links</cell></row><row><cell cols="2">during each epoch. Our initial experimental results showed</cell></row><row><cell cols="2">this technique to be effective for training our tracing network.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Best Configuration for Each RNN Unit Type -number of layers in the RNN model, D r -hidden dimension in RNN unit, D s -hidden dimension in Integration layer, lr -initial learning rate, gc -gradient clipping value, λ -regularization strengthFigure</figDesc><table><row><cell>RNN Unit</cell><cell>Dev. Loss</cell><cell>Word Emb.</cell><cell>L</cell><cell>D r</cell><cell>D s</cell><cell>lr</cell><cell>gc</cell><cell>λ</cell></row><row><cell>BI-GRU</cell><cell cols="2">.1045 PTC</cell><cell>1</cell><cell>30</cell><cell>10</cell><cell cols="2">.01 10</cell><cell>.0001</cell></row><row><cell>GRU</cell><cell cols="2">.1301 PTC</cell><cell>1</cell><cell>30</cell><cell>10</cell><cell cols="2">.01 10</cell><cell>.0001</cell></row><row><cell>BI-LSTM</cell><cell cols="2">.1434 PTC</cell><cell>1</cell><cell>30</cell><cell>10</cell><cell cols="2">.01 100</cell><cell>.0001</cell></row><row><cell>LSTM</cell><cell cols="3">.2041 PTC+Wiki 1</cell><cell>30</cell><cell>10</cell><cell>.01</cell><cell>10</cell><cell>.0001</cell></row><row><cell>L</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig.7: Precision-Recall Curve on test set -10% total data. trained with 45% and 80% of the data respectively) to generate trace links against the same small test set (sized at 10%). To reduce the effect of random data selection, we repeated this process five times and report the average results.With a larger training set, the MAP was .834 compared to .803 for the smaller training set. Results are depicted in the Precision-Recall Curve in Figure</figDesc><table><row><cell></cell><cell>&amp;</cell><cell></cell></row><row><cell></cell><cell>"&amp;</cell><cell></cell></row><row><cell>!</cell><cell>"</cell><cell>#</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. Acknowledgments</head><p>The work in this paper was partially funded by the US National Science Foundation Grant CCF-1319680.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering traceability links between code and documentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Canfora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Lucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Merlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic checking of conformance to requirement boilerplates via text chunking: An industrial case study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabetzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gnaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement</title>
		<meeting><address><addrLine>Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">October 10-11, 2013. 2013</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Change impact analysis for natural language requirements: An NLP approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabetzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goknil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd IEEE International Requirements Engineering Conference, RE 2015</title>
		<meeting><address><addrLine>Ottawa, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">August 24-28, 2015. 2015</date>
			<biblScope unit="page" from="6" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ontologybased multiperspective requirements traceability framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Assawamekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sunetnanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pluempitiwiriyawej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="522" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Software traceability with topic modeling</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">U</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd ACM/IEEE International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Program understanding and the concept assignment problem</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Biggerstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Mitbander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="72" to="82" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The relationship between recall and precision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buckland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Software traceability: trends and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gotel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Future of Software Engineering</title>
		<meeting>the on Future of Software Engineering<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-31">2014. May 31 -June 7, 2014. 2014</date>
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards more intelligent trace retrieval algorithms</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cleland-Huang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">(RAISE) Workshop on Realizing Artificial Intelligence Synergies in Software Engineering</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Achieving lightweight trustworthy traceability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, (FSE-22)</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, (FSE-22)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">November 16 -22, 2014. 2014</date>
			<biblScope unit="page" from="849" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing an artefact management system with traceability recovery features</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Lucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tortora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Maintenance</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="306" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Technique integration for requirements assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dekhtyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Holbrook</surname></persName>
		</author>
		<author>
			<persName><surname>Dekhtyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Requirements Engineering Conference (RE)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning effective query transformations for enhanced requirements trace retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013</title>
		<meeting><address><addrLine>Silicon Valley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">November 11-15, 2013. 2013</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integrating information retrieval, execution and link analysis analysis algorithms to improve feature location in software</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Revelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poshyvanyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="309" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DO-178B: Software Considerations in Airborne Systems and Equipment Certification</title>
	</analytic>
	<monogr>
		<title level="m">Federal Aviation Authority (FAA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Food and Drug Administration. Guidance for the Content of Premarket Submissions for Software Contained in Medical Devices</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards mining replacement queries for hard-to-retrieve traces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gibiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Czauderna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASE &apos;10: Proceedings of the IEEE/ACM international conference on Automated software engineering</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Gotel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Egyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grünbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dekhtyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Maletic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Traceability fundamentals. In Software and Systems Traceability</title>
		<imprint>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An analysis of the requirements traceability problem</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C Z</forename><surname>Gotel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First IEEE International Conference on Requirements Engineering, ICRE &apos;94</title>
		<meeting>the First IEEE International Conference on Requirements Engineering, ICRE &apos;94<address><addrLine>Colorado Springs, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">April 18-21, 1994. 1994</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><surname>Ontology</surname></persName>
		</author>
		<title level="m">Encyclopedia of Database Systems</title>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1963" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Foundations for an expert system in domain-specific traceability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st IEEE International Requirements Engineering Conference</title>
		<meeting><address><addrLine>RE; Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>Rio de Janeiro-RJ</publisher>
			<date type="published" when="2013-07-15">2013. July 15-19, 2013. 2013</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards an intelligent domain-specific traceability solution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Monaikul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM/IEEE international conference on Automated software engineering</title>
		<meeting>the 29th ACM/IEEE international conference on Automated software engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="755" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cold-start software analytics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vierhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Mining Software Repositories, MSR 2016</title>
		<meeting>the 13th International Conference on Mining Software Repositories, MSR 2016<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">May 14-22, 2016. 2016</date>
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sentence-to-code traceability recovery with domain ontologies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APSEC</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Thu</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Neural Networks: A Comprehensive Foundation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Prentice Hall PTR</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Advancing candidate link generation for requirements tracing: The study of methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Huffman</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dekhtyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M B</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Jackson</surname></persName>
		</author>
		<title level="m">Introduction To Expert Systems</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>3 ed.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1506.02078</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Safety critical systems: challenges and directions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conf. on Software Engineering, ICSE</title>
		<meeting><address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05-25">2002, 19-25 May 2002. 2002</date>
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Combining deep learning with information retrieval to localize buggy files for bug reports (n)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="476" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ontology-based trace retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Traceability in Emerging Forms of Software Engineering (TEFSE2013</title>
		<meeting><address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving trace accuracy through data-driven configuration and composition of tracing features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amornborvornwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovering traceability links in software artifact management systems using information retrieval methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Lucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tortora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology (TOSEM)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Strategic traceability for safety-critical projects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A semantic relatedness approach for traceability link recovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 20th International Conference on Program Comprehension, ICPC 2012</title>
		<meeting><address><addrLine>Passau, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 11-13, 2012. 2012</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting, classifying, and tracing nonfunctional software requirements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Requir. Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1310.4546</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Enhancing candidate link generation for requirements tracing: The cluster hypothesis revisited</title>
		<author>
			<persName><forename type="first">N</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 20th IEEE International Requirements Engineering Conference (RE)</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 24-28, 2012. 2012</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6026</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A high-throughput screening approach to discovering good forms of biologically inspired visual representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1000579</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Code completion with statistical language models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Traceability gap analysis for assessing the conformance of software traceability to relevant guidelines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering &amp; Management 2015, Multikonferenz der GI-Fachbereiche Softwaretechnik (SWT) und Wirtschaftsinformatik (WI)</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="120" to="121" />
		</imprint>
	</monogr>
	<note>FA WI-MAW</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>CoRR, abs/1509.06664</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning complex, extended sequences using the principle of history compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A comparative evaluation of two user feedback techniques for requirements trace retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1069" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ontology matching: State of the art and future challenges</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shvaiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Euzenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="158" to="176" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rule-based generation of requirements traceability relations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Spanoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pérez-Miñana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="127" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Application of swarm techniques to requirements tracing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sultanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Huffman</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Requirements Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="https://www.fra.dot.gov/Page/P0358" />
		<title level="m">Federal Railroad Administration,PTC System Information</title>
		<imprint>
			<publisher>US Department of Railroads</publisher>
			<biblScope unit="page" from="2016" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatically learning semantic features for defect prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Software Engineering</title>
		<meeting>the 38th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="297" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improving automated requirements trace retrieval: a study of term-based enhancement methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Settimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="146" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
