<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Models Can Teach Themselves to Program Better</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-29">29 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Haluptzok</surname></persName>
							<email>haluptzok@live.com</email>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Bowers</surname></persName>
							<email>mlbowers@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><forename type="middle">Tauman</forename><surname>Kalai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Microsoft Research Preprint. Under review</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Models Can Teach Themselves to Program Better</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-29">29 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.14502v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work shows how one can use large-scale language models (LMs) to synthesize programming problems with verified solutions, in the form of "programming puzzles," which can then in turn be used to fine-tune those same models, improving their performance. This work builds on two recent developments. First, LMs have achieved breakthroughs in non-trivial reasoning and algorithm implementation, generating code that can solve some intermediate-level competitive programming problems [e.g., 6, 15, 7]. However, training code LMs involves curated sets of natural-language problem descriptions and source-code tests and solutions, which are limited in size. Second, a new format of programming challenge called a programming puzzle was introduced, which does not require a natural-language description and is directly specified by a source-code test <ref type="bibr" target="#b17">[18]</ref>. In this work we show how generating synthetic programming puzzles and solutions, verified for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles. Additionally, we release a dataset of 1 million puzzles and solutions generated by the Codex model, which we show can improve smaller models through fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref> show that transformer-based language models (LMs), pre-trained on massive corpora of text and code, can complete programming tasks from English instructions. At a high level they share a common structure: they all start with multi-billion-parameter LMs pre-trained on multi-gigabyte corpora including source code (e.g., GitHub), they all curate smaller "problem sets" of standalone programming problems, and they all find that pre-trained LMs can generate code to solve some fraction of held-out test problems. Each problem is defined by a precise English description, as well as test cases to further reduce ambiguity and facilitate correctness evaluation. The problem sets draw from programming competitions and other sources of standalone tests. Each problem can be solved by a self-contained function g(x). Fine-tuning the pre-trained models on training problems from these problem sets further improves performance on test problems.</p><p>A key challenge to improving these data hungry LMs is procuring large quantities of high quality training programming problems and matching solutions. Programming problem datasets are expensive to author and curate, and thus limited in size. Manually generating problem descriptions in a natural language such as English, with correct code solutions and comprehensive test cases, is time consuming. While significant efforts have been made to harvest the existing competitive programming websites and GitHub repositories for standalone programming problems, the number of human authored problem descriptions with matching code solutions is limited. Second, one may consider augmenting these datasets with synthetically generated problems but evaluating the quality and correctness of the English language problem descriptions would be difficult. Many problem descriptions wouldn't assert f(g())</p><p>Figure <ref type="figure">1</ref>: Illustrative puzzles and solutions that were synthesized by the Codex language model: the first is a simple equation; the second requires finding a palindrome (string same forwards and backwards) with exactly n copies of each of a given list of substrings.</p><p>make sense, and it is difficult with standard metrics like perplexity to verify which English problem descriptions correspond to a meaningful and precise problem. On top of that, a corresponding code solution needs to be synthesized for each generated English problem description and predicting whether synthesized code solves problems as described would be yet another challenge.</p><p>Programming puzzles <ref type="bibr" target="#b17">[18]</ref> represent programming problems using code only, <ref type="foot" target="#foot_0">2</ref> emphasizing problemsolving and understanding code (rather than English). A puzzle is specified by a function f (y, x) together with zero or more inputs x. The goal is to find y such that f (y, x) = True. A puzzle f is an output verifier that validates y as a solution to f on input x. The answer y = g(x) is the output of a synthesized program g. Examples of (synthetic) puzzles are given in Figure <ref type="figure">1</ref>. In order to find a solution g, a code synthesizer is given the source code of f (and x), with the goal of generating a program g such that f (g(x), x) = True. Even if the ultimate goal is solving problems in English, disentangling problem-solving from English-understanding can be useful for evaluating progress in learning to code.</p><p>The open-source P3 dataset <ref type="foot" target="#foot_1">3</ref> of Python Programming Puzzles demonstrates that programming puzzles can capture a wide range of challenges from various domains, from trivial string manipulation to longstanding open problems in algorithms and mathematics. Recursion, dynamic programming, and other fundamental programming techniques are all useful in solving the puzzles in P3. Puzzles circumvent the natural-language issues mentioned above, because the validity of puzzles and solutions can be directly verified by simply executing code. Puzzles can be used in a RL-like fashion without requiring matching human-authored solutions, by exploring the space of code problems and solutions, synthesizing problem-solution pairs and reinforcing the model with the pairs that correctly evaluate.</p><p>In our work, we use human-authored puzzles but not human-authored solutions.</p><p>Generating new puzzles and solutions. This paper investigates data generation using LMs, by generating synthetic programming puzzles with verified solutions to improve performance on solving future puzzles. While Schuster et al. <ref type="bibr" target="#b17">[18]</ref> showed how to use LMs to solve puzzles, we show how to use LMs to generate puzzles as well, selecting random subsets of P3 puzzles to create few-shot prompts to prime the LM to generate more puzzles. An LM is also used to synthesize the corresponding code solutions g for each programming problem f . The correctness of f (g(x), x) is programmatically evaluated, thus allowing for the creation of a large training dataset of verifiedcorrect puzzles and solutions to fine-tune on. This process is iterated: starting with a base language model, the same language model is used in each iteration to generate and solve puzzles, the correct solutions are filtered by a Python interpreter, and the model is fine-tuned.</p><p>We measure the utility of this process based on how well the LM performs at solving new, held-out test puzzles. Such an approach may fail for numerous reasons. The generated puzzles may be too hard and thus discarded because no solutions were found, or too easy with trivial solutions that are not instructive. The puzzles may also be too similar to one another, in which case the model may become too focused on one type of problem. Nonetheless, we hypothesized that this process would improve P3 test performance.</p><p>To test this hypothesis we synthesize four datasets of 1 million (1M) puzzle-solution pairs that are verified correct. Each dataset is synthesized using a different LM. The largest model is Codex <ref type="bibr" target="#b5">[6]</ref> which is accessed via an API. Codex is a GPT3-like transformer model <ref type="bibr" target="#b4">[5]</ref> that has been trained on a large corpus of code and a smaller corpus of standalone programming problems. The other three models we generate data from are open-source GPT-Neo 125M, 1.3B and 2.7B models <ref type="bibr" target="#b3">[4]</ref>. <ref type="foot" target="#foot_2">4</ref>P3 provides hundreds of hand-written training and test puzzles. We synthesize four datasets of 1M verified-correct synthetic puzzle-solution pairs, by running the LMs on prompts derived from random subsets of P3 training puzzles. There is no limit on the number of puzzles and solutions that could be synthesized in this manner. Since fine-tuning Codex is not yet publicly available, we instead fine-tune just the three smaller GPT-Neo models on the synthetic datasets. GPT-Neo is another GPT3-like model which has been pre-trained on the Pile <ref type="bibr" target="#b8">[9]</ref>, a dataset including publicly available natural language data and a large sample of code from GitHub repositories. Finally, the GPT-Neo models are used to solve test puzzles in a few-shot manner, following Schuster et al. <ref type="bibr" target="#b17">[18]</ref>.</p><p>As a baseline, we compare to GPT-Neo with no fine-tuning, which solves few of P3's test puzzles. GPT-Neo, once fine-tuned on any one of the four 1M verified synthetic puzzle-solution pairs, solves approximately five times as many puzzles as the baseline model, and tends to do best when trained on the 1M data generated from the largest model. We also evaluate two alternative strategies for fine-tuning GPT-Neo in our studies. The first is GPT-Neo fine-tuned on just the 155 P3 training puzzles with synthetic solutions without any additional synthesized puzzles. Second, we fine-tune GPT-Neo on a set of 1M unverified synthetic puzzle-solution pairs without correctness filtering. This second baseline enables us to evaluate the effect of automatic correctness filtering. Both alternatives performed significantly worse than fine-tuning on verified synthetic puzzle-solution pairs.</p><p>There are two main contributions of this work. First is demonstrating how one can use LMs, together with a Python interpreter, to generate verified programming challenges which can then be used to further improve LMs at solving such challenges. (Fine-tuning each of the three models on puzzles generated by that same model at least doubles all pass rates.) Second, we release a dataset of 1M puzzles generated by the Codex LM which we demonstrate is useful for fine-tuning smaller models. The puzzles generated by Codex and code used in this project will be available under an MIT license at https://GitHub.com/Microsoft/PythonProgrammingPuzzles. This work complements the growing body of work on extracting standalone tests from human-generated code.</p><p>The paper is organized as follows. Section 2 describes our approach for generating puzzles and verified solutions, and for using these to improve an LM's ability to solve puzzles. Section 3 presents the evaluation of our approach. Section 4 discusses related work. Finally, we conclude and discuss future work. The Appendix gives further implementation details, and also provide comparisons between synthesized and human-generated puzzles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Pipeline</head><p>We first present an overview of the data generation, fine-tuning, and evaluation process. A high-level view of our data generation pipeline can be seen in Figure <ref type="figure" target="#fig_1">2</ref>. While we did not have access to fine-tuning for the Codex model, interestingly, we did find its puzzles useful for improving the smaller GPT-Neo models. We also used the puzzles generated by Codex as an opportunity to perform an ablation study to understand the importance of using the interpreter to filter puzzles for correctness, as is summarized in Figure <ref type="figure" target="#fig_2">4</ref>. We want to emphasize that no human hand-written solutions are used for fine-tuning or evaluation, other than the five illustrative examples used in the prompt for few-shot learning to solve puzzles. Those specific examples are shown in the Appendix.  Since the API access to Codex doesn't allow for fine-tuning, Codex stopped at step 3 in the first loop after generating 1 million verified puzzles and solutions. GPT-Neo 125M, 1.3B and 2.7B support fine-tuning and we passed through the loop 2 times for the GPT-Neo models. The first loop produced 25K unique puzzle/solution samples from each model to finetune on, a bootstrapping step which greatly sped up the data generation rate in the second loop using the finetuned models. In the second loop we generated 1 million unique puzzle/solution samples from each model to finetune on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The data generation pipeline can be run for multiple iterations to improve the language model as follows. In each iteration, the same language model is used for both generation and fine-tuning. Each iteration consists of two steps: (a) generating puzzles with verified solutions, and (b) fine-tuning on the verified solutions. To generate puzzles, we create prompts composed of puzzles sampled from the 155 training puzzles of the P3 dataset. Each prompt consists of a series of puzzles one after another (see Fig. <ref type="figure">3</ref>). The prompts were fed to the language model in the few-shot learning approach to generate additional puzzles. For each synthetic generated puzzle, we also synthesize solutions.</p><p>Solving was performed by prompting the language model with a few-shot prompt demonstrating several problem-solution pairs. Finally, we verified the generated solutions for correctness through execution in a Python interpreter, discarding any incorrect solutions. Further details are in Section 2.2. The other step in each iteration is fine-tuning the LM on the verified synthetic data (Section 2.3). These two steps are alternated until some budget is reached. We evaluated the fine-tuned model on the held-out test split of puzzles provided by P3 (Section 2.4). To solve puzzles, at all stages we use the same prompt as in Schuster et al. <ref type="bibr" target="#b17">[18]</ref> but vary the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating Puzzles and Solutions</head><p>This section describes how the puzzles and solutions were generated in each iteration. Further details of the process appear in the Appendix. At the time of writing, the P3 repository contained 397 programming puzzles with a training and test split, as well as Codex-generated solutions to those puzzles that were solved based on 1,000 attempts.</p><p>In order to generate puzzles, we created a simple prompt which consisted of a sample of training puzzles as large as possible for the LM (for Codex specifically given the API limit, it was a median of 43 puzzles) as illustrated in Figure <ref type="figure">3</ref>. We then applied filtering, eliminating duplicate puzzles, puzzles with an invalid argument type-hint <ref type="foot" target="#foot_3">5</ref> , puzzles which did not parse in Python, and puzzles which had a "trivial" solution. For example, if a puzzle took an int solution, we tested to ensure that it did not have a solution in {-10, -9, . . . , 100}. In total, approximately half of the generated puzzles were eliminated during this pre-filtering process.</p><p>We then used the LM to attempt to solve each of these puzzles, in turn, using the same few-shot tutorial prompt used in P3, which consists of five sample puzzles and solutions-these same five samples were given to humans in their P3 user study. For each puzzle, 128 candidate solutions were generated by the GPT-Neo and Codex model. Each of these solutions was judged as correct or incorrect based on whether it solved the generated puzzle, using a one-second timeout as in the P3 evaluation. We then take the solutions judged to be correct, with up to a maximum of 8 distinct solutions per puzzle, taking the shortest 8 for the puzzles that had more than 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fine-tuning</head><p>Each of the 3 GPT-Neo model sizes was fine-tuned for 1 epoch using each of the 4 different 1M length synthetic verified datasets generated, yielding 12 fine-tuning runs with those results shown in Figure <ref type="figure">6</ref>. The format of the fine-tuning data mirrors that of the few-shot solving prompt shown in Figure <ref type="figure">1</ref>.</p><p>A breakdown of the datasets used for fine-tuning is given in Table <ref type="table" target="#tab_2">2</ref>. The BASELINE is included for comparison and measures the model without any fine-tuning. The HUMAN dataset consists of correct solutions to 95 puzzles out of the 155 puzzle P3 training. These solutions were generated by a Codex model, as in the original work on Programming Puzzles <ref type="bibr" target="#b17">[18]</ref>, and verified in the same fashion as described above (Section 2.2) for solving the synthetic puzzles. From Codex only, the UNVERIFIED dataset was created for an ablation study by the procedure in Section 2.2 but all Codex-proposed solutions to the synthetic puzzles were used, without discarding the problem-solution pairs that were not correct. Note that the solutions were longer on average in the UNVERIFIED dataset indicating that correct solutions tended to be shorter. The VERIFIED datasets consist of four datasets, one for each model used for generating the data, and each has 1M puzzles that were generated from the 155 puzzle train set, along with their generated solutions that passed verification as detailed in Section 2.2. In each case, the puzzle-solution pairs were shuffled (keeping each solution following its corresponding puzzle) and concatenated into a single corpus for fine-tuning.</p><p>The experiments on fine-tuning LMs to improve their performance on solving programming puzzles were done with the GPT-Neo model <ref type="bibr" target="#b3">[4]</ref>, as described in the introduction. GPT-Neo is publicly available pre-trained in 3 different sizes of 125 million, 1.3 billion, and 2.9 billion parameters. These pre-trained models were used as the baseline models for these experiments, and fine-tuning was done starting from these models. Experiments were run across all 3 model sizes, yielding qualitatively similar results across the different model sizes. In Figure <ref type="figure">7</ref> we compare different amounts of finetuning, and in all other cases models were fine-tuned for one epoch. learning rate of 5 ? 10 -6 with the AdamW variant <ref type="bibr" target="#b15">[16]</ref> of the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with ? 1 = .9 and ? 2 = .99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Solving test puzzles</head><p>The same few-shot approach used for solving synthetic puzzles was used to solve test puzzles using GPT-Neo models, as described in Section 2.2. We did not measure solving test puzzles using Codex since we could not fine-tune it. The same tutorial prompt used in P3 was also used, as detailed in the Appendix, and a 1-second timeout for evaluation. The models used to solve test puzzles were the fine-tuned models discussed above. It is also worth noting that, while the bulk of the time was spent generating synthetic puzzles and fine-tuning the models, this is a one-time cost where the model could be used to solve many future puzzles, more than we presently have in our test set. Hence, amortizing the cost, generating synthetic puzzles would be less computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental evaluations</head><p>For solving test puzzles, GPT-Neo was given 100 attempts to generate a solution at temperature of 0.8 as used by Chen et al. <ref type="bibr" target="#b5">[6]</ref> using the few-shot prompt as described in 2.2. Temperature sweeps for the GPT-Neo models solving the Codex puzzles was performed as shown in Figure <ref type="figure">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pass@K solving metric</head><p>Consistent with prior work, results are presented using the Pass@K metric <ref type="bibr" target="#b5">[6]</ref>. In generating the 100 solutions for each of the 228 problems, the index of the first correct solution obtained for each problem is recorded. Pass@K indicates how many problems had a correct solution generated within the first K solutions. Higher values for K result in solving more problems, as shown in Figure <ref type="figure">5</ref>. A refined N@K metric was introduced in Li et al. <ref type="bibr" target="#b14">[15]</ref>, where K solutions are generated and N ? K of them must be chosen for "submission." This metric is inspired by settings such as competitive programming where a system must choose to submit a limited number of candidate solutions for evaluation. Thus in general, Pass@K is equivalent to K@K, and AlphaCode used 10@K for large values of K (e.g., millions). For puzzles, since at most one submission is need, Pass@K is in fact equivalent to the 1@K metric, because no solver need ever submit an incorrect solution since the puzzle verifies if the solution is correct. In some competitions such as those on the popular codeforces.com website, there is a cost for incorrect submissions. For puzzles, one would never incur such a cost. Also, we expect pass@1 performance to improve at lower temperatures, but a single temperature was used to conserve resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We measured how successfully GPT-Neo could solve the 228 test programming puzzles in the fewshot regime using the Pass@K metric. The first experiment involved fine-tuning GPT-Neo on the small HUMAN dataset described in Section 2.3, which was constructed from 635 Codex-synthesized solutions to the 155 human handwritten puzzles in the training set. Fine-tuning on that small dataset One might expect that fine-tuned models could learn in a zero-shot manner. We tested this hypothesis, but, as seen in Figure <ref type="figure">7</ref>, the fine-tuned models benefit from few-shot learning. Even after extensive fine-tuning on the puzzle problem format for over 1 billion tokens, the LM still performed better when prompted with the five examples of puzzles/solutions to prime the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge-distillation ablation study</head><p>When Codex-generated puzzles are used to fine-tune a GPT-Neo model, the smaller model may be learning both from the larger Codex model (a form of what is called knowledge distillation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref>) as well as from the interpreter which filters puzzles for correctness (which might be called interpreter distillation). This presented an opportunity for an ablation study to disentangle the effects of the two. We compared the GPT-Neo baseline models to ones fine-tuned on the UNVERIFIED-CODEX dataset versus those fine-tuned on VERIFIED-CODEX dataset. This experiment is summarized in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>As shown in Figure <ref type="figure">5</ref>, fine-tuning on the unverified data improved the Pass@K performance across all models, and verified data gave an additional considerable boost to performance. To facilitate evaluation, many related datasets of programming problems have been curated, including especially relevant standalone programming challenges described in English and code <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>. Schuster et al. <ref type="bibr" target="#b17">[18]</ref> and similarly Li et al. <ref type="bibr" target="#b14">[15]</ref> make an important distinction between two types of programming problems: those that only involve translation and those that require problem-solving. Generally, data generated from larger models helps more than the data from the smaller models, as larger models appear able to distill more knowledge to the smaller models through the verified fine-tuning data set they generate.</p><p>Translation problems, such as "Add up all the odd numbers in array x," require the LM to translate a procedure from natural language to code. Problem-solving is required when the description does not state how to solve the problem. For example, "Find a path of length at most 17 between nodes 1 and 2 in graph x" conveys the problem to solve but not how to go about finding a path. Puzzles focus on problem-solving rather than translation.</p><p>In knowledge distillation <ref type="bibr" target="#b12">[13]</ref> a student model is trained to imitate the behavior of a teacher model on some data, and in the data-free paradigm the training data itself is synthetically generated. Related work on knowledge distillation can be found in the survey of Gou et al. <ref type="bibr" target="#b9">[10]</ref>. Recent work in commonsense knowledge graphs by West et al. <ref type="bibr" target="#b19">[20]</ref> has explored filtering language model outputs for quality during knowledge distillation using a neural filter. This shares the filtering aspect of our work, but given the ambiguity of their natural language task they can't evaluate correctness directly, unlike in the programming puzzle paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Impact and Limitations</head><p>The automation of writing code may enable software engineers to be more productive and produce higher value products for society. However, increasing software engineer's productivity does risk impacting the total number of software engineers needed, so if substantial gains are made, care would need to be taken when releasing it. Also, automated software development has serious risks if bugs (e.g., security holes) that are common in the code samples used for training the LM will be reproduced in the LM's output.</p><p>The approach presented here focuses on teaching the model to solve a problem described in code.</p><p>Although many natural language problems can be described as a programming puzzle that verifies a solution, other problems are not translatable into code. Also training exclusively on Programming Puzzles would likely hurt the model's ability to understand natural language. All experiments in our paper were done with a fixed temperature of 0.8, based on the recommendation for Pass@100 in Chen et al. <ref type="bibr" target="#b5">[6]</ref>. A hyper-parameter sweep on temperature across all 3 model sizes verified that 0.8 was also optimal for our model and dataset at a 0.2 search step size for maximizing Pass@100 which is the percentage of problems solved at least once with 100 generated code solutions per problem. GPT-Neo was fine-tuned for 1 epoch (?92 million tokens) in these graphs.</p><p>While we do not have access to the data that these models were trained on, given their massive sizes it is possible that they include some Personally Identifiable Information. Despite care taken in their curation, it is also almost certain that they contain offensive content. One symptom of this is the fact that source code of the puzzles we generate contains occasional expletives, not present in P3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper describes a new method for generating programming problems and verified solutions automatically without the need for human-authored problems and solutions. We use the programming puzzle paradigm, which eliminates the ambiguity in judging whether a solution to a synthetically generated English problem description is correct, as puzzle solutions can be verified with a Python interpreter. We show that fine-tuning language models on synthetic problems and solutions which have been verified for correctness enhances the ability of the model to solve new problems. We find that fine-tuning the LM on verified generated data is superior to using unverified generated data, indicating that the interpreter is providing a valuable signal. We also find that the small human-authored data set was inadequate to improve performance over the baseline LM.</p><p>Of primary interest is the improved performance of each of the three GPT-Neo models on data that was generated by that model itself, with the aid of a Python interpreter filtering for correctness (at least doubling all pass rates). This demonstrates the feasibility of self-improvement in a bootstrapping manner through puzzle generation and verification. We also find that the best results for fine-tuning a specific model are achieved by generating data with the largest LM available, and fine-tuning the smaller model with that generated data, similar to results in distillation. We also analyze the importance of the few-shot prompt in puzzle solving which yields a significant performance boost even after extensive fine-tuning. In future work, it would be interesting to design more advanced RL pipelines for self improvement by generating and selecting more "instructive" puzzles. It would also be interesting to see how much larger models, such as Codex and AlphaCode, are improved with this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Details of puzzle generation and solving</p><p>Figure <ref type="figure" target="#fig_5">9</ref> shows the prompt used to solve puzzles: the same prompt used (a) in P3 to solve the training puzzles, (b) to solve the generated puzzles, and (c) to solve the test puzzles. It is worth noting that fewer than 1% of puzzles were duplicates. The fixed temperature of 0.9 from prior work <ref type="bibr" target="#b17">[18]</ref> was used in all puzzle-solving for generating fine-tuning data, where temperature of 0.8 was used for testing the fine-tuned model per Chen et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>In solving puzzles, both synthetic puzzles and P3 puzzles, we use the same judging code from the P3 repository. <ref type="foot" target="#foot_4">6</ref> Their evaluation identifies syntax errors and aborts infinite loops using timeouts.</p><p>Their judge prevents some malicious instructions from being executed by automated code checks, though other judging systems perform full sand-boxing of the computation to prevent a generated code sample from doing harm like deleting files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Further examples of generated puzzles</head><p>A hand examination was performed on a subset of the generated puzzles, where we attempted to understand how the puzzles may originate. We found several concepts repeated from the training, other human concepts such as days of the week, and other puzzles that appear to be derived from programming challenges on the web. We found many human concepts misused, such as the perimeter of a triangle being confused with its side. Additionally input variables were sometimes unused, or puzzles did not test what they appeared like they should test because of certain issues they contained. Finally, comments were sometimes generated of varying quality.</p><p>For several puzzles, we attempted to delve deeper to understand the origin for the puzzle. For instance, f2 from Figure <ref type="figure">1</ref>   can be solved by a simple loop. The P3 dataset does contain a discrete log problem but it is in the test set. While we could not find the exact code above, the problem itself does appear to be equivalent to the English challenge stated on this programming challenge website: https: //adventofcode.com/2015/day/25. It is still unclear how exactly the system generated this code.</p><p>The following puzzle asks for a list of triangles of perimeter 5, but uses the variable name side, suggesting that it may not understand the difference between perimeter and side. The puzzle has an additional constraint which is clearly poor programming as it refers to undefined variables a1 and a2. Consequently, solving this requires finding a list of a single triangle of perimeter 5, such as Several puzzles included concepts (like vowels) and specific strings (like the famous pangram below) that appeared in the training data.</p><p>def f(w: str, z="The quick brown fox jumps over the lazy dog", n=2): return w.count("a") + w.count("e") + w.count("i") + w.count("o") + w.count("u") == n and w in z and w != z Many puzzles were not particularly interesting such as the two below, which involve finding a string of a given length containing a given substring, and finding a list of 21 numbers between 1-9 that sum to 100. Other puzzles involved very human-like strings:</p><p>def f(m: str): assert m.startswith("Hello, Salif") assert "But, but..." in m assert m.endswith("You're great!") return len(m) == 282 def g():</p><p>return "Hello, Salif. But, but... If a friend ever said hello to me, I wonder where are you from? A freaky fellow? Are you from a freaky galaxy?" + \ " or are you from a freaky universe or a freaky planet? The answer is no: I'm megalomaniac!" + \ " I know because I don't translate meaning. You're great!"</p><p>Other puzzles involved human concepts such as the day of week which did not appear in the training data:</p><p>def f(days: List[str], x="tue", k=3, n=4): nums = {"mon": 0, "tue": 1, "wed": 2, "thu": 3, "fri": 4, "sat": 5, "sun": 6} numx = nums The comments that are generated are sometimes useful and sometimes incorrect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>x: str, chars=['Hello', 'there', 'you!'], n=4600): return x == x[::-1] and all([x.count(c) == n for c in chars]) def g(chars=['Hello', 'there', 'you!'], n=4600): s = "".join([c*n for c in chars]) return s + s[::-1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Data Generation Pipeline, used to iteratively generate data and fine-tune the LMs. The pipeline was run on 4 different large language models: Codex and GPT-Neo 125M, 1.3B and 2.7B. Since the API access to Codex doesn't allow for fine-tuning, Codex stopped at step 3 in the first loop after generating 1 million verified puzzles and solutions. GPT-Neo 125M, 1.3B and 2.7B support fine-tuning and we passed through the loop 2 times for the GPT-Neo models. The first loop produced 25K unique puzzle/solution samples from each model to finetune on, a bootstrapping step which greatly sped up the data generation rate in the second loop using the finetuned models. In the second loop we generated 1 million unique puzzle/solution samples from each model to finetune on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of the Codex ablation experiment and results. Generating and fine-tuning on verified synthetic puzzles and solutions, is shown in green, while using unverified puzzles is shown in red. The GPT-Neo baseline is shown in yellow. All performance results are from the 2.7B model after one epoch of fine-tuning.</figDesc><graphic url="image-1.png" coords="7,127.80,72.00,356.40,186.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure5: Pass@K for the three fine-tuned models. Pass@K corresponds to the number of puzzle problems that had at least one correct solution generated in K attempts. On the test set increasing the number of K solutions generated by GPT-Neo results in an increase in Pass@K. The graph shows how fine-tuning on verified Codex generated data dominates using unverified Codex generated data, and that fine-tuning significantly outperforms the baseline model. Based on these results all other experiments were done using verified data</figDesc><graphic url="image-6.png" coords="8,242.64,252.26,130.53,84.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Few-shot vs. zero-shot and fine-tuning epochs. Across all 3 model sizes, testing GPT-Neo in few-shot beats zero-shot significantly even after 11 epochs of fine-tuning which is over 1 billion tokens of Codex-generated puzzle-problem/solution pairs. The LM still benefits from providing the P3 tutorial puzzle prompt.</figDesc><graphic url="image-12.png" coords="9,242.64,232.47,130.52,77.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example of the prompt used for solving puzzles, identical to the "medium prompt" of P3 [18, Figure C.3]. The first five example puzzles f1-f5 are always the same. The puzzle to be solved is also provided in the prompt as f6, and the solution function signature is provided as g6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>[[ 2</head><label>2</label><figDesc>,2,1]. def f(ls: List[List[int]], a=24, b=16, c=24, target=None, side=5): for a, b, c in ls: assert a &lt;= side and b &lt;= side and c &lt;= side and a + b + c == side, "Invalid triangle" if not target: target = ls[-1] def legal_move(m): (a, b, c), (i, j, k) = m return ((a == side or a == b + c) and a == a1 and a != a2) or a == a2 and a != a1 and a != b + c a1, a2, a3 = target moves = list(zip(ls, ls[1:])) return all(legal_move(m) for m in moves) return [[a,b,c] for a in range(side+1) for b in range(side-a+1) for c in range(side-a-b+1) if a + b + c == side and (a == side or a == b + c)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>def f(s: str, t="rome", length=14): return len(s) == length == len(set(s.upper())) and t.upper() in s.upper() def f(li: List[int]): return len(li) == 21 and all(i in li for i in range(1, 10)) and sum(li) == 100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>[x] return (len(set(days)) &lt;= k and (n -len(set(days))) * n &gt;= n * (1 + (n -1) // k) and numx &lt;= n // 2 and numx != nums[days[n // 2]] and numx &gt; nums[days[0]] and numx &lt; nums[ days[-1]]) # right half of week is weekdays days[:n//2] # left half of week is weekends</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Figure 3: An example prompt for generating puzzles. For each request for a prompt completion, the LM would generate a new puzzle. Statistics from the training and test set used from the P3 dataset.</figDesc><table><row><cell cols="4">def f(inds: List[int], li=[42, 18, 21, 103, 2, 11], target=[2, 21, 42]):</cell></row><row><cell>i, j, k = inds</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">return li[i:j:k] == target</cell><cell></cell><cell></cell></row><row><cell cols="3">def f(path: List[List[int]], m=8, n=8, target=35):</cell><cell></cell></row><row><cell cols="2">def legal_move(m):</cell><cell></cell><cell></cell></row><row><cell cols="2">(a, b), (i, j) = m</cell><cell></cell><cell></cell></row><row><cell cols="3">return {abs(i -a), abs(j -b)} == {1, 2}</cell><cell></cell></row><row><cell>. . .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>def f(</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Name Puzzle source Num. puzzles Num. human solutions</cell></row><row><cell cols="2">Train P3</cell><cell>155</cell><cell>0</cell></row><row><cell>Test</cell><cell>P3</cell><cell>228</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Fine-tuning was done at a Statistics for the datasets used for fine-tuning in our experiments. The same puzzle may be repeated in a single dataset with multiple (fewer than 8) distinct solutions. Pass@100 is shown for 1 epoch of fine-tuning of the GPT-Neo2.7B model on the dataset.</figDesc><table><row><cell>Fine-tune dataset</cell><cell cols="2">Verified Puzzles</cell><cell cols="3">Solutions (Count) # Tokens pass@100</cell></row><row><cell>BASELINE</cell><cell>N/A</cell><cell cols="2">No puzzles No solutions (0)</cell><cell>0</cell><cell>7.5%</cell></row><row><cell>HUMAN</cell><cell>Yes</cell><cell>Human</cell><cell>Synthetic (635)</cell><cell>74K</cell><cell>7.5%</cell></row><row><cell>VERIFIED-125M</cell><cell>Yes</cell><cell>Synthetic</cell><cell>Synthetic (1M)</cell><cell>74M</cell><cell>15.4%</cell></row><row><cell>VERIFIED-1.3B</cell><cell>Yes</cell><cell>Synthetic</cell><cell>Synthetic (1M)</cell><cell>65M</cell><cell>18.9%</cell></row><row><cell>VERIFIED-2.7B</cell><cell>Yes</cell><cell>Synthetic</cell><cell>Synthetic (1M)</cell><cell>66M</cell><cell>20.6%</cell></row><row><cell cols="2">UNVERIFIED-CODEX No</cell><cell>Synthetic</cell><cell>Synthetic (1M)</cell><cell>113M</cell><cell>21.5%</cell></row><row><cell>VERIFIED-CODEX</cell><cell>Yes</cell><cell>Synthetic</cell><cell>Synthetic (1M)</cell><cell>98M</cell><cell>38.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This problem requires computing a discrete log. While the discrete log problem is notoriously difficult and is the basis of numerous cryptography systems, the number is small enough that it</figDesc><table><row><cell>from typing import List</cell></row><row><cell>def f1(s: str):</cell></row><row><cell>return "Hello " + s == "Hello world"</cell></row><row><cell>def g1():</cell></row><row><cell>return "world"</cell></row><row><cell>assert f1(g1())</cell></row><row><cell>def f2(s: str):</cell></row><row><cell>return "Hello " + s[::-1] == "Hello world"</cell></row><row><cell>def g2():</cell></row><row><cell>return "world"[::-1]</cell></row><row><cell>assert f2(g2())</cell></row><row><cell>def f3(x: List[int]):</cell></row><row><cell>return len(x) == 2 and sum(x) == 3</cell></row><row><cell>def g3():</cell></row><row><cell>return [1, 2]</cell></row><row><cell>assert f3(g3())</cell></row><row><cell>def f4(s: List[str]):</cell></row><row><cell>return len(set(s)) == 1000 and all((x.count("a") &gt; x.count("b")) and ('b' in x)</cell></row><row><cell>Both involve testing palindromes and substrings. for x in s)</cell></row><row><cell>More surprisingly, the following sophisticated problem was generated: def g4():</cell></row><row><cell>def f(n: int, target=20151120): return ["a"*(i+2)+"b" for i in range(1000)]</cell></row><row><cell>assert 0 &lt;= n &lt;= 1e14 next = lambda x: (x * 252533) % 33554393 assert f4(g4())</cell></row><row><cell>seen = set() def f5(n: int):</cell></row><row><cell>now = 20151120 return str(n * n).startswith("123456789")</cell></row><row><cell>while now not in seen:</cell></row><row><cell>def g5(): seen.add(now) now = next(now) return int(int("123456789" + "0"*9) ** 0.5) + 1</cell></row><row><cell>if now == target: return n == 0 assert f5(g5())</cell></row><row><cell>n -= 1 def f6(inds: List[int], string="Sssuubbstrissiingg"): return False next = lambda x: (x * 252533) % 33554393 return inds == sorted(inds) and "".join(string[i] for i in inds) == "substring"</cell></row><row><cell>now = 20151120 n = 0 def g6(string="Sssuubbstrissiingg"):</cell></row><row><cell>while next(now) != target:</cell></row><row><cell>n += 1</cell></row><row><cell>now = next(now)</cell></row><row><cell>return n</cell></row></table><note><p>seems similar in spirit (but not identical) to several of the training problems. Here is a P3 training problem that is somewhat related: def train(s: str, substrings=['foo', 'bar', 'baz']): return all(sub in s and sub[::-1] in s for sub in substrings)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Code may contain helpful natural-language comments, but correctness is evaluated based solely on code.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://GitHub.com/Microsoft/PythonProgrammingPuzzles (MIT license)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Model numbers indicate parameter counts, and the models (MIT License) were pre-trained by EleutherAI.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>A valid puzzle has a single required argument with a type that must be a bool, float, int, str, or List[]'s thereof, nested to arbitrary depth.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We additionally set the PYTHONHASHSEED environment variable to 0 to make Python set functions deterministic.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large-scale benchmark for few-shot program induction and synthesis</title>
		<author>
			<persName><forename type="first">Ferran</forename><surname>Alet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Lopez-Contreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Lozano-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/alet21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<idno>arXiv:2108.07732 [cs.PL</idno>
		<ptr target="https://arxiv.org/abs/2108.07732" />
		<title level="m">Program Synthesis with Large Language Models</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepCoder: Learning to Write Programs</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5551208</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5551208" />
		<title level="m">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<idno>arXiv:2107.03374</idno>
		<ptr target="https://arxiv.org/abs/2107.03374" />
		<title level="m">Evaluating Large Language Models Trained on Code</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<idno>arXiv:2204.02311 [cs.CL</idno>
		<ptr target="https://arxiv.org/abs/2204.02311" />
		<title level="m">PaLM: Scaling Language Modeling with Pathways</title>
		<meeting><address><addrLine>Jeff Dean, Slav Petrov, and Noah Fiedel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DreamCoder: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Sabl?-Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Cary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1145/3453483.3454080</idno>
		<ptr target="https://doi.org/10.1145/3453483.3454080" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="835" to="850" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge Distillation: A Survey</title>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-021-01453-z</idno>
		<ptr target="https://doi.org/10.1007/s11263-021-01453-z" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program synthesis. Foundations and Trends? in Programming Languages</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="119" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Measuring Coding Challenge Competence With APPS</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09938[cs.SE]</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<ptr target="http://arxiv.org/abs/1503.02531" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Sutherland Robson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf" />
		<title level="m">Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-Level Code Generation with AlphaCode</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Machine Learning Framework for Programming by Example</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Butler W Lampson</surname></persName>
		</author>
		<author>
			<persName><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Programming Puzzles</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fe_hCc4RBrg" />
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Trabucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=ryeOSnAqYm" />
		<title level="m">Synthetic Datasets for Neural Program Synthesis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07178[cs.CL]</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">NAPS: Natural Program Synthesis Dataset</title>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Zavershynskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Skidanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03168</idno>
		<ptr target="http://arxiv.org/abs/1807.03168" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
