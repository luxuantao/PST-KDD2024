<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Synopses for Distinct-Value Estimation Under Multiset Operations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Beyer</surname></persName>
							<email>kbeyer@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
							<email>phaas@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Berthold</forename><surname>Reinwald</surname></persName>
							<email>reinwald@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannis</forename><surname>Sismanis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IBM Almaden Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
							<email>gemulla@inf.tu-dresden.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technische Universit Ã¤t Dresden San Jose</orgName>
								<address>
									<settlement>Dresden</settlement>
									<region>CA</region>
									<country>USA, Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Synopses for Distinct-Value Estimation Under Multiset Operations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AECA5A9394BF6CFD3442E2EB2EB1A091</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2 [Database Management]: Miscellaneous Algorithms</term>
					<term>theory distinct-value estimation</term>
					<term>synopsis warehouse</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of estimating the number of distinct values (DVs) in a large dataset arises in a wide variety of settings in computer science and elsewhere. We provide DV estimation techniques that are designed for use within a flexible and scalable "synopsis warehouse" architecture. In this setting, incoming data is split into partitions and a synopsis is created for each partition; each synopsis can then be used to quickly estimate the number of DVs in its corresponding partition. By combining and extending a number of results in the literature, we obtain both appropriate synopses and novel DV estimators to use in conjunction with these synopses. Our synopses can be created in parallel, and can then be easily combined to yield synopses and DV estimates for arbitrary unions, intersections or differences of partitions. Our synopses can also handle deletions of individual partition elements. We use the theory of order statistics to show that our DV estimators are unbiased, and to establish moment formulas and sharp error bounds. Based on a novel limit theorem, we can exploit results due to Cohen in order to select synopsis sizes when initially designing the warehouse. Experiments and theory indicate that our synopses and estimators lead to lower computational costs and more accurate DV estimates than previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The task of determining the number of distinct values (DVs) in a large dataset arises in a wide variety of settings in computer science, including data integration <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">8]</ref>, query optimization <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b29">29]</ref>, network monitoring <ref type="bibr" target="#b12">[12]</ref>, and OLAP <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b31">31]</ref>. The number of distinct values can be computed exactly by sorting the dataset and then executing a straightforward scan-and-count pass over the data; alternatively, a hash table can be constructed and used to compute the number of distinct values. Neither of these approaches scales well to the massive datasets often encountered in practice, because of heavy time and memory requirements. A great deal of research over the past twenty five years has therefore focused on approximate methods that scale to very large datasets. These methods work either by drawing a random sample of the data items and using the observed frequencies of the values in the sample as a basis for estimation <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> or by taking a single pass through the data and using hashing techniques to compute an estimate using a bounded amount of memory <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b33">33]</ref>.</p><p>Almost all of this work has focused on producing a given synopsis of the dataset and then using the synopsis to obtain a DV estimate; issues related to combining and exploiting synopses in the presence of set operations on multiple datasets have gone largely unexplored. Such issues are the focus of this paper, which is about DV estimation methods in the context of a "synopsis warehouse" environment as described in <ref type="bibr" target="#b5">[5]</ref>. <ref type="foot" target="#foot_0">1</ref> In a synopsis warehouse, incoming data is split into partitions, i.e., multisets of values, and a synopsis is created for each partition; the synopses are used to quickly estimate various partition properties. As partitions are rolled in and out of a full-scale warehouse, the corresponding synopses are rolled in and out of the synopsis warehouse. The architecture requires that synopses can be created in parallel, ensuring scalability, and that synopses can be combined to create a synopsis corresponding to the union, intersection, or difference of the corresponding partitions, providing flexibility. We use the term "partition" here in a very general sense. Data may be partitioned -e.g., by timestamp, by data value, and so forth -for purposes of parallel processing and dealing with fluctuating data-arrival rates. Data may also, however, be partitioned by its source -e.g., SAP customer addresses versus PeopleSoft customer addresses. In the latter scenario, comparison of data characteristics in different partitions may be of interest for purposes of metadata discovery and automated data integration <ref type="bibr" target="#b4">[4]</ref>. For example, DV estimates can be used to detect keys and duplicates in a partition, can help discover subsetinclusion and functional-dependency relationships, and can be used to approximate the Jaccard distance or other similarity metrics between the domains of two partitions <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">8]</ref>.</p><p>Our goal is therefore to provide "warehouse-ready" synopses for DV estimation, as well as corresponding DV estimators that exploit these synopses. As indicated above, it is essential that, whenever a compound data partition is created via multiset operations on a collection of input partitions, the synopsis for the compound partition can be easily obtained by combining the synopses of the input partitions. We also strive to maintain the best possible accuracy in our DV estimates, especially when the size of the synopsis is small: as discussed in the sequel, the size of the synopsis for a compound partition is limited by the size of the smallest input synopsis.</p><p>We bring together a variety of ideas from the literature -see Section 2 -to obtain a solution to our problem, resulting in bestof-breed DV estimation methods. More specifically, we propose the use of an "AKMV synopsis," extending an idea in <ref type="bibr" target="#b3">[3]</ref> in order to handle multiset operations gracefully; our extension involves adding counters to the basic synopsis, in the spirit of <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b31">31]</ref>. We then provide methods for combining AKMV synopses such that the collection of these synopses is "closed" under multiset operations on the parent partitions. The AKMV synopsis can also handle deletions of individual partition elements. Our new DV estimator is a deceptively simple modification of an estimator proposed in <ref type="bibr" target="#b3">[3]</ref>. Using a probabilistic model of hashing, we apply results from the theory of order statistics to show that our proposed estimator is unbiased and has lower mean-squared error than the "basic" DV estimator that underlies most current methods. We also derive exact moment formulas and probabilistic error bounds for the unbiased estimator, along with asymptotic approximations to these quantities that can be employed when the number of DVs is known to be large. Our asymptotic analysis rests on a new limit theorem that allows us to exploit results in <ref type="bibr" target="#b7">[7]</ref>, which were originally developed for estimating the size of a transitive closure. The asymptotic error bounds can be used to help determine appropriate synopsis sizes when designing the synopsis warehouse. We also provide a maximum-likelihood estimator that is asymptotically equivalent to our unbiased estimator as the synopsis size and number of DVs becomes large; it follows that when there are many distinct values and the synopsis size is large, the unbiased estimator has essentially the minimal possible variance of any DV estimator. We then show how our new estimator can be modified to obtain unbiased DV estimates in the presence of multiset operations.</p><p>The remainder of the paper is organized as follows. We review previous approaches to DV estimation in Section 2. In Section 3, we describe the KMV synopsis, a simpler version of our ultimate AKMV synopsis that was essentially proposed in <ref type="bibr" target="#b3">[3]</ref>, and discuss its space requirements and construction cost. We then introduce our unbiased estimator in Section 4, and give both an exact and an asymptotic analysis of the estimator's behavior. The issues involved in combining synopses are covered in Section 5. We show that the KMV synopsis can be used successfully in certain estimation scenarios, but needs to be augmented in order to deal with multiset difference operations; the resulting AKMV synopsis has the desirable closure property mentioned above. We also show how to extend our simple unbiased estimator to provide unbiased estimates in the presence of set operations, obtaining a direct unbiased estimate of the Jaccard distance in the process. We provide an empirical evaluation of our methods in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We now discuss previously-proposed synopses, DV estimators, and methods for handling compound partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Synopses for DV Estimation</head><p>In general, the literature on DV estimation does not discuss synopses explicitly, and hence does not discuss issues related to combining synopses in the presence of set operations on the corre-sponding partitions. We can, however, directly infer potential candidate synopses from the various algorithm descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Bit-Vector Synopses</head><p>The oldest class of synopses comprises various types of bit vectors. The "linear counting" technique <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b33">33]</ref> uses a bit vector V of length M = O(D), together with a hash function h : D(A) â { 1, 2, . . . , M }, where D(A) denotes the domain of the partition A of interest. The function h is applied to each element v â A, and the h(v)th bit of V is set to 1. After A has been scanned, the estimate of D, the number of distinct values in A, is the total number of 1bits in V , multiplied by a correction factor. The correction factor compensates for undercounting due to "hash collisions" in which h(v) = h(v ) for v = v ; see, for example, <ref type="bibr" target="#b2">[2]</ref>. The O(D) storage requirement for linear counting is often prohibitive in applications where D can be very large and multiple DV estimators must be maintained.</p><p>The "logarithmic counting" method of Flajolet and Martin <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b14">14]</ref> uses a bit vector of length L = O(log D). The idea is to hash each of the distinct values in A to the set { 0, 1 } L of binary strings of length L, and keep track of r, the position (counting from the left, starting at 0) of the leftmost 0 bit over all of the hashed values. The estimate is roughly of the form 2 r (multiplied by a certain factor that corrects for "bias" and hash collisions). This tracking of r is achieved by taking each hashed value, transforming the value by zeroing out all but the leftmost 1, and computing the bitwise-OR of the transformed values. The value of r is then given by the leftmost 0 bit in the resulting bit vector. In the complete algorithm, several independent values of r are, in effect, averaged together (using a technique called "stochastic averaging") and then exponentiated. Alon et al. <ref type="bibr" target="#b1">[1]</ref> analyze a variant of the logarithmic counting algorithm under an assumption of pairwise-independent hashing. Recent work by Durand and Flajolet <ref type="bibr" target="#b11">[11]</ref> improves on the storage requirement of the logarithmic counting algorithm by tracking and maintaining r, the position of the leftmost 0, directly. The number of bits needed to encode r is O(log log D), and hence the technique is called LogLog counting.</p><p>The main drawback of the above bit-vector data structures, when used as synopses in our warehouse setting, is that union is the only supported set operation. One must, e.g., resort to the inclusion/exclusion formula to handle set intersections. As the number of set operations increases, this approach becomes extremely cumbersome, expensive, and inaccurate.</p><p>Several authors <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b31">31]</ref> have proposed replacing each bit in the logarithmic-counting bit vector by an exact or approximate counter, in order to permit DV estimation in the presence of both insertions and deletions to the dataset. This modification does not ameliorate the inclusion/exclusion problem, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Random Samples</head><p>Another synopsis possibility is to use a random sample of the data items in the specified partition <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. The key drawback is that DV estimates computed from such a synopsis can be very inaccurate, especially when the data is skewed or when there are many distinct values, each having a low frequency (but not all unique); see <ref type="bibr" target="#b6">[6]</ref> for a negative result on the performance of samplebased estimators. Moreover, combining synopses to handle unions of partitions can be expensive <ref type="bibr" target="#b5">[5]</ref>, and it appears that the inclusion/exclusion formula is needed to handle intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Sample-Counting Synopsis</head><p>Another type of synopsis arises from the "sample counting" DVestimation method -also called "adaptive sampling" -credited to Wegman <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b13">13]</ref>. Here the synopsis for partition A comprises a subset of { h(v) : v â D(A) }, where h : D(A) â { 0, 1, . . . , M } is a hash function. In more detail, the synopsis comprises a fixedsize buffer that holds binary strings of length L = log(M), together with a "reference" binary string s, also of length L. The idea is to hash the distinct values in the partition, as in logarithmic counting, and insert the hashed values into a buffer that can hold up to k &gt; 0 hashed values; the buffer tracks only the distinct hash values inserted into it. When the buffer fills up, it is purged by removing all hashed values whose leftmost bit is not equal to the leftmost bit of s; this operation removes roughly half of the hashed values in the buffer. From this point on, a hashed value is inserted into the buffer if and only if the first bit matches the first bit of s. The next time the buffer fills up, a purge step (with subsequent filtering) is performed by requiring that the two leftmost bits of each hashed value in the buffer match the two leftmost bits of the reference string. This process continues until all the values in the partition have been hashed. The final DV estimate is roughly equal to K2 r , where r is the total number of purges that have occurred and K is the final number of values in the buffer.</p><p>The algorithms in <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref> embody the same idea, essentially with a "reference string" equal to 00 â¢ â¢ â¢ 0. Indeed, the number of purges in the sample-counting algorithm corresponds to the "die level" used in <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. One difference in these algorithms is that the actual data values, and not the hashed values, are stored: the level at which a data value is stored encodes the number of leading 0's in its hashed representation. In <ref type="bibr" target="#b16">[16]</ref>, the stored values are augmented with additional information. Specifically, for each distinct value in the buffer, the algorithm maintains the number of instances of the value in the dataset (here a relational table) and also maintains a reservoir sample <ref type="bibr" target="#b32">[32]</ref> of the rows in the table that contain the value. This additional information can be exploited to obtain approximate answers, with probabilistic error guarantees, to a variety of SELECT DISTINCT queries over a partition. Such queries include, as a special case, the SELECT COUNT(DISTINCT) query that corresponds to our desired DV estimate. In <ref type="bibr" target="#b3">[3]</ref>, the basic sample-counting algorithm is enhanced by compressing the stored values.</p><p>For sample-counting algorithms with reference string equal to 00 â¢ â¢ â¢ 0, the synopsis holds the K smallest hashed values encountered, where K lies roughly between k/2 and k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">The Bellman Synopsis</head><p>In the context of the Bellman system, the authors in <ref type="bibr">[8]</ref> propose a synopsis related to DV estimation. This synopsis comprises k entries and uses independent hash functions h 1 , h 2 , . . . , h k ; the ith synopsis entry is given by the ith minHash value m i = min vâD(A) h i (v). The synopsis for a partition is not actually used to directly compute the number of DVs in the partition, but rather to compute the Jaccard distance between partitions (see Section 2.3 below). When constructing the synopsis, each scanned data item in the partition incurs a cost of O(k), since the item must be hashed k times for comparison to the k current minHash values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DV Estimators</head><p>The motivation behind virtually all DV estimators can be viewed as follows. If D 1 points are placed randomly and uniformly on the unit interval, then, by symmetry, the expected distance between any two neighboring points is 1/(D + 1) â 1/D, so that the expected value of U (k) , the kth smallest point, is <ref type="foot" target="#foot_1">2</ref> and yields the basic estimator</p><formula xml:id="formula_0">E[U (k) ] â â k j=1 (1/D) = k/D. Thus D â k/E[U (k) ]. The simplest estimator of E[U (k) ] is simply U (k) itself,</formula><formula xml:id="formula_1">DBE k = k/U (k) .</formula><p>The simplest connection between the above idea and the DVestimation problem rests on the observation that a hash function often "looks like" a uniform random number generator. In particular, let v 1 , v 2 , . . . , v D be an enumeration of the distinct values in dataset A and let h be a hash function as before. For many hash functions, the sequence h(v 1 ), h(v 2 ), . . . , h(v D ) will look like the realization of a sequence of independent and identically distributed (i.i.d.) samples from the discrete uniform distribution on { 0, 1, . . . , M }. Provided that M is sufficiently greater than D, the sequence</p><formula xml:id="formula_2">U 1 = h(v 1 )/M,U 2 = h(v 2 )/M, . . . ,U D = h(v D</formula><p>)/M will approximate the realization of a sequence of i.i.d. samples from the continuous uniform distribution on [0, 1]. This assertion requires that M be much larger than D to avoid collisions, i.e., to ensure that, with high probability, h(v i ) = h(v j ) for all i = j. A "birthday problem" argument <ref type="bibr">[27, p. 45]</ref> shows that collisions will be avoided when M = O(D 2 ). We assume henceforth that, for all practical purposes, any hash function that arises in our discussion avoids collisions. We use the term "looks like" in an empirical sense, which suffices for applications. Thus, in practice, the estimator DBE k can be applied with U (k) taken as the kth smallest hash value (normalized by a factor of 1/M). Note that the function f (x) = 1/x is strictly convex on (0, â), so that</p><formula xml:id="formula_3">E[ DBE k ] = E k/U (k) &gt; k/E U (k) â D</formula><p>by Jensen's inequality. That is, the estimator DBE k is biased upwards for each possible value of D. In Section 4, we provide an unbiased estimator that also has lower mean-squared error than DBE k . Note that, in a certain sense, the foregoing view of hash functions -as algorithms that effectively place points on the unit interval according to a uniform distribution -represents a worst-case scenario with respect to the basic estimator. To the extent that a hash function spreads points evenly on [0, 1], i.e., without the slight clumping that is a byproduct of randomness, the estimator DBE k will yield more accurate estimates. We have observed this phenomenon experimentally; see Section 6.</p><p>The estimator DBE k was proposed in <ref type="bibr" target="#b3">[3]</ref>, along with conservative error bounds based on Chebyshev's inequality; the motivation behind the estimator is essentially the one given above. Interestingly, both the logarithmic and sample-counting estimators can be viewed as approximations to the basic estimator. For logarithmic counting -specifically the Flajolet-Martin algorithm -consider the binary decimal representation of the normalized hash values h(v)/M, where M = 2 L . E.g., a hash value h(v) = 00100110, after normalization, will have the binary decimal representation 0.00100110. It can be seen that the smallest normalized hash value is approximately equal to 2 -r , so that, modulo the correction factor, the Flajolet-Martin estimator (without stochastic averaging) is 1/2 -r , which roughly corresponds to DBE 1 . The final F-M estimator uses stochastic averaging to average independent values of r and hence compute an estimator Ã of E[log 2 DBE 1 ], leading to a final estimate of D = c2 Ã , where the constant c approximately unbiases the estimator. (Our new estimators are exactly unbiased.) For sample counting, suppose, without loss of generality, that the reference string is 00 â¢ â¢ â¢ 0 and, as before, consider the normalized binary decimal representation of the hashed values. Thus the first purge leaves behind normalized values of the form 0.0 â¢ â¢ â¢ , the second purge leaves behind values of the form 0.00 â¢ â¢ â¢ , and so forth, the last (rth) purge leaving behind only normalized hashed values with r leading 0's. Thus the number 2 -r (which has r -1 leading 0's) is roughly equal to the largest of the K normalized hashed values in the size-k buffer, so that the estimate K/2 -r is roughly equal to DBE k .</p><p>Giroire <ref type="bibr" target="#b18">[18]</ref> studies a variant of DBE k in which the hashed values are divided into m &gt; 1 subsets, leading to m i.i.d. copies of the basic estimator. These copies are obtained by dividing the unit interval into m equal segments; the jth copy of the basic estimator is based on all of the hashed values that lie in the jth segment, after shifting and scaling the segment (and the points therein) into a copy of the unit interval. (Note that for a fixed synopsis size k, each copy of the basic estimator is based on approximately k/m observations.) Each copy of the basic estimator is then subjected to a nonlinear transformation g, and multiplied by a correction factor c. The function g is chosen to "stabilize" the estimator, and the constant c is chosen to ensure that the estimator is asymptotically unbiased as k becomes large. Finally, the i.i.d. copies of the transformed estimators are averaged together. The motivation behind the transformation g is to avoid the instability problem, discussed previously, that arises when k = 1. In Section 4.1, we show that our proposed estimator is unbiased for any values of D and k, while being less cumbersome to compute. Moreover, when D k 0, our estimator has approximately the minimal possible variance (see Section 4.2), and hence is at least as statistically efficient as any estimator in <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Estimates for Compound Partitions</head><p>To our knowledge, the only prior discussion of how to construct DV-related estimates for compound partitions is found in <ref type="bibr">[8]</ref>. DV estimation for the intersection of partitions A and B is not computed directly. Instead, the Jaccard distance 3  <ref type="bibr">[8]</ref>) is estimated first and then, using the estimator Ï, the number of values in the intersection is estimated as</p><formula xml:id="formula_4">Ï = D(A â© B)/D(A âª B) (called the "resemblance" in</formula><formula xml:id="formula_5">D(A â© B) = Ï Ï + 1 D(A) + D(B) .<label>(1)</label></formula><p>The quantities D(A) and D(B) are computed exactly, by means of GROUP BY queries; our proposed estimators avoid the need to compute or estimate these quantities. There is no discussion in <ref type="bibr">[8]</ref> of how to handle any set operations other than the intersection of two partitions. If one uses the principle of inclusion/exclusion to handle other set operations, the resulting estimation procedure will not scale well as the number of operations increases. Our new techniques handle arbitrarily complex combinations of set operations (multiset unions, intersections, and differences) in an efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE KMV SYNOPSIS</head><p>As indicated above, we will use estimators that are closely related to the basic estimator DBE k (see Section 4). This relationship immediately implies a choice of synopsis for a partition A: we first hash each value in D(A) using a hash function h with domain { 0, 1, . . . , M }, and then we record the k smallest of the hashed values. We call this synopsis a KMV synopsis (for k minimum values). The KMV synopsis can be viewed as originating in <ref type="bibr" target="#b3">[3]</ref>, but there is no discussion in <ref type="bibr" target="#b3">[3]</ref> about implementing, constructing, or combining such synopses.</p><p>As discussed previously, we need to have M = O(D 2 ) to avoid collisions. Thus each of the k hashed values requires O(log M) = 3 Here D(S) denotes the number of distinct values in the set S.</p><p>Algorithm 1 (KMV Computation) 1: h: hash function from domain of dataset to { 0, 1, . . . , M } 2: L: list of k smallest hashed values seen so far 3: maxVal(L): returns the largest value in L 4: 5: for each item x in the dataset do A KMV synopsis can be computed from a single scan of the data partition, using Algorithm 1. The algorithm uses a sorted list of k hashed values, which can be implemented using, e.g., a priority queue. The membership check in line 7 avoids unnecessary processing of duplicate values in the input data partition, and can be implemented using a temporary hash table that is discarded after the synopsis has been built.</p><formula xml:id="formula_6">6: v = h(x) 7: if v / â L then 8: if |L| &lt; k then 9: insert v into L 10: else if v &lt; maxVal(L) then</formula><p>Assuming that the scan order of the items in a partition is independent of the hashed item values, and using reasoning similar to <ref type="bibr" target="#b17">[17]</ref>, we obtain the following result. Each subsequent new DV encountered will incur an O(log k) cost if it is inserted (line 11), or an O(1) cost otherwise. (Note that a given DV will be inserted at most once, at the time it is first encountered, regardless of the number of times that it appears in A.) The ith new DV encountered is inserted only if its normalized hash value U i is less than M i , the largest normalized hash value currently in the synopsis. Because points are placed uniformly, we have</p><formula xml:id="formula_7">P {U i &lt; M i | M i } = M i ,</formula><p>so that, by the law of total expectation,</p><formula xml:id="formula_8">P {U i &lt; M i } = E P {U i &lt; M i | M i } = E[M i ] â k/i.</formula><p>Thus, writing H D for the Dth harmonic number, the expected cost for handling the remaining Dk distinct values is</p><formula xml:id="formula_9">E[Cost] â D â i=k+1 (k/i)O(log k) + 1 -(k/i) O(1) &lt; D â i=1 [(k/i)O(log k)] + O(D) = O(D) + O(k log k) D â i=1 (1/i) = O(D) + O(k log k)H D = O(D + k log k log D),</formula><p>and the overall expected cost is</p><formula xml:id="formula_10">O(N + D + k log k + k log k log D) = O(N + k log k log D).</formula><p>The foregoing construction cost compares favorably to the O(kN) cost for the Bellman synopsis. Moreover, when D is small, the KMV synopsis contains more information in a statistical sense than the Bellman synopsis, since the former synopsis essentially samples distinct values without replacement, whereas the latter synopsis samples distinct values with replacement. The cost for the KMV synopsis is comparable to that of the sample-counting synopsis <ref type="bibr" target="#b17">[17]</ref>. Indeed, the sample-counting synopsis is very similar to KMV, except that the size is a random variable K whose value ranges between roughly k/2 and k. Thus the KMV synopsis contains more statistical information for a given space requirement, and yields DV estimators that are statistically more stable.</p><p>We show in Section 5 that the KMV synopsis gracefully handles a variety of set operations, but needs to be augmented with counters to handle multiset differences. This augmentation has a negligible effect on the construction cost, and results in a desirable "closure" property that permits efficient estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NEW DV ESTIMATORS</head><p>As discussed previously, the basic estimator DBE k is biased upwards for the true number of distinct values D. Inspired by results in <ref type="bibr" target="#b7">[7]</ref>, we consider the estimator</p><formula xml:id="formula_11">DUB k = (k -1)/U (k)<label>(2)</label></formula><p>and show that DUB k is unbiased. The DUB k estimator forms the basis for the extended DV estimators, discussed in Section 5, used to estimate the number of DVs in a compound partition. Another standard estimation approach is the method of maximum likelihoodwe therefore also develop a maximum-likelihood estimator (MLE) for our problem. Finally, we analyze DUB k as D becomes large, and show that our results coincide asymptotically with those in <ref type="bibr" target="#b7">[7]</ref>, yielding convenient error bounds. This "large D" analysis facilitates the choice of k when initially designing a synopsis warehouse. Henceforth, we assume without further comment that D &gt; k; if D â¤ k, then we can easily detect this situation and compute the exact value of D from the synopsis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of the Unbiased Estimator</head><p>Let U 1 ,U 2 , . . . ,U D be the normalized hash values of the distinct items in the dataset; we model these values as a sequence of independent and identically distributed (i.i.d.) random variables from the uniform[0, 1] distribution -see the discussion in Section 2.2. Denote by U (k) the kth smallest of U 1 ,U 2 , . . . ,U D , that is, U (k) is the kth order statistic. Results from the theory of order statistics [9, Sec. 2.1] imply that U (k) follows the beta distribution with parameters k and Dk + 1. That is, the probability density function (pdf) of U (k) is given by</p><formula xml:id="formula_12">f k,D (t) = t k-1 (1 -t) D-k /B(k, D -k + 1),<label>(3)</label></formula><p>where B(a, b) = 1 0 t a-1 (1-t) b-1 dt denotes the beta function <ref type="bibr" target="#b23">[23]</ref>. Moreover,</p><formula xml:id="formula_13">P{U (k) â¤ x } = x 0 f k,D (t)dt = I x (k, D -k + 1) (4) = D â i=k D i x i (1 -x) D-i<label>(5)</label></formula><p>where I x (a, b) is the regularized incomplete beta function and is defined for all real a and b. An efficient algorithm for the computation of the most significant digits of I x (a, b) is given in <ref type="bibr" target="#b10">[10]</ref>. Observe that the sum in ( <ref type="formula" target="#formula_13">5</ref>) is simply the probability that at least k of the U i values are smaller than x.</p><p>To facilitate the analysis of DUB k , we first derive the moments of 1/U (k) . For k &gt; r â¥ 0, we have</p><formula xml:id="formula_14">E[U -r (k) ] = 1 0 f k,D (t) t r dt = B(k -r, D -k + 1) B(k, D -k + 1) .</formula><p>If r is an integer, we can exploit the identity B(a, b) = a -1 a+b-1 a -1</p><p>to obtain</p><formula xml:id="formula_15">E[U -r (k) ] = D r /(k -1) r ,<label>(6)</label></formula><p>where a b denotes the falling power a(a -1) â¢ â¢ â¢ (ab + 1). Regarding DUB k , we find that</p><formula xml:id="formula_16">E[ DUB k ] = E[(k -1)/U (k) ] = (k -1)E[U -1 (k) ] = D,</formula><p>so that DUB k is indeed unbiased for D, and</p><formula xml:id="formula_17">Var[ DUB k ] = (k -1) 2 E[U -2 (k) ] -(k -1) 2 E[U -1 (k) ] 2 = D(D -k + 1) k -2 .</formula><p>Because DUB k is unbiased, its mean-squared error (MSE) is equal to its variance.</p><p>For comparison, note that, by ( <ref type="formula" target="#formula_15">6</ref>),</p><formula xml:id="formula_18">E[ DBE k ] = kD/(k -1) and MSE[ DBE k ] = k k -1 2 MSE[ DUB k ] + D k -1 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus, as discussed earlier, DBE</head><p>k is biased high for D, and has infinite mean when k = 1, as observed in <ref type="bibr" target="#b16">[16]</ref>. Moreover, it can be seen that DUB k has lower MSE than DBE k . We now provide probabilistic (relative) error bounds for the estimator DUB k . Specifically, given 0 &lt; Î´ &lt; 1, we give a value of Îµ such that DUB k lies in the interval</p><formula xml:id="formula_19">[(1 -Îµ)D, (1 + Îµ)D] with probability Î´ . THEOREM 2. For 0 &lt; Îµ &lt; 1 and k â¥ 1, P | DUB k -D| D â¤ Îµ = I u(D,k,Îµ) (k, D -k + 1) -I l(D,k,Îµ) (k, D -k + 1),<label>(7)</label></formula><p>where</p><formula xml:id="formula_20">u(D, k, Îµ) = k -1 (1 -Îµ)D and l(D, k, Îµ) = k -1 (1 + Îµ)D .<label>(8)</label></formula><p>PROOF. The desired result follows directly from (4) after using (2) to obtain</p><formula xml:id="formula_21">P | DUB k -D| D â¤ Îµ = P (1 -Îµ)D â¤ DUB k â¤ (1 + Îµ)D = P k -1 (1 + Îµ)D â¤ U (k) â¤ k -1 (1 -Îµ)D .</formula><p>Error bounds for a given value of Î´ can be obtained by equating the right side of (7) to Î´ and solving for Îµ using a root-finding algorithm. Although useful for theoretical analysis, these bounds cannot be used directly in practice, since they involve the unknown parameter D. Using a standard approach from statistics, practical approximate error bounds based on the observed value of U (k) can be obtained by replacing D with DUB k in the above formulas. Figure <ref type="figure" target="#fig_1">1</ref>  superior to DBE k when k is small; for example, when k = 16 and Î´ = 0.95, use of the unbiased estimator yields close to a 20% reduction in Îµ. As k increases, k -1 â k and both estimators perform similarly. Note that the error bound is very stable even for large values of D. E.g., it follows the results of Section 4.3 that, for Î´ = 0.95 and k = 1024, the upper bound on Îµ as D â â is Îµ â 0.06127, whereas we observe a value of Îµ â 0.06124 for D = 1, 000, 000.</p><p>To further examine the behavior of the unbiased estimator, we derive the expected value of the absolute ratio error (ARE), where the ARE is defined as | DUB k -D|/D. The expected ARE is a common metric for comparing the performance of statistical estimators. THEOREM 3. The expected ARE of DUB k is given by</p><formula xml:id="formula_22">E | DUB k -D| D = 2 D k -1 k -1 D k-1 1 - k -1 D D-k+2</formula><p>PROOF. We have</p><formula xml:id="formula_23">E[ARE] = 1 D 1 0 k -1 t -D f k,D (t) dt = 1 D (k-1)/D 0 k -1 t -D f k,D (t) dt + 1 D 1 (k-1)/D D - k -1 t f k,D (t) dt = 2I (k-1)/D (k -1, D -k + 1) -2I (k-1)/D (k, D -k + 1),</formula><p>where the last equality is obtained after expanding the integrals and applying the identity</p><formula xml:id="formula_24">(k -1)(tD) -1 f k,D (t) = f k-1,D-1 (t)</formula><p>. The desired result now follows after applying (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Maximum Likelihood Estimator</head><p>The classical statistical approach to estimating unknown parameters is the method of maximum likelihood <ref type="bibr" target="#b30">[30,</ref><ref type="bibr">Sec. 4.2]</ref>. We apply this approach by casting our DV-estimation problem as a parameter estimation problem. Specifically, recall that U (k) has the pdf f k,D given in <ref type="bibr" target="#b3">(3)</ref>. The MLE estimate of D is defined as the value D that maximizes the likelihood L(D;U (k) ) of the observation U (k) , defined as L(D;U (k) ) = f k,D (U (k) ). We find this maximizing value by solving the equation L (D;U (k) ) = 0, where the prime denotes differentiation with respect to D. We have</p><formula xml:id="formula_25">L (D;U (k) ) = ln(1 -U (k) ) -Î¨(D -k + 1) + Î¨(D + 1),</formula><p>where Î¨ denotes the digamma function. If x is sufficiently large, then Î¨(x) â ln(x -1) + Î³, where Î³ denotes Euler's constant. <ref type="bibr">Ap</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis with Many DVs</head><p>Our asymptotic analysis for large D rests on Theorem 4 below. As before, for a sequence U 1 ,U 2 , . . . ,U n of i.i.d. uniform[0,1] random variables, denote by U (1) &lt; U (2) &lt; â¢ â¢ â¢ &lt; U (n) the order statistics of the sequence, and define the spacings</p><formula xml:id="formula_26">W 1 = U (1) ,W 2 = U (2) - U (1) , . . . ,W n = U (n) -U (n-1) . Write X n â X if and only if the se- quence { X n : n â¥ 1 } converges in distribution to X, that is, lim nââ P { X n â¤ x } = P { X â¤ x }</formula><p>for all x at which the function F(x) = P { X â¤ x } is continuous. We say that a random variable Y has an exponential distribution with rate parameter Î» , denoted Exp(Î» ), if </p><formula xml:id="formula_27">P {Y â¤ y } = 1 -e -Î» y if y â¥ 0; 0 if y &lt; 0.</formula><formula xml:id="formula_28">P {W i â¤ x } = P { DW i â¤ Dx } â 1 -e -Dx</formula><p>for 1 â¤ i â¤ n, it follows that the unscaled spacings W 1 ,W 2 , . . . ,W k are approximately i.i.d. Exp(D), so that U (k) is distributed approximately as the sum of k i.i.d. Exp(D) random variables. This exponential scenario is precisely the one analyzed in <ref type="bibr" target="#b7">[7]</ref>, in the context of estimating the size of a transitive closure. It follows from <ref type="bibr" target="#b7">[7]</ref> that</p><formula xml:id="formula_29">P | DUB k -D| D â¤ Îµ â e -k-1 1+Îµ 1 + k-1 â i=1 (k -1) i (1 + Îµ) i i! -e -k-1 1-Îµ 1 + k-1 â i=1 (k -1) i (1 -Îµ) i i! and E | DUB k -D| D â 2(k -1) k-2 (k -2)!e k-1 â 2 Ï(k -2) .</formula><p>As might be expected, the above formulas can also be obtained by letting D â â in the corresponding formulas from Section 4.1. Though slightly conservative, the asymptotic error bounds have the advantageous property that, unlike the exact bounds, they do not involve the unknown quantity D. Thus, given desired values of Îµ and Î´ , they can be used to help determine target synopsis sizes when initially designing a synopsis warehouse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMBINING SYNOPSES</head><p>The discussion up until now has focused on creating and using a synopsis to estimate the number of DVs in a single base partition. We now focus on DV estimation for a compound partition, i.e., a partition that is created from a set of base partitions using the multiset operations of intersection, union, and difference. When a compound partition G has been created from base partitions using only union and intersection, or when each base partition contains no duplicates, we can estimate the number of DVs in G directly from the KMV synopses for the base partitions. To handle multiset difference, however, we need to augment our KMV synopses with counters; we show that the resulting AKMV synopses are "closed" under multiset operations on the parent partitions. The closure property implies that if E and F are compound partitions and G is obtained from E and F via a set operation, then we can compute an AKMV synopsis for G from the corresponding AKMV synopses for E and F, and unbiasedly estimate the number of DVs in G from this resulting synopsis. This procedure avoids the need to access the synopsis for each of the base partitions that were used to create E and F. The AKMV synopsis can also handle deletions of individual items from the warehouse. As discussed below, the actual DV estimators that we use for compound partitions are, in general, extensions of the simple DUB k estimator developed in Section 4.</p><p>We assume throughout that all synopses are created using the same hash function h : D â { 0, 1, . . . , M }, where D denotes the domain of the data values that appear in the partitions and M = O(|D| 2 ) as discussed previously. We denote ordinary set-union, set-intersection, and set-difference operators by { âª, â©, \ } and the corresponding multiset operators by { âª m , â© m , \ m }. 4    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Union Operations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B).</head><p>Observe that G contains the k smallest values in h(A) for some k â¤ k, and these k values therefore are also contained in L A , i.e.,</p><formula xml:id="formula_30">G â© h(A) â L A . Similarly, G â© h(B) â L B , so that G â L A âª L B . For any h â (L A âª L b ) \ G, we have that h &gt; max h âG h by definition of G, because h â h(A âª B). Thus G in fact comprises the k smallest values in L A âª L B , so that L = G. Now observe that, by definition, G is precisely the size-k KMV synopsis of A âª m B.</formula><p>Thus we can immediately apply the results of the Section 4 to estimate D âª by Dâª = (k -1)/U (k) where Dâª is computed from the size-k KMV synopsis L = L A âL B . This result extends immediately to multiple partitions: the number of DVs in</p><formula xml:id="formula_31">A 1 âª m A 2 âª m â¢ â¢ â¢ âª m A n can be estimated from L = L A 1 â L A 2 â â¢ â¢ â¢ â L A n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Intersection Operations</head><p>As before, consider two partitions A and B, with corresponding KMV synopses L A and L B of sizes k A and k B , respectively. Our goal now is to estimate </p><formula xml:id="formula_32">D â© = |D(A â© m B)| = |D(A) â© D(B)|. Set L = L A â L B and write L = { h(v 1 ), h(v 2 ), . . . , h(v k ) }, where k = min(k A , k B ) as before and each distinct value v i is an element of D(A) âª D(B). Also write V L = { v 1 , v 2 , . . . , v k }, and set K â© = |{ v â V L : v â D(A) â© D(B) }|. LEMMA 1. For each v â V L , we have v â D(A) (resp., v â D(B)) if and only if h(v) â L A (resp., h(v) â L B ). PROOF. Let v â V L , so that h(v) is among the k smallest values of h(A âª m B). Then h(v) is among the k smallest values of h(A) if v â D(A), so that h(v) â L A if v â D(A). Conversely, if h(v) â L A ,</formula><formula xml:id="formula_33">P { K â© = j } = D â© j D âª -D â© k -j D âª k . (<label>9</label></formula><formula xml:id="formula_34">)</formula><p>We now use K â© to estimate D â© . From Section 5.1, we know that Dâª = (k -1)/U (k) is an unbiased estimator of D âª ; we would like to "correct" this estimator via multiplication by the Jaccard distance Ï = D â© /D âª . We do not know Ï, but a reasonable estimate is</p><formula xml:id="formula_35">Ï = K â© /k, (<label>10</label></formula><formula xml:id="formula_36">)</formula><p>the fraction of sample elements in V L â D(A âª B) that belong to D(A â© B). This leads to our proposed estimator</p><formula xml:id="formula_37">Dâ© = K â© k k -1 U (k) .</formula><p>We now establish some basic properties of the estimator. For</p><formula xml:id="formula_38">n â¥ k â¥ 1, set â(n, k, Îµ) = I u(n,k,Îµ) (k, n -k + 1) -I l(n,k,Îµ) (k, n -k + 1),</formula><p>where I x (a, b) is the regularized incomplete beta function, and u(n, k, Îµ) and l(n, k, Îµ) are defined as in <ref type="bibr">(8)</ref>. Take â(â, k, Îµ) = 0.</p><p>Denote by</p><formula xml:id="formula_39">H( j; N, M, n) = M j N -M n -j N n</formula><p>the hypergeometric probability distribution function. THEOREM 6. When based on a combined synopsis of size k, the estimator Dâ© satisfies E</p><formula xml:id="formula_40">[ Dâ© ] = D â© if k &gt; 1, Var[ Dâ© ] = D â© (kD âª -k 2 -D âª + k + D â© ) k(k -2)</formula><p>if k &gt; 2, and, if D â© &gt; 0, Îµ â (0, 1), and k â¥ 1,</p><formula xml:id="formula_41">P | Dâ© -D â© | D â© â¤ Îµ K â© = j = â(kD â© / j, k, Îµ)<label>(11)</label></formula><p>for 0 â¤ j â¤ min(k, D â© ), and</p><formula xml:id="formula_42">P | Dâ© -D â© | D â© â¤ Îµ = min(k,D â© ) â j=0 â(kD â© / j, k, Îµ)H( j; D âª , D â© , k).<label>(12)</label></formula><p>PROOF. A can be seen from ( <ref type="formula" target="#formula_33">9</ref>), the distribution of K â© does not depend on the hash values { h(v) : v â D(A âª B) }. It follows that the random variables K â© and U (k) are statistically independent, as are Ï and U (k) , where Ï = K â© /k as above. By <ref type="bibr" target="#b9">(9)</ref> and standard properties of the hypergeometric distribution, we have</p><formula xml:id="formula_43">E [K â© ] = k D â© D âª<label>(13)</label></formula><p>and</p><formula xml:id="formula_44">Var[K â© ] = D â© (D âª -D â© )k(D âª -k) D 2 âª (D âª -1) . (<label>14</label></formula><formula xml:id="formula_45">)</formula><p>It follows from ( <ref type="formula" target="#formula_43">13</ref>) that E [ Ï] = Ï. Using independence and the unbiasedness of Dâª , we find that</p><formula xml:id="formula_46">E[ Dâ© ] = E[ Ï Dâª ] = E[ Ï]E[ Dâª ] = ÏD âª = D â© .</formula><p>The formula for Var[ Dâ© ] follows from ( <ref type="formula" target="#formula_15">6</ref>), <ref type="bibr" target="#b13">(13)</ref>, and ( <ref type="formula" target="#formula_44">14</ref>), after some straightforward algebra. To obtain the relation in <ref type="bibr" target="#b11">(11)</ref>, use the fact that K and Dâª are independent, and write</p><formula xml:id="formula_47">P | Dâ© -D â© | D â© â¤ Îµ K â© = j = P |( j/k) Dâª -D â© | D â© â¤ Îµ = P | Dâª -D * | D * â¤ Îµ ,</formula><p>where D * = (k/ j)D â© . The desired result then follows by mimicking the proof of Theorem 2. The final relation in <ref type="bibr" target="#b12">(12)</ref> follows from ( <ref type="formula" target="#formula_41">11</ref>) by unconditioning on K â© and using <ref type="bibr" target="#b9">(9)</ref>.</p><p>Thus Dâ© is unbiased for D â© . It also follows from the proof that the estimator Ï is unbiased for the Jaccard distance Ï. Using (12), we can compute exact confidence bounds numerically, analogously to the single-partition case. To obtain practical approximate bounds based on observation of K â© and U (k) , use the representation in <ref type="bibr" target="#b11">(11)</ref>, but replace D â© by Dâ© .</p><p>Interestingly, Dâ© can be viewed as being in the spirit of the Bellman estimator <ref type="bibr" target="#b1">(1)</ref>. Specifically, the quantity D(A) + D(B) /( Ï + 1) in <ref type="bibr" target="#b1">(1)</ref>, after some algebra, can be viewed as an estimator of D âª , so the overall estimator can be viewed as an estimator of Ï times an estimator of D âª . We emphasize, however, that Dâ© is based on very different estimators of Ï and D âª .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Compound Partitions with No Duplicates</head><p>We now consider the more general problem of estimating D E , the number of DVs in E, where E is a compound partition created from n â¥ 2 base partitions A 1 , A 2 , . . . , A n , all of which are ordinary sets, using the ordinary union, intersection, and set-difference operators. Some examples are</p><formula xml:id="formula_48">E = A 1 \ A 2 and E = (A 1 âª A 2 ) â© (A 3 âª A 4 ) \ A 5 .</formula><p>Note that E is also an ordinary set, so that E = D(E), and estimating the number of DVs in E is equivalent to estimating the cardinality of E. Our key observation is that the foregoing analysis for the intersection operator applies essentially unchanged in the current setting. Specifically, we form the synopsis</p><formula xml:id="formula_49">L = L A 1 â L A 2 â â¢ â¢ â¢ â L A n of size k = min(k A 1 , k A 2 , . . . , k A n ).</formula><p>Define V L as before and set</p><formula xml:id="formula_50">K E = |{ v â V L : v â D(E) }|.<label>(15)</label></formula><p>By a trivial extension of Lemma 1, we can compute</p><formula xml:id="formula_51">K E from L A 1 , L A 2 , . . . , L A n alone. The estimator DE = K E k k -1 U (k)<label>(16)</label></formula><p>is unbiased for D E , and the other properties of DE are derived exactly as for Dâ© . For the special case E = A 1 âª A 2 , we have K E = k with probability 1, and DE reduces to Dâª . Note that the approach of this section can also be applied when the base partitions are multisets, provided that the only operations used are âª m and â© m . To see this, observe that the computation in <ref type="bibr" target="#b15">(15)</ref> effectively replaces each A i with D(A i ); as indicated previously, D E is unchanged by this transformation. Of course, we can also estimate D E when E is any ordinary set expression involving the D(A i ) sets, even if the A i 's are themselves multisets. This latter task requires that we augment our KMV synopsis. We define an augmented synopsis of a base partition A, which we call an AKMV synopsis, as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multiset Difference</head><formula xml:id="formula_52">L + A = (L A , c A ), where L A = { h(v 1 ), h(v 2 ), . . . , h(v k ) } is a KMV synopsis of size k, and c A = { c A (v 1 ), c A (v 2 ), . . . , c A (v k ) } is a set of k nonnegative counters. The quantity c A (v)</formula><p>is the multiplicity in A of the value v; this use of counters is in the spirit of <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b31">31]</ref>. The size of the AKMV synopsis is O(k log D+ k log M), where M is the maximum multiplicity of a value in the multiset A. Note that, if A is a set, then it suffices to maintain a bit vector rather than a vector of counts, so that the size of the synopsis is O(k log D), just as with an ordinary KMV synopsis. It is easy to modify Algorithm 1 to create and maintain counters via O(1) operations. The modified algorithm retains the original algorithm complexity of O(N + k log k log D).</p><p>We now proceed almost exactly as in Section 5.3, taking</p><formula xml:id="formula_53">E = A \ m B in (15). Observe that a value v â V L is an element of K E if and only if h(v) â L A and either (i) h(v) â L B (which implies that v â D(B) by Lemma 1) or (ii) h(v) â L B with c A (v) -c B (v) &gt; 0.</formula><p>Thus we can determine K E from L + A and L + B alone. This approach extends to any quantity of the form D E , where E is a compound (multiset) partition created by applying the operations âª m , â© m , and \ m to n â¥ 2 base partitions A 1 , A 2 , . . . , A n , each of which may be a multiset. As with multiset difference, we use the count vectors to compute K E ; see the following section for another example of such a computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">The Closure Property</head><p>In the previous section, we defined the AKMV synopsis of a base partition. We extend this definition to the AKMV synopsis</p><formula xml:id="formula_54">L + E = (L E , c E ) of a compound partition E by taking L E = L A 1 â L A 2 â â¢ â¢ â¢ â L A n</formula><p>, where A 1 , A 2 , . . . , A n are the base partitions used to construct E, and c E (v) is the multiplicity of v in E. With this definition, the collection of AKMV synopses over compound partitions is closed under multiset operations. For example, if we combine compound partitions E and F -having respective AKMV synopses</p><formula xml:id="formula_55">L + E = (L E , c E ) and L + F = (L F , c F ) -to create G = E â© m F, then the combined synopsis is L + G = (L E â L F , c G ). Here c G (v) = min c E (v), c F (v) for each v â V L G , where we take c X (v) = 0 if h(v) â L X . The cases G = E âª m F and G = E \ m F are handled similarly.</formula><p>Then K E is computed as the number of non-zero elements in c G , and the number of DVs in G is estimated as in ( <ref type="formula" target="#formula_51">16</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Deletions</head><p>We now show how AKMV synopses can easily support deletions of individual items. Consider a partition A that receives a stream of transactions of the form +v or -v, corresponding to the insertion or deletion, respectively, of value v.</p><p>A naive approach maintains two AKMV synopses: a synopsis L + i for the multiset A i of inserted items and a synopsis L + d for the multiset A d of deleted items. Computing the AKMV synopsis of the multiset difference A i \ m A d yields the AKVM synopsis L + A of the true multiset A. Because A i and A d are always growing with time, they can become significantly larger than the actual partition A, so that DV estimates based on (15) will be of low quality. Therefore, whenever the number of deletions causes the error bounds to become unacceptable, a fresh scan of the data can be used to produce a new pair of synopses L i and L d corresponding to A i = A and A d = / 0. This method does not actually require two synopses. We can simply maintain the counters in a single AKMV synopsis L by incrementing the counter at each insertion and decrementing at each deletion. If we retain synopsis entries having counter values equal to 0, we produce precisely the synopsis L described above.</p><p>We conjecture that the above procedure can be further improved as follows. Whenever an insertion transaction +v arrives and v &lt; maxVal(L), remove a 0-item (i.e., an item with a 0-valued counter) from L and insert the new value v , with corresponding counter value c A (v ) = 1. The victim 0-item can be chosen arbitrarily from among all 0-items in the synopsis, except that a 0-item with hash value equal to maxVal(L) should be removed only if it is the sole 0-item in the synopsis. If the synopsis contains no 0-items when the insertion transaction arrives, then remove the maximum item as usual and insert the new item. Thus, if the synopsis contains at least one 0-item with hash value less than maxVal(L), then maxVal(L) stays the same after the insertion; otherwise, maxVal(L) decreases. Using this process, L produces its best estimates when there are very few 0-items. This situation occurs whenever the current size of D(A) is roughly the maximum size that D(A) has ever attained. For example, when insertions significantly outnumber deletions, L will yield reasonably accurate estimates. If, however, A shrinks significantly, a fresh synopsis may be required to achieve the desired accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL EVALUATION</head><p>We implemented the AKMV synopsis and the corresponding DV estimators, and applied our prototype to various real-world and synthetic datasets. First we examined the average ARE of the new estimators over synthetic data in order to establish a baseline; the synthetic datasets correspond to a scenario in which the hash function behaves exactly like a random number generator, in accordance with the assumptions underlying the derivation of our methodology. Then, using real data, we examined the impact on estimation accuracy of using different hash functions when creating the synopsis. We next compared the accuracy of the new estimators to that of the current best-of-breed estimators. Finally, we investigated the accuracy of our DV estimators on compound partitions of the form A âª m B and A â© m B, as well as the accuracy of our Jaccard-distance estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We evaluated our KMV prototype using three real-world datasets. The OPIC dataset contains product information for a large computer company. The BASEBALL dataset contains data about baseball players, teams, awards, hall-of-fame memberships, and both game and player statistics for the baseball championship in Australia. The RDW dataset was obtained from the data warehouse of a large financial company. Table <ref type="table" target="#tab_1">1</ref> displays some summary characteristics of these datasets. All experiments were performed on a UNIX machine with one 2.8 GHz processor and 1GB of RAM. Unless stated otherwise, results are reported for experiments on the RDW dataset; the results for other datasets are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accuracy Comparison</head><p>We first compared the expected accuracy of the KMV-based estimators using a synthetic baseline dataset. Then we compared these estimators with the current best-of-breed DV estimators on real data, using various hashing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Baseline Comparison on Synthetic Data</head><p>In order to compare the DV estimators while controlling for hashing effects, we used a high-quality pseudorandom number generator <ref type="bibr" target="#b26">[26]</ref> to produce synthetic datasets, each with a specified number of distinct values. In effect, we generated the hashed values directly, according to a "truly" uniform[0,1] probability distribution. Since we derived our estimators under precisely this uniformdistribution assumption, the synthetic dataset provides a baseline for performance, allowing us, in subsequent experiments, to clearly see the effects on accuracy of using non-ideal hash functions.</p><p>We studied the behavior of the various estimators we have discussed: the basic estimator DBE k , the unbiased estimator DUB k and the MLE estimator DMLE k . To gauge the accuracy of a given estimator D for a specified number D of distinct values, we generated 1000 datasets, each containing D distinct values, using a different pseudorandom number seed each time ensure independence between the datasets. For each estimator, we computed the ARE on each dataset and then averaged these 1000 ARE values.</p><p>Figure <ref type="figure" target="#fig_7">2</ref> shows the average ARE as a function of the synopsis size k when the true numbers of distinct values are D = 1 million and D = 100, 000, respectively. Not surprisingly, the primary factor that affects the accuracy is the size of the KMV synopsis, with relative estimation errors decreasing as k increases. As expected from theory, the relative estimation error is relatively insensitive to the true number of distinct values. Observe that the unbiased esti-  mator consistently provides the best accuracy for all synopsis sizes, and that the basic and MLE estimators are indistinguishable. For small synopses, the benefit of the unbiased estimator increases as the true number of distinct values increases. Since the unbiased estimator introduces no overhead with respect to the basic estimator, we recommend using it for all synopsis sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Hashing Effect</head><p>The development in Section 4 assumes that the hash function can be viewed as a 1-to-1 mapping from the D distinct values in the dataset to a set of D uniform random numbers. Such a mapping can be constructed perfectly using O(D) memory, but this memory requirement is infeasible for very large datasets. For practical purposes, we need to approximate such a mapping using a hash function that requires a small amount of memory (logarithmic in D).</p><p>In this section we study how the use of real-world hash functions effects the accuracy of our estimators. More specifically, we compute estimates from real data, using various hash functions, and compare the accuracy of these results to the accuracy of the baseline estimates from the previous section. Our accuracy measure is again the ARE, averaged over all of the datasets in the database. We used three different hashing methods: AES The Advanced Encryption Standard (AES) hash function is a well established cipher function that has been studied extensively. For example, Hellekalek and Wegenkittl <ref type="bibr" target="#b21">[21]</ref> showed that AES behaves empirically like a high-quality randomnumber generator when applied in an iterative fashion, making it a promising candidate for DV estimation (although we use AES in a slightly different fashion here than in <ref type="bibr" target="#b21">[21]</ref>). Since AES is a cipher function, its output size is equal to the input size. In our implementation, we only used the most significant 32 bits of the output as the hash value.</p><p>FLH This hash function, due to Wegman, 5 typifies the sort of hash function used in current computer systems. The function is rather complicated, so we omit details here. Note that FLH is designed merely to avoid collisions, and so does not provide any guarantees on the distribution of its output.</p><p>GRM This Golden-Ratio Multiplicative (GRM) hash function is based on classical results of Knuth <ref type="bibr" target="#b25">[25]</ref>. The method is based on the observation that multiplying each member of the sequence 1, 2, . . . , n by the golden ratio, and keeping the frac- 5 Personal communication. Figures 3-5 display the average ARE of the unbiased KMV estimator as a function of synopsis size for the three hash functions described above, as well as for the ideal hash function implicit in the baseline scenario of Figure <ref type="figure" target="#fig_7">2</ref>. The three figures correspond to the three real-world datasets described previously.</p><p>The FLH hash function is dominated by the other hash functions on all datasets. For the RDW and OPIC datasets, the GRM hashing function is clearly superior, even outperforming the ideal baseline in certain cases. The main reason behind GRM's high accuracy, however, is that the RDW and OPIC datasets contain many machine-generated surrogate integer keys of the form 1, 2, . . . , n. As discussed above, the hashed values are very evenly distributed for such input. Thus, even when the synopsis size k is quite small, the spacings are very stable and even, leading to very accurate DV estimates. The BASEBALL dataset, on the other hand, contains almost no surrogate keys, and the accuracy of GRM drops significantly. For this dataset we see that the AES hash function is reasonably close to the baseline ARE, performs comparably to the other hash functions for all values of k, and has superior performance for small values of k. Overall, we recommend using AES as a hash function, because it tracks the baseline output reasonably closely for all datasets, which results in relatively reliable accuracy that is independent of the presence of surrogate keys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Comparison with Best-of-Breed Estimators</head><p>We next compared the unbiased KMV estimator to two current best-of-breed estimators: the SDLogLog estimator, which is a highly tuned implementation of the loglog estimator given in <ref type="bibr" target="#b11">[11]</ref>, and a variant of the sample-counting algorithm described in <ref type="bibr" target="#b3">[3]</ref>. In preliminary experiments, we found that these latter two estimators were the best of the probabilistic-counting and sample-counting types, respectively. All of the algorithms used exactly the same amount of available memory, which corresponded to a synopsis size of k = 8192. We chose this value because it maximized the performance of our own hand-tuned optimized SDLogLog estimator. The box plot in Figure <ref type="figure" target="#fig_10">6</ref> summarizes, for each estimator, the distribution of the ARE values over all of the datasets in the RDW database. 6 For comparison, we also plotted the ARE distribution for the baseline scenario when using the unbiased KMV estimator DUB k together with a synopsis of size k = 8192. As can be seen, the unbiased KMV estimator is significantly more accurate than both SDLogLog and sample-counting on real datasets. The main reason is that both SDLogLog and sample-counting merely approximate the basic estimator -and hence the MLE estimator -even when k is large, whereas the unbiased KMV estimator essentially coincides with the MLE estimator for large k. If the synopsis size were small, so that bias effects were important, then the unbiased KMV estimator would have a further accuracy advantage. In these experiments, we found that, for the large value of k that we used, the AES hash function distributed points somewhat more evenly than we would expect from a true random number generator, and this phenomenon resulted in a slight accuracy improvement; see Section 2.2. Thus these results provide further empirical evidence for the suitability of the AES hash function for DV estimation. 6 The top, midpoint, and bottom of a box represent the 75th, 50th, and 25th percentiles of the ARE values, and the top of the thin line corresponds to the maximum ARE value that was observed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Compound Partitions</head><p>The main motivation behind our work on the AKMV synopsis was the desire for efficient and accurate DV estimates within the synopsis-warehouse scenario, where different synopses need to be combined in order to efficiently extract interesting information about corresponding compound partitions. In practice, the most common operations in this setting are multiset union and intersection. We therefore evaluated the accuracy of the KMV-synopsis estimators in the presence of these operations, as well as the accuracy of our Jaccard-distance estimator.</p><p>For this experiment, we computed a KMV synopsis of size k = 8192 for each dataset in the RDW database. Then, for every possible pair of synopses, we used the unbiased estimator in <ref type="bibr" target="#b16">(16)</ref> to estimate the DV count for the union and intersection, and also estimated the Jaccard distance using our new estimator Ï defined in <ref type="bibr" target="#b10">(10)</ref>. We also estimated these quantities using the SDLogLog estimator; specifically, we estimated the number of DVs in the union directly, and then used the inclusion/exclusion rule to estimate the DV count for the intersection and then for the Jaccard distance.</p><p>Figure <ref type="figure" target="#fig_11">7</ref> displays, for each estimator, a histogram for each of these three multiset operations. (The histogram shows, for each possible value of ARE, the number of dataset pairs for which the DV estimate yielded that specific ARE value.) For the majority of the datasets, the unbiased estimator based on the KMV synopsis provides estimates that are almost ten times more accurate than the SDLogLog estimates, even though both methods used exactly the same amount of available memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">SUMMARY AND CONCLUSIONS</head><p>We have revisited the classical problem of DV estimation, but from a synopsis-oriented point of view. By combining and extend-ing previous results on DV estimation, we have obtained a solution that is well suited to the synopsis warehouse architecture. Our solution comprises the AKMV synopsis, along with novel unbiased DV estimators that exploit an AKMV synopsis.</p><p>Our theoretical contributions include (1) using the theory of order statistics to derive a new class of unbiased DV estimators and to provide error bounds, (2) providing a connection to the results in <ref type="bibr" target="#b7">[7]</ref> via an asymptotic analysis, and (3) providing a connection to the theory of maximum likelihood estimators, thereby establishing asymptotic efficiency. From a practical point of view, we have shown empirically that the AKMV synopsis, in combination with our new unbiased estimators, provides superior accuracy, especially when estimating the number of distinct values in compound partitions. Moreover, because our methods require only a single hash function, constructing the synopsis is relatively inexpensive. We have shown how to combine synopses in order to handle compound partitions. Based on our experiments, we have also provided guidance in selecting a hash function, and have identified the AES hash function as a reasonably good choice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>end if 15: end for O(log D) bits of storage, and the required size of the KMV synopsis is O(k log D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>THEOREM 1 .</head><label>1</label><figDesc>The expected cost to construct a KMV synopsis of size k from a partition A comprising N data items having D distinct values is O(N + k log k log D) PROOF. The hashing step and membership check in lines 6 and 7 incur a cost of O(1) for each of the N items in A, for a total cost of O(N). To compute the expected cost of executing the remaining steps of the algorithm, observe that the first k DVs encountered are inserted into the priority queue (line 9), and each such insertion has a cost of at most O(log k), for an overall cost of O(k log k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Error bounds for D = 1, 000, 000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>THEOREM 4 .</head><label>4</label><figDesc>Let U 1 ,U 2 , . . . ,U n be a sequence of i.i.d. uniform[0,1] random variables, define W 1 ,W 2 , . . . ,W n as above, and fix k â¥ 1. Then lim nââ (nW 1 , nW 2 , . . . , nW k ) â (Y 1 ,Y 2 , . . . ,Y k ), where Y 1 ,Y 2 , . . . ,Y k are i.i.d. with each Y i having an Exp(1) distribution. PROOF. Let {Y n : n â¥ 1 } be an infinite sequence of i.i.d. Exp(1) random variables, and write W = (W 1 ,W 2 , . . . ,W k ) and Y = (Y 1 ,Y 2 , . . . ,Y k ). It follows from a well known result for order statisticssee [9, p. 134] or [24, p. 105-107] -that, for any fixed n â¥ k, we have W D = Y /S n , where S n = Y 1 + Y 2 + â¢ â¢ â¢ + Y n+1 and D = denotes equality in distribution. Since nW D = Y /(S n /n) and lim nââ S n /n = E[Y 1 ] = 1 with probability 1 by the strong law of large numbers, Slutsky's theorem [30, p. 19] implies that Y /(S n /n) â Y /1 = Y , and hence nW â Y , as n â â. Thus, for large D and fixed k â¤ D, the scaled spacings DW 1 , DW 2 , . . . , DW k are approximately i.i.d. Exp(1). Since</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>THEOREM 5 .</head><label>5</label><figDesc>Consider two partitions A and B, along with their KMV synopses L A and L B of sizes k A and k B , respectively. (For purposes of this discussion, we view the synopses as sets of hashed values.) We wish to estimate D âª = |D(Aâª m B)|, where, as before, D(S) denotes the set of DVs in multiset S. Observe that D(A âª m B) = D(A) âª D(B), so that D âª can also be interpreted as |D(A) âª D(B)|. Define L A â L B to be the set comprising the k smallest values in L A âª L B , where k = min(k A , k B ). Observe that the â operator is symmetric and associative. The set L = L A â L B is the size-k KMV synopsis of A âª m B, where k = min(k A , k B ). PROOF. For a multiset S with D(S) â D, write h(S) = { h(v) : v â D(S) }, and denote by G the set of k smallest values in h(A âª m 4 Recall that if n A (v) and n B (v) denote the multiplicities of value v in multisets A and B, respectively, then the multiplicity of v in A âª m B, A â© m B, and A \ m B are given respectively by n A (v) + n B (v), min n A (v), n B (v) , and max n A (v)n B (v), 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>then it follows immediately from our running assumption of no hash collisions that v â D(A). An analogous argument holds for partition B. Lemma 1 implies that v â D(A)â©D(B) if and only if h(v) â L A â© L B , and we can compute K â© from L A and L B alone. Observe that, under our random hashing model, V L can be viewed as a uniform random sample of size k drawn from D(A âª m B). The quantity K â© is a random variable that represents the number of elements in V L that also belong to the set D(A â© m B). It follows that K â© has a hypergeometric distribution: setting D â© = |D(A â© m B)| and D âª = |D(A âª m B)| as before, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Multiset difference is more complicated than union or intersection in that there are two possible, nonequivalent quantities to estimate: D * -= |D(A) \ D(B)| or D -= |D(A \ m B)|. The quantity D * -can be estimated as in Section 5.3 by taking E = D(A) \ D(B), and so we concentrate on the estimation of D -.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline: Accuracy vs. KMV Synopsis size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hashing Effect on the RDW Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Hashing Effect on the BASEBALL Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Accuracy Comparison for the RDW Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy Comparison for Union, Intersection, and Jaccard Distance on the RDW Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Real Dataset Characteristics</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Tables Total #Attributes # Tuples</cell></row><row><cell>OPIC</cell><cell>106</cell><cell>1802</cell><cell>27,757,807</cell></row><row><cell>BASEBALL</cell><cell>12</cell><cell>192</cell><cell>262,432</cell></row><row><cell>RDW</cell><cell>24</cell><cell>504</cell><cell>2,661,506</cell></row><row><cell>207</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Similar ideas appear in[8], where the synopses are called "signatures."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In the statistical literature, this estimator is called the method-ofmoments estimator of E[U (k) ].</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The space complexity of approximating the frequency moments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sys. Sci</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximating the number of unique values of an attribute without sorting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Astrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schkolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sys</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counting distinct elements in a data stream</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bar-Yossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RANDOM</title>
		<meeting>RANDOM</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward automated large-scale information integration and discovery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myllymaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirahesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reinwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sismanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Management in a Connected World</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="161" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Techniques for warehousing of sample data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards estimation error guarantees for distinct values</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Narasayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM PODS</title>
		<meeting>ACM PODS</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Size-estimation framework with applications to transitive closure and reachability</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sys. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="441" to="453" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining database structure; or, how to build a data quality browser</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shkapenyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="240" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nagaraja. Order Statistics</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithm 708; significant digit computation of the incomplete beta function ratios</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Didonato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Morris</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="373" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Loglog counting of large cardinalities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Eur. Symp. Algorithms (ESA 2003)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>11th Eur. Symp. Algorithms (ESA 2003)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2832</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bitmap algorithms for counting active flows on high speed links</title>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCOMM 02</title>
		<meeting>SIGCOMM 02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="323" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive sampling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Encyclopaedia of Mathematics, Supplement I. Kluwer</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hazewinkel</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic counting algorithms for data base applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computer Sys. Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="182" to="209" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tracking set-expression cardinalities over continuous update streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garofalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="369" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinct sampling for highly-accurate answers to distinct values queries and event reports</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating simple functions on the union of data streams</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tirthapura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp. Parallel Algorithms and Architecture</title>
		<meeting>ACM Symp. Parallel Algorithms and Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="281" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Order statistics and estimating cardinalities of massive data sets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Giroire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Analysis Algorithms</title>
		<meeting>Intl. Conf. Analysis Algorithms</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An estimator of the number of species from quadrat sampling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stokes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="135" to="141" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimating the number of classes in a finite population</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stokes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1475" to="1487" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Empirical evidence concerning AES</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hellekalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wegenkittl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Modelling Comput. Simulation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="322" to="333" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The history of histograms (abridged)</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balakrishnan</surname></persName>
		</author>
		<title level="m">Continuous Univeriate Distributions -2</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">A Second Course in Stochastic Processes</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">of The Art of Computer Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Addison-Wesley</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Sorting and Searching</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mersenne twister: A 623-dimensionally equidistributed uniform pseudorandom number generator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nishimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Modeling Computer Simulation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Randomized Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-dimensional clustering: a new data layout scheme in DB2</title>
		<author>
			<persName><forename type="first">S</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malkemus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cranston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="637" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Access path selection in a relational database management system</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Astrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Chamberlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lorie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Serfling</surname></persName>
		</author>
		<title level="m">Approximation Theorems of Mathematical Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Storage estimation for multidimensional aggregates in the presence of hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random Sampling with a Reservoir</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A linear-time probabilistic counting algorithm for database applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Vander-Zanden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Sys</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="208" to="229" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
