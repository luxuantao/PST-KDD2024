<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Runtime-Driven Shared Last-Level Cache Management for Task-Parallel Programs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abhisek</forename><surname>Pan</surname></persName>
							<email>pana@purdue.edu</email>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
							<email>vpai@purdue.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Runtime-Driven Shared Last-Level Cache Management for Task-Parallel Programs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Task-parallel programming models with input annotationbased concurrency extraction at runtime present a promising paradigm for writing parallel programs for today's multi or many-core heterogeneous systems. Through management of dependencies, data-movements, task assignments, and orchestration, these models markedly simplify the programming effort for parallelization while exposing higher levels of concurrency. In addition, the use of a runtime platform enables innovations in the hardware-software interface that allows the hardware to be highly responsive to the characteristics of the application and vice versa.</p><p>In this paper, we show that, for task-parallel applications running on multicores with a shared last-level cache (LLC), the concurrency extraction framework can be used to substantially improve the efficiency of the shared LLC. We develop a task-based cache partitioning technique that leverages the dependence tracking and look-ahead capabilities of the runtime. Based on the input annotations for future tasks, the runtime instructs the hardware to prioritize data blocks with future reuse and evict blocks with no future reuse. These instructions allow the hardware to preserve all the blocks for a subset of the future tasks, thus creating partitions for tasks rather than threads. This leads to a considerable improvement in cache efficiency over what is achieved by existing thread-centric cache management policies. Thread-centric cache management policies fail to track the complex patterns of data-reuse among tasks that can be assigned to arbitrary cores and hence replace blocks for all future tasks resulting in poor overall hit-rates. The proposed hardware-software technique leads to a mean improvement of 18% in application performance and a mean reduction of 26% in misses over an LRU-replacement based LLC for a set of input-annotated task-parallel programs using the OmpSs programming model implemented on the NANOS++ runtime. In contrast, the state-of-the-art thread-based partitioning scheme suffers an average performance loss of 2% and an average increase of 15% in misses over the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Current architectural trends of rising on-chip core counts and worsening power-performance penalties of off-chip memory accesses have made the shared last-level caches (LLC) one of the major determinants of multicore performance. Traditional thread-agnostic Least Recently Used (LRU)based cache replacement schemes have been found to be ineffective for shared LLCs since they are neither able to pre-vent the destructive interference of high-demand low-reuse threads on other threads, nor are they adept in handling thrashing or scan-type access streams <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Shared LLC management techniques for multicore processors have focussed on either multiprogramming workloads or multithreaded applications. Partitioning techniques for multiprogramming workloads have focussed on managing contention among applications through explicit partitioning of the cache among co-running applications for throughput or fairness improvement <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>. Proposals for replacement policy modification have concentrated on tuning the replacement policy for better prediction of future reuse of cached data, with separate parameter tuning for each application <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Researchers have also proposed dynamic partitioning policies for multithreaded programs with static thread assignments and mapping, with an aim to ensure balanced progress for all threads while optimizing throughput <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In this work we focus on shared LLC managment for an alternative model of concurrency management for parallel programs -task-based parallism. As the number of on-chip cores increase, runtime-managed task-based programming models have become an important vehicle for expressing parallelism. Through management of dependencies, task assignments, and orchestration, these models markedly simplify the programming effort for parallelization while exposing higher levels of concurrency. These task-based models are especially important for today's many-core heterogeneous chips because of their ability to simplify the effort for parallelization by relieving the programmer of complex scheduling, load-balancing, and data-movement considerations.</p><p>Recently researchers have proposed dependency-aware task-parallel models where the programmer specifies the input and output for each task through pragmas or code snippets, and the runtime uses this information to build the task dependency graph and schedule tasks for execution once the dependencies are resolved <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref>. These models further ease programming effort through automatic handling of synchronization and forcing deterministic execution, and at the same time improve performance by exposing higher levels of concurrency than what is usually extracted by the programmer <ref type="bibr" target="#b3">[4]</ref>. Additionally researchers have exploited the information tracked by the runtime to improve the efficiency of hardware optimizations such as prefetching and coherence for private caches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Figure <ref type="figure">1</ref> illustrates the two competing models to express parallelism. In the thread-based model, parallelism is ex-Figure <ref type="figure">1</ref>: Parallel programming models: thread-based vs. task-based pressed through a few concurrent long running threads (usually equal to the number of logical cores available), all of which synchronize at barriers. In contrast, execution in taskbased models proceeds through a series of relatively small units of concurrency called tasks. All tasks which are independent of each other can be executed in parallel as long as there are enough resources available.</p><p>While partitioning the ways of a shared LLC among threads have been shown to improve the performance for thread-based applications, such thread-centric partitioning techniques are not well-suited for task-parallel applications, since these schemes are effective only under the following two conditions:</p><p>? LLC associativity much higher than number of threads, in order to create imbalance of allocation among the threads, and ? Long-running pinned threads with substantial intrathread data reuse, in order to build per-thread datareuse models. In thread-centric partitioning models, cache blocks are tagged with the identity (id) of the thread or core that allocates the block. Since there is substantial intra-thread data reuse, data brought in by one thread can be reasonably expected to be reused by the same thread. Hence it makes sense to use thread-based partitions, where data-blocks allocated by one thread are protected at the expense of the blocks brought in by other threads. However, a thread-based partitioning paradigm does not work well for task-parallel programs running on higher number of on-chip cores with fine-grained tasks and dynamic task-core assignments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. Tasks have much shorter lifetimes than threads, and there is not much intra-task data-reuse at the LLC level. Hence effective management of cached data for task-parallel applications require tracking of inter-task data reuse and creating partitions that encompass multiple groups of tasks.</p><p>This paper presents a hardware-software technique to efficiently manage the shared LLC for applications using the dependency-aware task-parallel model on multicores. As mentioned above, in task-based applications, the dominant form of data-reuse at the LLC level is of the inter-task variety. Hence for task-based applications, in order to partition the cache space among tasks, inter-task reuse of data blocks needs to be tracked. For the dependency-aware task-parallel programming models, the runtime already tracks the intertask data-reuse among the tasks as part of dependency resolution. Moreover, when a data-block is allocated by a task, the runtime can identify which task will reuse the block in the future. Our technique leverages the reuse-tracking and look-ahead capabilities of the runtime in order to create a task-based partition for the LLC.</p><p>With multiple tasks simultaneously accessing the LLC, a large percentage of the data blocks are evicted before they can be reused by future tasks, often leading to poor utilization. Our technique address this inefficiency in two steps. First, when a data block is accessed by a task, the runtime communicates to the hardware the identity of the next task that is going to reuse the data-block. This allows the hardware to group the cache-resident data blocks by the identity of the task-ids that will reuse the data-block. Second, during victim selection for replacement, the replacement engine uses the task-data mapping to create partitions for future tasks -it attempts to retain all the blocks to be reused by a subset of the future tasks at the cost of evicting blocks belonging to other tasks. This considerably increases the cache utilization for the tasks in the preferred subset, which in turn leads to better LLC utilization overall, and hence reduced execution time. The runtime also detects blocks which will no longer be reused by any future tasks and instructs the hardware to de-prioritize these blocks.</p><p>The task-aware cache management framework is developed as an extension of the NANOS++ implementation of the OmpSs programming model <ref type="bibr" target="#b11">[12]</ref>. The framework does not require any change in the OmpSs API and hence is transparent to the application programmer. OmpSs is a taskbased programming model which requires the programmer to annotate each task with the data objects that the task is going to read from or write to. OmpSs provides compiler directives for creating these annotations. The runtime evaluates the annotations at task-creation time and builds a task-dependency graph based on these annotations. If a task is found to be dependent on a previously created task, it is added as a successor to the previous task. A task is scheduled for execution once all its dependencies are resolved. We extend the dependence-resolution framework of the runtime to record, for each created task, the mapping of data objects to the successor tasks who are going to use the objects next. At the start of execution of a task, the runtime communicates to the hardware this stored task-data mapping. This id of the future task that is going to use the block is stored with the tag of the block in the LLC. During replacement, if the replacement engine finds that all blocks in a set are tagged with future task-ids, it replaces the LRU block and considers the task-id of this block to be de-prioritized. So henceforth all blocks belonging to this task are replaced before any block belonging to another task. If no blocks belonging to the de-prioritized task is found in a set, another future task is de-prioritized. This allows at least some of the future tasks to preserve all their blocks in the cache. The extra hits obtained through this preservation usually outweighs the extra misses suffered by the de-prioritized tasks, leading to an overall improvement in cache efficiency and program performance. The runtime decides on the candidate tasks for prioritization based on the size of the data accessed by the tasks. Only the more prominent tasks (in terms of data used) are selected as candidate for prioritization.</p><p>Our hardware-software shared LLC management technique leads to a 18% increase in performance and a 26% reduction in misses over a LRU-replacement based LLC for a set of input-annotated task-parallel programs us-ing the OmpSs programming model implemented on the NANOS++ runtime.</p><p>The rest of the document is organized as follows: Section 2 provides a brief background on dependence-aware task-parallel models. Section 3 motivates the need for taskbased cache management approaches for task-parallel applications. The proposed hardware-software technique is discussed in Section 4. Section 5 outlines the experimental framework. Section 6 analyses the performance of the proposed technique. Section 7 discusses the implementation challenges. Section 8 reviews the related work, and finally Section 9 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DEPENDENCE-AWARE TASK PARAL-LELISM</head><p>Dependence-aware task-parallel models are a class of taskparallel models that extract concurrency among tasks at runtime based on programmer-provided inputs and outputs for each task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref>. Such a model substantially simplifies the programming effort required to parallelize a sequential application. All the programmer needs to do is to encapsulate sequential blocks of computation (such as functions) into tasks and specify the data to be used by each task. The input-output data can be specified in the form of compiler directives (OpenMP tasks <ref type="bibr" target="#b2">[3]</ref>, OmpSs <ref type="bibr" target="#b11">[12]</ref>), or code snippets (serialization sets <ref type="bibr" target="#b3">[4]</ref>). During program execution, the runtime thread first evaluates the data-specification clauses for the tasks it encounters in order to build a task-dependence graph. Tasks are inserted in the task-dependence graph in program order but are executed out of order. The task-dependence graph is used to schedule tasks for execution once all dependencies are resolved.</p><p>The OmpSs programming model works with C programs and allows the programmer to create tasks from functions or code blocks using the task directive. The Mercurium compiler interprets the task directives to create actual tasks. The NANOS++ runtime system is responsible for the actual execution of the application, and manages all aspects of execution such as thread-pool management, target-device management, dependence resolution, and scheduling. Listing 1 shows the skeleton code for an implementation of the Fast Fourier Transform (FFT) kernel in OmpSs. The task directive in OmpSs can be appended with in, out, inout, or concurrent dependence clauses to specify which data objects a task depends on (for example Listing 1, line 1). Currently the dependency clauses can efficiently express data objects ranging from simple variables to segments of multidimensional arrays <ref type="bibr" target="#b29">[30]</ref>. A multidimensional array segment, known as a region, represent a discontiguous region of memory made from a set of contiguous memory segments <ref type="bibr" target="#b29">[30]</ref>. Since the dependencies are computed at task creation time, dependencies can be tracked through pointer aliasing as well.  Internally, the runtime represents each region in a compact form. For 64-bit virtual addresses, a region is represented by an ordered sequence of digits such that each digit can be 0,1, or X(unknown). This in turn can be represented by a pair of 64 bit binary fields, called the value and the mask respectively. A one in the mask field denotes that the bit in the value field at the corresponding position is known, otherwise the bit-value is at that position is unknown, and the corresponding position in the value field is set to zero by convention (more details in <ref type="bibr" target="#b29">[30]</ref>). For example, if we consider a 2-dimensional 4x4 array represented in a 4-bit virtual address space (Figure <ref type="figure">2</ref>), a region that consists of two ranges &lt;0x2 -0x3, 0x6 -0x7&gt; can be denoted by the sequence 0X1X, which is equivalent to the &lt;value, mask&gt; pair of &lt;1010, 0010&gt;. The compact representation also allows for inexpensive membership tests -only a couple of operations, a bit-wise AND followed by an equality test, are required to check whether an address belongs to a region. We leverage the low storage and computation costs of storing a region and testing membership in the proposed cache management framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Specifying Data Dependencies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CACHE MANAGEMENT FOR TASK-PARALLEL PROGRAMS</head><p>A task is the smallest unit of concurrency in task-parallel applications. The execution of a task-parallel application proceeds through a series of possibly parallel tasks, the dependence among which are determined by the data-objects each tasks reads or writes. Once all dependencies of a task are resolved, it is scheduled for execution, and is executed by an idle thread among a pool of worker threads. A task is usually of much smaller duration than a thread and does not show appreciable data-reuse at the LLC level during execution. The dominant form of data-reuse in such programs are of the inter-task variety, and the data reuse patterns among tasks can be quite complex.</p><p>Thread-based dynamic cache partitioning policies attempt to partition the ways of the shared LLC among co-running threads in order to maximize throughput and achieve balanced progress for all threads <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. We find that such thread-based partitioning schemes are quite ineffective in improving the shared LLC efficiency for task-parallel applications. Figure <ref type="figure" target="#fig_3">3</ref> shows the misses due to three threadbased cache partitioning techniques for a 16 MB, 32-way LLC shared among 16 cores, relative to the baseline Global LRU replacement for a series of task-parallel applications (application details in Section 5). The STATIC policy statically partitions the cache ways equally among all threads.</p><p>The utility-based cache partitioning (UCP ) policy allocates space to each thread based on a runtime estimate of the marginal utility of the space to each thread with the goal of maximizing total cache throughput <ref type="bibr" target="#b31">[32]</ref>. The imbalancebased cache partitioning technique (IMB RR), designed specifically for symmetric multithreaded applications, creates temporary imbalance of allocation among threads to accelerate each thread in turn so that all threads are accelerated in the long run. Cache misses incurred by these schemes is seldom appreciably better than the the baseline and often worse ( up to 3.7X worse). On an average, the STATIC policy incurs 1.54X, the UCP policy incurs 1.31X, and the IMB RR policy incurs 1.15X times the misses incurred by the baseline. In contrast, the well-known OP-TIMAL replacement policy (due to Belady <ref type="bibr" target="#b6">[7]</ref>) incurs only 0.65X as many misses on an average as the baseline.</p><p>There are a few reasons for the inefficiency of threadcentric techniques for task-parallel applications. These schemes determine the per-thread allocations based on some form of a runtime model of allocation-vs-efficiency for each thread. Building this model requires each thread to be long-running and pinned to a specific core. Since tasks are short-lived and data referenced by a task running on a particular core can be reused by another task on a different core depending on scheduling considerations, such models are not meaningful in a task-based environment. Second, since major data-reuse at the LLC level is between tasks and not within a task, any runtime model needs to track data-reuse across groups of tasks and distribute the cache space across these groups of tasks. Intra-task data reuse with short reuse distances (due to spatial locality for example) is usually captured at lower levels of the cache hierarchy. Third, thread-based allocation models evaluate the allocation at pre-specified coarse-grain static interval of instructions (tens of millions of instructions), whereas allocation decisions for task-based applications should ideally happen at task-boundaries. Finally the patterns of inter-task data reuse are complex -for example, a single task can consume data generated by multiple producer tasks, individual portions of the data touched by a single task can be reused by different tasks, or a single data-object can be heavily reused by multiple tasks. The thread-based models cannot capture these complex reuse patterns.</p><p>As an example, Figure <ref type="figure" target="#fig_4">4</ref> shows the nature of inter-task data reuse for the two-dimensional FFT task-parallel application. Assume that the input matrix is broken into 16 disjoint chunks (d1 -d16), which are touched by tasks t1 -t14. Figure <ref type="figure" target="#fig_4">4a</ref> shows part of the task-dependency graph for the application. Figure <ref type="figure" target="#fig_4">4b</ref> shows the mapping of tasks to data. FFT proceeds in alternate stages (see Listing 1), with all tasks in each stage being independent of each other. In the first stage the trsp blk and trsp swap tasks (tasks t1 -t10) operate on the smallest chunks. A trsp blk task touches 1 chunk (t1 ? d1), and a trsp swap task touches 2 chunks(t2 ? d2, d5). In the second stage however (tasks t11 -t14), each fft1d task operates on a set complete rows (t11 ? d1, d2, d3, d4). Hence 1 fft1d task reuses data block from 4 tasks from the previous stage, and conversely chunks touched by 1 trsp swap task is used by two fft1d tasks (for example, blocks touched by t2 are used by t11 and t12). Thread-centric cache management techniques do not capture such reuse relationships among tasks.</p><p>Furthermore, the effectiveness of thread-based way- (b) Task-data mapping for FFT2D tasks. partitioning techniques reduces as the number of cores increase relative to the shared LLC associativity, since the number of possible partitioning configurations decrease. The goal of this work is to develop a cache management technique that captures the complex data-reuse patterns among tasks and partitions the cache accordingly. Referring to the FFT example, the tasks in the first stage (t1-t10) operate in parallel and attempt to bring the entire input matrix in the shared LLC. However if the input matrix, which represents the combined working set for all tasks, is too large to fit in the LLC, the baseline policy starts replacing the earliest blocks touched. This leads to a high percent of miss rates for all future tasks (t11-t14). In our scheme, when the first set of tasks (t1 -t10) touch the data-blocks, the runtime informs the hardware about the identity of the future tasks that would reuse the data blocks. For example, when the chunks in the topmost row are touched by tasks t1 -t4, they are marked to be touched by task t11 next. This information allows the partitioning engine to be smarter about which blocks to replace. The engine essentially partitions the cache for future tasks, so as to preserve all blocks belonging to a single task. By default, the partitioning engine tries to protect all blocks for every future task. However that is not possible if the working set is larger than the LLC capacity. Hence, at the time of replacement, if all the blocks in a set are found to be protected, the engine replaces the LRU block. This also means that the task that owns the LRU block is marked as low-priority. Since this task is marked as low-priority, all the data blocks for this low-priority task becomes candidates for replacement across all sets. If in another set, there are no blocks belonging to this low-priority task, then another task, which owns the LRU block in that set, is identified as low-priority. This implicitly creates a partition shared by a group of low priority tasks across all sets, while allowing the other tasks to remain at high priority and entirely preserve their data. Depending on the relative size of the working set and the LLC capacity, a subset of the tasks t11 -t14 will be protected and the protected tasks will enjoy very high hit-rates in the cache, thus increasing the overall hit-rate. In contrast, if global LRU replacement is used, all future tasks suffer from poor hitrates, which leads to inefficient cache usage. If the runtime determines that there are no future tasks that will reuse a data block, it instructs the hardware to consider such blocks as dead and hence candidates for immediate eviction. The future task-data mapping is updated by the runtime at the start of each task.</p><p>The runtime can also decide not to update the task-data mapping for future tasks that have small memory footprints. In that case the data blocks are marked belonging to a default task (a common task-id is used for all such blocks), and remain at a priority lower than the protected tasks but higher than any de-prioritized task. Only the more prominent tasks (in terms of data used) are selected as candidates for prioritization. This allows us to limit the overheads and achieve better performance by protecting only the tasks that have a high impact on application performance. For applications using matrix-vector computations, tasks that involve only vector-vector computations can be ignored since the memory footprint of these tasks are orders of magnitude smaller than that of the tasks that involve matrix-vector computations. In this work, the candidate tasks are chosen by the programmer and are communicated to the runtime through the priority directive available through the API. However it is possible to let the runtime select such tasks at runtime based on the relative size of the memory footprints of tasks. For applications which have only a single type of task (matrix multiplication) or comparable memory footprints for all tasks (parallel sort), the runtime considers all tasks as candidates for prioritization. In the following section, we describe in detail the hardware and software support required to implement the above technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HARDWARE-SOFTWARE SUPPORT</head><p>The objectives of our hardware-software cache management framework are twofold:</p><p>1. For any LLC-resident data block, communicate the identity of the future task(s) that will use the block to the LLC so that updated task-data mapping can be maintained. The runtime communicates the changes in task-data mapping to the hardware only at the start and end of a task execution. 2. Design an LLC replacement engine that uses the taskdata mapping to create task-based partitions that attempts to preserve data blocks for as many future tasks as possible. The priority levels of each live task needs to be tracked in order to create the partitions. In this section we describe the proposed modifications in the runtime, hardware-software interface, cache storage, and the replacement engine required to achieve these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Runtime Modifications</head><p>When a task is created, the dependence analysis engine of the NANOS++ runtime creates a unique id for the task and stores the data regions accessed by the task in a data structure called the region tree. Each region is tagged with the last writer task and the reader tasks of the latest produced value. The dependencies for a newly-created task can then be computed by comparing its data regions with the data-regions inserted in the tree by previous tasks. We ex-tend the state of this newly created task to store a mapping of data-regions accessed by this task to the id of the next future task(s) that will reuse these regions. A special taskid is used to represent the fact that no future task is going to use this block (henceforth called the dead task). The mapping is updated as the dependence engine adds future tasks to the tree and computes the dependencies. For example, Figure <ref type="figure" target="#fig_6">5</ref> shows a simple task-dependence graph that the runtime might generate. Assume that all tasks have a read-write relationship with the data regions. Task t2 is dependent on t1 through region d1; task t3 is dependent on t2 through d1, and on t1 through d2. When t1 is created, regions d1, d2 are mapped to the dead task. As tasks t2 and t3 are added, the task-data region mapping for t1 is updated.</p><p>When this task starts execution, the runtime informs the hardware about the data-blocks and the associated future tasks. The runtime sends this information if the memory footprint of the future task is prominent enough to warrant protection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interfacing with Hardware</head><p>As discussed in Section 2.1, a data region is represented in the OmpSs in a very compact form, through two 64 bit fields, which can represent a set of non-contiguous virtual addresses. Hence the runtime can communicate a task-region pair using two 64-bit fields for region id and a 32 bit field for a task-id. Accordingly, we propose a simple instruction set extension in the form of a memory-mapped interface with specific user-level commands. The fields to communicate for one data region are as follows:</p><p>? value (64-bit)</p><p>? mask (64 bit)</p><p>? software task-id (32 bit)</p><p>? group-id(1 bit) A small per-core hardware engine translates the software task-id to a hardware task-id and stores this mapping in a Task-Region Table and determines the future task-id of each memory access instruction. Task-id translation mechanism and the purpose of the group-id is explained in Section 4.3. This table is flushed and updated by the runtime at the start of each task. Each memory access instruction during the task execution does a lookup of this table to identify the task-id that the address belongs to. The membership test for an address requires two bitwise logical operations. If no future task-id is found, a special default task-id is assumed. The number of entries in this table is equal to the number of tasks a particular task depends on, and only a few entries are needed. With the use of composite taskids (described in 4.3), we find that 16 entries per core is more than enough. This setup is similar to the well-known programmable memory-access-interception frameworks proposed for transparent memory access monitoring, debugging, or dynamic optimizations <ref type="bibr" target="#b36">[37]</ref>.</p><p>Once a task-id is obtained the task-id is carried with the memory transaction and stored in the memory hierarchy. If the access is a miss in lower-level (L1) cache, the futuretask id is sent to the LLC as part of the miss request, and is updated in the tags of the LLC. If the access is hit in the lower-level cache, the task-id is compared with the last task-id. If the ids are found to be different, an id-update request is sent to the LLC to update the tag of the block with the new owner.</p><p>At the end of a task, the runtime informs the hardware that the task with that software task-id has finished execution. This allows the hardware to update the status of the corresponding hardware task-id to Not-Used (task status values are described in Section 4.4), and also free the hardware task-id for recycling. Figure <ref type="figure" target="#fig_7">6</ref> tabulates the commands sent by the runtime at different stages of execution for the example shown in Figure <ref type="figure" target="#fig_6">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hardware-Software Task Translation</head><p>The runtime communicates the tasks-data mapping to the hardware in terms of the unique task-id that it has determined. While these software task-ids can be directly used in the hardware framework, we propose a translation framework between the software and hardware task-ids (HW-SW Task Map) because of two reasons. First, software task-ids are monotonically increasing and are equal to the number of tasks created throughout the application. However at a particular time, only a limited number of tasks are active from the perspective of our framework (currently executing tasks, and the tasks that are directly dependent on these currently executing tasks). So a HW-SW task mapping maintained at the LLC level allows us to recycle hardware task-ids and limit the bit-budget of storing a task-ids. So before the runtime updates the Task-Region Table, the hardware sends a request to a centralized engine to obtain a hardware task-id. If a hardware id has been already assigned to the requested software id, or a free hardware id is available for allocation, the Task-Region Table is updated, otherwise the update is skipped. The runtime requests to release the hardware-id corresponding to the software id at the end of each task. Figure <ref type="figure" target="#fig_8">7</ref> shows when and where hardware tasks are mapped to software tasks, and also lists the active hardware taskids required during the execution for the example shown in Figure <ref type="figure" target="#fig_6">5</ref>.</p><p>Second, hardware task-ids allow us to effectively deal with multiple reader tasks. In most cases a data-region touched by a task has a clearly identifiable single next user task (all cases of WAW, WAR dependencies, and most cases of RAW dependencies). However if a data object written by a task is read by multiple tasks which are independent of eachother, the object has multiple future users, which can all proceed in parallel (Figure <ref type="figure" target="#fig_9">8</ref>). Hence the object should be preserved as long as any of these multiple protected tasks are high-priority. Also, the the id change from the multiple tasks (t1, t2, t3 in the figure) to the next task (t5) should happen only after each of t1, t2, and t3 has used the data. In order to achieve these goals, we assign a composite task id to this region, and the mapping between composite taskid to its constituent tasks is maintained in the LLC level. The priority of the block is determined to be the highest of all tasks that own the block. Also the ownership transition from a composite task to the next task happens only when all constituent tasks are released. The group-id that was mentioned as a part of the interface is used to identify groups of tasks for a single region that make up a composite task. A data-region with a group-id of 0 signifies that there are more tasks for this particular data-region. A group-id of 1 implies the end of a group of task for a region. In the common case of a single task for a region, the group-id is always 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Last-Level Cache Modifications</head><p>At the LLC level the future task-ids are stored along with the cache tags. There are two special task-ids, the dead task, and the default task. The partitioning engine maintains a table, indexed by the task-id, called the Task Status Table, maintaining the status of the task-ids. A task-id can be in  the following states: 1. High-Priority: The data blocks that belong to this task-id are protected and would not be replaced unless necessary, 2. Not-Used: The task-id is not in use, so the blocks belonging to this task-id will be replaced after lowpriority blocks but before high-priority blocks, 3. Low-Priority: At least one block belonging to the task block has been replaced already, and blocks belonging to this tasks are first candidates for replacement. Hence 2 bits per task-id are required to represent the task status. A hardware task-id can be used to represent either a single task-id or a composite task-id. Hence a third bit is required to identify if the task-id is a composite id. For a non-composite task-id, the status is read directly from the Task-Status Table . Otherwise the composite Task-Status Map is read to find the tasks that the composite task-id belongs to, and the highest priority among all member tasks (again read from the Task-Status Table ) is chosen as the block priority. Figure <ref type="figure" target="#fig_10">9</ref> shows a system-level view of the proposed framework, with per-core Task-Region Tables and the Task Status Table at the LLC level.</p><p>Algorithm 1 describes the victim selection algorithm used by the replacement engine. The replacement policy is still LRU-based but is modified such that the replacement engine chooses the LRU block based on the following overriding priority order (most-likely to least-likely to be replaced): blocks belonging to the dead task, blocks belonging to the low-priority task, blocks not tied to any task(default-task) or blocks with task-ids which are not being used, and blocks belonging to the high-priority task . So all blocks with the lowest priority status will be replaced before any higherpriority block, and within the same priority group, the LRU block will be replaced first.</p><p>When a block belonging to a high-priority task is replaced, the task priority in the Task-Status Table changed to a low-priority task. For a block with a composite task-id, if all the constituent tasks are high-priority, then a randomly chosen task from the group is downgraded. This is the key step that creates the partitioning implicitly, since once a task is downgraded, its blocks will be replaced from all sets, until there is a set where no such blocks are found. Then another task will be downgraded. This creates a common partition for all downgraded tasks, while letting the other tasks preserve their data. The number of tasks that are downgraded are not controlled explicitly, but is decided automatically based on the size of the working set relative to the cache capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION FRAMEWORK</head><p>We evaluate the performance of our hardware-software LLC management technique using the GEMS executiondriven full-system multiprocessor simulator <ref type="bibr" target="#b24">[25]</ref>. GEMS is a detailed timing simulator for the memory hierarchy that uses the Simics full-system simulator as its functional simulation engine <ref type="bibr" target="#b13">[14]</ref>. We model a multicore chip with a highlyassociative shared last-level L2 cache with private L1 caches. Table <ref type="table" target="#tab_3">1</ref> lists the relevant system parameters. We modify the Perfect-regions dependence plugin supplied with the Nanos++ runtime to implement the sofware hints frameowrk, and use the default breadth-first scheduler in our experiments. Workloads: We use the following seven task parallel applications obtained from the the OmpSs application repository <ref type="bibr" target="#b0">[1]</ref>. After warming up the cache until the start of execution of the first batch of tasks, we run the benchmarks to completion.We parallelize the input initialization where appropriate.</p><p>1 where at each stage the input is split into quarters and sorted in parallel, and then merged back in pairs. Quicksort is used for the smallest chunks. We use 4K integers as input with chucks of 256 elements for each task.</p><p>6. Gauss-Siedel(Heat): Iterative heat distribution solver using the 5-point Gauss-Seidel algorithm. The input matrix consists of 2048 X 2048 double precision elements.  We compare the performance of the proposed Task-Based Partitioning technique(TBP ) with other hardware-based shared cache partitioning techniques. The STATIC policy is the simplest partitioning policy that statically partitions the cache ways equally among all cores / threads. Utilitybased Cache Partitioning(UCP ) is a well-known dynamic cache partitioning technique for multiprogramming workloads. UCP policy allocates space to each thread based on a runtime estimate of the marginal utility of the space to each thread with the goal of maximizing total cache throughput <ref type="bibr" target="#b31">[32]</ref>. It is expected to maximize the overall hit-rate of the cache, but not necessarily ensure balanced progress. The imbalance-based cache partitioning technique (IMB RR) is a recent dynamic scheme that creates temporary imbalance of allocation among the threads of a parallel application to accelerate each thread in turn so that all threads are accelerated in the long run <ref type="bibr" target="#b26">[27]</ref>. It uses round-robin policy of thread-prioritization to accelerate all threads. This technique also has the ability to turn off partitioning and use the baseline LRU policy, if partitioning does not provide any benefit. We also evaluate the performance of a wellknown replacement-policy modification technique -the dy-namic re-reference interval prediction (DRRIP) method proposed by Jaleel et al. <ref type="bibr" target="#b19">[20]</ref>. DRRIP is a modification of NRU replacement policy, which aims to make the replacment policy both scan and thrash-resistant. A policy change from SRRIP (static RRIP) to BRRIP (Bimodal RRIP) is effected when the policy selection counter shows a bias of 1024 for one policy over another. Figure <ref type="figure" target="#fig_12">10</ref> shows the relative performance of the STATIC, UCP, IMB RR, DRRIP, and the proposed TBP techniques relative to an unpartitioned cache (using a thread-agnostic LRU policy) for a 32-way 16 MB shared LLC. The x-axis shows the benchmarks. The Yaxes for 10a show relative performance (higher is better) and for 10b show relative miss-rates (lower is better). On average, usr the TBP technique leads to a 18% improvement in performance and 26% reduction in cache misses over the baseline. DRRIP achieves better performance than the baseline, with a 5% speed up and 13% reduction in misses. Other thread-based partitioning schemes actually perform worse than the baseline in average. The STATIC policy suffers from a 27% drop in performance with 54% increase in misses with respect to the baseline. The corresponding numbers for UCP and IMB RR are 11% performance loss with 31% miss increase, and 2% performance loss with 15% increase in misses respectively. IMB RR performs best among the competing partitioning techniques since it has the ability to turn off partitioning all-together if it determines partitioning to be harmful. As expected, TBP achieves very little performance gain for matrix multiplication because of the compute-intensive nature of the application. For Heat, TBP suffers a performance loss compared to UCP and IMB RR despite a miss-rate reduction because the application can not recover from the temporary imbalances create in the task performance due to task-prioritization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">IMPLEMENTATION OVERHEAD</head><p>The major overhead of the proposed technique comes from maintaining an updated task-to-block mapping. We used 8bit task-ids, so that the hardware has 256 task-ids that can be recycled. The core-level Task-Region Table has <ref type="bibr">16 20</ref> byte entries, which results in a total space overhead of 5KB over 16 cores. For 256 tasks, the Task-Status Table of 256 entries has a total overhead of less than 128 bytes. We also note that the overhead of hardware-software task mapping can be obviated if the dependency resolution is implemented in hardware, as proposed by Etsion et al. <ref type="bibr" target="#b14">[15]</ref>. The LLC tags carry task-ids, but for thread-based partitions the tags carry thread-ids which would be 4-bits for 16 cores. On the other hand the proposed scheme does not incur any overheads for creating runtime models or dynamically computing allocations unlike other thread-based partitioning techniques. For example the UMON circuits used in the UCP technique incur 2KB storage per-core, adding up to 32 KB for 16 cores. UCP also runs a greedy algorithm at pre-specified intervals of instruction to compute the partition sizes. IMB RR scheme does not have shadow monitors, use a hardware program phase detection policy to adapt to change in program phases, and needs to repeatedly re-partition the cache space to find the best configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Shared Cache Partitioning</head><p>The problem of partitioning a shared cache among multiple concurrently executing threads has received considerable attention in the community. The partitioning techniques can be categorized under two groups -one targeting the multiprogramming environment (one application per core), the other targeting multithreaded programs (single application using all cores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Multi-programming Workloads</head><p>The idea of minimizing the over-all miss rate for all competing threads in a multi-programming workload has been the focus of several partitioning techniques <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>. A partitioning scheme that always allocates space to a thread which has maximum marginal utility for that space can minimize the over-all miss rate. If the marginal utility for each thread decreases monotonically, then the optimal partitioning can be achieved by simply continuing to allocate one unit at each iteration to the thread which has the maximum utility for it, till there are no free units left <ref type="bibr" target="#b34">[35]</ref>. For realistic applications with no such monotonicity in their utilization behavior, this greedy algorithm has been modified to obtain near-optimal algorithms.</p><p>Qureshi and Patt propose the use of auxiliary tag directories and hit counters for each thread to track its cacheutilization behavior at runtime <ref type="bibr" target="#b31">[32]</ref>. Cache ways are partitioned among threads using this information, in order to maximize the combined utility for all threads. The authors propose an algorithm that relaxes the requirement of allocating only one way in each iteration. At each iteration the algorithm greedily finds the thread that has maximum utility considering all free ways and the minimum number of ways that are needed to achieve that utility. The winning thread is then allocated the minimum number of ways.</p><p>Suh et al. <ref type="bibr" target="#b35">[36]</ref> compute the marginal utility of each thread from the actual cache itself by counting the recency positions of the hits for each thread into the shared cache. The scheme does not incur the overhead of per-thread shadow tags, but suffers form inaccuracy in the marginal utility since the utility obtained for each thread is affected by the behavior of all other threads sharing the main cache. They employ another modification to Stone's algorithm where the marginal utility curve for each thread is broken into piece-wise monotonically decreasing regions, and Stone's algorithm is invoked for each combination of non-convex points in the curves, and finally the best partition is chosen from all the candidates. Jaleel et al. proposes a shared cache management technique which partitions the cache implicitly by choosing a specific insertion policy for each of the competing applications based on its memory access behavior <ref type="bibr" target="#b18">[19]</ref>. The key insight behind this scheme is that for a reference stream that has a working set larger than the cache capacity, cache utilization can be improved by increasing the lifetime of some cache blocks beyond that allowed by a traditional LRU scheme <ref type="bibr" target="#b30">[31]</ref>. To this end, the authors propose the Bimodal Insertion Policy (BIP) replacement method, where most of the incoming blocks are inserted in the LRU position instead of the MRU position, the default position for traditional LRU-based schemes. As a result, any block that reaches the MRU position gets the opportunity to live in the cache for more time than it could in an aging-based scheme like LRU. For the rest, the MRU insertion position is maintained, so that the aging-based replacement is not eliminated completely. For multi-programming workloads, the BIP method would work best for the applications whose working sets could not be accommodated in the shared cache, whereas the LRU scheme would be suitable for applications with high temporal locality and small working sets. Hence, a portion of the shared cache sets is used to choose between the BIP and LRU policies for each thread in the cache at runtime and the chosen thread-aware replacement policy is enforced for the rest of the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Multi-threaded Workloads</head><p>Muralidhara et al. investigate the problem of partitioning a shared L2 cache among the threads of a multi-threaded application <ref type="bibr" target="#b25">[26]</ref>. The authors proposed a dynamic partitioning scheme that focuses on making the slowest running thread (critical path thread) faster so that the application becomes faster <ref type="bibr" target="#b25">[26]</ref>. The proposed technique involves dividing the entire execution time into equally spaced intervals of dynamic instruction count, computing the IPC values after each interval, and allocating more cache space to the slowest thread. A record of IPC values vs. cache sizes is maintained for all past intervals. Cubic spline interpolation is used on the recored data points in order to predict the change in IPC for additional cache space allocation. Pan and Pai propose an imbalance-based partitioning scheme for symmetric multithreaded programs <ref type="bibr" target="#b26">[27]</ref>. The authors show that the memory reuse behavior of each thread in these programs is symmetric, and, in most cases, non linear. They exploit the non-linearity by creating high levels of imbalance in thread allocations, such that one thread at a time can accelerate by securing large share of the cache at the expense of all other threads. Overall performance improvement is secured by prioritizing each thread in a round-robin fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Software-assisted Cache Management</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Software Hints</head><p>A key component of our cache management technique for task-based applications is the framework that allows the runtime to control the shared LLC replacement by informing the hardware about the tasks which are future consumers of the data blocks. In recent times, hardware vendors have allowed software to influence the replacement policy through appropriate hints, such as non-temporal access hints in Intel processors, or target cache-level specifications for allocating cache blocks in the Itenium architecture. Researchers have explored compiler and profile-based techniques to improve cache utilization through these hints <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Wang et al. propose a software replacement hint in the form of an evict-me bit, which when set for a block (through extended load/store instructions) makes it the most likely candidate to be replaced. The authors develop a static compiler-level analysis to obtain a rough estimate of reuse distances of blocks for loop-based programs, and set the evict-me bit for the blocks with reuse distance greater than the cache size <ref type="bibr" target="#b37">[38]</ref>.</p><p>Beyls and D'Hollander develop profile-based and analytical techniques to generate target cache hints for loop-based applications running on the IA64 architecture <ref type="bibr" target="#b7">[8]</ref>. Target cache hints inform the hardware about the highest(fastest) level of cache a particular memory access is expected to obtain reuse hits from. The hardware uses these hints to guide allocation and promotion decisions for the caches. First, the authors estimate the reuse distance histogram for all accesses from each memory access instruction through profiling and extrapolation and generate static hints (hints that remain constant for all accesses due to an instruction). Second, for applications conforming to the polyhedral model, the authors develop equations to generate forward reuse distance estimations for accesses based on program parameters and use these to insert extra code which dynamically generates the hints for different instances of the same instruction. Brock et al. propose a similar profile-based method to identify accesses with high OPT distances (forward reuse distances for accesses under the optimal replacement policy) and annotate these accesses with MRU replacement hints during the compile phase <ref type="bibr" target="#b9">[10]</ref>. The motivation of using OPT-distance over forward reuse distance stems from the fact that for cyclic access-patterns with a period larger than the cache capacity, all accesses have reuse distance greater that the capacity and hence are candidates for MRU eviction, whereas an optimal replacement policy would have retained some of them. A static memory reference inside a loop can lead to many dynamic accesses with differing OPT distances. To address this issue, the authors present an analysis to group accesses by OPT distances in loop-based programs, and performs loop splitting to enable the compiler insert appropriate hints to these separate groups. The analysis is profile-driven. First OPT-distances are measured for all dynamic accesses, and then they are grouped by static references. For each static references the authors aim to find patterns of OPT-distances across the loop iterations. Two distinct patterns are found to be dominant -for some iterations, the distances are bounded (spatial-locality), and for others they increase linearly with iteration number (temporal locality due to cyclic access patterns for loops). The authors use automatic grid-regression method to learn this patterns from training inputs. They also find that, for linear patterns, the offset depends on loop size/input size, and the slope depends on loop-shape. This enables OPT-distance prediction for different inputs sizes. Then, based on number of static references in the loop, input size and cache size, loop-splitting is performed. These profile-based or analytical estimations of reuse distance are reasonably accurate for single threaded loop-based applications, but do not work well for parallel applications because actual shared reuse distance values diverge from the predicted values due to interference from co-running threads or tasks, and the indeterminism in access interleaving.</p><p>Sandberg et al. explore the effects of identifying and eliminating non-temporal accesses in a multicore multiprogramming environment <ref type="bibr" target="#b33">[34]</ref>. The authors develop a profile-based approach to identify memory access instructions whose data is never reused during its lifetime in the cache hierarchy as non temporal accesses through offline analysis of reuse distance profiles. These non-temporal accesses are installed only in L1 cache but not in the outer level-caches. A nontemporal access instruction is identified by the following criteria: at least one access with reuse distance greater than capacity and the number of accesses that reuse data within a distance range between L1 capacity and L2 capacity is smaller than a threshold. If an L1 data block is set as nontemporal it remains so until it is evicted from the L1 cache, even if it is reused through some other instruction, it remains non-temporal. Due to this stickiness of the non-temporal status, the above-mentioned condition must also hold for any memory access instruction that reuses the same data through the L1 cache. Confining non-temporal data to the private L1 caches allows for better utilization of the shared outer level caches. Rus et al. proposed profile-based techniques to selectively use non-temporal access instructions for string operations with poor reuse behavior to improve cache utilization for datacenter applications <ref type="bibr" target="#b32">[33]</ref>. Yang et al. explored the benefits of using non-temporal accesses to reduce the cache pollution effects of zero-intializtions in virtual machine-managed applications <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Software Cache Partitioning</head><p>Lu et al. partition the cache space among heap and global objects allocated by an application in order to segregate objects with poor data locality from the rest of the objects <ref type="bibr" target="#b22">[23]</ref>. This improves whole-program locality. Data locality signatures of individual objects are collected through profiling runs with training inputs, and are used to predict the locality behavior for the test inputs. Partitioning is done at the start of a run, using the stored locality information, run parameters, and cache configuration. Page coloring is used to partition the cache in software.</p><p>Ding et al. implement a library for user-level allocation of cache space that is implemented through the well-known page-coloring based cache-set partitioning method <ref type="bibr" target="#b10">[11]</ref>. The library allows users to allocate private/shared space for specific data-structures and threads in the cache. Design of the allocation policy is left to the programmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Runtime-driven Architecture Optimizations</head><p>The runtime for task-parallel applications have been used by researchers to guide architectural optimizations for multicores. Papaefstathiou et al. develop a runtime-guided prefetch engine that can be used to prefetch data blocks to be accessed by future tasks for multicores with private caches. The authors also partition each cache between the data blocks belonging to the current and future tasks in order to prevent the prefetched data from polluting the caches <ref type="bibr" target="#b27">[28]</ref>. Manivannan and Stenstrom propose using the runtime to guide coherence optimizations such as downgrading and self-invalidation in order to improve performance of task-parallel applications <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSION</head><p>In this paper, we proposed and evaluated a hardwaresoftware technique to partition a shared last-level cache among the tasks of a task-parallel application. We show that current thread-based partitioning techniques are ineffective in improving the efficiency of shared LLC for task-parallel applications, since they fail to track complex data-reuse patterns present among short-lived tasks and also to adapt to the dynamism of task-core assignments. Instead we design a scheme based on the ideas of using the runtime to map cache-resident task blocks to the tasks that are going to reuse them in future, and directing the replacement engine to preserve data blocks for as many future tasks as possible. The scheme also identifies blocks that are no longer going to be used in future and flags them for early eviction. On average,the proposed technique achieves 10% increase in application performance and 26% reduction miss-rate over a LRU-based unpartitioned cache for task-parallel applications through improved utilization of the cache space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 : 2 -</head><label>22</label><figDesc>Figure 2: 2-dimensional 4x4 array repesented in row-major order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>38 } 39 /FFT1D 2 (Listing 1 :</head><label>383921</label><figDesc>/ Second FFT round 40 f o r ( long J =0; J&lt;N SQRT ; J+=FFT BS ) 41 N SQRT , FFT BS , &amp;A[ J ] [ 0 ] ) ; 42 // Transpose 43 f o r ( long JJ =0; JJ&lt;N SQRT ; JJ+=TR BS ) { 44 t r s p b l k (N, N SQRT , TR BS , &amp;A[ JJ ] [ JJ ] ) ; 45 f o r ( long J=JJ+TR BS ; J&lt;N SQRT ; J+=TR BS ) 46 t r s p s w a p (N, N SQRT , TR BS , &amp;A[ JJ ] [ J ] , &amp;A [ J ] [ JJ ] ) ; Skeleton code snippet for FFT implementation in OmpSs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LLC Misses for thread-based partitioning techniques applied to task-based applications running on 16 cores sharing a 32-way 16 MB LLC.</figDesc><graphic url="image-2.png" coords="5,97.53,293.57,180.45,73.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Inter-task data reuse for FFT2D application. A single task can consume data generated by more than one tasks, vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Task-dependence graph and task-data mapping. The circles show the task-ids subscripted with taskcreation time. White boxes show data regions touched by a task, grey boxes show how task-data mapping changes as more tasks are created. t? denotes the dead task ie. no task is going to use the data in future.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hardware-software communication wrt Figure 5. The runtime sends the new task-data map at the start of execution of each task, and requests release of the software task-id at the end of execution of the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Hardware-software task-id translation wrt Figure 5. t1, t2, t3 are software task-ids and x1 and x2 are hardware task-ids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Task-dependence graph for multiple readers. All three tasks t2, t3, t4 are future users of region d1 after it is touched by t1. Also all of them have t5 as the next future task-id. Hence t2, t3, t4 are mapped to a composite task-id in hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Task-based cache management framework. Additional structures are highlighted in grey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Heat MEAN Misses rela/ve to baseline LRU STATIC UCP IMB_RR DRRIP TBP (b) Cache misses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Relative performance of STATIC, UCP, IMB RR, DRRIP, and TDP schemes for 16 MB Cache, normalized to the baseline unpartitioned LRU cache (32-way associative, shared by 16 cores).</figDesc><graphic url="image-9.png" coords="10,95.47,310.51,181.35,91.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>System Parameters.</figDesc><table><row><cell>Number of Cores</cell><cell>16</cell></row><row><cell>Cache Line Size</cell><cell>64 bytes</cell></row><row><cell>L1 Cache Associativity</cell><cell>4</cell></row><row><cell>L1 Cache Size</cell><cell>256KB</cell></row><row><cell>L2 Cache Associativity</cell><cell>32</cell></row><row><cell>L2 Cache Size</cell><cell>16 MB</cell></row><row><cell>L2 Cache Request Latency</cell><cell>4 cycles</cell></row><row><cell>L2 Cache Response Latency</cell><cell>4 cycles</cell></row><row><cell>Coherence Protocol</cell><cell>MESI directory</cell></row><row><cell>Frequency</cell><cell>1 GHz</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://pm.bsc.es/projects/bar/wiki/Applications" />
		<title level="m">Barcelona Supercomputing Center bsc application repository</title>
		<imprint>
			<date type="published" when="2015-04-02">2015-04-02</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.threadingbuildingblocks.org" />
		<title level="m">Intel Corporation intel threading builing blocks</title>
		<imprint>
			<date type="published" when="2015-03-21">2015-03-21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="//www.openmp.org/mp-documents/OpenMP4.0.0.pdf" />
	</analytic>
	<monogr>
		<title level="j">OpenMP Application Programming Interface</title>
		<imprint>
			<biblScope unit="page" from="2015" to="2019" />
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
	<note>Version 4.0, howpublished =http</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Serialization sets: A dynamic dependence-based parallel execution model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;09</title>
		<meeting>the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The design of openmp tasks. Parallel and Distributed Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Copty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoeflinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massaioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Teruel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="404" to="418" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Legion: Expressing locality and independence with logical regions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Slaughter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC &apos;12</title>
		<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis, SC &apos;12<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtual-storage computer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Syst. J</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966-06">June 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating cache hints for improved program efficiency</title>
		<author>
			<persName><forename type="first">K</forename><surname>Beyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hollander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Archit</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="223" to="250" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cilk: An efficient multithreaded runtime system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Joerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kuszmaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP &apos;95</title>
		<meeting>the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP &apos;95<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pacman: Program-assisted cache management</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Symposium on Memory Management, ISMM &apos;13</title>
		<meeting>the 2013 International Symposium on Memory Management, ISMM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ulcc: A user-level facility for optimizing shared cache performance on multicores</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;11</title>
		<meeting>the 16th ACM Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ompss: A proposal for programming heterogeneous multi-core architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Druan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aygude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martinell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martorell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Planas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="173" to="193" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving cache management policies using dynamic reuse distances</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;12</title>
		<meeting>the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Full-system simulation from embedded to high-performance systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engblom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aarno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processor and System-on-Chip Simulation</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Leupers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</editor>
		<meeting>essor and System-on-Chip Simulation</meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="25" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Task superscalar: An out-of-order task pipeline</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cabarcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2010 43rd Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<date type="published" when="2010-12">Dec 2010</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quality of service shared cache management in chip multiprocessor architecture</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Communist, utilitarian, and capitalist cache policies on cmps: caches as a shared resource</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int&apos;l Conf. Parallel Architectures and Compilation Techniques, PACT &apos;06</title>
		<meeting>15th Int&apos;l Conf. Parallel Architectures and Compilation Techniques, PACT &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cqos: a framework for enabling qos in shared caches of cmp platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Annual Int&apos;l Conf. Supercomputing, ICS &apos;04</title>
		<meeting>18th Annual Int&apos;l Conf. Supercomputing, ICS &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for managing shared caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sebot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. Parallel Architectures and Compilation Techniques, PACT &apos;08</title>
		<meeting>17th Int&apos;l Conf. Parallel Architectures and Compilation Techniques, PACT &apos;08</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="208" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (rrip)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on Computer architecture, ISCA &apos;10</title>
		<meeting>the 37th annual international symposium on Computer architecture, ISCA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cache replacement based on reuse-distance prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Design, 2007. ICCD 2007. 25th International Conference on</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fair cache sharing and partitioning in a chip multiprocessor architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int&apos;l Conf. Parallel Architectures and Compilation Techniques, PACT &apos;04</title>
		<meeting>13th Int&apos;l Conf. Parallel Architectures and Compilation Techniques, PACT &apos;04<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
	<note>IEEE CS</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Soft-olp: Improving hardware cache performance through software-controlled object-level partitioning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architectures and Compilation Techniques, 2009. PACT &apos;09. 18th International Conference on</title>
		<imprint>
			<date type="published" when="2009-09">Sept 2009</date>
			<biblScope unit="page" from="246" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Runtime-guided cache coherence optimizations in multi-core architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Manivannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel and Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2014-05">2014. May 2014</date>
			<biblScope unit="page" from="625" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multifacet&apos;s general execution-driven multiprocessor simulator (gems) toolset</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Intra-application cache partitioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Muralidhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2010 IEEE Int&apos;l Symp. Parallel &amp; Distributed Processing (IPDPS)</title>
		<meeting>2010 IEEE Int&apos;l Symp. Parallel &amp; Distributed essing (IPDPS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-04">Apr. 2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imbalanced cache partitioning for balanced data-parallel programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-46</title>
		<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-46<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="297" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prefetching and cache management using task lifetimes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papaefstathiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Katevenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Nikolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pnevmatikatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International ACM Conference on International Conference on Supercomputing, ICS &apos;13</title>
		<meeting>the 27th International ACM Conference on International Conference on Supercomputing, ICS &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A dependency-aware task-based programming environment for multi-core architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008-09">2008. Sept 2008</date>
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
	<note>Cluster Computing</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Handling task dependencies under strided and aliased references</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Supercomputing, ICS &apos;10</title>
		<meeting>the 24th ACM International Conference on Supercomputing, ICS &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th annual Int&apos;l Symp. Computer Architecture, ISCA &apos;07</title>
		<meeting>34th annual Int&apos;l Symp. Computer Architecture, ISCA &apos;07</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="381" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th Ann</title>
		<meeting>39th Ann</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
	<note>IEEE CS</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated locality optimization based on the reuse distance of string operations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Code Generation and Optimization (CGO), 2011 9th Ann</title>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing cache pollution through detection and elimination of non-temporal memory accesses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ekl?v</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, SC &apos;10</title>
		<meeting>the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, SC &apos;10<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimal partitioning of cache memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1054" to="1068" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic partitioning of shared cache memory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="7" to="26" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memtracker: Efficient and programmable support for memory access monitoring and debugging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA &apos;07</title>
		<meeting>the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA &apos;07<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="273" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using the compiler to improve cache replacement decisions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Weems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;02</title>
		<meeting>the 2002 International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;02<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">199</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Why nothing matters: The impact of zeroing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications, OOPSLA &apos;11</title>
		<meeting>the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications, OOPSLA &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="307" to="324" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
