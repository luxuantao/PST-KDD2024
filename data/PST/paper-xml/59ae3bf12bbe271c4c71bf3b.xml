<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iris: A Conversational Agent for Complex Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ethan</forename><surname>Fast</surname></persName>
							<email>ethan.fast@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binbin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julia</forename><surname>Mendelsohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Bassen</surname></persName>
							<email>jbassen@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Iris: A Conversational Agent for Complex Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96E40B0B7882614D9146609D9B12E463</idno>
					<idno type="DOI">10.1145/3173574.3174047</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>5</term>
					<term>2 Information Interfaces and Presentation: Natural Language conversational agents; data science</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Iris allows users to combine commands through nested conversations to accomplish open-ended data science tasks. (1) Users interact with Iris through natural language requests and (2) the system responds with real-time feedback on the command the request will trigger. Once a command is triggered, Iris (3) converses with users to resolve arguments. When resolving arguments, users can (1) initiate a nested conversation via a new command, or (4) reference the result of previous conversation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>For decades, the promise of computers that communicate with us through natural language has been depicted in works of science fiction and driven research agendas in artificial intelligence (AI) and human-computer interaction (HCI). As early as 1964, Joseph Weizenbaum demonstrated how a computer program could hold open-ended conversations using a large set of pattern matching rules <ref type="bibr" target="#b45">[46]</ref>. Terry Winograd later developed a more sophisticated program that could act upon natural language requests within a simplified blocks world <ref type="bibr" target="#b46">[47]</ref>. In recent years, researchers have begun to apply these assistants to more complex tasks, such as data visualization workflows <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Speak to today's agents as you would to a colleague or friend student, however, and it becomes clear that they have many limitations. In particular, agents are typically oriented towards executing standalone commands rather than complex conversation. Modern NLP techniques such as semantic parsing <ref type="bibr" target="#b44">[45]</ref> and LSTMs <ref type="bibr" target="#b3">[4]</ref> can decompose complex requests such as "run a t-test on the log-transform of x and y" into function composition over several independent commands. However, these techniques require large amounts of training data, making bootstrapping a new domain or command a difficult task that worsens combinatorially with the number of combinable commands. As a result, commanding a modern conversational agent to run a t-test on the log transform of two variables is more likely than not to result in a response like "I don't know what you mean". Linguistic theory suggests a complementary path towards conversational agents that support greater complexity. In daily life, people often build meaning through combinations of speech acts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. Instead of asking an graduate student, "can you run a t-test on the log-transform of x and y", we might make the same request in stages via a nested conversation. For example, first we might ask, "can you run a t-test?" and when the student replies, "on what?", we can then clarify, "on the log-transform of x and y". This enables the same kind of command composition as a state-of-the-art NLP (and more expressivity than today's commercial agents such as Siri), but in a form that requires less training data and is more robust to errors. This is possible because instead of inferring an entire parse tree from a single statement, the agent only needs to correctly classify and execute each step of the conversation. The resulting conversation data and parse tree can then be used to train more advanced models.</p><p>We leverage this insight-that humans build meaning from combinations of atomic speech acts-to create a system that can nest and combine commands with far less training effort than modern deep learning systems. We draw on techniques from functional programming, such as first-order and partial functions, to create a domain-specific language (DSL) for authoring commands. Our system transforms Python functions into composable automata and regulates the set of possible automata compositions through a conversational type system. The result is a set of programming abstractions that enable an agent to compose one command through the argument of another via nested conversation, a form of function composition, or sequence commands through a referencing to some previous conversations, a form of assignment. From nearly one hundred atomic commands, our system enables thousands of composed commands.</p><p>We showcase this architecture in Iris, a conversational agent that helps users with data science tasks (Figure <ref type="figure">1</ref>). Data science is a domain where complex tasks are often executed by non-expert programmers; yet these tasks are difficult to support with standalone commands <ref type="bibr" target="#b20">[21]</ref>. To interact with Iris, you type natural language requests into a chat window (1.1) and receive real-time feedback on what command your request will trigger <ref type="bibr">(1.2)</ref>. When you enter a command, Iris converses with you to resolve its arguments <ref type="bibr">(1.3)</ref>, which you may populate by calling new commands, a form of conversational composition <ref type="bibr">(1.1)</ref>. You can also use the results of previous commands by storing them in named variables or referencing previous command results, forms of sequencing <ref type="bibr">(1.4)</ref>. If you are an expert user, Iris exposes an API that allows you to extend it with new commands.</p><p>The primary contributions of this paper include:</p><p>• An approach that allows agents to combine commands through nested conversation, inspired by linguistic theory.</p><p>• A DSL for transforming Python functions into composable commands that can be leveraged by a conversational agent.</p><p>• The Iris system: a conversational agent for data science.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONVERSATION ANALYSIS THEORY</head><p>Linguistic models of human conversation suggest that agents can compose commands through nested conversations, as opposed to the more challenging task of inferring these compositions automatically from a single request. We use conversation analysis (CA), a theory that has been influential in sociolinguistics and discursive psychology <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>, to help us explain the human conversational strategies that allow agents to compose commands.</p><p>CA theory models all conversations through a basic unit called an adjacency pair: a pair of statements spoken by two conversational participants <ref type="bibr" target="#b17">[18]</ref>. These pairs are typed with labels such as greeting-greeting or question-answer. More complex conversations can be described by joining adjacency pairs through expansions: In particular, insert expansions nest an adjacency pair within a conversation that allows one speaker to resolve other issues before continuing, for example, asking what time a World Cup game airs before answering a question about when to schedule a meeting.</p><p>Insert expansions are useful to agents because they can make conversation more expressive. Today's agents leverage a form of insert expansion called a clarification request <ref type="bibr" target="#b28">[29]</ref>. Humans use these requests to clarify the meaning of previous statements. Agents often use them in a similar way, for example, asking for or confirming arguments before execution: "By Elena, did you mean Elena Ferrante?". Unlike today's agents, Iris supports an insert expansion called the dependent question: a nested conversation where the meaning of one statement is grounded in a subsequent request <ref type="bibr" target="#b28">[29]</ref>. For example, a request like "Who is going to the game tonight?", might depend a new conversation initiated by, "I don't remember". Iris supports dependent questions through command composition. For example, when running a command to compute the mean of some data, Iris might ask, "What array would you like to use?", and a user might answer, "I'd like to generate one from the normal distribution". After further conversation, the resulting array value will be used to compute the mean.</p><p>Finally, the linguistic idea of anaphora describes expressions of language that depend on previous expressions <ref type="bibr" target="#b36">[37]</ref>. For Iris, anaphoric expressions are values produced by a previous command that are necessary to the execution of the present command, enabling command sequencing. Iris supports such expressions through named variables and a simple model of pronoun co-reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TODAY'S CONVERSATIONAL AGENTS</head><p>Iris is inspired by many existing conversational agents <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref> and we examine the interactions these agents support in Table <ref type="table" target="#tab_0">1</ref>. All systems have the ability to execute standalone commands, such as "set a timer" or "generate a random number"; extract arguments from a user query, for example, parsing the name Oyeyemi from "send Oyeyemi an email"; and resolve arguments by asking follow-up questions, for example, responding to the request "schedule a meeting" with "when would you like to set it?'' pressivity inspired by linguistic theory, allowing open-ended command combination through a simple statistical model. From a system design perspective, Iris also contributes a DSL that makes creating command automata as simple as writing and annotating a Python function.</p><p>Iris belongs to a line of HCI systems that map natural language to underlying functionality such as commands, code snippets, and APIs. Query-feature graphs provided an early foundation for these methods, connecting user requests with system commands in an image editing application <ref type="bibr" target="#b11">[12]</ref>. Others have since extended this approach: for example, using word embedding models to connect user vocabulary with system keywords <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> or programming syntax <ref type="bibr" target="#b37">[38]</ref>, and statistical language models to predict functions a user wants to call <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref>. All of these systems solve the vocabulary problem <ref type="bibr" target="#b9">[10]</ref> through statistical models that map natural language to the domain language of a system. While Iris takes a similar tack, it extends user requests through conversation, allowing users to combine atoms of functionality -through composition and sequencing-into complex commands.</p><p>Iris also draws on insights from tools designed to help with data science tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41</ref>]. Wrangler, for example, combines spreadsheet visualizations with natural language command descriptions to help users manipulate data <ref type="bibr" target="#b19">[20]</ref>. Iris leverages similar natural language descriptions in the hints it displays as a user formulates a command. Other tools such as Burrito and Variolite are oriented more towards organizing data science code for reuse <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>. Iris also aims to enable reusable code, but contributes a different perspective: by wrapping high-level functions in natural language, users can redeploy these functions in future conversations.</p><p>When designing for natural language interfaces, systems must manage the ambiguity of user language. DataTone and PixelTone provide guidance here, illustrating how systems can surface decisions about ambiguity <ref type="bibr" target="#b12">[13]</ref> and constrain these decisions by direct manipulation <ref type="bibr" target="#b23">[24]</ref> or other modalities <ref type="bibr" target="#b41">[42]</ref>. Iris combines these ideas with an understanding One important advance of Iris over existing agents is support for composition (i.e., nested conversations). For example, when Iris asks you a question, such as "what model should I use?" you can respond with a request that initiates a new conversation such as "make a logistic regression model". The result of that nested conversation will then pass back to the initial request. Other systems either do not support these interactions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, or only support them when they have been hard-coded into the agent via a dialog tree <ref type="bibr" target="#b4">[5]</ref>. For example, when Siri asks, "What's the date of your event?", you can respond with some commands such as "when do I get back from Australia?", but not with others, such as "how about on Elena's birthday?". Support for composition enables other new interactions that are absent from agents today, such as commands that take references to other commands as arguments (similar to first-class functions in programming languages), or commands that generate new commands (similar to partial functions). For example, Iris can combine "transform the dataframe" with "square each value", where the second command is captured as an argument of the first and applied repeatedly over the data.</p><p>Finally, Iris supports the sequencing of any of its commands (i.e., anaphora resolution). For example, you might ask Iris to "get the petal-length column from data.csv", then to "take the mean of that column''. A few existing agents support such open-ended sequencing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, but most support these interactions only when hard-coded.</p><p>In sum, most of today's agents support command combination through hard-coded logic. The enormous training difficulty of machine learning methods that infer command compositions automatically put these methods out of reach for many applications. By instead allowing a user to combine commands through nested conversations, agents can immediately unlock a much more complex set of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Iris is inspired by other dialogue systems that engage with complex tasks. Like Iris, these systems often represent commands as automata, and in some cases they allow users to combine commands interactively. PLOW and Ava, for example, allow users to sequence commands through variable assignment for complex tasks such as filling out a form on the web <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, but do not support command composition through nested conversation. Similarly, Ravenclaw and Siri allow users to compose some commands through nested conversation, but only if the composition has been pre-defined in a logic tree <ref type="bibr" target="#b4">[5]</ref>. A related class of methods such as semantic parsing or deep LSTMs also enable atomic command composition <ref type="bibr" target="#b44">[45]</ref>. These methods require large amounts of training data, however, and do not support back-and-forth dialog with a user to clarify arguments or mistaken compositions. The key contribution of Iris over prior work is greater ex-of how humans resolve ambiguity (e.g., through clarification requests <ref type="bibr" target="#b28">[29]</ref>) to manage the execution of user commands.</p><p>Systems that interact with users through speech have unique psychological constraints <ref type="bibr" target="#b35">[36]</ref>, such as the tendency of users to anthropomorphize them <ref type="bibr" target="#b31">[32]</ref>. To better design for these constraints, other work in HCI has examined how humans communicate through theories such as the language/action perspective <ref type="bibr" target="#b47">[48]</ref> and applied these theories to agents <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref>. Such theories have centered around individual speech acts, which describe how language relates to the world <ref type="bibr" target="#b38">[39]</ref>. Iris expands on this perspective by capturing the interleaving of multiple speech acts as described by CA theory <ref type="bibr" target="#b17">[18]</ref>. A second class of work has turned a practical lens on the challenges of implementing speech interfaces, providing tools for rapid prototyping <ref type="bibr" target="#b22">[23]</ref>, triggering user queries <ref type="bibr" target="#b48">[49]</ref>, or extending the limits of machine reasoning with a crowd <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. We similarly aim to provide a framework that allows others to design and bootstrap conversational agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCENARIO</head><p>To motivate how combining commands through composition and sequencing can empower interaction with a conversational agent, we present a scenario in which we use Iris to replicate work analyzing linguistic signals of dogmatism (intractability of opinion) in social media <ref type="bibr" target="#b9">[10]</ref>. For this scenario, Iris is seeded with a large set of statistical commands based on the scipy.stats and sklearn libraries, as well as commands that enable text analysis through lexicons <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The original study collected dogmatism labels for Reddit posts through a crowdsourcing task. The dataset has two columns: post, the text of a post; and score, a dogmatism label between 0 (non-dogmatic) and 15 (dogmatic). We begin the scenario with the data loaded into the dogmatism_data, a dataframe indexed on these column names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Analysis of Dogmatism</head><p>To discover linguistic features associated with dogmatic posts, we first need to divide the posts into two populations: dogmatic and non-dogmatic. The original study considered posts in the highest quartile (top 25%) as dogmatic and those in the lowest quartile (bottom 25%) as non-dogmatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Interactions with Iris</head><p>We type "quartiles" into Iris and the hint box fills with compute quartiles for a {dataframe}. We trigger that command and Iris asks, "What dataframe do you want to analyze?" The hint box fills with "dogmatism_data" because it is a variable in the environment that matches the type the command requires. We accept this suggestion, and Iris displays a spreadsheet view with the columns in the dataframe. We click on the "score" column to select it. Iris responds: "Q1 is from 2.0 to 7.0, Q2 is from 7.0 to 9.0, Q3 is from 9.0 to 12.0, and Q4 is from 12.0 to 15.0".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex Command Composition</head><p>Next we need to select the highest and lowest quartiles of posts: those with scores less than 7 (non-dogmatic) and those with scores greater than 12 (dogmatic). We type "filter dog-matism_data" which triggers the command filter {data-frame} by {column}. Iris outputs a spreadsheet view of the data and again asks us to choose a column to filter on. We select score, and Iris asks, "How would you like to filter the column?", explaining that we can provide any command that takes a single argument and produces a boolean value. We say "use less than", which via composition triggers {x} less than {y}. Iris asks for the first argument (x) of the command, and we say "use the column" which triggers yet another command, create reference to column variable. Iris then asks for the second (y) and we say "7". Given the nested create reference to column variable command, Iris returns a partial function ({x} less than 7) that the filter command applies to each row of the dataframe, returning rows where the selected column is less than 7 (Figure <ref type="figure" target="#fig_0">2</ref>). The interaction above would not be possible in any other conversational agent today. First, command composition allows us to call another command within the filter function, then partial command application allows us to generate a new command dynamically from the less than command.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversational Type System</head><p>What if we had made a mistake and passed filter a command like log base 10 of {x}, which does not return a boolean value? In this case, Iris would evaluate the log command on the first row of data, returning a floating point number. This would fail a dynamic type check because filter requires a command that returns the type boolean and not float. Iris would then respond, "That filter argument requires a com- mand that returns true or false, but your command returned a float", and repeat its original request. We could then try again, or exit the conversation via "quit". This type system provides a means of sanitizing inputs and composed commands before they are executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saving and Sequencing Commands</head><p>We need to reuse the dataframe we just filtered, so we ask Iris to "save that as non_dogmatic_posts", which triggers save {that} as {non_dogmatic_posts}. A new variable named dog-matic_posts appears in the right side-bar. This interaction is possible because Iris can sequence commands, referencing the result of the previous command (via the keyword "that") in the current conversation. Through a similar interaction, we save posts with scores greater than 12 in dogmatic_posts. Now we would like to test these dataframes of posts for differences in linguistic features. To replicate the original paper, we will use LIWC (Linguistic Inquiry and Word Count) <ref type="bibr" target="#b34">[35]</ref>, a popular tool for computational social science supported by Iris. LIWC analyzes text for signals across many linguistic categories, such as dominance, anger, or sentiment.</p><p>We tell Iris to "run an analysis using LIWC", which triggers the command liwc analysis on {dataframe}. We type "dogmatic_posts" and Iris outputs a spreadsheet view of the data ands asks us to choose a column. We select the post column, and Iris then runs the analysis and returns a dataframe indexed on word counts for each of LIWC's linguistic categories. We save these category scores in dogmatic_liwc, and then repeat this process for the content of non-dogmatic posts and save them in non_dogmatic_liwc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning from User Language</head><p>We can now test for linguistic signals that are different between dogmatic and non-dogmatic posts. We type "run Mann-Whitney tests between the columns in dogmatic_liwc and non_dogmatic liwc", which triggers statistical test {test} between {dogmatic_liwc} and {non_dogmatic_liwc}.</p><p>Iris does not understand which statistical test we want (the test argument), so it responds, "Sure, I can run statistical tests between two dataframes. What test would you like to run?", along with a set of options that appear in the hint pane. We select Mann-Whitney U, and Iris connects this with "Mann-Whitney" in the original request, learning a new template for this command that it will remember in the future. Iris then executes the command to generate a dataframe of test statistics, which we save in dogmatism_stats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introspecting Iris Commands</head><p>The original study corrected these test statistics via the Holmes method. Has Iris already done so? We click on the statistical test function in the conversation pane to open up documentation in the right sidebar, which includes the command's source code and a description of what it does. Since we see that the statistics have not been corrected, we ask Iris to "apply Holmes correction to dogmatism_stats". After correction, we see dogmatic associations for swearing, negative sentiment, and sexual language, and non-dogmatic associations for first-person pronouns and past tense. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing Data</head><p>We would now like to investigate the relationships we just uncovered through data visualization. In particular, we can examine the relationship between the swearing LIWC category and dogmatism ratings through a scatter plot. We ask Iris to "make a scatter plot" using the LIWC data we already computed. By default the scatter plot command will plot a relationship between two columns in the same dataframe, but by using command composition, we can ask Iris to switch to a different dataframe to select the dogmatism score data for the y-axis (Figure <ref type="figure" target="#fig_2">3</ref>). From the resulting plot, it is clear that while swearing often happens in both dogmatic and non-dogmatic posts, posts with high amounts of swearing are far more likely to be rated dogmatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saving Conversations as Code</head><p>Finally, we would like to save our analysis as Python code to share with others. When conversing with Iris, the system both executes commands and dynamically constructs an abstract syntax tree (AST) behind the scenes that can be used Function composition: Although Iris commands are automata, they can also be composed much like traditional functions. To support this, the states that handle argument requests can set their transitions dynamically to point to a new command, then loop back with a return value once the new command has executed. This functionality enables open-ended command compositions via nested conversation. This is a novle component of Iris, powered by other contributions below.</p><p>Memory: To share data between calls to Iris commands (i.e., sequencing), and to store data returned by argument requests and nested conversations (i.e., composition), automata need to read from and write to shared memory. To enable this, automata pass a dictionary between state transitions.</p><p>Scope: When composing two Iris commands with the same argument name, such as x in add {x} {y} and subtract {x} {y}, automata need scoping rules that prevent one command's argument from overwriting another's. To enable this, scoped commands store data in different namespaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type System:</head><p>To regulate what kinds of values can be passed an Iris command, and which commands can be composed, some automata need type constraints over their transitions. This allows Iris to give commands a type signature that it can use to gracefully reject inputs at runtime.</p><p>First-class Functions: Some Iris commands take other commands as arguments. For example the command argument in filter {dataframe} by {command} requires a command that returns a boolean value when applied to its argument. To enable this, automata must be able to read and write other automata to and from shared memory.</p><p>Partial Functions: Iris can create new commands from commands it already knows. For example, a command such as {x} greater than {y} can be parameterized with arbitrary y values to create an infinite number of other single-argument functions, such as {x} greater than 1, {x} greater than 2, and so on. These partial functions are useful for filtering or transforming data, where you would otherwise need to register an intractable number of functions with Iris (one for each constant value to support). To enable this, Iris's automata support closures over argument bindings.</p><p>While existing work in conversational agents supports some of these concepts, such as function application or memory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, Iris is the first to support many others, such as composition, scope, and first-class functions. These concepts work together to allow users to combine commands via conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Structure of Iris Commands</head><p>Iris commands are building blocks that users can combine to accomplish complex tasks. These commands are defined by applying a to save or re-run an interaction. We click on "export" in the right sidebar, which translates this AST into a standalone Python file that can be edited and run independently of Iris.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYSTEM</head><p>Iris is a conversational agent that supports open-ended data science tasks. In this section, we describe how we built Iris, with emphasis on its compositional architecture, conversational type system, and API for command creation.</p><p>We present an overview of how Iris works in Figure <ref type="figure" target="#fig_2">3</ref>. The entry point to interactions with Iris is a simple statistical model that maps user input onto commands. These commands require arguments that Iris either extracts through template strings or converses with a user to resolve. Users can answer an argument request by executing another command via a nested conversations (composition) or referencing a previous conversation (sequencing). This system is focused on a new approach to combining atomic commands, not command classification or argument extraction: other work in NLP will continue to improve on these core methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Programming Model for Conversation</head><p>Iris draws upon many existing programming abstractions to support open-ended command combination. Previous dialog systems have used automata, or state machines <ref type="bibr" target="#b47">[48]</ref>, to model conversation. Automata provide useful scaffolding for backand-forth interactions with a user, but they are not powerful enough to support nested conversations or commands that can reference and call other commands. We describe how we Iris has augmented traditional automata below:</p><p>Function application: Iris commands are functions wrapped by automata that interact with a user. For example, when arguments cannot be extracted from a user's request, automata will loop through clarification requests to request them. This basic functionality is shared by existing agents <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. We present an example Iris command in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>The title field describes how Iris will reference the command in conversation. The examples provide initial grounding for what user requests will trigger the command and how to extract arguments. The argu-ment_types connect arguments with types that dictate how Iris should converse with a user to resolve them.</p><p>The explanation function defines how the return value of a command will be presented to the user.</p><p>Iris commands inherit from a base class (IrisCommand) that transforms them into automata (Figure <ref type="figure" target="#fig_2">3</ref>). An outermost automaton extracts arguments from a user's request and binds them to a command. This automaton transitions to another that gathers missing arguments, cycling through clarification requests with the user (driven by argument type). This in turn transitions to an execution automaton that looks up the argument bindings and runs the code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Conversational Type System</head><p>Iris's conversational type system provides a framework for sanity checking user input and resolving a command's arguments through conversation. For example, if Iris knows it needs an Integer, it can execute conversational logic to collect that value, such as, "What integer would you like to use for n?" or reject user input if the user provides a string.</p><p>Types in Iris consist of (1) logic that defines what Python values match the type, (2) a means of converting user input into a value of the type, such as a function to convert the string value "9" into an integer, (3) a clarification request that determines how Iris will interact with a user to resolve a type, and (4) a set of type converters that can transform values of non-matching types into the correct type, such as converting a floating point number into an integer.</p><p>Iris currently contains types for integers, strings, arrays, dataframes (multi-dimensional data with named columns), models and metrics (based on the sklearn API), and plots (based on the matplotlib API). These types all inherit from a base automata class (IrisType), so an expert user can add types to Iris without writing state transition logic. For example, the type definition for integers is only 8 lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Statistical Model of User Requests</head><p>Iris connects user language with commands through a multiclass logistic regression model that is trained to predict commands from user language. Iris bootstraps training based on a few examples associated with each command, but as users enter new requests, the model is updated to incorporate them. With more data, it is possible to swap out this classifier with something more sophisticated, such as a deep neural network <ref type="bibr" target="#b39">[40]</ref>. Iris does not contribute over existing NLP work for command classification: the classifier it uses is only important to enable the other, novel parts of its architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Executing Requests via Iris Commands</head><p>To connect user requests with commands, Iris acts much like an interpreter in a traditional programming environment. Upon receiving a request, Iris calls its statistical model to determine which command the user wants. Iris then executes the desired command's automata, which handle argument extraction and clarification requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracting Arguments from Requests</head><p>The automata for argument extraction use an exact match procedure that aligns requests word-by-word with a set of templates and extracts values from matching words in the template so long as they can be converted into the correct type. As users enter requests it has not seen, Iris learns new templates for executing commands and updates its statistical model. When arguments cannot be extracted, these automata transition to others that handle clarification requests.</p><p>Given its modular architecture, Iris can easily be extended to support more advanced models for argument extraction (such as LSTMs) once we have more training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composing Command Execution</head><p>Composition allows users to nest commands within each other through conversation (Figure <ref type="figure" target="#fig_1">2A</ref>). For example, a user can compose one command to "transform the post column in dogmatism_data" with another to "take the absolute value".</p><p>To achieve composition, automata that handle clarification requests must distinguish between responses that are intended as primitive values (e.g., an integer, or the name of an array value) and those that correspond to new commands. These automata first use the conversion methods provided by their types to attempt to parse a user response into a value of the correct type. If this process fails, the automata will process the input as a new, composed command. Iris's interpretation of an input is transparent to the user and updated in real-time as a hint above the text field.</p><p>To compose one command's automata (the child) with another's (the parent), the parent initializes the child with a new scope to prevent argument bindings from overwriting each other -for example, two commands that use the argument "x"-passes control of the conversation to the child, and adds a state transition from the child to the itself. After the child executes and transitions to the parent, the parent binds the child's result to the original argument in question, then continues its own execution (Figure <ref type="figure" target="#fig_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequencing Command Execution</head><p>Sequencing allows users to reference the value of a previous command in their current interaction. To accomplish this, Iris saves the result of each command as it executes in a history variable that can be referred to in future conversations through pronoun keywords such as "this", "those", or "that". For example, after a command has just returned an array, users can ask Iris to "take the mean of that". This simple model works well in practice; in the future, it is possible to swap in more advanced co-reference models <ref type="bibr" target="#b32">[33]</ref>.</p><p>Iris also provides an internal API that commands can call to add new, named variables to the Iris runtime environment.</p><p>We have used this API to create an general purpose command to save {value} to {variable name}, which can be issued to save conversation results for future use, such as in the request, "save that to array result", and then a follow-up, "tell me the variance of array result".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transforming Conversations into Programs</head><p>Users can export conversations with Iris as a Python script to save and replicate their work. To accomplish this, Iris incrementally builds an abstract syntax tree (AST) that represents the current state of the conversation. When a user asks to export their conversation, Iris uses rules associated with each AST node type to compile the AST into a Python program. Concretely, Python source code for each relevant command is generated at the top of the program, sequenced commands are translated as variable assignment, and nested commands are translated as function composition (Figure <ref type="figure" target="#fig_4">5C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Iris User Interface</head><p>Users interact with Iris through a chat window that is augmented with hints and metadata about the current state of the system (Figure <ref type="figure">1</ref>). Users enter requests into an input box (1.1) and receive real-time feedback about what command their re-quest will execute (1.2), which they can use to reformulate the request if necessary. Once a user has entered a command, Iris will begin a conversation in the chat window, asking questions if it needs information about a command's arguments <ref type="bibr">(1.3)</ref>. Users can respond to these questions with concrete values (for example, "Elena" or "2"), new commands that Iris should execute (1.4) or references to the results of previous conversations (1.5). The chat window has a right sidebar that provides information about the names, types, and values of existing environment variables.</p><p>Because Iris is designed for data science tasks, it must also display non-textual data. In particular, Iris outputs complex array and matrix data using numpy's string formatting tools, pretty prints Python objects such as lists and dictionaries, and can embed images directly in the chat window.</p><p>The Iris user interface is built in JavaScript with React and Redux and communicates with a Python backend that runs the automata-based logic encoded by the conversational DSL. All backend and frontend components of the user interface are open source at http://github.com/Ejhfast/ iris-agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>Can Iris help users accomplish data science tasks? What benefits and drawbacks does a conversational interface provide over programming? We ran a study to validate Iris's design, and compare it to the high-level data mining API provided by the Python package sklearn. Following the study, we asked participants about their impressions of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Our study aims to compare Iris to how a data scientist would accomplish a predictive modeling task in Python. Study participants built and cross-validated a model to predict a flower's species based on measurements of the length and width of its petals and sepals <ref type="bibr" target="#b2">[3]</ref>, then reported on feature relationships by examining model coefficients. The task description was written by a third party expert who was not familiar with the training examples for Iris commands. We asked participants to complete the task as quickly as possible and measured the time it took them to complete the task as well as the correctness of their final output. Finally, we conducted semi-structured interviews, asking participants what they liked and disliked about Iris, and following up on any problems we noticed as they completed the study. These interviews lasted about ten minutes.</p><p>Experimental design: Participants completed the task using both sklearn (in a Jupyter notebook) and the Iris conversational interface, in random order to counterbalance learning effects. We chose to compare to sklearn due to the one-to-one correspondence between the commands in each tool (many Iris commands are built on sklearn).</p><p>Participants: eight people participated in the study. All were trained computer scientists with past experience in data science and sklearn, and none had used Iris prior to the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data format:</head><p>In the sklearn condition, we presented the flower data as a dictionary (with column names such as "sepallength" as the keys). In the Iris condition, we presented these data as a dataframe (indexed on the same column names).</p><p>Participants did not need to manipulate the data in either condition, besides selecting features to input to a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User instruction:</head><p>In the sklearn condition, we provided a high-level explanation of how the library works, and URLs for all the sklearn methods that a participant would need to complete the task. We also answered any questions about sklearn in the course of the task. In the Iris condition, we explained how the interface worked, but we did not provide details about the commands that the participant would need, or answer questions about the purpose of Iris commands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Participants completed the task 2.6 times faster on average in the Iris condition (Figure <ref type="figure" target="#fig_5">6</ref>). This effect was statistically significant under a Mann-Whitney U test (U 8 =3.0, p&lt;0.01).</p><p>There was a small learning effect (participants were 1.31 times faster on average for the second interface), re-emphasizing the importance of our randomization, but the effect was not significant (U 8 =26, p=0.28). All participants completed the task correctly in both conditions.</p><p>Participants approached the modeling task in different ways. Some participants worked backwards from a high level goal that was several steps in the future. For example, P1 asked Iris to "cross-validate" before they had created a model or selected features. When Iris then asked, "what model do you want to use?", P1 created it on demand though a nested con- versation (made possible by composition). Other participants worked more incrementally. For example, P4 first selected features from the flower data and saved them in named variables, then referenced those variable names in conversation when creating a logistic regression model (made possible by sequencing). Most participants took a middle ground approach that leveraged both kinds of combination: for example, composing model creation and data selection, then sequencing that with a new request for cross-validation.</p><p>Participants also differed in the vocabulary they used to interact with Iris. For example, P1 asked Iris for a "logistic regression", while P3 asked to "build a classifier", and P2 asked to "select columns" while P5 asked to "select features". Sometimes users reformulated a request when nothing appropriate appeared in the hint box, as occurred when one participant attempted to access a column of data using the '.' notation common to Python objects.</p><p>While three participants interacted with Iris almost entirely through full natural language queries, most interacted via keywords, as if they were querying a search engine (for example, typing "logistic regression" to trigger create a clas- sification model). When asked why they chose to converse this way, participants said it was faster, as the hint box indicated when keywords would trigger the correct command. Notably, keyword searches still allowed participants to build complex commands through composition and sequencing. In contrast, participants who interacted with Iris through full language queries (for example, "cross-validate model1 with accuracy and 10 folds") reported that they wanted Iris to extract a command's arguments automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages to Conversation</head><p>In interviews following the study, participants mentioned aspects of Iris that they liked and disliked. These aspects varied to some degree with a user's level of experience with sklearn. For example, less experienced users of sklearn found value in the structural guidance provided by conversation with Iris (for example, how APIs fit together). P1 said: Other participants commented on the complexity of what the system could accomplish. For example, P3 said: "It's so cool that Iris can do such complex stuff, when legit companies out there have nothing this sophisticated."</p><p>Four participants expressed interest in using Iris for future data science work. One participant offered to connect us with a colleague in the Psychology department who was teaching an introductory statistics class as applied through R. "Iris would be so much better," P4 said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges for Conversation</head><p>We also asked participants what they found challenging about Iris's conversational interface. One common view was that the expressivity of Iris presented more opportunities for mistakes. For example, P2 said: "It's great that Iris can combine anything together, but that creates more opportunities for something to go wrong."</p><p>Iris's conversational type system can prevent many common user errors, such as combining commands that return incompatible types, providing guardrails on user data entry that encourage exploration and experimentation <ref type="bibr" target="#b30">[31]</ref>. However, as the complexity of conversations increased, we noticed that participants occasionally got lost when dealing with many layers of sub-conversations. Visual cues such as tab indendation might help add clarity to these interactions.</p><p>Similarly, other kinds of mistakes can emerge from miscommunication between a system and user about what the system is actually doing. Along these lines, some participants expressed the concern that a natural language based system should be transparent about the operations it is executing, especially for data science work. P5 said:</p><p>"Say I have some data that's not normally distributed, and I ask Iris to do a t-test. How do I know it's doing the right one?"</p><p>While Iris repeats back the commands it is executing, this is not always enough to give users a clear sense of what is happening. In response, we have added a feature to Iris that allows users to inspect a command's underlying Python code. Iris code is largely written using high level APIs such as scikit, so it is possible to quickly inspect the constituent functions and determine what methods are being executed.</p><p>Other participants mentioned the vocabulary problem as a potential challenge. For example, P4 said:</p><p>"Everything worked for me, but I can imagine another user entering the wrong words and getting stuck."</p><p>The vocabulary problem has long been an issue for systems based on natural language. As Iris gains more users and language data, we aim to address this problem more formally by learning from logs of mistakes <ref type="bibr" target="#b15">[16]</ref>. In the course of the pilot study, three participants entered requests that triggered the wrong command. For example, "make model" triggered "create a regression model" instead of "create a classification model"; and for two participants, entering the name of a column did not resolve to a command to extract that column from a dataframe. While these mistakes were foreshadowed by text in the system's hint box, users ignored or misinterpreted these hints, and future versions of the Iris might emphasize them further.</p><p>Finally, several participants suggested that text is not always the best medium for communicating. For example, P2 said:</p><p>"There are certain things, like selecting data columns, that I'd prefer to do by clicking on things."</p><p>Prior work has demonstrated strong results with mixed modality interfaces <ref type="bibr" target="#b23">[24]</ref>. Such feedback drove our decision to include a spreadsheet view of dataframes in Iris, and we plan to incorporate other modalities in the future. Manipulating and transforming data, for example, can often be enhanced by interactive visualizations, and we plan to explore how such visual feedback might augment user conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIMITATIONS AND FUTURE WORK</head><p>Here we discuss challenges to overcome as Iris's user base and range of functionalities grow.</p><p>First, as Iris expands to support many more commands, interpreting user language may become more difficult. The system currently supports 95 atomic commands with a manually authored dataset of examples (roughly 5 examples per command). How accurate will command classification become as the set of commands grows larger? How many user examples are necessary to support a new command among a library of thousands of others? Addressing these concerns will become more important as we deploy Iris in the wild.</p><p>Second, Iris creates AST representations of conversations with users and so has the ability to save these recorded programs, which may combine multiple commands. This ability connects with work in programming by demonstration <ref type="bibr" target="#b26">[27]</ref>, and offers the potential for users to create complex, reusable workflows through natural language. Saving ASTs for reuse presents a usability challenge, however. For example, how should Iris ask a user which parameters in the AST are arguments, and which should be captured as constants? And is it be possible to mine useful higher-level commands by analyzing the ASTs of hundreds or thousands of users? We aim to explore these questions as Iris grows.</p><p>Finally, Iris enables exploratory and interactive data analyses, as you might conduct today in R or a Jupyter notebook. This is distinct from other data science work that requires enormous datasets, where training a model may take hours, days, or even weeks. To run these models, researchers typically setup and debug long-running pipelines of commands, which is not something Iris is currently designed to do. As Iris expands to allow users to create and save workflows though PBD, such pipelines may be more feasible to run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In this paper, we show how conversational agents can draw on human conversational strategies to combine commands together, allowing them to assist us with tasks they have not been explicitly programmed to support. We showcase these ideas in Iris, an agent designed to help users with data science and machine learning tasks. More broadly, our work demonstrates how simple models of conversation can lead to surprisingly complex emergent outcomes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Composition allows Iris to support nested conversations. Here a user interacts with Iris to filter a dataframe to select rows where the score column is less than 7. Behind the scenes, Iris generates a new command using partial function generation to filter the data.</figDesc><graphic coords="4,315.99,61.20,242.01,316.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Iris displays a scatter plot for the relationship between sweating and dogmatism. By default, Iris interacts with users to visualize data from a single dataframe, but here we use command composition to select a column from a different dataframe for the y-axis data.</figDesc><graphic coords="5,315.80,279.20,242.23,398.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Iris allows users to combine commands by transforming them into automata that it can compose and sequence. Here we depict how these automata fit into the system architecture. Composition (short dash) allows comands to be called recursively within each other. Sequencing (long dash) allows commands to happen in series, referencing previous command results.</figDesc><graphic coords="6,54.00,466.20,345.98,203.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Implementation of an Iris command from a scipy function. Iris extends the function with conversational affordances, such as argument requests (lines 8-9) and an explanation (lines 14-17). These affordances translate into automata powered by the conversational DSL.</figDesc><graphic coords="7,282.00,61.20,276.00,165.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Conversations with Iris are programs under composition: (A) user conversation with Iris, (B) the abstract syntax tree that Iris builds at run-time, and (C) Python code generated by Iris from the AST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Study participants completed a data science task 2.6 times faster on average with Iris than with sklearn and Jupyter (p &lt; 0.01). The y-axis describes time in seconds.</figDesc><graphic coords="9,316.80,61.20,241.20,71.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,56.30,154.40,501.70,200.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,237.60,62.00,320.40,175.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Interactions that today's conversational agents have been designed to support. Iris enables broader support for composition, calling one command within another command, and sequencing, using the results of previous commands in a new command.</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>All you need to do is say something close, like 'rename column' and it can figure out the rest."</figDesc><table><row><cell>"It took a while to remember how to thread together the com-</cell></row><row><cell>ponents of the sklearn API. Like, first you need to initiate a</cell></row><row><cell>model instance, then pass that to the cross-validation func-</cell></row><row><cell>tion-it wasn't totally obvious. But with Iris, once it knew I</cell></row><row><cell>wanted to cross-validate, it walked me backwards though the</cell></row><row><cell>steps, like giving it a model, what type of model, and so on."</cell></row></table><note><p>In contrast, more experienced users thought Iris would save time as a wrapper for a set of functions they frequently call in smaller scripts. For example, P4 said: "For simple scripts, this is so much faster... I really like that you don't have to remember the name of the function you want to call.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>CHI 2018, April 21-26, 2018, Montréal, QC, Canada Paper 473</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Command-Space: modeling the relationships between tasks, descriptions and features</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Laput</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual ACM symposium on User interface software and technology</title>
		<meeting>the 27th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Plow: A collaborative task learning agent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Galescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Taysom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The species problem in Iris</title>
		<author>
			<persName><forename type="first">E</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of the Missouri Botanical Garden</title>
		<imprint>
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The RavenClaw dialog management framework: Architecture and systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Speech &amp; Language</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Calendar. help: Designing a Workflow-Based Scheduling Agent with Humans in the Loop</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cranshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elwany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monroy-Hernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augur: Mining Human Behaviors from Fiction to Power Interactive Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emergent, crowd-scale programming practice in the IDE</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steffee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd annual ACM conference on Human factors in computing systems</title>
		<meeting>the 32nd annual ACM conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empath: Understanding topic signals in large-scale text</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying dogmatism in social media: Signals and models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enabling Programming Languages to Learn from the Crowd</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><surname>Meta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query-feature graphs: bridging user vocabulary and system functionality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Managing ambiguity in natural language interfaces for data visualization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><surname>Datatone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology</title>
		<meeting>the 28th Annual ACM Symposium on User Interface Software &amp; Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to discourse analysis: Theory and method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wrapping Your Lab Notebook in Computational Infrastructure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><surname>Burrito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TaPP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What would other programmers do: suggesting solutions to error messages</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Macdougall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rovinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><surname>Sirius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Hutchby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wooffitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conversation analysis</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From Data to Insights Through Conversations</title>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Potti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Ava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive visual specification of data transformation scripts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><surname>Wrangler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enterprise data analysis and visualization: An interview study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variolite: Supporting Exploratory Programming by Data Scientists</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Suede: A Wizard of Oz Prototyping Tool for Speech User Interfaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aboobaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 13th Annual ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pixeltone: A multimodal interface for image editing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wilensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chorus: a crowd-powered conversational assistant</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual ACM symposium on User interface software and technology</title>
		<meeting>the 26th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Answering visual questions with conversational crowd assistants</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility</title>
		<meeting>the 15th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SUGILITE: Creating Multimodal Smartphone Automation by Demonstration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Keyword programming in Java</title>
		<author>
			<persName><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering</title>
		<meeting>the twenty-second IEEE/ACM international conference on Automated software engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A corpus-based taxonomy of question responses</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lupkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ginzburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWCS 2013 (International Workshop on Computational Semantics)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Agents that reduce work and information overload</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CACM</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Scratch Programming Language and Environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eastmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Trans. Comput. Educ</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Wired for speech: How voice activates and advances the human-computer relationship</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised noun phrase coreference research: The first fifteen years</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gestalt: integrated support for implementation and analysis in machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bancroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd annual ACM symposium on User interface software and technology</title>
		<meeting>the 23nd annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: LIWC</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Do animals have accents?&quot;: talking with agents in multi-party conversation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Porcheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharples</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The syntactic domain of anaphora</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reinhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Massachusetts Institute of Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Assisting Interactive Programming with Bimodal Embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><surname>Codemend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Speech acts: An essay in the philosophy of language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Searle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04808</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Eviza: A Natural Language Interface for Visual Analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Battersby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gossweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal error correction for speech user interfaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Suhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on computer-human interaction (TOCHI)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An intelligent assistant for high-level task understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Intelligent User Interfaces</title>
		<meeting>the 21st International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">EnsembleMatrix: Interactive Visualization to Support Machine Learning with Multiple Classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning Language Games through Interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ELIZA-a computer program for the study of natural language communication between man and machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Understanding computers and cognition: A new foundation for design</title>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Flores</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Intellect Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A language/action perspective on the design of cooperative work</title>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Computer Interaction</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><surname>Almond</surname></persName>
		</author>
		<title level="m">The Architecture of an Open, Crowdsourced, Privacy-Preserving, Programmable Virtual Assistant</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
