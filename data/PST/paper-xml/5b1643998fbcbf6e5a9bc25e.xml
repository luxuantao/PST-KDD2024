<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
							<email>dwang@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Le Roux</surname></persName>
							<email>leroux@merl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deliang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Cognitive and Brain Sciences</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep clustering</term>
					<term>chimera++ network</term>
					<term>iterative phase reconstruction</term>
					<term>cocktail party problem</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes an end-to-end approach for single-channel speaker-independent multi-speaker speech separation, where time-frequency (T-F) masking, the short-time Fourier transform (STFT), and its inverse are represented as layers within a deep network. Previous approaches, rather than computing a loss on the reconstructed signal, used a surrogate loss based on the target STFT magnitudes. This ignores reconstruction error introduced by phase inconsistency. In our approach, the loss function is directly defined on the reconstructed signals, which are optimized for best separation. In addition, we train through unfolded iterations of a phase reconstruction algorithm, represented as a series of STFT and inverse STFT layers. While mask values are typically limited to lie between zero and one for approaches using the mixture phase for reconstruction, this limitation is less relevant if the estimated magnitudes are to be used together with phase reconstruction. We thus propose several novel activation functions for the output layer of the T-F masking, to allow mask values beyond one. On the publiclyavailable wsj0-2mix dataset, our approach achieves state-ofthe-art 12.6 dB scale-invariant signal-to-distortion ratio (SI-SDR) and 13.1 dB SDR, revealing new possibilities for deep learning based phase reconstruction and representing a fundamental progress towards solving the notoriously-hard cocktail party problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed exciting advances towards solving the cocktail party problem. The inventions of deep clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, deep attractor networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and permutation free training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> have dramatically improved the performance of single-channel speaker-independent multispeaker speech separation, demonstrating overwhelming advantages over previous methods including graphical modeling approaches <ref type="bibr" target="#b7">[8]</ref>, spectral clustering approaches <ref type="bibr" target="#b8">[9]</ref>, and CASA methods <ref type="bibr" target="#b9">[10]</ref>.</p><p>However, all of these conduct separation on the magnitude in the time-frequency (T-F) domain and directly use the mixture phase for time-domain re-synthesis, largely because phase is difficult to estimate. It is well-known that this incurs a phase inconsistency problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, especially for speech processing, where there is typically at least half overlap between consecutive frames. This overlap makes the STFT representation of a speech signal highly redundant. As a result, the enhanced STFT representation obtained using the estimated magnitude and mixture phase would not be in the consistent STFT Part of this work was done while Z.-Q. Wang was an intern at MERL. domain, meaning that it is not guaranteed that there exists a time-domain signal having that STFT representation.</p><p>To improve the consistency, one stream of research is focused on iterative methods such as the classic Griffin-Lim algorithm <ref type="bibr" target="#b13">[14]</ref>, multiple input spectrogram inverse (MISI) <ref type="bibr" target="#b14">[15]</ref>, ISSIR <ref type="bibr" target="#b15">[16]</ref>, and consistent Wiener filtering <ref type="bibr" target="#b16">[17]</ref>, which can recover the clean phase to some extent starting from the mixture phase and a good estimated magnitude by iteratively performing STFT and iSTFT <ref type="bibr" target="#b12">[13]</ref>. There are some previous attempts at naively applying such iterative algorithms as a post-processing step on the magnitudes produced by deep learning based speech enhancement and separation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. However, this usually only leads to small improvements, even though the magnitude estimates from DNNs are reasonably good. We think that this is possibly because the T-F masking is performed without being aware of the later phase reconstruction steps and hence may not produce spectral structures that are appropriate for iterative phase reconstruction.</p><p>This study hence proposes a novel end-to-end speech separation algorithm that trains through iterative phase reconstruction via T-F masking for signal-level approximation. On the publicly-available wsj0-2mix corpus, our algorithm reaches 12.6 dB scale-invariant SDR, which surpasses the previous best by a large margin and is comparable to the oracle 12.7 dB result obtained using the so-called ideal ratio mask (IRM). Our study shows, for the first time and based on a large open dataset, that deep learning based phase reconstruction leads to tangible and large improvements when combined with state-of-the-art magnitude-domain separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Chimera++ Network</head><p>To elicit a good phase via phase reconstruction, it is necessary to first obtain a good enough magnitude estimate. Our recent study <ref type="bibr" target="#b2">[3]</ref> proposed a novel multi-task learning approach combining the regularization capability of deep clustering with the ease of end-to-end training of mask inference, yielding significant improvements over the individual models.</p><p>The key idea of deep clustering <ref type="bibr" target="#b0">[1]</ref> is to learn a highdimensional embedding vector for each T-F unit using a powerful deep neural network (DNN) such that the embeddings of the T-F units dominated by the same speaker are close to each other in the embedding space while farther otherwise. This way, simple clustering methods like k-means can be applied to the learned embeddings to perform separation at run time. More specifically, the network computes a unit-length embedding vector vi ∈ R 1×D corresponding to the i th T-F element. Similarly, yi ∈ R 1×C is a one-hot label vector representing which source in a mixture dominates the i th T-F unit. Vertically stacking these, we form the embedding matrix V ∈ R T F ×D and the label matrix Y ∈ R T F ×C . The embeddings are learned arXiv:1804.10204v1 [cs.SD] 26 Apr 2018 by approximating the affinity matrix from the embeddings:</p><formula xml:id="formula_0">LDC,classic = V V T − Y Y T 2 F (1)</formula><p>Our recent study <ref type="bibr" target="#b2">[3]</ref> suggests that an alternative loss function, which whitens the embedding in a k-means objective, leads to better separation performance.</p><formula xml:id="formula_1">LDC,W = V (V T V ) − 1 2 − Y (Y T Y ) −1 Y T V (V T V ) − 1 2 2 F = D − tr (V T V ) −1 V T Y (Y T Y ) −1 Y T V<label>(2)</label></formula><p>To learn the embeddings, bi-directional LSTM (BLSTM) is usually used to model the context information from past and future frames. The network architecture is shown at the bottom of Fig. <ref type="figure" target="#fig_1">1</ref>, where the DC embedding layer is a fully-connected layer with a non-linearity such as a logistic sigmoid, followed by unit-length normalization for each frequency.</p><p>Another permutation-free training scheme was proposed for mask-inference networks first in <ref type="bibr" target="#b0">[1]</ref>, and was later found to be working very well in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b5">[6]</ref>. The idea is to train a maskinference network to minimize the minimum loss over all permutations. Following <ref type="bibr" target="#b6">[7]</ref>, the phase-sensitive mask (PSM) <ref type="bibr" target="#b20">[21]</ref> is used as the training target. It is common in phase-sensitive spectrum approximation (PSA) to truncate the unbounded mask values. Using T b a (x) = min(max(x, a), b), the truncated PSA (tPSA) objective is</p><formula xml:id="formula_2">LtPSA = min π∈P c |X| − T γ|X| 0 (|Sc| cos(∠Sc − ∠X)) 1 ,<label>(3)</label></formula><p>where ∠X is the mixture phase, ∠Sc the phase of the c-th source, P the set of permutations on {1, . . . , C}, |X| the mixture magnitude, Mc the c-th estimated mask, |Sc| the magnitude of the c-th reference source, denotes element-wise matrix multiplication, and γ is a mask truncation factor. Sigmoidal activation together with γ = 1 is commonly used in the output layer of T-F masking. To endow the network with more capability, multiple activation functions that can work with γ &gt; 1 will be discussed in Section 3.4. Following <ref type="bibr" target="#b21">[22]</ref>, our recent study <ref type="bibr" target="#b2">[3]</ref> proposed a chimera++ network combining the two approaches via multi-task learning, as illustrated in the bottom of Fig. <ref type="figure" target="#fig_1">1</ref>. The loss function is a weighted sum of the deep clustering loss and the mask inference loss.</p><formula xml:id="formula_3">L chi ++ α = αLDC,W + (1 − α)LtPSA<label>(4)</label></formula><p>Only the MI output is needed to make predictions at run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Iterative Phase Reconstruction</head><p>There are multiple target sources to be separated in each mixture in our study. The Griffin-Lim algorithm <ref type="bibr" target="#b13">[14]</ref> only performs iterative reconstruction for each source independently. In <ref type="bibr" target="#b2">[3]</ref>, we therefore proposed to utilize the MISI algorithm <ref type="bibr" target="#b14">[15]</ref> (see Algorithm 1) to reconstruct the clean phase of each source starting from the estimated magnitude of each source and the mixture phase, where the sum of the reconstructed time-domain signals after each iteration is constrained to be the same as the mixture signal. Note that the estimated magnitudes remain fixed during iterations, while the phase of each source are iteratively reconstructed. In <ref type="bibr" target="#b2">[3]</ref>, the phase reconstruction was only added as a post-processing, and it was not part of the objective function during training, which remained computed on the time- </p><formula xml:id="formula_4">for i = 1, . . . , K do δ (i−1) = x − C c=1 ŝ(i−1) c ; θ(i) c = ∠STFT ŝ(i−1) c + δ (i−1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head><p>, for c = 1, . . . , C;</p><formula xml:id="formula_5">ŝ(i) c = iSTFT( Âc, θ(i) c</formula><p>), for c = 1, . . . , C; end Algorithm 1: Iterative phase reconstruction based on MISI. STFT(•) extracts the STFT magnitude and phase of a signal, and iSTFT(•,•) reconstructs a time-domain signal from a magnitude and a phase. frequency representation of the estimated signal, prior to resynthesis. In this paper, we go several steps further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Waveform Approximation</head><p>The first step in phase reconstruction algorithms such as MISI is to reconstruct a waveform from a time-frequency domain representation using the inverse STFT. We thus consider a first objective function computed on the waveform reconstructed by iSTFT, denoted as waveform approximation (WA), and represent iSTFT as various layers on top of the mask inference layer, so that end-to-end optimization can be performed. The label permutation problem is resolved by minimizing the minimum L1 loss of all the permutations at the waveform level. We denote the model trained this way as WA. The objective function to train this model is</p><formula xml:id="formula_6">LWA = min π∈P c ŝ(0) π(c) − sc 1 ,<label>(5)</label></formula><p>where sc denotes the time-domain signal of source c, and ŝ(0) c denotes the c-th time-domain signal obtained by inverse STFT from the combination of the c-th estimated magnitude and the mixture phase. Note that mixture phase is still used here and no phase reconstruction is yet performed. This corresponds to the initialization step in Algorithm 1.</p><p>In <ref type="bibr" target="#b22">[23]</ref>, a time-domain reconstruction approach is proposed for speech enhancement. However, their approach only trains a feed-forward mask-inference DNN through iDFT separately for each frame using squared error in the time domain. By Parseval's theorem, this is equivalent to optimizing the mask for minimum squared error in the complex spectrum domain, when using the noisy phases, as in <ref type="bibr" target="#b20">[21]</ref>, proposed in the same conference. A follow-up work <ref type="bibr" target="#b18">[19]</ref> of <ref type="bibr" target="#b22">[23]</ref> supplies clean phase during training. However, this makes their approach equivalent to conventional magnitude spectrum approximation <ref type="bibr" target="#b23">[24]</ref>, which does not perform as well as the phase-sensitive mask <ref type="bibr" target="#b24">[25]</ref>. Closest to the above WA objective, an adaptive front-end framework was recently proposed <ref type="bibr" target="#b25">[26]</ref> in which the STFT and its inverse are subsumed by the network, along with the noisy phase, so that training is effectively end-to-end in the time-domain. The proposed method then replaces the STFT and its inverse by trainable linear convolutional layers. Unfortunately the paper does not compare training through the STFT to the conventional method so the results are uninformative about this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unfolded Iterative Phase Reconstruction</head><p>We further unfold the iterations in the MISI algorithm as various deterministic layers in a neural network. This can be achieved by further growing several layers representing STFT and iSTFT operations on top of the mask inference layer. By performing end-to-end optimization that trains through MISI, the network can become aware of the later iterative phase reconstruction steps and learn to produce estimated magnitudes that are well-suited to that subsequent processing, hence producing better phase estimates for separation. The model trained this way is denoted as WA-MISI-K, where K ≥ 1 is the number of unfolded MISI iterations. The objective function is</p><formula xml:id="formula_7">LWA-MISI-K = min π∈P c ŝ(K) π(c) − sc 1 ,<label>(6)</label></formula><p>where ŝ(K) c denotes the c-th time-domain signal obtained after K MISI iterations as described in Algorithm 1. The whole separation network, including unfolded phase reconstruction steps at the output of the mask inference head of the Chimera++ network, is illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>. The STFT and iSTFT can be easily implemented using modern deep learning toolkits as deterministic layers efficiently computed on a GPU and through which backpropagation can be performed.</p><p>A recent study by Williamson et al. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> proposed a complex ratio masking approach for phase reconstruction and speech enhancement, where a feed-forward DNN is trained to predict the real and imaginary components of the ideal complex filter in the STFT domain, i.e., Mc = Sc/X = |Sc|e j(∠Sc−∠X) /|X| for source c for example. The real component is equivalent to the earlier proposed phase-sensitive mask <ref type="bibr" target="#b20">[21]</ref>, which contains patterns clearly predictable from energy-based features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. However, recent studies along this line suggest that the patterns in the imaginary component are too random to predict <ref type="bibr" target="#b28">[29]</ref>, possibly because it is difficult for a learning machine to determine the sign of sin(∠Sc − ∠X) only from energy-based features. In contrast, the cos(∠Sc − ∠X) in the real component is typically much smaller than one for T-F units dominated by other sources and close to one otherwise, making itself predictable from energy-based features. The proposed method thus only focuses on estimating a mask in the magnitude domain and uses the estimated magnitude to elicit better phase through iterative phase reconstruction.</p><p>Another recent trend is to avoid the phase inconsistency problem altogether by operating in the time domain, using convolutional neural networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, WaveNet <ref type="bibr" target="#b31">[32]</ref>, generative adversarial networks <ref type="bibr" target="#b32">[33]</ref>, or encoder-decoder architectures <ref type="bibr" target="#b33">[34]</ref>. Although they are promising approaches, the current stateof-the-art approach for supervised speech separation is via T-F masking <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3]</ref>. The proposed approach is expected to produce even better separation if the phase can be reconstructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Activation Functions with Values Beyond One</head><p>Sigmoidal units are dominantly used in the output layer of deep learning based T-F masking <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, partly because they can model well data with bi-modal distribution <ref type="bibr" target="#b36">[37]</ref>, such as the IRM <ref type="bibr" target="#b37">[38]</ref> and its variants <ref type="bibr" target="#b35">[36]</ref>. Restricting the possible values of the T-F mask to lie in [0, 1] is also reasonable when using the mixture phase for reconstruction: indeed, T-F mask values larger than one would in theory be needed in regions where interferences between sources result in a mixture magnitude smaller than that of a source; but the mixture phase is also likely to be different from the phase of that source in such regions, in which case it is more rewarding in terms of objective measure to oversuppress than to go even further in a wrong direction. This is no longer valid if we consider phase reconstruction in the optimization. Moreover, capping the mask values to be between zero and one is more likely to take the enhanced magnitude further away from the consistent STFT domain, posing potential difficulties for later phase reconstruction.</p><p>To obtain clean magnitudes, the oracle mask should be |Sc|/|X| (also known as the FFT mask in <ref type="bibr" target="#b37">[38]</ref> or the ideal amplitude mask in <ref type="bibr" target="#b20">[21]</ref>). Clearly, this mask can go beyond one, because the underlying sources, although statistically independent, may have opposite phase at a particular T-F unit, therefore cancelling with each other and producing a mixture magnitude that is smaller than the magnitude of a given source. It is likely much harder to predict the mask values of such T-F units, but we believe that it is still possible based on contextual information.</p><p>In our study, we truncate the values in PSM to the range [0, 2] (i.e., γ = 2 in Eq. ( <ref type="formula" target="#formula_2">3</ref>)), as only a small percentage of mask values goes beyond this range. Multiple activation functions can be utilized in the output layer. We here consider:</p><p>• doubled sigmoid: sigmoid non-linearity multiplied by 2;</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>We validate the proposed algorithms on the publicly-available wsj0-2mix corpus <ref type="bibr" target="#b0">[1]</ref>, which is widely used in many speakerindependent speech separation tasks. It contains 20,000, 5,000 and 3,000 two-speaker mixtures in its 30 h training, 10 h validation, and 5 h test sets, respectively. The speakers in the validation set (closed speaker condition, CSC) are seen during training, while the speakers in the test set (open speaker condition, OSC) are completely unseen. The sampling rate is 8 kHz. Our neural network contains four BLSTM layers, each with 600 units in each direction. A dropout of 0.3 is applied on the output of each BLSTM layer except the last one. The network is trained on 400-frame segments using the Adam algorithm. The </p><formula xml:id="formula_8">L chi ++ α LWA LWA−MISI−1 LWA−MISI−2 LWA−MISI−3 LWA−MISI−4 LWA−MISI−5</formula><p>Figure <ref type="figure">2</ref>: SI-SDR vs number of MISI iterations at test time window length is 32 ms and the hop size is 8 ms. The square root Hann window is employed as the analysis window and the synthesis window is designed accordingly to achieve perfect reconstruction after overlap-add. A 256-point DFT is performed to extract 129-dimensional log magnitude input features. We first train the chimera++ network with α set to 0.975. Next, we discard the deep clustering branch (i.e., we set α to 0) and train the network with LWA. Subsequently, the network is trained using LWA-MISI-1, then LWA-MISI-2, and all the way to LWA-MISI-K, where here K = 5, as performance saturated after five iterations in our experiments. We found this curriculum learning strategy to be helpful. At run time, for the models trained using LWA-MISI-5, we run MISI with 5 iterations, while results for other models are obtained without phase reconstruction unless specified.</p><p>We report the performance using scale-invariant SDR (SI-SDR) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>, as well as the SDR metric computed using the bss eval sources software <ref type="bibr" target="#b39">[40]</ref> because it is used by other groups. We believe SI-SDR is a more proper measure for singlechannel instantaneous mixtures <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation Results</head><p>Table <ref type="table" target="#tab_0">1</ref> reports the SI-SDR results on the wsj0-2mix dataset. We first present the results using sigmoidal activation. The chimera++ network obtains significantly better results than the individual models (11.2 dB vs. 10.4 dB and 10.0 dB SI-SDR). With the mixture phase and estimated magnitudes, performing five iterations of MISI pushes the performance to 11.5 dB, while 11.3 dB is obtained when applying five iterations of Griffin-Lim on each source independently, as is reported in <ref type="bibr" target="#b2">[3]</ref>. Performing end-to-end optimization using LWA improves the results to 11.6 dB from 11.2 dB, without requiring phase reconstruction postprocessing. Further applying MISI post-processing for five iterations (MISI-5) on this model however does not lead to any improvements, likely because the mixture is used during training and the model compensates for it without expecting further processing. In contrast, training the network through MISI using LWA-MISI-5 pushes the performance to 12.2 dB. Among the three proposed activation functions, the convex softmax performs the best, reaching 12.6 dB SI-SDR. It thus seems effective to model the multiple peaks in the histogram of the truncated PSM, and important to produce estimated magnitudes that are closer to the consistent STFT domain. As expected, activations going beyond 1 only become beneficial when training through phase reconstruction.</p><p>In Fig. <ref type="figure">2</ref>, we show the evolution of the SI-SDR performance of the convex softmax models trained with different objective functions against the number of MISI iterations at test time (0 to 5). Training with LWA leads to a magnitude that is very well suited to iSTFT, but not to further MISI iterations. As we train for more MISI iterations, performance starts lower, but reaches higher values with more test-time iterations.</p><p>Table <ref type="table" target="#tab_1">2</ref> lists the performance of competitive approaches on the same corpus, along with the performance of various oracle masks with or without applying MISI for five iterations. The first three algorithms use mixture phase directly for separation. The fourth one, time-domain audio separation network (Tas-Net), operates directly in the time domain. Our result is 1.1 dB better than the previous state-of-the-art by <ref type="bibr" target="#b2">[3]</ref> in terms of both SI-SDR and SDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>We have proposed a novel end-to-end approach for singlechannel speech separation. Significant improvements are obtained by training the T-F masking network through an iterative phase reconstruction procedure. Future work includes applying the proposed methods to speech enhancement, considering the joint estimation of magnitude and an initial phase that improves upon the mixture phase, and improving the estimation of the ideal amplitude mask. We shall also consider alternatives to the waveform-level loss, such as errors computed on the magnitude spectrograms of the reconstructed signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Input:</head><label></label><figDesc>Mixture time-domain signal x, mixture complex spectrogram X, mixture phase ∠X, enhanced magnitudes Âc = Mc • |X| for c = 1, . . . , C, and iteration number K Output : Reconstructed phase θ(K) c and signal ŝ(K) c for c = 1, . . . , C ŝ(0) c = iSTFT( Âc, ∠X), for c = 1, . . . , C;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training through K MISI iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>clipped ReLU: ReLU non-linearity clipped to [0, 2]; • convex softmax: the output non-linearity is a threedimensional softmax for each source at each T-F unit. It is used to compute a convex sum between the values 0, 1, and 2: y = [x0, x1, x2][0, 1, 2] T where [x0, x1, x2] is the output of the softmax. This activation function is designed to model the three modes concentrated at 0, 1 and 2 in the histogram of the PSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>SI-SDR (dB) performance on wsj0-2mix.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Approaches</cell><cell></cell><cell cols="2">CSC OSC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L DC,W</cell><cell></cell><cell></cell><cell>10.4</cell><cell>10.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L tPSA</cell><cell></cell><cell></cell><cell>10.1</cell><cell>10.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L chi ++ α</cell><cell>(sigmoid)</cell><cell></cell><cell>11.1</cell><cell>11.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+ Griffin-Lim-5</cell><cell></cell><cell>11.2</cell><cell>11.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+ MISI-5</cell><cell></cell><cell>11.4</cell><cell>11.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+ L WA</cell><cell></cell><cell>11.6</cell><cell>11.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+MISI-5</cell><cell></cell><cell>11.6</cell><cell>11.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA-MISI-5</cell><cell></cell><cell>12.4</cell><cell>12.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L chi ++ α</cell><cell cols="2">(doubled sigmoid)</cell><cell>10.0</cell><cell>10.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA</cell><cell></cell><cell>11.5</cell><cell>11.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA-MISI-5</cell><cell></cell><cell>12.5</cell><cell>12.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L chi ++ α</cell><cell cols="2">(clipped RelU)</cell><cell>10.4</cell><cell>10.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA</cell><cell></cell><cell>11.7</cell><cell>11.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA-MISI-5</cell><cell></cell><cell>12.6</cell><cell>12.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L chi ++ α</cell><cell cols="2">(convex softmax)</cell><cell>11.0</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA</cell><cell></cell><cell>11.8</cell><cell>11.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">+L WA-MISI-5</cell><cell></cell><cell>12.8</cell><cell>12.6</cell></row><row><cell></cell><cell>12.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>12.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>12.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SI-SDR [dB]</cell><cell>11.75 12.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>11.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>11.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>11.00</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Number of MISI iterations at test time</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with other systems on wsj0-2mix.</figDesc><table><row><cell></cell><cell cols="2">SI-SDR (dB)</cell><cell cols="2">SDR (dB)</cell></row><row><cell>Approaches</cell><cell cols="2">CSC OSC</cell><cell cols="2">CSC OSC</cell></row><row><cell>Deep Clustering [1, 2]</cell><cell>-</cell><cell>10.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep Attractor Networks [4, 5]</cell><cell>-</cell><cell>10.4</cell><cell>-</cell><cell>10.8</cell></row><row><cell>PIT [6, 7]</cell><cell>-</cell><cell>-</cell><cell>10.0</cell><cell>10.0</cell></row><row><cell>TasNet [34]</cell><cell>-</cell><cell>10.2</cell><cell>-</cell><cell>10.5</cell></row><row><cell>Chimera++ Networks [3]</cell><cell>11.1</cell><cell>11.2</cell><cell>11.6</cell><cell>11.7</cell></row><row><cell>+ MISI-5 [3]</cell><cell>11.4</cell><cell>11.5</cell><cell>12.0</cell><cell>12.0</cell></row><row><cell>WA (proposed)</cell><cell>11.8</cell><cell>11.8</cell><cell>12.3</cell><cell>12.3</cell></row><row><cell>WA-MISI-5 (proposed)</cell><cell>12.8</cell><cell>12.6</cell><cell>13.2</cell><cell>13.1</cell></row><row><cell>Oracle Masks:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Magnitude Ratio Mask</cell><cell>12.5</cell><cell>12.7</cell><cell>13.0</cell><cell>13.2</cell></row><row><cell>+ MISI-5</cell><cell>13.5</cell><cell>13.7</cell><cell>14.1</cell><cell>14.3</cell></row><row><cell>Ideal Binary Mask</cell><cell>13.2</cell><cell>13.5</cell><cell>13.7</cell><cell>14.0</cell></row><row><cell>+ MISI-5</cell><cell>13.1</cell><cell>13.4</cell><cell>13.6</cell><cell>13.8</cell></row><row><cell>PSM</cell><cell>16.2</cell><cell>16.4</cell><cell>16.7</cell><cell>16.9</cell></row><row><cell>+ MISI-5</cell><cell>18.1</cell><cell>18.3</cell><cell>18.5</cell><cell>18.8</cell></row><row><cell>Ideal Amplitude Mask</cell><cell>12.6</cell><cell>12.8</cell><cell>12.9</cell><cell>13.2</cell></row><row><cell>+ MISI-5</cell><cell>26.3</cell><cell>26.6</cell><cell>26.8</cell><cell>27.1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Clustering: Discriminative Embeddings for Segmentation and Separation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single-Channel Multi-Speaker Separation using Deep Clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alternative Objective Functions for Deep Clustering</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Attractor Network for Single-Microphone Speaker Separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speaker-Independent Speech Separation with Deep Attractor Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-Talker Speech Separation with Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Language Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-Human Multi-Talker Speech Recognition: A Graphical Modeling Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kristjansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Spectral Clustering, with Application to Speech Separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m">Computational Auditory Scene Analysis: Principles, Algorithms, and Applications</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-IEEE Press</publisher>
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explicit consistency constraints for STFT spectrograms and their application to phase reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA Workshop on Statistical and Perceptual Audition (SAPA)</title>
				<meeting>ISCA Workshop on Statistical and Perceptual Audition (SAPA)</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Signal Reconstruction from STFT Magnitude: A State of the Art</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sturmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Audio Effects</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Phase Processing for Single-Channel Speech Enhancement: History and Recent Advances</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krawczyk-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Signal Estimation from Modified Short-Time Fourier Transform</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative Phase Estimation for the Synthesis of Separated Sources from Single-Channel Mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Letters</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Informed Source Separation using Iterative Reconstruction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sturmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consistent Wiener Filtering for Audio Source Separation</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Spectral Mapping for Speech Dereverberation and Denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Merks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Two-stage Algorithm for Noisy and Reverberant Speech Enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Iterative Phase Recovery Framework with Phase Mask for Spectral Mapping with an Application to Speech Enhancement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Phase-Sensitive and Recognition-Boosted Speech Separation using Deep Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Clustering and Conventional Networks for Music Separation: Stronger Together</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Deep Neural Network for Time-Domain Signal Reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminatively Trained Recurrent Neural Networks for Singlechannel Speech Separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Global Conference on Signal and Information Processing</title>
				<meeting>IEEE Global Conference on Signal and Information essing</meeting>
		<imprint>
			<publisher>GlobalSIP</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speech Enhancement with LSTM Recurrent Neural Networks and its Application to Noise-Robust ASR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Latent Variable Analysis and Signal Separation</title>
				<meeting>International Conference on Latent Variable Analysis and Signal Separation</meeting>
		<imprint>
			<publisher>LVA/ICA</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end source separation with adaptive front-ends</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02514</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Complex Ratio Masking for Monaural Speech Separation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Time-Frequency Masking in the Complex Domain for Speech Dereverberation and Denoising</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech Dereverberation and Denoising using Complex Ratio Masks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03658</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Raw Waveform-Based Speech Enhancement by Fully Convolutional Networks</title>
		<idno type="arXiv">arXiv:1703.02205</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speech Enhancement using Bayesian WaveNet</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SEGAN: Speech Enhancement Generative Adversarial Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">TasNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00541</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Supervised Speech Separation Based on Deep Learning: An Overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07524</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent Deep Stacking Networks for Supervised Speech Separation</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On Training Targets for Supervised Speech Separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SDR -half-baked or well done</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mitsubishi Electric Research Laboratories (MERL)</title>
				<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech and Language Processing</title>
				<imprint>
			<date type="published" when="2006-07">Jul. 2006</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
