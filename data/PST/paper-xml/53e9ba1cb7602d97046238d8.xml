<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human detection using a mobile platform and novel features derived from a visual saliency mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Montabone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Universidad Catolica de Chile</orgName>
								<address>
									<addrLine>Casilla 306, Santiago 22</addrLine>
									<country>Pontificia, Chile</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
							<email>asoto@ing.puc.cl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Universidad Catolica de Chile</orgName>
								<address>
									<addrLine>Casilla 306, Santiago 22</addrLine>
									<country>Pontificia, Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human detection using a mobile platform and novel features derived from a visual saliency mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E62CAE238BFEA0B47DFCA1DA819AAA28</idno>
					<idno type="DOI">10.1016/j.imavis.2009.06.006</idno>
					<note type="submission">Received 7 July 2008 Received in revised form 9 January 2009 Accepted 7 June 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human detection Visual saliency Visual features Moving cameras</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human detection is a key ability to an increasing number of applications that operates in human inhabited environments or needs to interact with a human user. Currently, most successful approaches to human detection are based on background substraction techniques that apply only to the case of static cameras or cameras with highly constrained motions. Furthermore, many applications rely on features derived from specific human poses, such as systems based on features derived from the human face which is only visible when a person is facing the detecting camera. In this work, we present a new computer vision algorithm designed to operate with moving cameras and to detect humans in different poses under partial or complete view of the human body. We follow a standard pattern recognition approach based on four main steps: (i) preprocessing to achieve color constancy and stereo pair calibration, (ii) segmentation using depth continuity information, (iii) feature extraction based on visual saliency, and (iv) classification using a neural network. The main novelty of our approach lies in the feature extraction step, where we propose novel features derived from a visual saliency mechanism. In contrast to previous works, we do not use a pyramidal decomposition to run the saliency algorithm, but we implement this at the original image resolution using the so-called integral image. Our results indicate that our method: (i) outperforms state-of-the-art techniques for human detection based on face detectors, (ii) outperforms state-of-the-art techniques for complete human body detection based on different set of visual features, and (iii) operates in real time onboard a mobile platform, such as a mobile robot (15 fps).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human detection is a key ability to an increasing number of applications that operates in human inhabited environments or needs to interact with a human user. As an example, cars provided with pedestrian protection systems require some type of human detection capability <ref type="bibr" target="#b11">[12]</ref>. In the same way, an autonomous mobile robot, such as a social robot <ref type="bibr" target="#b7">[8]</ref>, needs to detect humans to perform tasks like aid for rehabilitation in hospitals <ref type="bibr" target="#b21">[21]</ref>, assistance in offices <ref type="bibr" target="#b0">[1]</ref>, or guidance in museums <ref type="bibr" target="#b3">[4]</ref>.</p><p>There is an extensive list of works dedicated to the problem of human detection <ref type="bibr" target="#b24">[24]</ref>. Most of these works are based on static cameras, where the goal is not only to detect but also to track humans <ref type="bibr" target="#b39">[39]</ref>. In this case, the most popular approach is to detect humans using background subtraction methods. These methods require that the image background does not change severely between frames. Systems using background subtraction first build a background model of the controlled environment. This has been done in many different ways <ref type="bibr" target="#b24">[24]</ref>; one example is generating a Gaussian distribution of the background as seen in <ref type="bibr" target="#b39">[39]</ref>. When the platform is mobile, however, as in the case of a mobile robot, none of the background subtraction methods can be applied.</p><p>An important aspect of human detection is that the system should be able to detect a human under different poses, however, most current solutions still rely on detecting human faces <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b20">20]</ref>. This has the disadvantage that a human can only be detected when is facing the camera. In particular, in the case of a mobile robot this leads to a loss of several social aspects. For instance, if the robot wants to initiate a conversation, the user has to already be paying attention to it, which is not always the case. Similar problems might arise in the case of a pedestrian detection system where the human can be in any pose with respect to the detecting system.</p><p>In this way, although important advances have been achieved in the development of algorithms for human detection, there is still space for further improvements, particularly in the case of a mobile platform that needs to detect humans under different poses <ref type="bibr" target="#b12">[13]</ref>. Furthermore, many applications also require real time operation, stressing the need for an efficient human detection system that can provide a timely detection. These are the main challenges that we face in this work.</p><p>In this work, we present a human detection system able to: (i) operate on a mobile platform, (ii) detect humans under different poses, including frontal, back, and profile views of the complete and upper human body, and (iii) operate in real time (15 fps).</p><p>The key principle that provides such flexibility in the detection is the use of novel features for human detection derived from a visual saliency mechanism. We call these features: visual saliency features or VSFs, and we refer to the method to extract them as VSF. These features are based on a biologically inspired attention system. Due to this, the size and shape of the filters used for calculations of the proposed features are not user defined like most previously used features for human detection <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b34">34]</ref>. Experiments in real life scenarios show that the proposed system: (i) outperforms state-of-the-art techniques for human detection based on face detectors, (ii) outperforms related systems based on different set of visual features, and (iii) operates in real time onboard a mobile robot.</p><p>This paper is organized as follows. Section 2 reviews relevant previous work on human detection using computer vision techniques. Section 3 presents relevant background material. Section 4 discusses the main details of our approach. Section 5 shows the results of testing our algorithm under different real case scenarios. Finally, Section 6 presents the main conclusions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>The state-of-the-art in human detection systems can be divided into two main categories: (i) Methods that require background substraction as a first step to detect the interesting objects. (ii) Methods that perform the detection using uncontrolled moving cameras. Our method belongs to this last category, hence, we concentrate our review in methods that do not rely in background subtraction techniques. For a more extensive review refer to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In 2004, Gavrila et al. proposed a pedestrian protection system for moving vehicles <ref type="bibr" target="#b11">[12]</ref>. Human bodies are detected using a shape-based method known as the Chamfer system. Every detected shape is then passed to a previously trained neural network as a verification step using texture as the feature. As stated by the authors, this system requires more accuracy for use in the real world.</p><p>Rajagopalan and Chellappa proposed an statistical approach for describing the shape of humans using clusters <ref type="bibr" target="#b29">[29]</ref> and later generalized it for detecting humans and vehicles <ref type="bibr" target="#b30">[30]</ref>.</p><p>In 2000, Zhao and Thorpe presented an interesting work where human detection is accomplished using the complete human body <ref type="bibr" target="#b40">[40]</ref>. It uses a neural network fed with the intensity gradient of the objects. Stereo information is used for removing areas of the image that do not belong to the object itself in order to diminish the negative effects of background clutter. The reported detection rate of this system is 85.4%.</p><p>In the work proposed by Papageorgiou and Poggio, human detection is performed using Haar-like features over a previously trained support vector machine classifier <ref type="bibr" target="#b27">[27]</ref>. Haar-like features are intensity differences at user defined rectangular regions. The size of these rectangular regions is object dependent. The authors manually select this parameter on their training data obtaining a detection rate of 90%.</p><p>In 2001, Viola and Jones constructed a fast frontal face detection system based on an extended set of Haar-like features and an Ada-Boost classifier <ref type="bibr" target="#b34">[34]</ref>. It also presents the use of an integral image in order to speed up the feature calculation process (Ref. Section 3.1). This system was later used in many works due to its real time operation and accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b37">37]</ref>. Also, this system was later updated to use motion in order to increase the detection rates <ref type="bibr" target="#b35">[35]</ref>.</p><p>Color cues have also being used to detect humans, mainly motivated by the robustness of skin color detection techniques <ref type="bibr" target="#b33">[33]</ref>. In <ref type="bibr" target="#b28">[28]</ref>, Pszczolkowski and Soto present a human detection system for a mobile platform based on depth segmentation and a new color based technique to detect faces. In <ref type="bibr" target="#b23">[23]</ref>, Munoz-Salinas et al. also use color and depth information for human detection by combining plan-view map information and a color face detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background information</head><p>In this section, first the concept of an integral image is explained. Afterwards, attention systems and saliency maps are presented. Finally, computational models of visual saliency are introduced and compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Integral image</head><p>This concept was first referred to as summed-area tables or texture mapping in the field of Computer Graphics <ref type="bibr" target="#b4">[5]</ref>. Later, this idea was brought to image processing in the work of <ref type="bibr" target="#b34">[34]</ref>. They presented a revolutionary investigation in the object detection area, producing results up to 15 times faster than previous works.</p><p>Given a grayscale image i, each pair ðx; yÞ of the integral image I of i represents the sum of the image values above and to the left of x, y: Iðx; yÞ ¼ X</p><formula xml:id="formula_0">x 0 6x;y 0 6y iðx 0 ; y 0 Þ:<label>ð1Þ</label></formula><p>Therefore, given any particular rectangular area defined by P1 ¼ ðx1; y1Þ and P2 ¼ ðx2; y2Þ its sum can be calculated in constant time using the integral image: rectSumðx1; y1; x2; y2Þ ¼ Iðx2; y2Þ À Iðx1; y2Þ À Iðx2; y1Þ þ Iðx1; y1Þ:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention systems and saliency maps</head><p>Attention systems are used to compute interesting areas of a given image or video. This is done because of the vast amounts of information that a computer vision system normally needs to process. It is impractical for those systems to exhaustively search through all the data. Therefore, only the interesting regions computed by an attention system are used, considerably decrementing the complexity of the tasks.</p><p>The most common attention systems are based on biological models <ref type="bibr" target="#b32">[32]</ref>. These models suggest that the human visual system uses only a portion of the received information in order to achieve faster results when dealing with complex scenes. This portion is known as the Focus of Attention. One of the most accepted theories about the Focus of Attention is the Treisman's Feature-integration Theory of Attention <ref type="bibr" target="#b32">[32]</ref>. Basically, he proposes that a saliency map is built by mixing parallel feature maps.</p><p>The retina of the human eye contains ganglion cells which receive the visual information from photo receptors through the bipolar cells. The receptive fields of the ganglion cells are composed of two areas, the surround and the center. There are two types of ganglion cells: (i) on-center ganglion cells which respond to bright areas surrounded by a dark background, and (ii) off-center ganglion cells which respond to dark areas surrounded by a bright background (Fig. <ref type="figure" target="#fig_0">1</ref>) <ref type="bibr" target="#b26">[26]</ref>. Most attention systems build upon the base of the Theory of Attention using center-surround differences <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computational models of visual saliency</head><p>Computational models of visual saliency often try to mimic the behavior of the receptive fields of ganglion cells. Because of calculations of on-center and off-center differences, expensive computa-tions needs to be done. Commonly, there is a trade-off between accuracy and speed of computation. As a result, usually attention systems only deliver coarse grained information to decrease processing time.</p><p>One of the most accepted and widely used computational models of attention was proposed by Itti et al. <ref type="bibr" target="#b17">[17]</ref>. This system has a solid theoretical background, is able to integrate different visual cues, <ref type="bibr" target="#b32">[32]</ref> and performs relatively well. In addition to this, a completely documented and supported implementation of the system, the iLab Neuromorphic Vision C++ Toolkit (iNVT), is publicly available <ref type="bibr" target="#b16">[16]</ref>.</p><p>Over the past few years, the original work of <ref type="bibr" target="#b17">[17]</ref> was improved in the system called VOCUS proposed by Frintrop <ref type="bibr" target="#b8">[9]</ref>. Retaining the same theory from the work of <ref type="bibr" target="#b17">[17]</ref>, but implementing it in a different manner, Frintrop managed to deliver more accurate results, but at the expense of more computational time.</p><p>The center-surround differences calculated by iNVT and VOCUS are slow. To increase the speed of the systems, both works adopted two approximations: (i) squared regions, and (ii) image pyramidal decomposition. In terms of squared regions, center-surround calculations in the human eye are circular <ref type="bibr" target="#b26">[26]</ref>. For simplicity, iNVT and VOCUS approximate these regions with squares. No substantial difference is presented in the results when using circular areas instead of squared ones <ref type="bibr" target="#b9">[10]</ref>. In terms of pyramidal decomposition, as the system works only with the most downsampled scales in the pyramid, the output quality gets compromised. This normally leads to poorly defined borders of the objects in the resulting saliency map. Although VOCUS uses the same concept of image pyramids as iNVT, the particular method used by Frintrop yields better results <ref type="bibr" target="#b8">[9]</ref>.</p><p>iNVT and VOCUS use center-surround differences to calculate saliency maps based on different visual cues. For simplicity, only intensity map computation will be described next for each system. The use of center-surround differences for the calculation of other saliency maps is analogous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">iNVT</head><p>This system first generates a grayscale version of the input image. Then, it calculates an image pyramid of eight grayscale images, each one of them scaled to one quarter of the previous one.</p><p>After that, center-surround differences are calculated as an across-scale difference between coarse and fine scales. Fine scales are defined as a scale s 2 f2; 3; 4g and coarse scales are defined as a scale s 2 f5; 6; 7; 8g.</p><p>An across-scale difference is calculated by scaling the coarse scale into the fine scale and then executing pixel by pixel subtraction. Next, all the maps are summed up to obtain the final intensity map. This yields fast but very poor feature maps <ref type="bibr" target="#b17">[17]</ref>.</p><p>Also, center-surround differences are calculated as absolute values. In other words, no difference exists between on-center and off-center differences for this system. This can be calculated faster, but lacks the flexibility of separated maps <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Vocus</head><p>In VOCUS <ref type="bibr" target="#b8">[9]</ref>, the intensity map is calculated as follows: first, the original color image is converted into grayscale. Then, a Gaussian image pyramid is created. This is achieved by applying a 3 Â 3 Gaussian filter to the grayscale image, and after that, scaling it down by a factor of two on each axis. The filtering and scaling are repeated four times, yielding five images: i 0 ; i 1 ; i 2 ; i 3 , and i 4 . From this moment on, the system only takes into account the information present in the smallest scales; images i 2 ; i 3 , and i 4 (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>The system now calculates on-center and off-center differences in the three images that represent scales s 2 f2; 3; 4g, respectively.</p><p>Centers are represented as a pixel and two surround values, r, are used: 3 and 7, based in the work of <ref type="bibr" target="#b17">[17]</ref>. Therefore, 12 intensity submaps are generated. The process of calculating these submaps is as follows: first, center and surround are defined: surroundðx; y; s; rÞ ¼</p><formula xml:id="formula_2">P x 0 ¼r x 0 ¼Àr P y 0 ¼r y 0 ¼Àr i s ðx þ x 0 ; y þ y 0 Þ À i s ðx; yÞ ð2r þ 1Þ 2 À 1 ;<label>ð3Þ</label></formula><p>centerðx; y; sÞ ¼ i s ðx; yÞ; ð4Þ then, every pixel of each intensity submap is calculated: Int On;s;r ðx; yÞ ¼ maxfcenterðx; y; sÞ À surroundðx; y; s; rÞ; 0g; ð5Þ Int Off ;s;r ðx; yÞ ¼ maxfsurroundðx; y; s; rÞ À centerðx; y; sÞ; 0g; ð6Þ where s 2 f2; 3; 4g represents the image scale, r 2 f3; 7g the surround, and On; Off , the on-center and off-center differences, respectively.</p><p>After that, an on-center intensity map is calculated. This is done scaling the six on-center intensity submaps into the largest scale; i 2 , and then summing pixel by pixel. An off-center intensity map is generated the same way, using the off-center submaps.</p><formula xml:id="formula_3">Int On ¼ È s;r Int On;s;r ;<label>ð7Þ</label></formula><formula xml:id="formula_4">Int Off ¼ È s;r Int Off ;s;r ;<label>ð8Þ</label></formula><p>where È denotes the across-scale sum previously explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Discussion</head><p>The are two main differences between VOCUS and iNVT: The first one is that VOCUS generates independent on-center and offcenter intensity maps, whereas iNVT only calculates the absolute difference between center and surround. This gives VOCUS the advantage to distinguish between these two features <ref type="bibr" target="#b8">[9]</ref>.</p><p>The other main difference is that when iNVT calculates the center surround differences, it subtracts images from fine and coarse scales, resizing to the finest scale, therefore yielding less defined borders. VOCUS instead, first calculates center surround differences in every scaled image and then resizes the images to the largest scale, adding all the computed intensity submaps pixel by pixel. This technique yields better results than the work of <ref type="bibr" target="#b17">[17]</ref> but the processing time is slower <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Drawbacks of iNVT and VOCUS</head><p>The main drawback of iNVT and VOCUS is that the resulting feature maps are very poor. They do not contain the same level of resolution provided by the original image. These works first scale down the image by a factor of 16 resulting in considerable detail loss. Then, they proceed to build an image pyramidal decomposition, further decreasing the details.</p><p>Generating good quality feature maps is needed in order to obtain fine grained information of the objects. Until now, all attention systems only provide coarse grained information in their feature maps. This is the deficiency that we exploit to use saliency information as a novel feature descriptor mechanism to directly achieve classification instead of only as a visual attention mechanism <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref> or a relevant region detector <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">31]</ref> as in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our approach</head><p>Our approach for human detection follows a standard pattern recognition scheme based on four main steps: (i) preprocessing for stereo pair calibration, (ii) segmentation using depth continuity information, (iii) Feature extraction based on visual saliency, and (iv) classification using a neural network. An overview of the overall approach is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. We describe next the details of each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preprocessing module 4.1.1. Rectification</head><p>The original images taken from the stereo camera are first rectified using the calibration parameters obtained using the small vi-  sion system (SVS) <ref type="bibr" target="#b19">[19]</ref> calibration routine. A standard chessboard printed image was used to calibrate the device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object segmentation module</head><p>Stereo information obtained from the SVS is used to segment objects from the scene. The system uses a segmentation approach based on the work of <ref type="bibr" target="#b14">[15]</ref>. There are three steps involved in this module: (i) depth segmentation, (ii) analysis of connected components on each previously segmented layer, and (iii) application of set of filters in cascade. A diagram of this module can be seen in Fig. <ref type="figure" target="#fig_3">4</ref>. The details of the different steps are presented next.</p><p>First, the system calculates the disparity image for the entire frame and a disparity histogram is generated. Each local maxima of the disparity image represents the existence of one or more solid objects at a particular depth. The original depth image is segmented into i layers, where i is the number of local maxima in the histogram of disparities. Closer objects often present more variable depth values than distant ones. When extracting the ith layer from the depth map a depth dependent value Dd i is generated first. This value is calculated using a polynomial fit on previously obtained correct human segmentation values at different depths. Only values that lie inside the depth range defined by ðd i À Dd i ; d i þ Dd i Þ are present in the ith layer. Every layer is separated from the original depth map generating isolated images of different objects at different distances. Afterwards, every layer is segmented using standard connected components analysis.</p><p>As a final post-processing step, every blob is passed through a cascade set of filters. The system filters out candidate blobs using size and shape constraints. An estimate of the object's real height, width and depth can be obtained using stereo vision. Also, the bounding box that defines each blob provides aspect ratio information. In this way, using thresholds adapted to the expected dimensions of a human in an image, the system filters out blobs that: (i) present a real world height that is less than 40 cm, (ii) are wider than its height, or (iii) have a height that is more than three times its width. Filtered out blobs are split using a simple method. This method consists in measuring the valid height for every line in the blob, yielding a maximum valid height. Every line that is shorter than half of the maximum height is eliminated, splitting the blob.</p><p>It is worth to mention that the previous scheme provides a high level of freedom to move the video cameras, as opposed to other segmentation methods commonly used to detect human by a mobile robot, such as methods based on floor plane assumptions <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature extraction module</head><p>Attention systems are commonly used in computer vision applications as preprocessing modules (Ref. Section 3.2). As we explained before, the high computational cost of running visual attention systems at full image resolution has avoided the use of saliency mechanisms as direct feature extraction methods. This explains why most common attention systems provide only coarse grained information. First, layers are obtained through depth segmentation using stereo analysis. Then, connected components analysis is used in order to extract blobs from each layer. Each blob is then filtered using size and shape constraints. This work proposes the use of visual saliency as direct features for human detection. In order to obtain high quality features, a novel way of calculating visual saliency is presented. The proposed features or VSFs use the same biological theory as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">17]</ref> but uses an integral image on the original scale in order to obtain high quality features in real time (Ref. Section 3.1).</p><p>First, a Gaussian filter with a 3 Â 3 window is used twice, in order to smooth the image and obtain the same robustness to noise as the previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">17]</ref>. Then, the system calculates on-center and off-center differences separately using a unique integral image with variable size filter windows over the original grayscale image (see Fig. <ref type="figure" target="#fig_4">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Filter windows</head><p>VOCUS filter windows are defined by the scale s 2 f2; 3; 4g and the surround r 2 f3; 7g. Therefore there are six different sized filter windows in VOCUS. Using them in order to calculate on-center and off-center differences yield the 12 intensity submaps previously referred to.</p><p>The VSF method implements all of the same filter windows used in VOCUS. The main difference is that the filter is applied to the entire original image, instead of scaled down versions. Therefore, the system uses only a single parameter to define all the filter windows that will be calculated on a single integral image:</p><formula xml:id="formula_5">1 ¼ r2 s :<label>ð9Þ</label></formula><p>where r represents the surround and s, the scale, both used in the VOCUS system. Also, 1 denotes the surround to be used in VSF in order to cover the same window as the VOCUS window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Feature calculation</head><p>In order to calculate the intensity submaps, we first define center and surround using Eq. (2): surroundðx; y;</p><formula xml:id="formula_6">1Þ ¼ rectSumðx À 1; y À 1; x þ 1; y þ 1Þ À iðx; yÞ ð21 þ 1Þ 2 À 1 ;<label>ð10Þ</label></formula><p>centerðx; yÞ ¼ iðx; yÞ:</p><p>Then, every pixel of each intensity submap is calculated as follows:</p><p>Int On;1 ðx; yÞ ¼ maxfcenterðx; yÞ À surroundðx; y; 1Þ; 0g; ð12Þ Int Off ;1 ðx; yÞ ¼ maxfsurroundðx; y; 1Þ À centerðx; yÞ; 0g; ð13Þ where 1 2 f12; 24; 28; 48; 56; 112g represents the surround, and On; Off , the on-center and off-center differences, respectively. Note that the values of 1 are specially calculated using Eq. ( <ref type="formula" target="#formula_5">9</ref>) in order to process the same windows as the VOCUS system (see Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>Then, an on-center intensity map is calculated. This is done summing the six on-center intensity submaps pixel by pixel. An off-center intensity map is generated the same way, using the off-center submaps. Finally, both maps are summed up.</p><formula xml:id="formula_8">Int On ¼ X 1 Int On;1 ;<label>ð14Þ</label></formula><formula xml:id="formula_9">Int Off ¼ X 1 Int Off ;1 :<label>ð15Þ</label></formula><p>All the details of the image are preserved because the surround window varies according to the surrounding and scaling values proposed in VOCUS. This is done in order to calculate the same center-surround differences but in a highly accurate way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Center-surround calculation results</head><p>The results shown in Figs. <ref type="figure" target="#fig_5">6</ref> and<ref type="figure" target="#fig_6">7</ref> demonstrate the positive effects of not scaling the images when calculating the center-surround differences. VSF provides fine grained feature maps and much more defined borders. Other systems, such as VOCUS, only generate coarse grained feature maps with poorly defined borders.</p><p>In this work, a novel feature set is proposed: VSFs. Each segmented object is passed to the feature extracting module, obtaining VSFs. These VSFs represent the most salient parts of each object. As these features are biologically inspired, filters size and shape are obtained from previous investigations on how the retina perceives light stimuli <ref type="bibr" target="#b26">[26]</ref> instead of being user defined as most previously used features for human detection. Examples of VSFs can be seen in Fig. <ref type="figure" target="#fig_7">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">VSF vs VOCUS computational costs</head><p>VOCUS explicitly uses Eq. ( <ref type="formula">4</ref>) to determine its features for each image pixel at every scale. Given a center-surround filter window of n 2 pixels, VOCUS feature calculation has order Oðn 2 Þ for each filter because it sums every term individually. On the other hand, VSF uses the integral image (Ref. Section 3.1) to obtain the same calculation in constant time ðOð1ÞÞ. This provides VSF the ability to calculate features with filters of any size at constant speed. Using this ability, VSF calculates the same features as VOCUS but without the need to down sampling the original image. Instead of keeping the same size of the filters and resizing the original image as VOCUS does, VSF uses filters of different size (emulating the ones from VOCUS) applied at the original image resolution. The overhead of VSF is the initial calculation of the integral image, however, this needs to be calculated only once by a straight-forward scan over the whole image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object classification module</head><p>The system uses a feedforward neural network trained with backpropagation in order to classify objects. The inputs of this module are the VSFs, scaled into 18 Â 36 pixels. Example images can be seen in Fig. <ref type="figure" target="#fig_7">8</ref>. The neural network is composed of three layers: (i) the input layer with 648 neurons, one for each pixel, (ii) the hidden layer with 5 neurons, and (iii) the output layer with 2 neurons, where the likelihood of being a human or non-human is stored. The criteria for human acceptance is calculated by comparing both output neurons. The one that has the largest value represents the output of the neural net. In order to obtain faster calculations Sigmoid activation functions were used. Therefore every pixel in the input image has to be scaled into the fÀ1 . . . 1g range. The values selected are similar to standard ones used in other works that rely on a neural network for object classification <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation and results</head><p>In order to test the proposed system, we measure its performance on three main experiments: (i) detection of humans under different poses, (ii) comparison with a commonly used technique to detect humans based on face detection, and (iii) comparison with other human detection systems based on different visual features.</p><p>The system was mounted on top of a mobile robot. This robot was exposed to real life scenarios in different human inhabited environments. Fig. <ref type="figure" target="#fig_8">9</ref> shows some of the environments used to test our method. The base of the robot is a Pioneer P3-DX. A stereo camera is placed on top of the robot and it is able to take up to 30 frames per second. The stereo camera is configured in order to deliver valid range data from around 1.5-10 m. The system is expected to operate in non-crowded scenarios. The stereo camera is placed on top of a pan-tilt system. A saccadic control to manipulate the motion of the camera is used. The stereo camera is con-nected via firewire directly to a notebook which processes the information in addition to controlling the pan-tilt system. The system is implemented in C++ on a laptop with an AMD Sempron 1.79 GHz, 1.12 GB of RAM, running Linux Ubuntu 7.04 with a firewire interface to the stereo camera. This implementation allows us to achieve real time operation.</p><p>As training data, several images were acquired from diverse environments segmenting different objects from the scenes. In order to obtain these images, the system wandered around those environments and stored every segmented object. Using this output, the authors manually labeled 2239 object images, where 985 of them represented humans and 1254 represented non-human objects, such as trees, windows, doors, light posts, etc. Images were taken in both indoor and outdoor locations, such as offices, parks, and streets. In the set, humans appear under six different poses: frontal, back, and profile, with full and upper body views for each case. An example of these poses can be seen in Fig. <ref type="figure" target="#fig_10">10</ref> and further examples can be seen in Fig. <ref type="figure" target="#fig_11">12</ref>. Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> present further details of the training images. Example training images can be seen in Fig. <ref type="figure" target="#fig_9">11</ref>. Each image has been masked with the available stereo information and resized to a common size of 18 Â 36 pixels. The database is publicly available at http://samontab.googlepages.com/pedDatabase.rar.</p><p>It can be seen in Table <ref type="table" target="#tab_0">1</ref> that the number of training images for frontal poses are slightly higher than the rest. This is explained because of the social nature of the robot. While obtaining training data, most people appeared in front of the robot staring at it. This difference in proportion that naturally appeared is kept in the   training set in order to represent the real type of interaction expected with people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Different poses detection</head><p>The idea of this experiment is to test the performance of the system under different human poses. According to the training set described before, each human detection was archived in one of the following six categories: (i) full body, frontal; (ii) full body, back; (iii) full body, profile; (iv) upper body, frontal; (v) upper body, back, and (vi) upper body, profile. As this system is designed for a social robot with live video input instead of still images, every detection is considered only as one per human per pose. This means that, given a video sequence, a single human can only generate up to six detections in the system, one for each pose.</p><p>Tables <ref type="table">3</ref> and<ref type="table">4</ref> present detailed information about system performance in this experiment. It can be seen from these results that the system can detect humans regardless of their pose. As these results were obtained in different types of environment, the system can be used for both indoor and outdoor applications, although bad illumination can affect the performance of the system, mostly in the segmentation module, as stated in previous works with the same hardware <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Human face vs complete human body detection</head><p>In this experiment, the system was tested in order to compare the overall detection rate of a face detection system compared to that of the proposed system using live camera input in the same scenarios as the previous test: a public park, in the street, in a house, and in an office environment. People appear in the scene naturally, therefore different poses are presented. Each object detection is counted only once. Consecutive detections of the same object, whether it is a human or not, are not considered. The Viola Jones face detector (VJFD) system is selected for the test as it is commonly used in human detection for social robots <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">20]</ref>. We use the openCV implementation of VJFD <ref type="bibr" target="#b25">[25]</ref>. Although this implementation was trained with a different image set, our goal here is to show how in common environments where social robots can operate, a general detector able to detect humans in different   poses can be of great help. Detailed results of this experiment are presented in Table <ref type="table">5</ref>. Table <ref type="table">6</ref> shows that the proposed method provides higher detection rates and less false positives than the Viola Jones face detector (Figs. <ref type="figure" target="#fig_12">13</ref> and<ref type="figure" target="#fig_13">14</ref>). In Table <ref type="table">7</ref> can be seen that the largest difference in performance is obtained in outdoor environments. This can be explained because people is less restricted in outdoor environments, they can appear in many different poses and distances to the camera. As face detection systems are restricted only to detect humans facing the camera, their detection rate should drop if humans appear in different poses. Table <ref type="table">6</ref> also shows that false positives are very low for the proposed method compared to Viola Jones. This is due in part to the fact that the proposed method uses real world measurements based on stereo vision and shape constraints in order to filter out non-human like regions. Table <ref type="table">5</ref> shows that the proposed system only missed one person. This situation is presented in Fig. <ref type="figure" target="#fig_4">15</ref> where an almost full occlusion prevents the system from correctly detecting both persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Feature comparison</head><p>In this experiment, we test the proposed VSFs against different visual features previously used for human detection. In particular, we choose to compare against features derived by: edge detectors, intensity gradients <ref type="bibr" target="#b40">[40]</ref>, and Haar-like wavelets <ref type="bibr" target="#b34">[34]</ref>. Furthermore, we include as features the result of a traditional saliency map such as VOCUS. Edge detectors are one of the earliest features used for object detection. Although these features can represent the shape of an object, they are not robust to noise. <ref type="bibr" target="#b40">[40]</ref> proposed the use of intensity gradient in order to obtain higher flexibility. In a related approach, recently <ref type="bibr" target="#b5">[6]</ref> used histograms of oriented intensity gradients to detect humans. <ref type="bibr" target="#b34">[34]</ref> made popular the use of Haar-like wavelets for object detection. The main drawback of these features is that filters size and shape are often user defined. In this paper we use the values used in <ref type="bibr" target="#b27">[27]</ref>. As we detailed before, the proposed VSFs use predefined filters shape and size, based on biological findings about the operation of the human visual system.</p><p>Exactly the same detection test is performed for all the features under evaluation. K-fold cross-validation over the training set is used in order to measure the system classification error estimate. The number of folds used in this experiment is ten as suggested in <ref type="bibr" target="#b38">[38]</ref>. Therefore, each test set is conformed of 223 random images and the other 2016 images are used as the training set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Detailed performance of the proposed system for human detection in different poses: front, back and profile views of the front body and the upper body.</p><p>Results of the comparison are presented in Table <ref type="table" target="#tab_2">8</ref>. Also Fig. <ref type="figure" target="#fig_5">16</ref> shows the resulting ROC curves. It can be seen that the proposed VSFs present a higher detection rate and a lower false positive rate than the previously used features. Also, it can be seen that traditional attention features such as VOCUS, perform very poorly due to the coarse grained feature maps they provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>In this paper, we propose and test a human detection system designed to operate onboard mobile platforms, such as a mobile robot. Using a stereo vision based segmentation algorithm, novel visual features based on a saliency mechanism (VSFs), and a neural network based classifier, the resulting system is able to: (i) detect humans on different poses, (ii) operate onboard a mobile platform, and (iii) operate in real time. After testing the system under real world conditions on several human inhabited environments, such as a public park and indoor buildings, our results indicate an average detection rate of 94.73% and a false positive rate of 6.15%, as estimated using 10-fold cross-validation.</p><p>Among the main novelties of the proposed approach is a center-surround saliency mechanism used to directly obtain the main visual features used for human detection. This type of saliency mechanism has been used before but as part of visual attention systems where the main goal is to detect interesting regions of an input image, therefore, feature map details are less relevant than timely computation. In our case, by using an efficient implementation of center-surround differences through the so-called integral image, we demonstrate a method to generate fine grained feature maps of visual saliency operating in real time at the original image resolution.</p><p>One of the main robustness of the proposed approach is its capacity to detect humans under different poses. In particular, we test the advantage of such flexibility by contrasting the proposed system against a popular face detector algorithm that is commonly used in the mobile robotics arena to detect humans. Our results on common situations faced by a mobile robot show a highly significant increase in human detection rate. In this way, by using the proposed system, a robot can significantly increase its social capabilities by not requiring to detect just the humans that are facing it.</p><p>We also test the new proposed VSFs against previously visual features used for human detection, such as intensity gradient or the common Haar-like features. Our result indicates that VSFs outperforms previous features. In particular, we believe that VSFs have applications further beyond human detection, as relevant visual features for other visual classification problems.</p><p>As a future work, detection of humans in crowded scenarios should be considered. Also, the benefits of the use of a fine grained saliency map in other areas can be investigated. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. On-center and off-center ganglion cells and their approximation on computational models of visual saliency.</figDesc><graphic coords="3,84.59,517.27,167.05,212.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. VOCUS image scales. In VOCUS, the original image is converted into grayscale. Next, five different image scales are created ði0; . . . ; i4Þ. The system then works with the smallest scales ði2; i3; i4Þ, represented in the bottom images.</figDesc><graphic coords="4,112.08,67.92,358.56,204.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. General diagram of the proposed system.</figDesc><graphic coords="4,95.07,507.52,393.97,231.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Object segmentation module diagram. First, layers are obtained through depth segmentation using stereo analysis. Then, connected components analysis is used in order to extract blobs from each layer. Each blob is then filtered using size and shape constraints.</figDesc><graphic coords="5,153.07,67.92,298.21,83.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Top: filter windows used in VOCUS for scales 2, 3, and 4, respectively. Red windows (larger) represent r ¼ 7, and green (smaller), r ¼ 3. Bottom: filter windows used in VSF, red windows represent 1 values of 28, 56, and 112, respectively, while green windows represent 1 values of 12, 24, and 48, respectively. Notice how the filters in VOCUS loose detail as the size of the filter window grows larger, in contrast, the ones in VSF preserve all their details at all scales.</figDesc><graphic coords="5,133.23,514.77,337.79,206.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparing results of VOCUS and VSF. Notice how the detail is preserved in VSF (right) and how the VOCUS results are poorly defined (left).</figDesc><graphic coords="6,306.71,67.92,241.20,324.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Image details. VOCUS (center) only calculates coarse grained center-surround differences, loosing important details of the image. Instead, VSF (right) uses all the available information to provide a fine grained feature map.</figDesc><graphic coords="7,119.06,67.92,363.60,135.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples of VSFs. Top row shows the resulting VSFs for several humans in different positions. Bottom row shows the resulting VSFs sets for random objects such as windows, walls, and doors.</figDesc><graphic coords="7,81.24,257.39,173.87,99.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Different environments where the system was run.</figDesc><graphic coords="7,348.49,255.97,177.11,167.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Training set. Examples of the training images used in this work. Top row shows humans while bottom row shows non-human objects.</figDesc><graphic coords="8,80.90,508.59,154.80,221.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Different poses detection. These images show the different poses that the system can detect.</figDesc><graphic coords="8,109.25,67.92,365.40,213.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Detection examples. The images show different detections of the system for human under different poses.</figDesc><graphic coords="9,133.23,67.92,335.90,555.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Human detection rates. Comparison of the human detection rates between the proposed method and the Viola Jones face detector.</figDesc><graphic coords="10,44.84,67.92,226.93,134.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. False positives comparison. Comparison of the false positives between the proposed method and the Viola Jones face detector.</figDesc><graphic coords="10,44.67,249.28,227.28,129.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Poses of training images. This table shows the number of training images for each pose the system can detect.</figDesc><table><row><cell></cell><cell>Frontal</cell><cell>Back</cell><cell>Profile</cell></row><row><cell>Full body</cell><cell>194</cell><cell>113</cell><cell>102</cell></row><row><cell>Upper body</cell><cell>351</cell><cell>137</cell><cell>88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Training set details.</figDesc><table><row><cell>Environment</cell><cell>Type</cell><cell>Human examples</cell><cell>Non-human examples</cell></row><row><cell>Public park</cell><cell>Outdoor</cell><cell>148</cell><cell>314</cell></row><row><cell>Street</cell><cell>Outdoor</cell><cell>134</cell><cell>392</cell></row><row><cell>House</cell><cell>Indoor</cell><cell>246</cell><cell>117</cell></row><row><cell>Office</cell><cell>Indoor</cell><cell>457</cell><cell>431</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 8</head><label>8</label><figDesc>Feature comparison. The table presents the comparison of results obtained using the default parameters for different features previously used for human detection against a traditional saliency map (VOCUS) and the proposed VSFs. Note that while a traditional saliency map such as VOCUS perform very poorly, VSFs present the best results.</figDesc><table><row><cell>Feature type</cell><cell>Det rate (%)</cell><cell>FP rate (%)</cell></row><row><cell>Edges</cell><cell>89.14</cell><cell>11.63</cell></row><row><cell>Intensity gradient</cell><cell>89.95</cell><cell>10.37</cell></row><row><cell>Haar like</cell><cell>92.39</cell><cell>7.26</cell></row><row><cell>VOCUS</cell><cell>79.84</cell><cell>34.08</cell></row><row><cell>VSFs</cell><cell>94.73</cell><cell>6.15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>S. Montabone, A. Soto / Image and Vision Computing 28 (2010) 391-402</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially funded by FONDECYT grant 1070760 and Anillos ACT-32.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jijo-2: an office robot that communicates and learns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Motomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayamizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunschoten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multisensor integration for human-robot interaction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bellotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Intelligent Cybernetic Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<title level="m">Towards a humanoid museum guide robot that interacts with multiple persons, in: 5th IEEE-RAS International Conference on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="418" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experiences with an interactive museum tour-guide robot</title>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hahnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lakemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Summed-area tables for texture mapping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 18</title>
		<meeting>SIGGRAPH 18</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised identification of useful visual landmarks using multiple segmentations and top-down feedback</title>
		<author>
			<persName><forename type="first">P</forename><surname>Espinace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="538" to="548" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of socially interactive robots</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="143" to="166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">VOCUS: A Visual Attention System for Object Detection and Goaldirected Search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Rheinische Friedrich-Wilhelms-Universitat Bonn Germany</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A real-time visual attention system using integral images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Computer Vision Systems</title>
		<meeting>the 5th International Conference on Computer Vision Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian protection systems: issues, survey, and challenges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="430" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision-based pedestrian detection: the protector system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Munder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicle Symposium</title>
		<meeting>the IEEE Intelligent Vehicle Symposium</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computer vision approaches to pedestrian detection: visible spectrum survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes In Computer Science</title>
		<imprint>
			<biblScope unit="volume">4477</biblScope>
			<biblScope unit="page" from="547" to="554" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design of a social mobile robot using emotion-based decision mechanisms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hollinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manfredi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pezzementi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stereovision-based object segmentation for automotive applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Applied Signal Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2322" to="2329" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">16. ROC curves for human detection using different visual features</title>
		<author>
			<persName><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The ilab neuromorphic vision C++ toolkit: free tools for the next generation of vision algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Neuromorphic Engineer</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An affine invariant salient region detector</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV-04)</title>
		<meeting>the European Conference on Computer Vision (ECCV-04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The SRI small vision system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<ptr target="&lt;http://www.ai.sri.com/~konolige/svs/&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Providing the basis for human-robot-interaction: a multi-modal attention system for a mobile robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kleinehagenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hohenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interfaces</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Socially assistive robotics for post-stroke rehabilitation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mataric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Feil-Seifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Winstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroengineering and Rehabilitation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An affine invariant interest point detector</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV-02)</title>
		<meeting>the European Conference on Computer Vision (ECCV-02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">People detection and tracking using stereo vision and color</title>
		<author>
			<persName><forename type="first">R</forename><surname>Munoz-Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garcia-Silvente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="995" to="1007" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A Survey of Techniques for Human Detection from Video, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ogale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">OpenCV: Open Source Computer Vision Library</title>
		<ptr target="&lt;http://www.intel.com/research/mrl/research/opencv/&gt;" />
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">Vision Science: Photons to Phenomenology</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A trainable system for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human detection in indoor environments using multiple visual cues and a mobile robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pszczolkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection of people in images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedinds of the IEEE International Joint Conference on Neural Networks (IJCNN&apos;99)</title>
		<meeting>eedinds of the IEEE International Joint Conference on Neural Networks (IJCNN&apos;99)<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Higher order statistics-based detection of people/vehicles in images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-view matching for unordered image sets, or how do I organize my holiday snaps?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV-02)</title>
		<meeting>the European Conference on Computer Vision (ECCV-02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on pixel-based skin color detection techniques</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sazonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphicon-03</title>
		<meeting>Graphicon-03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<idno>CVPR-01</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV03)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the usefulness of attention for object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV-04)</title>
		<meeting>the European Conference on Computer Vision (ECCV-04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A multi-modal system for tracking and analyzing faces on a mobile robot</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Bohme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eibe</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pfinder: real-time tracking of the human body</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="780" to="785" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stereo and neural network-based pedestrian detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="148" to="154" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
