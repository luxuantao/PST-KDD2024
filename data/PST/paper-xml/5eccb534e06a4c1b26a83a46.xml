<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Adversarial Examples from the Mutual Influence of Images and Perturbations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
							<email>chaoningzhang1990@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology (KAIST)</orgName>
								<orgName type="laboratory">indicates equal contribution Robotics and Computer Vision (RCV) Laboratory Korea Advanced</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tooba</forename><surname>Imtiaz</surname></persName>
							<email>timtiaz@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology (KAIST)</orgName>
								<orgName type="laboratory">indicates equal contribution Robotics and Computer Vision (RCV) Laboratory Korea Advanced</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Adversarial Examples from the Mutual Influence of Images and Perturbations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A wide variety of works have explored the reason for the existence of adversarial examples, but there is no consensus on the explanation. We propose to treat the DNN logits as a vector for feature representation, and exploit them to analyze the mutual influence of two independent inputs based on the Pearson correlation coefficient (PCC). We utilize this vector representation to understand adversarial examples by disentangling the clean images and adversarial perturbations, and analyze their influence on each other. Our results suggest a new perspective towards the relationship between images and universal perturbations: Universal perturbations contain dominant features, and images behave like noise to them. This feature perspective leads to a new method for generating targeted universal adversarial perturbations using random source images. We are the first to achieve the challenging task of a targeted universal attack without utilizing original training data. Our approach using a proxy dataset achieves comparable performance to the state-of-the-art baselines which utilize the original training dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have shown impressive performance in numerous applications, ranging from image classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref> to motion regression <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">47]</ref>. However, DNNs are also known to be vulnerable to adversarial attacks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref>. A wide variety of previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref> explore the reason for the existence of adversarial examples, but there is a lack of consensus on the explanation <ref type="bibr" target="#b0">[1]</ref>. While the working mecha- nism of DNNs is not fully understood, one widely accepted interpretation considers DNNs as feature extractors <ref type="bibr" target="#b15">[16]</ref>, which inspires the recent work <ref type="bibr" target="#b16">[17]</ref> to link the existence of adversarial examples to non-robust features in the training dataset.</p><p>Contrary to previous works analyzing adversarial examples as a whole (summation of image and perturbation), we instead propose to analyze adversarial examples by disentangling image and perturbations and studying their mutual influence. Specifically, we analyze the influence of two independent inputs on each other in terms of contributing to the obtained feature representation when the inputs are combined. We treat the network logit outputs as a means of feature representation. Traditionally, only the most important logit values, such as the highest logit value for classification tasks, are considered while other values are disregarded. We propose that all logit values contribute to the feature representation and therefore treat them as a 1 logit vector. We utilize the Pearson correlation coefficient (PCC) <ref type="bibr" target="#b1">[2]</ref> to analyze the extent of linear correlation between logit vectors. The PCC values computed between the logit vectors of each independent input and the input combination gives insight on the contribution of the two independent inputs towards the combined feature representation. Our proposed general analysis framework is shown to be useful for analyzing influence of any two independent inputs, such as images, Gaussian noise, perturbations, etc. In this work, we limit the focus on analyzing the influence of image and perturbation in universal attacks. Our findings show that for a universal attack, the adversarial examples (AEs) are strongly correlated to the UAP, while a low correlation is observed between AEs and input images (see <ref type="bibr">Figure 4)</ref>. This suggests that for a DNN, UAPs dominate over the clean images in AEs, even though the images are visually more dominant. Treating the DNN as feature extractor, we naturally conclude that the UAP has features that are more dominant compared to the features of the images to attack. Consequently we claim that "UAPs are features while images behave like noise to them". This is contrary to the general perception that treats the perturbation as noise to images in adversarial examples. Our interpretation thus provides a simple yet intuitive insight on the working of UAPs.</p><p>The observation, that images behave like noise to UAPs motivates the use of proxy images to generate targeted UAPs without original training data, as shown in Figure <ref type="figure">1</ref>. Our proposed approach is more practical because the training data is generally inaccessible to the attacker <ref type="bibr" target="#b31">[32]</ref>. Our contributions can be summarized as follows:</p><p>• We propose to treat the DNN logits as a vector for feature representation. These logit vectors can be used to analyze the contribution of features of two independent inputs when summed towards the output. In particular, our analysis results regarding universal attacks reveal that in an AE, the UAP has dominant features, while the image behaves like noise to them.</p><p>• We leverage this insight to derive a method using random source images as proxy dataset to generate targeted UAPs without original training data. To our best knowledge, we are the first to fulfill this challenging task while achieving comparable performance to the state-of-the-art baselines utilizing the original training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We summarize previous works with two focuses: (1) explanations of adversarial vulnerability and (2) existing adversarial attack methods.</p><p>Explanation of adversarial vulnerability. Goodfellow et al. attribute the reason of adversarial examples to the local linearity of DNNs, and support their claim by their proposed simple yet effective FGSM <ref type="bibr" target="#b13">[14]</ref>. However, this linearity hypothesis is not fully compatible with the existence of adversarial examples which violate local linearity <ref type="bibr" target="#b23">[24]</ref>. Moreover, it can not fully explain the phenomenon that greater robustness is not observed in less linear classifiers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. Another body of works attributes the reason for low adversarial robustness to highdimensional input properties <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13]</ref>. However, reasonably robust DNNs of high-dimensional inputs can be trained in practice <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. One recent work <ref type="bibr" target="#b16">[17]</ref> attributes the reason for the existence of adversarial examples to non-robust features in the dataset. Some previous explanations, ranging from limited training data induced over-fitting <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref> to robustness under noise <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>, are well aligned with their framework <ref type="bibr" target="#b16">[17]</ref>. The concept of non-robust features is also implicitly explored in other works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. On the other hand, possible reasons for vulnerability against universal adversarial perturbations have been explored in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>. Their analysis is mainly based on the network decision boundaries, in particular, the existence of universal perturbations is linked to the large curvature of decision boundary. Our work mainly focuses on the explanation of universal adversarial vulnerability. One core aspect that differentiates our analysis framework from previous works is that we explore the influence of images and perturbations on each other, while previous works mainly analyze adversarial example as a whole <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18]</ref>. We explicitly analyze how the image and perturbations influence each other. Our analysis framework is mainly based on the proposed logit vector interpretation of how DNNs respond to the features in the input, without relying on the curvature property of decision boundaries <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Existing adversarial attack methods. The existing attacks are commonly categorized under image-dependent attacks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref> and universal (i.e. image-agnostic) attacks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34]</ref> which devise one single perturbation to attack most images. Image-dependent attack techniques have been explored in a variety of works ranging from optimization based techniques <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref> to FGSM related techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45]</ref>. Universal adversarial perturbations (UAPs) were first proposed by <ref type="bibr" target="#b26">[27]</ref>, and deploy the DeepFool attack <ref type="bibr" target="#b29">[30]</ref> iteratively on single data samples. Due to the nature of being image-agnostic, universal attacks constitute a more challenging task than image-dependent ones.</p><p>Another way to categorize attacks is non-targeted vs. targeted attacks. Generative targeted universal perturbations have been explored by <ref type="bibr" target="#b34">[35]</ref>. Targeted attacks can be seen as a special, but more challenging case of non-targeted attacks. Class discriminative (CD) UAPs were proposed in <ref type="bibr" target="#b45">[46]</ref>, aiming to fool only a subset of classes. The above mentioned universal attacks require utilization of the original training data. However, in practice the attacker often has no access to the training data <ref type="bibr" target="#b31">[32]</ref>. To overcome this limitation, Mopuri et al. propose to generate universal perturbation without training data <ref type="bibr" target="#b31">[32]</ref>. However, their approach is specifically designed for non-targeted attacks by maximizing the activation scores in every layer, and their performance is inferior to approaches with access to original training data. Another attempt for data-free non-targeted universal attack by training a network to generate proxy images is explored in <ref type="bibr" target="#b37">[38]</ref> . No prior work is found to have achieved targeted universal attack without access to the original training data, and our work is the first attempt in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Logit Vector</head><p>Following the common consensus that DNNs are feature extractors, we intend to analyze adversarial examples from the feature perspective. The logit values are often used as an indicator of feature presence in an image. Previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>, however, mainly focus only on the DNN highest logit output indicating the predicted class, while all other logits are usually neglected. "Logits" refer to the DNN output before the final softmax layer. In this work, we assume that all DNN output logit values represent the network response to features in the input. One concern about this vector interpretation is that only the logits of the ground-truth classes or other semantically similar classes are meaningful, while the other logits might be just random (small) values and thus do not carry important information. We address this concern after introducing the terms and notation used throughout this work.</p><p>A deep classifier Ĉ maps an input image x ∈ R d with a pixel range of [0, 1] to an output logit vector L x = Ĉ(x). The vector L x has K entries corresponding to the total number of classes. The predicted class y x of an input x can then be calculated from the logit vector as y x = arg max(L x ). We adopt the logit vector to facilitate the analysis of the mutual influence of two independent inputs in terms of their contribution to the combined feature representation. We mainly consider two independent inputs a ∈ R d and b ∈ R d , which can be images, Gaussian noise, perturbations, etc., whose corresponding logit vectors are denoted as L a and L b , respectively. The summation of these two inputs c = a + b, when fed to a DNN, leads to the feature representation L c . Both inputs a and b contribute partially to L c . Moreover, it is reasonable to expect that the contribution of each input will be influenced by the other one. Specifically, the extent of influence will be reflected in the linear correlation between the individual logit vector L a (or L b ) and L c . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pearson Correlation Coefficient</head><p>In statistics, the Pearson correlation coefficient (PCC) <ref type="bibr" target="#b1">[2]</ref> is a widely adopted metric to measure the linear correlation between two variables. In general, this coefficient is defined as</p><formula xml:id="formula_0">PCC X,Y = cov(X, Y ) σ X σ Y ,<label>(1)</label></formula><p>where cov indicates the covariance and σ X and σ Y are the standard deviation of vector X and Y , respectively, and the PCC values range from −1 to 1. The absolute value indicates the extent to which the two variables are linearly correlated, with 1 indicating perfect linear correlation, 0 indicating zero linear correlation, and the sign indicates whether they are positively or negatively correlated. Treating the logit vector as a variable, the PCC between different logit vectors can be calculated. We are mainly concerned about PCC La,Lc and PCC L b ,Lc , since PCC La,L b is always close to zero due to independence. Comparing PCC La,Lc and PCC L b ,Lc can provide insight about the contribution of the two inputs to L c , with a higher PCC value indicating the more significant contributor. For example, if PCC La,Lc is larger than PCC L b ,Lc , input a's share can be seen as more dominant than input b towards the final feature response. The relationship of two logit vectors, L a and L c for instance, can be visualized by plotting each logit pair. The extent of their correlation can be observed and quantified by the PCC. As a basic example, we show the logit vector analysis of two randomly sampled images from ImageNet <ref type="bibr" target="#b40">[41]</ref> in Figure <ref type="figure" target="#fig_1">2</ref>. The plot shows a strong linear correlation between L b and L c (PCC L b ,Lc = 0.88), while L a and L c are practically uncorrelated (PCC La,Lc = 0.19). These observations sug-gest a dominant contribution of input b towards logit vector L c . As a result, the same label "Wood rabbit" is predicted for c and b. Such combination of images has also been explored in Mixup <ref type="bibr" target="#b48">[49]</ref> for training classifiers. To establish the reliability of the PCC value as a metric, we repeat the above experiment with 1000 image pairs and report results on the effectiveness of PCC to predict label c in Table <ref type="table" target="#tab_0">1</ref>. We divide the image pairs into two groups: S m and S n . S m comprises of image pairs having the same predicted class y c as the prediction y a or y b . For S n , the predicted class y c is different from both y a and y b . Moreover, we use the parameter P PCC to show the proportion of predictions correctly inferred from the PCC values relative to the network predictions for c. For the image pairs from set S m , the P PCC is 96%, confirming the reliability of the PCC as our metric. The high gap between PCC h and PCC l further provides evidence for the high P PCC . For the image pairs from S n , PCC h − PCC l is smaller, implying that neither of the inputs is significantly dominant.</p><p>Recall that there is a concern that most logit values might be just random values, which is partially addressed by observing the correlation between PCC and y c as shown in Figure <ref type="figure" target="#fig_1">2</ref>. If the concern were valid, such that only a few logits are meaningful (i.e. only the highest logits or the logits for semantically similar classes), a high divergence should be observed for the less significant logits. However, this assumption does not align well with the results in Figure <ref type="figure" target="#fig_1">2</ref>, thus confirming the importance of all logit values. A higher PCC value for the dominant input further rules out the concern that the lower logit values are random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Influence of Images and Perturbations on Each Other</head><p>In this section, we analyze the interaction of clean images with Gaussian noise perturbation, universal perturbations and image-dependent perturbations. In doing so, input a is the image and input b the perturbation. The analysis is performed on VGG19 pretrained on ImageNet. For consistency, a randomly chosen a (shown in Figure <ref type="figure" target="#fig_1">2</ref>, top left) is used for all experiments. Along the same lines, for targeted perturbations we randomly set 'sea lion' as the target class t. For more results with different images and target classes on different networks, please refer to the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of Universal Perturbations</head><p>Universal perturbations come in two flavors: targeted and non-targeted. We use Algorithm 1 with loss function L t CL2 to generate targeted universal perturbations, and generate non-targeted universal perturbations using Equation <ref type="formula">4</ref>as the loss function. The results of this analysis are shown for a targeted and non-targeted UAP in Figure <ref type="figure">4</ref> and Figure <ref type="figure">5</ref>, respectively. For the targeted scenario, two major observations can be made: First, PCC La,Lc is smaller than PCC L b ,Lc , indicating a higher linear correlation between L c and L b than L c and L a . In other words, the features of the perturbation are more dominant than that of the clean image. Second, PCC La,Lc is close to 0, indicating that the influence of the perturbation on the image is so significant that the clean image features are seemingly unrecognizable to the DNN. In fact, comparing the logit analysis of L a and L c in Figure <ref type="figure">4</ref> with that of Gaussian noise and image in Figure <ref type="figure">3</ref> (bottom), a striking similarity is observed. This offers a novel interpretation of targeted universal perturbations: Targeted universal perturbations themselves (independent of the images to attack) are features, while images behave like noise to them. We further explore the nontargeted perturbations, and report the results in Figure <ref type="figure">5</ref>. Similar to targeted universal perturbations, the PCC La,Lc is smaller than PCC L b ,Lc for the non-targeted perturbation. However the dominance of the non-targeted perturbation is not as significant as that of the targeted perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of Image-Dependent Perturbations</head><p>The logit vector analysis results for targeted and nontargeted image-dependent perturbations are reported in Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref>, respectively. Contrary to the universal perturbations, the image-dependent perturbations are weakly correlated to c, and have a noise-like behaviour (Figure <ref type="figure">3</ref>). However, the image gets misclassified even Figure <ref type="figure">7</ref>. Logit vector analysis for input image (a) and nontargeted image-dependent perturbation (b). The perturbation was crafted with PGD <ref type="bibr" target="#b23">[24]</ref> though the image features appear to be more dominant than the perturbation. This is because the image features are more strongly corrupted through the image-dependent perturbation than Gaussian noise. This special behavior appears due to the fact that the image-dependent perturbations are crafted to form concrete features only in combination with the image. Such image-dependent behavior violates our assumption of independent inputs. However, we include these results since they offer additional insight into adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Why Do Adversarial Perturbations Exist?</head><p>A wide variety of works have explored the existence of adversarial examples as discussed in section 2. Based on our previous analyses, we arrive at the following explanation for the existence of UAPs:</p><p>Universal adversarial perturbations contain features independent of the images to attack. The image features are corrupted to an extent of being unrecognizable to a DNN, and thus the input images behave like noise to the perturbation features.</p><p>The finding in <ref type="bibr" target="#b17">[18]</ref> that universal perturbations behave like features of a certain class aligns well with our statement. Jetley et al. argue that universal perturbations exploit the high-curvature image-space directions to behave like features, while our finding suggests that universal perturbations themselves contain features independent of the images to attack. Utilizing the perspective of positive curvatures of decision boundaries, Jetley et al. adopt the decision boundary-based attack DeepFool <ref type="bibr" target="#b29">[30]</ref>. However, our explanation does not explicitly rely on the decision boundary properties, but focuses on the occurrences of strong features, robust to the influence of images. We can therefore deploy the PGD algorithm to generate perturbations consisting of target class features similar to <ref type="bibr" target="#b16">[17]</ref>.</p><p>If universal perturbations themselves contain features independent of the images to attack, do image-dependent perturbations behave in a similar way? As previously discussed, the analysis results in Figure <ref type="figure">6</ref> reveal that the behavior of image-dependent perturbations is not like features, but noise. On the other hand, the original image features are retained to a high extent. Ilyas et al. <ref type="bibr" target="#b16">[17]</ref> revealed that image-dependent adversarial examples include the features of the target class. However, as seen from the analysis in subsection 4.4, the isolated perturbation seems not to retain independent features due its low PCC value, but rather interacts with the image to form the adversarial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Targeted UAP with Proxy Data</head><p>Our above analysis demonstrates that images behave like noise to the universal perturbation features. Since the images are treated like noise, we can exploit proxy images as background noise to generate targeted UAPs without the original training data. The proxy images do not need to have any class object belonging to the original training class and their main role is to make the targeted UAP have strong background-robust target class features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Problem Definition</head><p>Formally, given a data distribution X ∈ R d of images, we compute a single perturbation vector v that satisfies</p><formula xml:id="formula_1">Ĉ(x + v) = t for most x ∼ X ||v|| p ≤ ǫ.<label>(2)</label></formula><p>The magnitude of v is constrained by ǫ to be imperceptible to humans. || • || p refers to the l p -norm and in this work, we set p = ∞ and ǫ = 10 for images in range [0, 255] 1 as in <ref type="bibr" target="#b26">[27]</ref>. Specifically, we assume having no access to original training data. Thus, the training data X v for v generation can be different from the original dataset X . We denote the proxy dataset as X v .</p><p>To evaluate targeted UAPs, we use the targeted fooling ratio metric <ref type="bibr" target="#b34">[35]</ref>, i.e. the ratio of samples fooled into the target class to the number of all data samples. We also use the non-targeted fooling ratio <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27]</ref>, calculating the ratio of misclassified samples to the total number of samples, for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Loss Function and Algorithm</head><p>To achieve the desired objective Eq. 2 most naively, the commonly used cross-entropy loss function L CE can be utilized. Since cross-entropy loss holistically incorporates logits of all classes, this loss function leads to overall lower fooling ratios. This behavior can be resolved by using a loss function L L that only aims to increase the logit of the target class.</p><p>Since we consider universal perturbations, to balance the above objective between different samples in training, we extend L L by clamping the logit values as follows: </p><formula xml:id="formula_2">L t CL1 = max(max i =t Ĉi (x v + v) − Ĉt (x v + v), −κ)<label>(3</label></formula><formula xml:id="formula_3">v v ← 0 ⊲ Initialize for iteration = 1, . . . , I do B ∼ X v : |B| = m ⊲ Randomly sample g v ← E x∼B [∇ v L] ⊲ Calculate gradient v ← Optim(g v ) ⊲ Update v ← ǫ v ||v||p ⊲ Norm projection end</formula><p>where κ indicates the confidence value, x v are samples from the proxy data X v and Ĉi indicates the i-th entry of the logit vector. In this case, the proxy data can be either a random source dataset or the original training data, depending on data availability. Note that similar techniques of clamping the logits have also been used in <ref type="bibr" target="#b4">[5]</ref>, however, their motivation is to obtain minimum-magnitude (image-dependent) perturbations. While the target logit in loss function L t CL1 is increased, the logit values of max Ĉi (x v + v) are decreased simultaneously during the training process. This effect is undesirable for generating a UAP with strong target class features, since other classes except the target classes will be included in the optimization, which might have negative effects on the gradient update. To prevent manipulation of logits other than the target class, we exclude the nontargeted class logit values in the optimization step, such that these values are only used as a reference value for clamping the target class logit. We indicate this loss function as L t CL2 . We report an ablation study of the different loss function performances in Table <ref type="table" target="#tab_1">2</ref>. The results suggest that L t CL2 , in general, outperforms all other discussed loss functions. We further provide a loss function resembling L t CL2 for the generation of non-targeted UAPs.</p><formula xml:id="formula_4">L nt = max( Ĉgt (x v + v) − max i =gt Ĉi (x v + v), −κ) (4)</formula><p>In the special case of crafting non-targeted UAPs, the proxy dataset has to be the original training dataset.</p><p>We provide a simple, yet effective algorithm in Algorithm 1. Our gradient based method adopts the ADAM <ref type="bibr" target="#b19">[20]</ref> optimizer and mini-batch training, which have also been adopted in the context of data-free universal adversarial perturbations <ref type="bibr" target="#b37">[38]</ref>. Mopuri et al. train a generator network for crafting UAPs with this configurations, which can be considered more complex.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Main Results</head><p>We generate the targeted UAPs for four different datasets, the ImageNet training set as well as three proxy datasets. In Algorithm 1, we set the number of iterations to 1000, use loss function L t CL2 and a learning rate of 0.005 with batch-size 32. As the proxy datasets, we use images from MS-COCO <ref type="bibr" target="#b22">[23]</ref> and Pascal VOC <ref type="bibr" target="#b8">[9]</ref>, two widely used object detection datasets, and Places365 <ref type="bibr" target="#b49">[50]</ref>, a large-scale scene recognition dataset. We generated targeted UAPs with the 4 datasets for 8 different target classes and evaluate them on the ImageNet test dataset. The average over the 8 target scenarios are reported in Table <ref type="table" target="#tab_2">3</ref>. Two major observations can be made: First, a significant difference can not be observed for the three different proxy datasets. Moreover, there is only a marginal performance gap between training with the proxy datasets and training with the original Ima-geNet training data. The results support our assumption that the influence of the input images on targeted UAPs is like noise.</p><p>We also explored generating targeted UAPs with white images and Gaussian noise as the proxy dataset. In both scenarios, inferior performance was observed. We refer the reader to the supplementary material for a discussion about possible reasons and further results.</p><p>Targeted perturbations for different networks are shown in Figure <ref type="figure" target="#fig_5">8</ref>. Since the target class is sea lion, we can notice the existence of sea lion-like patterns by taking a closer look. Samples of clean images and perturbed images misclassified as sea lion are shown in Figure <ref type="figure" target="#fig_6">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with Previous Methods</head><p>To the best of our knowledge, this is the first work to achieve targeted UAP without original training data, thus we can only compare our performance with previous works on related tasks. The authors of <ref type="bibr" target="#b34">[35]</ref> report a targeted fool-  ing ratio of 52% for Inception-V3 with access to the Ima-geNet training dataset. We use COCO as the proxy dataset and achieve a superior performance of 53.4%. We can not find any other targeted UAP method available in the literature but other previous works report the (non-targeted) fooling ratio and we compare our performance with them and the results are available in Table <ref type="table" target="#tab_3">4</ref>. We distinguish between methods with and without data availability. To compare with the methods with data-availability we trained a nontargeted UAP on ImageNet utilizing our introduced nontargeted loss function from Equation <ref type="formula">4</ref>. Note that we do not block the gradient for max i =gt Ĉi (x v + v) to let the algorithm automatically search a dominant class for an effective attack. We observe that our approach achieves superior performance than both UAP <ref type="bibr" target="#b26">[27]</ref> and GAP <ref type="bibr" target="#b34">[35]</ref>. For the case without access to the original training dataset, we use the COCO dataset to generate the UAP, and report the averages of performance on 8 target classes. Note that our method still generates a targeted UAP, but we use the nontargeted metric for performance evaluation. This setting is in favor of other methods, since ideally, we could report the best performance of a certain target class. Without bells and whistles, our method achieves comparable performance to the state-of-the-art data-free methods, constituting evidence that our simple approach is efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Transferability</head><p>The transferability results are available in Table <ref type="table">5</ref>. We observe that the non-targeted transferability performs reasonably well, while targeted transferability does not. We find no previous work reporting the targeted transferability for universal perturbations. For image-dependent perturbations, the targeted transferability has been explored in <ref type="bibr" target="#b14">[15]</ref>, which reveals that the targeted transferability is unsatisfactory when source network and target network belong to different network families. When the networks belong to the same network family, relatively higher transferability can be observed <ref type="bibr" target="#b14">[15]</ref>. This aligns well with our finding that VGG16 and VGG19 transfer reasonably well between each other as presented in Table <ref type="table">5</ref>. We further report the PCC of the two network UAPs in Table <ref type="table" target="#tab_4">6</ref>. We observe that the PCC values are relatively higher between VGG16 and VGG19 than other networks, indicating an additional benefit of PCC to provide insight to network transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we treat the DNN logit output as a vector to analyze the influence of two independent inputs in terms of contributing to the combined feature representation. Specifically, we demonstrate that the Pearson correlation coefficient (PCC) can be used to analyze relative contribution and dominance of each input. Under the proposed analysis framework, we analyze adversarial examples by disentangling images and perturbations to explore their mutual influence. Our analysis results reveal that universal perturbations have dominant features and the images to attack behave like noise them. This new insight yields a simple yet effective algorithm, with a carefully designed loss function, to generate targeted UAPs by exploiting a proxy dataset instead of the original training data. We are the first to achieve this challenging task and the performance is comparable to state-ofthe-art baselines utilizing the original training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>L x, 1 L x, 2 L x, 3 LFigure 1 .</head><label>1231</label><figDesc>Figure 1. Based on our observation that adversarial perturbations contain dominant features and images behave like noise to them, we design a new method of generating targeted universal adversarial perturbations without data, by using a proxy dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Images and their logit vector analysis. The first row shows the sample images a and b and the resulting image c. The second row shows the plots of logit vector Lc over La (left) and L b (right), with their respective PCC values.</figDesc><graphic url="image-4.png" coords="3,308.86,151.74,236.25,107.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Logit vector analysis for an input image and Gaussian noise N (µ, σ). The analysis is shown for µ = 0 and σ = 0 (left), σ = 0.1 (middle) and σ = 0.2 (right))</figDesc><graphic url="image-5.png" coords="4,308.86,72.00,236.25,147.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 . 4 Figure 6 .</head><label>546</label><figDesc>Figure 5. Logit vector analysis for input image (a) and nontargeted UAP (b). The UAP was trained with loss function Equation 4</figDesc><graphic url="image-7.png" coords="5,50.11,72.00,236.25,107.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>) 1 For 10 255Algorithm 1 :</head><label>1101</label><figDesc>images in the range [0, 1], ǫ = UAP algorithm Input: Proxy data X v , Classifier Ĉ, Loss function L, mini-batch size m, Number of iterations I, perturbation magnitude ǫ Output: Perturbation vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Targeted universal perturbations (target class 'sea lion') for different network architectures.</figDesc><graphic url="image-10.png" coords="8,50.11,72.00,236.25,157.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative Results. Clean images (top) and perturbed images (bottom) for VGG19</figDesc><graphic url="image-11.png" coords="8,50.11,267.46,236.25,118.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>PCC analysis for VGG19 using 1000 image pairs randomly sampled from the ImageNet test set. Here, for each image pair, the mean and standard deviations of higher and lower PCC values are reported under PCC h and PCC l , respectively.</figDesc><table><row><cell>|S|</cell><cell>PCC h</cell><cell>PCC l</cell><cell>PCC</cell></row></table><note>h − PCC l PPCC Sm 445 0.74 ± 0.10 0.27 ± 0.23 0.47 ± 0.27 96% Sn 555 0.63 ± 0.13 0.33 ± 0.20 0.30 ± 0.22 -</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the performance of different loss functions, for the proposed targeted UAP. The values in each column represent mean and standard deviation of the non-targeted fooling ratio (%) and targeted fooling ratio (%) obtained for 5 runs and target class 'sea lion'. ± 0.6 55.4 ± 1.0 70.8 ± 1.5 55.2 ± 2.2 89.1 ± 0.3 75.9 ± 0.9 87.9 ± 0.5 70.8 ± 1.1 78.2 ± 0.9 66.<ref type="bibr" target="#b4">5</ref> ± 1.3 LL 89.2 ± 0.4 47.1 ± 1.1 71.6 ± 0.8 56.9 ± 1.1 91.0 ± 0.3 79.0 ± 0.6 90.8 ± 0.2 73.1 ± 0.8 80.1 ± 0.8 69.1 ± 0.4</figDesc><table><row><cell>Loss</cell><cell>AlexNet</cell><cell>GoogleNet</cell><cell>VGG16</cell><cell>VGG19</cell><cell>ResNet152</cell></row><row><cell cols="6">LCE 90.5 L t CL1 90.2 ± 0.3 57.6 ± 1.4 71.7 ± 1.4 57.9 ± 2.3 90.1 ± 0.4 80.3 ± 0.5 88.2 ± 0.3 75.5 ± 0.6 80.2 ± 0.3 71.4 ± 0.5</cell></row><row><cell cols="6">L t CL2 90.5 ± 0.3 49.4 ± 1.2 73.0 ± 1.5 58.4 ± 2.2 93.5 ± 0.3 82.8 ± 0.7 92.7 ± 0.1 72.3 ± 2.5 81.3 ± 1.1 70.6 ± 2.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results for targeted UAPs trained on four different datasets. The values in each column represent mean and standard deviation of the non-targeted fooling ratio (%) and targeted fooling ratio (%) obtained for 8 different target classes. ± 2.2 48.6 ± 13.3 77.7 ± 3.2 59.9 ± 6.6 92.5 ± 1.3 75.0 ± 7.8 91.6 ± 1.3 71.6 ± 6.9 80.8 ± 2.6 66.3 ± 7.0 COCO [23] 89.9 ± 2.6 47.2 ± 13.1 76.8 ± 3.7 59.8 ± 7.5 92.2 ± 1.7 75.1 ± 12.3 91.6 ± 1.5 68.8 ± 9.4 79.9 ± 2.9 65.7 ± 7.8 VOC [9] 88.9 ± 2.6 46.9 ± 12.7 76.7 ± 3.2 58.9 ± 6.0 92.2 ± 1.6 74.7 ± 7.9 90.5 ± 2.3 68.8 ± 8.2 79.1 ± 3.3 65.2 ± 7.1 Places365 [50] 90.0 ± 2.1 42.6 ± 16.4 76.4 ± 3.7 60.0 ± 5.4 92.1 ± 1.5 73.4 ± 9.6 91.5 ± 1.6 64.5 ± 17.0 78.0 ± 3.2 62.5 ± 9.9</figDesc><table><row><cell>Proxy Data</cell><cell>AlexNet</cell><cell>GoogleNet</cell><cell>VGG16</cell><cell>VGG19</cell><cell>ResNet152</cell></row><row><cell>ImageNet [41] 89.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the proposed method to other methods. The results are divided in universal attacks with access to the original ImageNet training data (upper) and data-free methods (lower). The metric is reported in the non-targeted fooling ratio (%)) GoogleNet 53.25 0.02 75.47 62.06 50.51 0.17 48.79 0.14 34.94 0.34 VGG16 53.71 0.03 41.26 0.02 93.62 82.90 82.99 13.69 36.73 0.01 VGG19 53.67 0.02 39.78 0.02 83.40 44.53 92.53 75.61 35.36 0.01 ResNet152 54.46 0.03 42.43 0.07 55.05 1.63 55.12 1.05 80.47 70.20</figDesc><table><row><cell>Method</cell><cell cols="6">AlexNet GoogleNet VGG16 VGG19 ResNet152</cell></row><row><cell>UAP [27]</cell><cell>93.3</cell><cell>78.9</cell><cell></cell><cell>78.3</cell><cell>77.8</cell><cell>84.0</cell></row><row><cell>GAP [35]</cell><cell>-</cell><cell>82.7</cell><cell></cell><cell>83.7</cell><cell>80.1</cell><cell>-</cell></row><row><cell cols="7">Ours(ImageNet) 96.17 88.94 94.30 94.98 90.08</cell></row><row><cell>FFF [32]</cell><cell>80.92</cell><cell cols="2">56.44</cell><cell cols="2">47.10 43.62</cell><cell>-</cell></row><row><cell>AAA [38]</cell><cell>89.04</cell><cell cols="2">75.28</cell><cell cols="2">71.59 72.84</cell><cell>60.72</cell></row><row><cell cols="2">GD-UAP [31] 87.02</cell><cell cols="2">71.44</cell><cell cols="2">63.08 64.67</cell><cell>37.3</cell></row><row><cell cols="2">Ours (COCO) 89.9</cell><cell>76.8</cell><cell></cell><cell cols="2">92.2 91.6</cell><cell>79.9</cell></row><row><cell cols="7">Table 5. Transferability results for the proposed targeted universal</cell></row><row><cell cols="7">adversarial attack. The attack was performed for target class 'sea</cell></row><row><cell cols="7">lion' and proxy dataset MS-COCO. The rows indicate the source</cell></row><row><cell cols="7">model and the columns indicates the target model. The values in</cell></row><row><cell cols="7">each column are reported in the non-targeted fooling ratio (%) and</cell></row><row><cell cols="2">targeted fooling ratio (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AlexNet</cell><cell cols="2">GoogleNet</cell><cell cols="2">VGG-16</cell><cell>VGG19</cell><cell>ResNet152</cell></row><row><cell cols="7">AlexNet 90.45 49.61 54.77 0.01 60.43 0.13 58.66 0.09 47.02 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Results for Transferability measured with PCC values. Generated with COCO as background, for target class sea lion. The rows indicate the source model and the columns indicates the target model.</figDesc><table><row><cell></cell><cell cols="5">AlexNet GoogleNet VGG-16 VGG19 ResNet152</cell></row><row><cell cols="2">AlexNet 1.00</cell><cell>0.09</cell><cell>0.24</cell><cell>0.14</cell><cell>−0.05</cell></row><row><cell cols="2">GoogleNet 0.24</cell><cell>1.00</cell><cell>0.24</cell><cell>0.14</cell><cell>0.00</cell></row><row><cell>VGG16</cell><cell>0.36</cell><cell>0.09</cell><cell>1.00</cell><cell>0.48</cell><cell>−0.11</cell></row><row><cell>VGG19</cell><cell>0.19</cell><cell>0.07</cell><cell>0.55</cell><cell>1.00</cell><cell>−0.09</cell></row><row><cell cols="2">ResNet152 0.28</cell><cell>0.11</cell><cell>0.36</cell><cell>0.30</cell><cell>1.00</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank Francois Rameau and Dawit Mureja Argaw for their comments and suggestions throughout this project. This work was supported by NAVER LABS and the Institute for Information &amp; Communications Technology Promotion (2017-0-01772) grant funded by the Korea government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An introduction to multivariate statistical analysis (wiley series in probability and statistics)</title>
		<author>
			<persName><surname>Tw Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints</title>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Security and Privacy (SP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: from adversarial to random noise</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial examples are a natural consequence of test error in noise</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Cubuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><surname>Raghu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02774</idno>
		<title level="m">Martin Wattenberg, and Ian Goodfellow. Adversarial spheres</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Once a man: Towards multi-target attack via learning multi-target adversarial network once</title>
		<author>
			<persName><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2006">2019. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">With friends like these, who needs adversaries?</title>
		<author>
			<persName><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Art of singular vectors and universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure</title>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Mahloujifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Diochnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahmoody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations against semantic image segmentation</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaithanya</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2008">2017. 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Analysis of universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09554</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robustness of classifiers to universal perturbations: A geometric perspective</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalizable data-free objective for crafting universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Babu Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast feature fool: A data independent approach to universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utsav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Conference on Machine Vision (BMVC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A discussion of &apos;adversarial examples are not bugs, they are features&apos;: Adversarial examples are just bugs, too. Distill</title>
		<author>
			<persName><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<ptr target="https://distill.pub/2019/advex-bugs-discussion/response-5.1,2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-domain transferability of adversarial perturbations</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Muzammal Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bicheng Gao, and Serge Belongie. Generative adversarial perturbations</title>
		<author>
			<persName><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isay</forename><surname>Katsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2008">2018. 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attacking optical flow</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ask, acquire, and attack: Data-free uap generation using class impressions</title>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phani</forename><surname>Krishna Uppala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babu</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Are adversarial examples inevitable?</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02104</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring the space of adversarial images</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690</idno>
		<title level="m">A boundary tilting persepective on the phenomenon of adversarial examples</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding and enhancing the transferability of adversarial examples</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09707</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cd-uap: Class discriminative universal adversarial perturbation</title>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Philipp Benz, Tooba Imtiaz, and In-So Kweon</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepptz: Deep self-calibration for ptz cameras</title>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawit</forename><surname>Mureja Argaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Charles</forename><surname>Bazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dawit Mureja Argaw, Jean-Charles Bazin, and In So Kweon. Revisiting residual networks with nonlinear shortcuts</title>
		<author>
			<persName><forename type="first">Chaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Benz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
