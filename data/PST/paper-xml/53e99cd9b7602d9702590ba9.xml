<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Virtualization for Heterogeneous Parallel Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Chafi</surname></persName>
							<email>hchafi@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
							<email>zdevito@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adriaan</forename><surname>Moors</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tiark</forename><surname>Rompf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><forename type="middle">K</forename><surname>Sujeeth</surname></persName>
							<email>asujeeth@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
							<email>hanrahan@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Odersky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Virtualization for Heterogeneous Parallel Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F4A6073836CC1EFB26A9046DD290D626</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.1.3 [Programming Techniques]: Concurrent Programming -Parallel programming; D.3.4 [Programming Languages]: Processors -Code generation</term>
					<term>Optimization</term>
					<term>Run-time environments General Terms Languages</term>
					<term>Performance Parallel Programming</term>
					<term>Domain Specific Languages</term>
					<term>Dynamic Optimizations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As heterogeneous parallel systems become dominant, application developers are being forced to turn to an incompatible mix of low level programming models (e.g. OpenMP, MPI, CUDA, OpenCL). However, these models do little to shield developers from the difficult problems of parallelization, data decomposition and machine-specific details. Most programmers are having a difficult time using these programming models effectively. To provide a programming model that addresses the productivity and performance requirements for the average programmer, we explore a domainspecific approach to heterogeneous parallel programming.</p><p>We propose language virtualization as a new principle that enables the construction of highly efficient parallel domain specific languages that are embedded in a common host language. We define criteria for language virtualization and present techniques to achieve them. We present two concrete case studies of domain-specific languages that are implemented using our virtualization approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Until the early 2000s, advances in out-of-order (OOO) superscalar processors provided applications with improved performance by increasing the CPU core clock frequency Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Onward! 2010, October 17-21, 2010, Reno/Tahoe, Nevada, USA. Copyright c 2010 ACM 978-1-4503-0236-4/10/10. . . $10.00 and the number of instructions per clock cycle (IPC). With each new generation of processors, software developers were able to leverage this performance increase to create more compelling applications without changing their programming model. Furthermore, existing applications also benefited from this performance increase with no extra effort. The inability of processor vendors to deliver higher performance from a single core without exceeding a reasonable power envelope has brought this so called "free lunch" era to an end <ref type="bibr" target="#b46">[47]</ref>.</p><p>Power efficiency is now the most dominant design driver for microprocessors. Power efficient microprocessor design favors chip-multiprocessors consisting of simpler cores <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref> and heterogeneous systems consisting of general purpose processors, SIMD units and special purpose accelerator devices such as graphics processing units (GPUs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44]</ref> and cryptographic units. Existing applications can no longer take advantage of the additional compute power available in these new and emerging systems without a significant parallel programming and program specialization effort. However, writing parallel programs is not straightforward because in contrast to the familiar and standard von Neumann model for sequential programming, a variety of incompatible parallel programming models are required, each with their own set of trade-offs. The situation is even worse for programming heterogeneous systems where each accelerator vendor usually provides a distinct driver API and programming model to interface with the device. Writing an application that directly targets the variety of existing systems, let alone emerging ones, becomes a very complicated affair.</p><p>While one can hope that a few parallel programming experts will be able to tackle the complexity of developing parallel heterogeneous software, expecting the average programmer to deal with all this complexity is simply not realistic. Moreover, exposing the programmer directly to the various programming models supported by each compute device will impact application portability as well as forward parallel scalability; as new computer platforms emerge, applications will constantly need to be rewritten to take advantage of any new capabilities and increased parallelism. Furthermore, the most efficient mapping of an application to a heterogeneous parallel architecture occurs when the characteristics of the application are matched to the different capabilities of the architecture. This represents a significant disadvantage of this approach: for each application and for each computing platform a specialized mapping solution must be created by a programmer that is an expert in the specific domain as well as in the targeted parallel architecture. This creates an explosion and fragmentation of mapping solutions and makes it difficult to reuse good mapping solutions created by experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Need for DSLs</head><p>One way to capture application-specific knowledge for a whole class of applications and simplify application development at the same time is to use a domain specific language (DSL). A DSL is a concise programming language with a syntax that is designed to naturally express the semantics of a narrow problem domain <ref type="bibr" target="#b48">[49]</ref>. DSLs, sometimes called "little languages" <ref type="bibr" target="#b3">[4]</ref> or "telescoping languages" <ref type="bibr" target="#b28">[29]</ref>, have been in use for quite some time. In fact, it is likely that most application developers have already used at least one DSL. Examples of commonly used DSLs are TeX and LaTeX for typesetting academic papers, Matlab for testing numerical linear algebra algorithms, and SQL for querying relational databases.</p><p>One can also view OpenGL as a DSL. By exposing an interface for specifying polygons and the rules to shade them, OpenGL created a high-level programming model for realtime graphics decoupled from the hardware or software used to render it, allowing for aggressive performance gains as graphics hardware evolves. Even the Unix shell can be considered a DSL that provides a command-line interface to underlying operating system functions such as file and process management. The use of DSLs can provide significant gains in the productivity and creativity of application developers, the portability of applications, and application performance.</p><p>The key benefits of using a domain-specific approach for enhancing application performance are using domain knowledge for static and dynamic optimizations of a program written using a DSL and the ability to reuse expert knowledge for mapping applications efficiently to a specialized architecture. Most domain specific optimizations would not be possible if the program was written in a general purpose language. General-purpose languages are limited when it comes to optimization for at least two reasons. First, they must produce correct code across a very wide range of applications. This makes it difficult to apply aggressive optimizationscompiler developers must err on the side of correctness. Second, because of the general-purpose nature needed to support a wide range of applications (e.g. financial, gaming, image processing, etc.), compilers can usually infer little about the structure of the data or the nature of the algorithms the code is using. In contrast, DSL compilers can use aggressive optimization techniques using knowledge of the data structures and algorithms derived from the DSL. This makes it possible to deliver good performance on heterogeneous architectures using DSLs.</p><p>Using DSLs naturally divides the application development process into two phases. First, a DSL developer designs a DSL for a specific domain. Then, a much larger number of domain experts make use of the DSL to develop applications. The ideal DSL developer would be a domain expert, a parallelism expert, and a language and compiler expert. Such developers are rare and so there is a need to simplify the process of developing DSLs for parallelism.</p><p>Traditionally, DSLs have been developed from the groundup using custom compiler infrastructure. This is called the "external DSL approach". There are two problems with this approach to DSLs. First, developing these new languages to a sufficient degree of maturity is an enormous effort. This investment would have to include not just language specifications and construction of their optimizing compilers and libraries, but also all the other aspects of modern tooling including IDEs, debuggers, profilers, build tools as well as documentation and training. It is hard to see how such an investment can be made repeatedly for each specialized domain. Second, DSLs do not exist in a vacuum but have to interface to other parts of a system. For instance, a climate simulation program could have a visualization component that is based on a graphics DSL. It is not clear how multiple separate DSLs would inter-operate without creating a new DSL that integrates the desired combination of the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Embedded DSLs and Language Virtualization</head><p>Embedded DSLs <ref type="bibr" target="#b22">[23]</ref> overcome the problems with external DSLs and make DSL development more tractable. An embedded DSL lives inside of a host language. It is quite like a framework or a library, consisting of a set of classes and operations on objects of those types. There are several advantages to using embedded DSLs for application writers. First, programmers do not have to learn a completely new syntax. Second, multiple DSLs can be combined in the same application. Third, the DSL infrastructure can be shared between different DSLs and DSL developers will all use the same infrastructure for building their DSLs.</p><p>The main problem with embedded DSLs is that they cannot exploit domain knowledge to efficiently map programs to specialized architectures because they live inside a general purpose language. There is no obvious solution to this problem apart from replicating much of the effort of building a stand-alone DSL implementation. To overcome this problem, we need embedding languages that are particularly suited to the task of serving as a host language to DSL implementations. A language with this capability supports what we call language virtualization. A language is virtualizable if and only if it can provide an environment to embedded languages that makes them essentially identical to corresponding stand-alone language implementations in terms of (1) expressiveness -being able to express a DSL in a way which is natural to domain specialists, Figure <ref type="figure">1</ref>. An environment for domain-specific programming of heterogeneous parallel architectures using language virtualization.</p><p>(2) performance -leveraging domain knowledge to produce optimal code, and</p><p>(3) safety -domain programs are guaranteed to have certain properties implied by the DSL, (4) while at the same time requiring only modestly more effort than implementing a simple embedding. A subset of these features has been achieved before -most notably by Lisp, as much as 50 years ago. However, we believe we are the first to provide all of them. Section 6 provides a detailed comparison with related work. We discuss the means to achieve all of these features at once in more detail in Section 2.</p><p>There is a close analogy between language virtualization and hardware virtualization using virtual machines. In data-centers, it is often desirable to have a range of differently configured machines at one's disposal (for provisioning, fault-tolerance, and isolation), but usually it is not feasible or even desirable to operate a corresponding number of physical machines. Hardware virtualization solves this problem by embedding a number of specific virtual machines on a general-purpose host machine. A key aspect of virtual hardware resources is that they are practically indistinguishable from their real counterparts. We believe the same should be true for an embedded DSL, in the sense that it should exhibit the same expressiveness, performance and safety as if a specialized language tool chain had been tailor-made for the particular DSL.</p><p>This paper describes key elements of an ongoing effort to virtualize the language Scala <ref type="bibr" target="#b0">[1]</ref> and how language virtualization can be used in a domain-specific programming environment for heterogeneous parallel computers. The components of this environment are shown in Fig. <ref type="figure">1</ref>. The environment is composed of four main components: Applications composed of multiple DSLs, DSLs (e.g. Liszt and OptiML) embedded in Scala using language virtualization, a Scala-based compiler infrastructure that can perform domain-specific optimizations and a framework and runtime for DSL parallelization and mapping to heterogeneous architectures.</p><p>The remainder of this paper is organized as follows. Section 2 explains the notion of language virtualization in more detail and discusses key elements of virtualizing Scala. The next two sections describe how language virtualization can be used to develop two very different DSLs. Section 3 introduces Liszt, a DSL for scientific simulation that statically generates parallel code in C++. Section 4 introduces Op-tiML, a DSL for machine learning and data analysis. Section 5 describes Delite, a framework that simplifies DSL parallelization. Section 6 presents the related work for parallel programming, DSLs and language virtualization. Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Language Virtualization</head><p>We propose the following definition of language virtualization to capture necessary conditions for a general purpose language to serve as a successful embedding environment for DSLs: Definition. A programming language is virtualizable with respect to a class of embedded languages if and only if it can provide an environment to these embedded languages that makes the embedded implementations essentially identical to corresponding stand-alone language implementations in terms of expressiveness, performance and safety-with only modestly more effort than implementing the simplest possible complete embeddings.</p><p>Expressiveness encompasses syntax, semantics and, in the case of domain-specific languages, general ease of use for domain experts. Just as virtual hardware resources are not exactly identical to real ones, we do not require that an embedded language can exactly model the syntax of a standalone language but settle for a syntax that is essentially the same, i.e. modulo syntactic sugar. The same consideration applies to the other criteria as well.</p><p>Performance implies that programs in the embedded language must be amenable to extensive static and dynamic analysis, optimization, and code generation, just as programs in a stand-alone implementation would be. For many embedded languages, in particular those that are the focus of this paper, this rules out any purely interpretation-based solutions.</p><p>Safety means that the embedded implementation is not allowed to loosen guarantees about program behavior. In particular, host-language operations that are not part of the embedded language's specification must not be available to embedded programs.</p><p>Modest effort is the only criterion that has no counterpart in hardware virtualization. However, it serves an important purpose since an embedded language implementation that takes a DSL program as a string and feeds it into an external, specialized stand-alone compiler would trivially satisfy criteria expressiveness, performance and safety. Building this implementation, however, would include the effort of implementing the external compiler, which in turn would negate any benefit of the embedding. In a strict sense, one can argue that virtualizability is not a sufficient condition for a particular language being a good embedding environment because the "simplest possible" embedding might still be prohibitively expensive to realize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Achieving Virtualization</head><p>What does it take to make a language virtualizable in practice? Various ways of fulfilling subsets of the requirements exist, but we are unaware of any existing language that fulfills all of them. The "pure embedding" approach <ref type="bibr" target="#b22">[23]</ref> of implementing embedded languages as pure libraries in a modern host language can likely satisfy expressiveness, safety and effort if the host language provides a strong static type system and syntactic malleability (e.g. custom infix operators). Achieving performance in addition, however, seems almost impossible without switching to a completely different approach.</p><p>Expressiveness We can maintain expressiveness by overloading all relevant host language constructs. In Scala, for example, a for-loop such as</p><formula xml:id="formula_0">for (x &lt;-elems if x % 2 == 0) p(x)</formula><p>is defined in terms of its expansion elems.withFilter(x =&gt; x % 2 == 0)</p><p>.foreach(x =&gt; p(x))</p><p>Here, withFilter and foreach are higher-order methods that need to be defined on the type of elems. By providing suitable implementations for these methods, a domain-specific language designer can control how loops over domain collections should be represented and executed.</p><p>To achieve full virtualization, analogous techniques need to be applied to all other relevant constructs of the host language. For instance, a conditional control construct such as if (cond) something else somethingElse would be defined to expand into the method call __ifThenElse(cond, something, somethingElse)</p><p>where __ifThenElse is a method with two call-by-name parameters:</p><formula xml:id="formula_1">def __ifThenElse[T]</formula><p>(cond: Boolean, thenp: =&gt; T, elsep: =&gt; T)</p><p>Domain languages can then control the meaning of conditionals by providing overloaded variants of this method which are specialized to domain types.</p><p>In the same vein, all other relevant constructs of the host language need to map into constructs that are extensible by domain embeddings, typically through overloading method definitions.</p><p>Performance As we have argued above, achieving performance requires the ability to apply extensive (and possibly domain-specific) optimizations and code generation to embedded programs. This implies that embedded programs must be available at least at some point using a lifted, ASTlike intermediate representation. Pure embeddings, even if combined with (hypothetical) powerful partial evaluation as suggested in <ref type="bibr" target="#b22">[23]</ref>, would not be sufficient if the target architecture happens to be different from the host language target. What is needed is essentially a variant of staged metaprogramming, where the embedded "object" program can be analyzed and manipulated by a "meta" program that is part of the embedding infrastructure. However, any DSL will also contain generic parts, some of which will be host language constructs such as function definitions, conditionals or loops. These must be lifted into the AST representation as well.</p><p>This ability to selectively make constructs 'liftable' (including their compilation) such that they can be part of (compiled) DSL programs while maintaining expressiveness, safety and effort is an essential characteristic of virtualizable languages.</p><p>Modest Effort However, having to implement the lifting for each new DSL that uses a slightly different AST representation would still violate the effort criterion. Using an existing multi-stage language such as MetaOCaml <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref> would also likely violate this criterion, since the staged representation cannot be analyzed (for safety reasons we will consider shortly) and any domain-specific optimizations would require effort comparable to a stand-alone compiler.</p><p>Likewise, compile-time metaprogramming approaches such as C++ templates <ref type="bibr" target="#b49">[50]</ref> or Template Haskell <ref type="bibr" target="#b42">[43]</ref> would not achieve the goal, since they are tied to the same target architecture as the host language and their static nature precludes dynamic optimizations (i.e. recompilation). What is needed here is a dynamic multi-stage approach with an extensible common intermediate representation (IR) architecture. In the context of Scala, we can make extensive use of traits and mixin-composition to provide building blocks of common DSL functionality (API, IR, optimizations, code generation), including making parts of Scala's semantics available as traits. This approach, which we call lightweight modular staging <ref type="bibr" target="#b38">[39]</ref>, is described below and allows us to maintain the effort criterion. A key element is to provide facilities to compile a limited range of Scala constructs to architectures different from the JVM, Scala's primary target.</p><p>Safety There are two obstacles to maintaining safety. The first is to embed a typed object language into a typed meta language. This could be solved using a sufficiently powerful type system that supports an equivalent of GADTs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref> or dependent types <ref type="bibr" target="#b32">[33]</ref>. The second problem is that with a plain AST-like representation, DSL programs can get access to parts of their own structure. This is unsafe in general and also potentially renders optimizations unsound. Fortunately, there is a technique known as finally tagless <ref type="bibr" target="#b7">[8]</ref> or polymorphic embedding <ref type="bibr" target="#b21">[22]</ref> that is able to solve both problems at once by abstracting over the actual representation used.</p><p>The combination of lightweight modular staging and polymorphic embedding provides a path to virtualize Scala and actually maintains all four of the criteria listed in the definition of language virtualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Virtualization in Practice</head><p>We illustrate our approach of virtualization through lightweight modular staging and polymorphic embedding by means of the following very simple linear algebra example. The embedded DSL program consists of the example method in the trait TestMatrix. It makes use of a type Matrix which needs to be defined in trait MatrixArith. The clause this: MatrixArith =&gt; in the first line of the example is a self-type annotation <ref type="bibr" target="#b30">[31]</ref>; it declares the type of this to be of type MatrixArith, instead of just TestMatrix, which it would be if no annotation was given. The annotation has two consequences: First, all MatrixArith definitions are available in the type of the environment containing the example method, so this effectively constitutes an embedding of the DSL program given in example into the definitions provided by MatrixArith. Second, any concrete instantiation of TestMatrix needs to mixin a concrete subclass of the MatrixArith trait, but it is not specified which subclass. This means that concrete DSL programs can be combined with arbitrary embeddings by choosing the right mix-in.</p><p>Using lightweight staging we can reason about the highlevel matrix operations in this example and reduce the number of matrix multiplications from four to a single multiplication. Optimizing matrix operations is one of the classic examples of the use of C++ expression templates <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> and is used by many systems such as Blitz++ <ref type="bibr" target="#b51">[52]</ref>, A++ <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, and others. We do not have to change the program at all, but just the way of defining Matrix.</p><p>Here is the definition of matrix operations in MatrixArith: Since we have not defined a concrete representation, we say that the example code, as well as the definitions of matrix arithmetic operations, are polymorphic in the chosen representation, and hence, we have polymorphically embedded <ref type="bibr" target="#b21">[22]</ref> the language of matrix arithmetic operations into the host language Scala. We also note that the embedding is tagless <ref type="bibr" target="#b7">[8]</ref>, i.e. resolution of overloaded operations is based on static types and does not require dispatch on runtime values.</p><formula xml:id="formula_2">trait MatrixArith { type Rep[T] type InternalMatrix type Matrix = Rep[InternalMatrix]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If the representation is abstract, in what way does that help?</head><p>The answer is that we gain considerable freedom in picking a concrete representation and, perhaps more importantly, that the chosen representation is hidden from the DSL program.</p><p>To implement the desired optimizations, we will use expression trees (more exactly, graphs with a tree-like interface), which form the basis of our common intermediate representation that we can use for most DSLs: This expression representation will do a number of useful bookkeeping tasks for us, among them automatic elimination of common sub-expressions and, more generally, preventing any expression from being generated twice (e.g. we would only need to compute a * c once in our example). The implementation of this bookkeeping is in method findOrCreateDefinition, which can be overridden by the DSL designer to further customize the building of the AST. While we are able to eliminate redundant computation and thus optimize the example, the true power of using domain specific language is our ability to use domain knowledge to perform optimizations. In this case, we can use our knowledge of matrix operations to rewrite some of our expressions into more efficient forms. Implementing these rewritings is very simple using the framework we have developed so far. All we have to do is override the corresponding operations in one of the traits: We assume the println operation to be overloaded such that it will compile and execute its argument if it is invoked with a staged expression. The use of domain knowledge in this case yields a tremendous amount of reduction in required computation. This was achieved only using a library with the power of polymorphic embedding and staging, without having to change or create a custom compiler. Note that while we have only shown a simple mechanism for defining optimizations through transformation on operators, much more sophisticated analyses and optimizations are possible by iterating through the entire AST of the program as opposed to one node at time. Liszt, a DSL for mesh-based partial differential equations (PDEs), uses this full-program optimization approach to enable large-scale parallel execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we pick</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Physical Simulation with Liszt</head><p>Simulation underlies much of scientific computing. The goal of Liszt is to express simulations at a high-level, independent of the machine platform. Liszt is inspired by the pioneering work of others to develop libraries, frameworks and DSLs for scientific computing, including the SIERRA Framework <ref type="bibr" target="#b45">[46]</ref>, Overture <ref type="bibr" target="#b5">[6]</ref>, POOMA <ref type="bibr" target="#b37">[38]</ref>, Sundance/Trilinos <ref type="bibr" target="#b20">[21]</ref>, and many others.</p><p>A major goal of the Stanford University PSAAP Center is to develop a predictive simulation of a hypersonic vehicle. More specifically, the goal of the Stanford Center is to characterize the operability limits of a hypersonic propulsion system using predictive computations, with a primary focus on the unstart phenomena triggered by thermal choking in a hydrogen-fueled scramjet.</p><p>To perform these simulations, the center is developing software, called Joe, to solve for the air-flow through the scramjet in the presence of shock waves. Joe solves a RANS (Reynolds-Averaged Navier Stokes) problem on unstructured meshes in the steady and non-steady state using the finite-volume method. Joe has been ported to several clusters and shows scalable performance to 1,000s of processors. The scalable version is based on MPI, and has evolved from the code used to simulate a jet engine.</p><p>Working with the developers of Joe, we have designed Liszt to abstract the machine-dependent parts of the program, allowing us to target Liszt to many architectures. Here is a fragment of Liszt code performing a simple scalar convection operation.</p><formula xml:id="formula_3">val Flux = new Field[Cell,Float] val Phi = new Field[Cell,Float] val cell_volume = new Field[Cell,Float] val deltat = .001 ... while(&lt;not converged&gt;) { for(f &lt;-interior_faces) { val flux = calc_flux(f) Flux(inside(f)) -= flux Flux(outside(f)) += flux } for(f &lt;-inlet_faces) { Flux(outside(f)) += calc_boundary_flux(f) } for(c &lt;-cells(mesh)) { Phi(c) += deltat * Flux(c) / cell_volume(c) } for(f &lt;-faces(mesh)) Flux(f) = 0.f }</formula><p>Liszt is designed to abstract the low-level details of meshbased codes while leaving the application writer enough freedom to implement whatever simulation scheme they need. The language itself contains most of the features of a C-like subset of Scala with three additional domain specific constructs: a built-in mesh interface, the ability to store data on the mesh through fields or sparse matrices, and a for-comprehension over sets of mesh topology that maps computation over the mesh.</p><p>The mesh interface abstracts the details of an unstructured mesh that can contain elements of arbitrary types. Liszt meshes are 3D manifolds in 3D space, and hence contain vertices, edges, faces, and cells (the volume enclosed by faces). Liszt provides a complete interface to the mesh topology. For example, a user can obtain sets of mesh elements such as faces(mesh), the set of all faces in the mesh, or for a particular face f, edges(f), the set of edges around the face. User-defined sets of mesh topology are also available. In the example, interior_faces and inlet_faces are user-defined sets representing the interior of the mesh and a particular boundary condition, respectively.</p><p>Codes in the domain often need to represent continuous functions over space. Liszt provides fields to represent these continuous functions given by basis functions associated with values defined on cells, faces, edges and vertices of the mesh. Fields are also abstracted and accessed through methods so that their internal representation is not exposed.</p><p>Accessing a field with a vertex returns the value of the field basis coefficient at that vertex; accessing a field with a set of vertices returns a set of values. Sparse matrices are also provided to support implicit methods. Like fields, sparse matrices are indexed by pairs of topological elements, not integers. Various solvers such as HYPRE <ref type="bibr" target="#b10">[11]</ref> and PETSC <ref type="bibr" target="#b2">[3]</ref> are built-in to the system.</p><p>Finally, a for-comprehension maps computation to each member of a particular topological set. The statement for(f &lt;-cells(mesh)) &lt;block&gt; will execute &lt;block&gt; for each cell in cells(mesh). The language definition stipulates that the individual &lt;block&gt;s may be executed independently. These semantics give Liszt the freedom to parallelize for-comprehensions and schedule them arbitrarily.</p><p>In addition to these special features, Liszt also imposes a set of domain assumptions. Liszt expects that the body of a for-comprehension operates locally on the mesh, accessing a limited neighborhood around the top-level element. Furthermore, Liszt requires that the topology of the mesh remain constant for the duration of the program. These domain assumptions allow the DSL implementor to make aggressive optimizations that would otherwise not be possible, and since these assumptions generally fit they way people write codes in this domain, they do not hamper the expressibility of the language.</p><p>This code appeals to the computational scientists because it is written in a form they understand; it also appeals to the computer scientists because it hides the details of the machine and exposes opportunities for parallel execution.</p><p>Liszt takes advantage of the language virtualization features that Scala provides to enable aggressive domainspecific optimizations. Using the technique of polymorphic embedding presented previously, Liszt code is lifted into an internal representation, where a series of program analyses and aggressive domain-specific optimizations are performed. The result is a native C++ program that can run efficiently on an MPI-based cluster. A different set of transformations is being developed that allows us to target Liszt to a GPU.</p><p>While it would be possible to write a version of Liszt in Scala without any embedding, a simple library-based implementation of Liszt could not achieve good scalable performance. Consider the first for-comprehension in the example. The body of the for-comprehension adds the value of flux to the field entries for the two cells on the sides of face f. Since each cell is surrounded by many faces, multiple instances of the statement body end up writing data to flux value of the same cell. To avoid race conditions in this situation, an implementation of Liszt must introduce some form of synchronization. A naive solution might use locking or atomic primitives to update the values of the Flux field. While this would perform correctly, it introduces high overheads to these frequently occurring operations, and would not work for distributed memory systems. Instead, the commonly used approach in the field is to decompose the domain by partitioning the mesh into many pieces and assigning each piece to a single thread. Now synchronization only needs to occur on the borders between partitions. This synchronization is normally accomplished using "ghost" cells. If a particular thread needs to write to a value outside of its partition, it will create a duplicate entry, a "ghost", that will hold the partial values that the thread produces for that value, and these temporaries will be sent to the owner of the value only after the end of the for-comprehension. This strategy decreases communication overhead by batching the messages to other threads. It has been used successfully by Joe to show scalable performance to 1,000s of processors.</p><p>However, efficient approaches like the one described above traditionally need to be explicitly managed by the enduser. Code to partition the mesh into many sections, manage the ghost-cells, set up communication between threads, and perform batched messages would need to be inserted to ensure the program would scale. For Joe, this boilerplate code and the libraries needed to support it account for nearly 80% of the code. Furthermore, if a different parallelization strategy were to be applied, the client code would need to be significantly modified to support it.</p><p>Liszt uses language virtualization to avoid these issues. The client code does not specify a strategy for parallelization. Instead, Liszt first creates an intermediate representation of a client program. It then uses domain knowledge to map the intermediate representation to heterogeneous parallel computers. We have identified several situations where this domain knowledge enables aggressive code transformations, allowing for automatic parallelization.</p><p>• Automatically perform domain decomposition. In Liszt, all mesh accesses are performed through a standard mesh interface. The Liszt compiler analyzes the pattern of access to neighbors and provides input to the domain decomposition software. In particular, Joe uses ParMETIS <ref type="bibr" target="#b27">[28]</ref> to perform domain decomposition. ParMETIS reads a graph of vertices and edges. The vertices of the graph are mesh cells and the edges are neighboring mesh elements accessed during the computation. Liszt can create this graph automatically for ParMETIS, which is then able to decompose the mesh into optimal domains. All this is done automatically by the system.</p><p>• Automatic determination and maintenance of ghost cells. The same neighborhood analysis used to partition the domain can be used to determine which neighboring mesh elements are shared across a boundary. Liszt automatically finds the neighbors of a domain, and handles all communication and synchronization across the boundaries of the domain. This very domain-specific knowledge is crucial for minimizing communication in Liszt programs on distributed memory clusters. In effect, Liszt chooses the optimal mesh data structure.</p><p>• Optimize layout of field variables. A major design choice is how to represent fields. Fields are associated with mesh elements. In general, there will be a set of fields stored on cells, on vertices, etc. There are two ways to represent these fields. The most natural is as an array of structures. That is, the cell contains a structure that stores all the fields associated with the cell. Another choice is as a structure of arrays. That is, each field is stored in a separate array. Which representation to use depends on how and when the fields are used, and on the machine architecture. Liszt can analyze the program for co-usage of fields and then optimally map to a given architecture.</p><p>All of these examples depend on domain-knowledge. In this case, knowledge about meshes and fields. No generalpurpose compiler could possibly do these types of optimizations. After these transformations are applied, Liszt can generate code for a specific runtime. Currently a MPI-based cluster runtime exists with a CUDA-based GPU runtime in progress. We eventually plan to unite the two runtimes, allowing Liszt to target a heterogeneous system consisting of some traditional cluster nodes each accelerated with GPUs.</p><p>While it would be possible to write Liszt as a stand-alone language, embedding it as a virtual language inside Scala has several important advantages. Since it uses the same syntax and typing rules as standard Scala, it is easy for someone who knows Scala to understand and create Liszt code. IDE and tool support for Scala also works on Liszt code, making it easier to use than a stand-alone language. Furthermore, Liszt can also benefit from the staged computation that a virtualized language provides. In many scientific codes, the programmer can make a tradeoff between using a low-order (e.g linear) basis function to get a fast reconstruction of a field, or a higher-order basis function to get a more accurate reconstruction at the cost of additional computation and memory access. Often a solution may contain bases of mixed orders, with higher-order bases chosen for important parts of the mesh. This determination of the basis functions normally occurs during program initialization, but is constant after it has been chosen. Since the order of basis functions can determine the way the fields are accessed and the sparsity patterns of the matrices, it is beneficial to know this information before choosing field or matrix layouts. Using staged computation, Liszt can first generate code to compute the order of basis functions and then use the resulting values as input into the next stage of compilation, specializing the code to the particular assignment of basis functions that the first stage produced.</p><p>Through language virtualization, Liszt is able to perform the strong optimizations of a stand-alone domain-specific language without the need for a new language and compiler infrastructure. In the future this approach offers the ability to interoperate with other DSLs as well as share a common infrastructure for program analysis and code generation for heterogeneous architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Analysis and Machine Learning with OptiML</head><p>The second example of the use of language virtualization is for the implementation of OptiML, a DSL for machine learning. Machine Learning (ML) algorithms are becoming increasingly prevalent in a variety of fields, including robotics, data analytics, and bioinformatics. Machine learning applications are often used to process and find structure within very large datasets or from real-time data sources. These workloads often require great computational resources, even with efficient algorithms. To meet these computational needs effectively, much painstaking effort has been focused on taking advantage of modern multi-core and heterogeneous hardware <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. However, these solutions are usually complex, platform-dependent, and require significant initial investment to understand and deploy. As a result, ML researchers and developers spend a disproportionate amount of their time hand-tuning kernels for performance, a process that has to be repeated for every kernel and every platform. An ML DSL, on the other hand, could leverage the underlying abstractions that are common across many ML algorithms to simplify the way that parallel ML applications are developed. To take advantage of these and speed up the development of parallel ML applications, we are developing the OptiML DSL. The primary design goal of OptiML is the development of implementation-independent abstractions that capture the essential operations in machine learning. Using machineindependent representations, machine learning developers can focus on algorithmic quality (i.e. improving accuracy) and correctness while allowing the runtime system to manage performance and scaling. OptiML is implicitly parallel, so users are not required to do any explicit decomposition or parallelization.</p><p>The following code snippet from k-means clustering is an example of what an OptiML program looks like prior to compilation: // best effort update c --// calculate distances to current centroids for (i &lt;-0 until x.length) { c(i) = findNearestCluster(x(i), mu) } // update mu --move each cluster centroid // to mean of the points assigned to it for (j &lt;-0 until k){ var (weightedpoints, points</p><formula xml:id="formula_4">) = sum(0,m) { i =&gt; if (c(i) == j) (x(i), 1) else (0,0) } mu(j) = weightedpoints/points } }</formula><p>The OptiML surface syntax is regular Scala code with high-level, domain specific abstractions introduced to make implementing ML algorithms easier. Underneath, however, OptiML is polymorphically embedded in Scala. The user program implicitly operates on abstract representations whose implementations can be chosen at stage time without modifying the application code. Like Liszt, virtualization allows OptiML to lift user applications into an internal representation. During staging, OptiML performs domainspecific analyses and transformations on this internal representation. At run-time, OptiML further attempts to optimize program execution using domain-specific optimizations that could not be applied statically. Before describing these optimizations, we first note the domain-specific characteristics that OptiML tries to exploit.</p><p>Most ML algorithms perform basic operations on vectors and matrices. Fundamental operations include summation, maximization and minimization, especially in the context of convex optimization. Many of the algorithms expose parallelism at multiple levels of granularity; most have coarsegrained data parallelism (e.g. k-means) while others have only a small amount of work per data sample (e.g. linear regression). Several algorithms (logistic regression (LR), support vector machines (SVM), reinforcement learning) perform gradient descent on some objective function, iterating until they converge. This usually results in running the same kernel repeatedly on the same dataset many times. Many ML datasets (training sets) are very large, bound only by the execution time required for training; usually algorithm performance is limited by the available memory bandwidth due to low arithmetic intensity. The datasets typically have some exploitable structure; the most common are audio, image, or video data. Furthermore, these datasets may contain large amounts of redundancy (e.g., network traffic) that can be exploited to reduce total computation <ref type="bibr" target="#b8">[9]</ref>. Finally, many ML al-gorithms are probabilistic in nature and are robust to random errors such as dropped or incorrect computations.</p><p>These observations lead to the following proposed domainspecific optimizations for OptiML programs. We group them by whether they are applied during staging-time (statically) or run-time (dynamically).</p><p>Optimizations done at staging-time include:</p><p>• Transparent compression: We plan to explore the possibility of opportunistically compressing OptiML data structures before transferring to or from heterogeneous computing devices. While there is significant overhead to compression, several aspects of ML discussed previously make this a candidate for a winning trade-off: large datasets, low arithmetic intensity, and compressible data types (images, audio, video). In practice, most ML algorithms are bound by memory bandwidth, so on-the-fly compression could significantly decrease the total execution time when using heterogeneous devices. Stage-time analysis can generate code paths with explicit compression and decompression for large inputs.</p><p>• Device locality: OptiML will use stage-time information about object lifetime and arithmetic intensity to determine the target device for individual kernels. Furthermore, when a dataset is iterated over multiple times for different operands, clever scheduling can interleave the memory accesses to improve cache utilization.</p><p>• Choice of data representation: OptiML will generate code for multiple input-dependent data implementations (e.g. sparse and dense matrices), allowing the developer to use simple high-level representations (e.g. Matrix) that have identical semantics without worrying about the performance impact. OptiML will also use program analysis to choose the best representation for particular kernels, and can insert conversions from one implementation to another when it is likely to be beneficial to do so.</p><p>Optimizations done at run-time include:</p><p>• Best-effort computing: for some datasets or algorithms, many operations are not strictly required, and can be executed with "best-effort" semantics. This allows the OptiML programmer to signal to the runtime that it can drop certain data inputs or operations that are especially expensive if it is already under heavy load.</p><p>• Relaxed dependencies: it is sometimes possible to tradeoff a small degree of accuracy for a large gain in efficiency, especially when dealing with large datasets. This implies that some dependencies, such as parameter updates (especially across loop iterations), can be relaxed to varying degrees. By defining the strictness of a dependency, the OptiML user can choose the trade-off between exposing additional parallelism and increasing accuracy. The runtime will then dynamically decide to drop depen-dencies if it is advantageous to do so under the current workload.</p><p>In addition to the optimizations described here, OptiML must eventually generate parallel code targeted at heterogeneous machines. This is enabled by Delite, the framework and runtime that OptiML is being built on top of. We introduce Delite in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DSL Parallelization with Delite</head><p>In the Liszt example, Liszt explicitly maps programs to heterogeneous platforms by directly generating code written in lower level programming models such as CUDA and MPI. This allows Liszt to have maximal control over each stage of its compilation and runtime. This flexibility comes at the cost of implementation complexity; even within a virtualized host language, handling all aspects of the parallelization of DSL programs places a heavy burden on DSL developers. Since many components of a parallel DSL are potentially reusable (e.g. scheduling), we believe that it is important to develop frameworks and runtimes that aid in the task of parallelizing DSLs.</p><p>Delite is a framework for simplifying DSL parallelization and development. It first provides a set of predefined AST nodes for the common parts of domain-specific languages. Essentially, this is equivalent to embedding the Scala language itself. This allows the DSL developer to start with the base Scala AST and then add domain-specific nodes and semantics to the language. Since the embeddings are modularized into a variety of traits, the DSL developer is free to choose the subset of functionality to include in his virtual language.</p><p>Delite also provides a set of special case classes that can be inherited from when defining the node types of the language's AST (recall MatrixArithRep in Section 2). In addition to a case class that represents a sequential task, Delite provides case classes for a variety of parallel execution patterns (e.g. Map, Reduce, ZipWith, Scan). Each of these case classes constrains what information the DSL developer needs to provide so that Delite can automatically generate parallel code. For example, in OptiML, the scalar multiplication AST node inherits from Map. The DSL developer would then only have to specify the mapping function; Delite handles generating the parallel code for that operation on a variety of targets such as the CPU and GPU.</p><p>A Delite program goes through multiple compilation stages before execution. The first compilation stage, written by the DSL developer, uses virtualization to lift the user program into an internal representation and performs applicable domain-specific optimizations (e.g. the stage-time OptiML transformations). However, instead of generating explicitly parallel code, the DSL generates an AST where DSL operations are represented as Delite nodes (e.g. a Map node).</p><p>The AST is then compiled in subsequent stages by Delite. Delite expands the nodes generated by the DSL developer to handle Delite-specific implementation details (e.g. data chunking) and perform generic optimizations (e.g. operation fusing). Delite also generates a static schedule for straightline subgraphs in the user program, which reduces the time required to make dynamic scheduling decisions during actual execution. In the last compilation stage, Delite maps each domain-specific operation to different hardware targets. The final result of compilation is an optimized execution graph along with the generated kernels. The Delite runtime executes the graph in parallel using the available hardware resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>There is a long history of research into parallel and concurrent programming. Parallel programming is now standard practice for scientific computations and in specialized application areas such as weather or climate forecasting.</p><p>Parallel programming codes are usually procedural/imperative with explicit segmentation between processors. Communication is done by specialized libraries such as MPI or, in shared-memory environments, parallelism is made explicit via compiler directives such as in OpenMP. Interesting alternatives such as SISAL <ref type="bibr" target="#b14">[15]</ref>, SaC <ref type="bibr" target="#b40">[41]</ref> or NESL <ref type="bibr" target="#b4">[5]</ref> have been tried but were either ahead of their time or too specialized to be adopted by a large community. The same holds for data-parallel Haskell <ref type="bibr" target="#b25">[26]</ref>, which is built around nested array parallelism similar to NESL. The explicit message passing paradigm of MPI is not without critique; some argue that a model based on send and receive primitives is too low-level and that communication should be represented in a more structured manner, e.g. using collective operations <ref type="bibr" target="#b17">[18]</ref>. Google's MapReduce framework is essentially such a model <ref type="bibr" target="#b12">[13]</ref>.</p><p>Recently, the high-performance programming language designs Chapel <ref type="bibr" target="#b9">[10]</ref>, X10 <ref type="bibr" target="#b39">[40]</ref> and Fortress <ref type="bibr" target="#b26">[27]</ref> have emerged from a DARPA initiative. These languages combine sophisticated type systems with explicit control of location and concurrency. They are targeted primarily at scientific applications for supercomputers. It remains to be seen to what degree they can be applied to general programming on a variety of heterogeneous hardware. We believe that some of the innovations of these languages (regions, configuration variables) can be generalized and made more powerful in the staged compilation setting that we propose.</p><p>When performance is paramount and the sheer number of possible combinations makes manual specialization intractable, program generation is an approach that is often used. Instead of building a multitude of specialized implementations, a program generator is built that, given the desired parameters as input, outputs the corresponding specialized program. A number of high-performance programming libraries are built in such a way, for example ATLAS <ref type="bibr" target="#b53">[54]</ref> (linear algebra), FFTW <ref type="bibr" target="#b16">[17]</ref> (discrete fourier transform), and Spiral <ref type="bibr" target="#b34">[35]</ref> (general linear transformations). However, good program generators still take a huge effort to built and often, the resulting generator implementation will no longer resemble the original algorithm.</p><p>The ideas underlying language virtualization are both very old and quite new. Recent related work includes the notion of universal languages <ref type="bibr" target="#b52">[53]</ref> and relating hardware virtualization criteria with partial evaluation <ref type="bibr" target="#b15">[16]</ref>. Several important aspects of language virtualization have been with us since the invention of Lisp <ref type="bibr" target="#b44">[45]</ref> more than 50 years ago. The Lisp model of "code as data", supported by macros, makes it very easy to define and process embedded domainspecific languages. A common viewpoint is that Lisp is not so much a programming language, but rather a way to express language abstractions. In the words of Alan Kay: "Lisp isn't a language, it's a building material".</p><p>On the other hand, language embeddings in Lisp can be too seamless in that they do not distinguish between the embedded DSL and the hosting framework. Embedded DSL programs can observe the way their embeddings work and can access fairly arbitrary parts of the host environment. These limitations can be overcome in principle, given enough effort. A hosting environment could statically analyze embedded domain-specific code for safety violations. However, such analyses are non-trivial tasks, as they basically subsume implementations of static type checkers.</p><p>A common approach to DSL embedding is to assemble an abstract syntax tree (AST) representation of embedded programs. However, even in statically typed settings such as LINQ <ref type="bibr" target="#b6">[7]</ref> or earlier work on compiling embedded languages <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> the type of the AST is publicly available. An embedded language might use this fact to manipulate its own representation, thus undermining its virtualization.</p><p>Tighter encapsulation is provided by staging (or multistage programming). Staging shares with language virtualization the idea that DSL programs are assembled and run as part of the host program execution. But it provides no control on the choice of representation of the language embeddings. The usual approach, taken e.g. by MetaOCaml <ref type="bibr" target="#b19">[20]</ref>, is to use different kinds of syntactic brackets to delineate staged expressions in a DSL, i.e., those that will be part of the generated program from the host program. Staged expressions can have holes, also marked by special syntax, into which the result of evaluating the contained generator stage expression will be placed. Finally, staged expressions can be run, which will cause them to be assembled as program source code, run through the compiler, with the resulting object code being dynamically loaded and executed. In essence, staging as implemented in MetaOCaml is similar to macros using quote/unquote/eval in Lisp, but with a static type system that ensures well-formedness and type safety for the generated code at the time the multi-stage program is compiled A closely related approach to code generation is template expansion, as implemented in C++ <ref type="bibr" target="#b49">[50]</ref> or Template Haskell <ref type="bibr" target="#b42">[43]</ref>. The main difference to multi-staged programming is that all template expansion is done at compile-time. In contrast to staging, some template expansion mechanisms have a concept of user-definable rewriting rules which enable a limited form of domain-specific optimizations. However, the target of the compilation of embedded languages is always the same as the host language's.</p><p>A generalized approach to language embedding which is referred to as finally tagless or polymorphic embedding was introduced by Carette et al. <ref type="bibr" target="#b7">[8]</ref> and taken up by Hofer et al. <ref type="bibr" target="#b21">[22]</ref>. The former focus on the basic mechanism of removing interpretive overhead while the latter stress modularity and the ability to abstract over and compose semantic aspects. We show in this paper how to use polymorphic embeddings and lightweight modular staging <ref type="bibr" target="#b38">[39]</ref> to optimize parallel domain specific languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Enabling mainstream programming to take advantage of heterogeneous parallel hardware is difficult. In fact it has been identified as a grand challenge problem (PPP: "Popular Parallel Programming") by the computer architecture community <ref type="bibr" target="#b24">[25]</ref>. In this paper we have proposed an embedded domain specific approach based on language virtualization to address this challenge.</p><p>Our approach is not a "silver bullet" that will magically parallelize arbitrary programs. But language virtualization provides a path to get there. As new application domains come up, one can put effort into designing good parallel embedded languages for these domains and use a virtualization framework to map them to parallel execution environments. Likewise, as future parallel hardware evolves one can extend the framework to accommodate new architectures. This creates a realistic prospect that heterogeneous parallel execution environments can be harnessed in an increasing number of domains without requiring unreasonable effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>trait</head><label></label><figDesc>TestMatrix { this: MatrixArith =&gt; //requires mixing-in a MatrixArith implementation //when instantiating TestMatrix def example(a: Matrix, b: Matrix, c: Matrix, d: Matrix) = { val x = a * b + a * c val y = a * c + a * d println(x+y) } }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>/</head><label></label><figDesc>/ allows infix(+, * ) notation for Matrix implicit def matArith(x: Matrix) = new { def +(y: Matrix) = plus(x,y) def * (y: Matrix) = times(x,y) } def plus(x: Matrix, y: Matrix): Matrix def times(x: Matrix, y: Matrix): Matrix } There is nothing in the definition of MatrixArith apart from the bare interface. The definition Rep[T] postulates the existence of a type constructor Rep, which we take to range over possible representations of DSL expressions. In the staged interpretation, an expression of type Rep[T] represents a way to compute a value of type T. The definition of InternalMatrix postulates the existence of some internal matrix implementation, and the definition Matrix = Rep[InternalMatrix] denotes that Matrix is the staged representation of this not further characterized internal matrix type. The remaining statements define what operations are available on expressions of type Matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>trait Expressions { // constants/symbols (atomic) abstract class Exp[T] case class Const[T](x: T) extends Exp[T] case class Sym[T](n: Int) extends Exp[T] // operations (composite, defined in subtraits) abstract class Op[T] // additional members for managing // encountered definitions def findOrCreateDefinition[T](op: Op[T]): Sym[T] implicit def toExp[T](d: Op[T]): Exp[T] = findOrCreateDefinition(d) object Def { // pattern-match on definitions def unapply[T](e: Exp[T]): Option[Op[T]] = ... } }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Rep[T] = Exp[T] and introduce suitable case classes to represent the different node types in our expression tree. We also have to provide a dummy implementation of InternalMatrix: trait MatrixArithRepExp extends MatrixArith with Expressions { //selecting expression tree nodes representation type Rep[T] = Exp[T] trait InternalMatrix //tree node classes case class Plus(x: Matrix, y: Matrix) extends Op[InternalMatrix] case class Times(x: Matrix, y: Matrix) extends Op[InternalMatrix] def plus(x: Matrix, y: Matrix) = Plus(x, y) def times(x: Matrix, y: Matrix) = Times(x, y) }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>trait</head><label></label><figDesc>MatrixArithRepExpOpt extends MatrixArithRepExp { override def plus(x: Matrix, y: Matrix) = (x, y) match { // (AB + AD) == A * (B + D) case (Def(Times(a, b)), Def(Times(c, d))) if (a == c) =&gt; Times(a, Plus(b,d)) case _ =&gt; super.plus(x, y) // calls default plus() if no match } } Instantiating our example with object MyMatrixApp extends TestMatrix with MatrixArithRepExpOpt constructs an object that generates an optimized version of our example code. It automatically rewrites the sum of multiplications into a single multiplication of a sum of matrices: a * (b + c + c + d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>val x = TestSet(inputFile) val c = BestEffortVector[Int](m) // number of clusters val k = 10 // number of data points val m = x.numRows // k randomly initialized cluster centroids val mu : Vector[Double] = initializeClusters(x) c.setPolicy(ConvergingBestEffortPolicy(m)) until converged(mu) {</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Selection of mesh representation. Again, because the mesh topology is accessed through standard APIs, Liszt can analyze those API calls to determine what neighborhood relationships are used by the program. Liszt can build a table of what neighborhood relationships are being used, and choose the best mesh representation for the application.</figDesc><table /><note><p>•</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.scala-lang.org" />
		<title level="m">Scala</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Industry-Changing Impact of Accelerated Computing</title>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<ptr target="http://sites.amd.com/us/Documents/AMD_fusion_Whitepaper.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient Management of Parallelism in Object Oriented Numerical Software Libraries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modern Software Tools in Scientific Computing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Arge</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruaset</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Langtangen</surname></persName>
		</editor>
		<imprint>
			<publisher>Birkhäuser Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="163" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Programming pearls: little languages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="711" to="721" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Provable Time and Space Efficient Implementation of NESL</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN International Conference on Functional Programming</title>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="213" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overture: An object-oriented framework for solving partial differential equations on overlapping grids</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Henshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Quinlan</surname></persName>
		</author>
		<idno>UCRL- JC-132017</idno>
	</analytic>
	<monogr>
		<title level="m">SIAM conference on Object Oriented Methods for Scientfic Computing</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linq</forename><surname>Essential</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Addison-Wesley Professional</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finally tagless, partially evaluated</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kiselyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APLAS</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4807</biblScope>
			<biblScope unit="page" from="222" to="238" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Best-effort parallel execution framework for recognition and mining applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd Annual Int&apos;l Symp. on Parallel and Distributed Processing (IPDPS&apos;09)</title>
		<meeting>of the 23rd Annual Int&apos;l Symp. on Parallel and Distributed essing (IPDPS&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel programmability and the chapel language</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Zima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJHPCA</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="312" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design of the hypre Preconditioner Library</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Falgout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Workshop on Object Oriented Methods for Inter-operable Scientific and Engineering Computing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Henderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lyons</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Map-reduce for machine learning on multicore</title>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compiling embedded languages</title>
		<author>
			<persName><forename type="first">C</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Finne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="455" to="481" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SISAL: Streams and iterators in a single assignment language, language reference manual</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1985-03">March 1985</date>
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report M-146</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jones optimality and hardware virtualization: a report on work in progress</title>
		<author>
			<persName><forename type="first">B</forename><surname>Feigin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mycroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PEPM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="169" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast fourier transform compiler</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Send-receive considered harmful: myths and realities of message passing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gorlatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Program. Lang. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Parallel support vector machines: The cascade svm</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In NIPS &apos;04</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Implementing DSLs in metaOCaml</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Taha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA &apos;04: Companion to the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="41" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of the Trilinos project</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Heroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Howle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lehoucq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Phipps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Salinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Thornquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Tuminaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Willenbring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="423" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Polymorphic embedding of dsls</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPCE</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Smaragdakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Siek</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="137" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modular domain specific languages and tools</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hudak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fifth International Conference on</title>
		<meeting>Fifth International Conference on</meeting>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="134" to="142" />
		</imprint>
	</monogr>
	<note>Software Reuse</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">From a Few Cores to Many: A Tera-scale Computing Research Review</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="http://download.intel.com/research/platform/terascale/terascale_overview_paper.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revitalizing Computer Architecture Research</title>
	</analytic>
	<monogr>
		<title level="j">Computing Research Association</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Irwin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005-12">dec 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Harnessing the Multicores: Nested Data Parallelism in Haskell</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leshchinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M T</forename><surname>Chakravarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSTTCS, volume 2 of LIPIcs</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Hariharan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mukund</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Vinay</surname></persName>
		</editor>
		<imprint>
			<publisher>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="383" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel programming and parallel abstractions in fortress</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L S</forename><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE PACT</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">157</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A parallel algorithm for multilevel graph partitioning and sparse matrix ordering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="95" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Telescoping languages: A system for automatic generation of domain languages</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Broom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koelbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="408" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>This provides a current overview of the entire Telescoping Languages Project</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain specific embedded compilers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSL: Proceedings of the 2 nd conference on Domainspecific languages</title>
		<meeting><address><addrLine>Austin, Texas, United States; New York, NY; USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery, Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="10036" to="15701" />
		</imprint>
	</monogr>
	<note>One Astor Plaza, 1515 Broadway</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable component abstractions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Odersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Gabriel</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="41" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The case for a single-chip multiprocessor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nayfeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;96</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tagless staged interpreters for typed languages</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pasalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sheard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="218" to="229" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Simple unification-based type inference for GADTs. SIG-PLAN Not</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Peyton</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weirich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Washburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spiral: A generator for platform-adapted libraries of signal processing alogorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJHPCA</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A++/P++ array classes for architecture independent finite differences computations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ONNSKI</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Treating a user-defined parallel library as a domain-specific language</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V W</forename><surname>Reynders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tholburn</surname></persName>
		</author>
		<title level="m">POOMA: A Framework for Scientific Simulation on Parallel Architectures</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled DSLs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rompf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Odersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPCE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">X10: Concurrent programming for modern architectures</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Saraswat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APLAS</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Single Assignment C: efficient support for high-level array operations in a functional setting</title>
		<author>
			<persName><forename type="first">S.-B</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Funct. Program</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1005" to="1059" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vytiniotis. Complete and decidable type inference for GADTs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schrijvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Peyton</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sulzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFP &apos;09: Proceedings of the 14th ACM SIGPLAN international conference on Functional programming</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="341" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Template meta-programming for Haskell</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sheard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="60" to="75" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Sih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Common Lisp the Language</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Steele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Digital Press</publisher>
			<pubPlace>Billerica, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A framework approach for developing parallel adaptive multiphysics applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Finite Elem. Anal. Des</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1599" to="1617" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The free lunch is over: A fundamental turn toward concurrency in software</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multistage programming: its theory and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Supervisor-Sheard</publisher>
			<pubPlace>Tim</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Domain-specific languages: an annotated bibliography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Visser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="36" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">C++ templates: the Complete Guide</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vandevoorde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Josuttis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Addison-Wesley Professional</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Expression templates, C++ gems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Veldhuizen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Arrays in Blitz++</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Veldhuizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCOPE, volume 1505 of Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Caromel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Oldehoeft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Tholburn</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Active Libraries and Universal Languages</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Veldhuizen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
		<respStmt>
			<orgName>Indiana University Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automated empirical optimizations of software and the ATLAS project</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="35" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
