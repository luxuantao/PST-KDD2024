<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
							<email>weihuahu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
							<email>liubowen@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Chemistry, 3 Bioengineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
							<email>joe-gomes@uiowa.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Chemical and Biochemical Engineering</orgName>
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
							<email>marinka@hms.harvard.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
							<email>pande@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na√Øve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transfer learning refers to the setting where a model, initially trained on some tasks, is re-purposed on different but related tasks. Deep transfer learning has been immensely successful in computer vision <ref type="bibr">(Donahue et al., 2014;</ref><ref type="bibr">Girshick et al., 2014;</ref><ref type="bibr">Zeiler &amp; Fergus, 2014)</ref> and natural language processing <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr">Peters et al., 2018;</ref><ref type="bibr">Mikolov et al., 2013)</ref>. Despite being an effective approach to transfer learning, few studies have generalized pre-training to graph data.</p><p>Pre-training has the potential to provide an attractive solution to the following two fundamental challenges with learning on graph datasets <ref type="bibr">(Pan &amp; Yang, 2009;</ref><ref type="bibr">Hendrycks et al., 2019)</ref>: First, task-specific labeled data can be extremely scarce. This problem is exacerbated in important graph datasets from scientific domains, such as chemistry and biology, where data labeling (e.g., biological experiments in a wet laboratory) is resource-and time-intensive <ref type="bibr">(Zitnik et al., 2018)</ref>. Second, graph data from real-world applications often contain out-of-distribution samples, meaning that graphs in the training set are structurally very different from graphs in the test set. Out-of-distribution prediction is common in real-world graph datasets, for example, when one wants to predict chemical properties of a brand-new, just synthesized molecule, which is different from all molecules synthesized so far, and thereby different from all molecules in the training set.</p><p>However, pre-training on graph datasets remains a hard challenge. Several key studies <ref type="bibr">(Xu et al., 2017;</ref><ref type="bibr">Ching et al., 2018;</ref><ref type="bibr">Wang et al., 2019)</ref> have shown that successful transfer learning is not only a * Equal contribution. Project website, data and code: http://snap.stanford.edu/gnn-pretrain</p></div>		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
