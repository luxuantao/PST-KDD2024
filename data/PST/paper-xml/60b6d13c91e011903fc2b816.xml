<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Attention Free Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-28">28 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
							<email>szhai@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
							<email>wtalbott@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
							<email>nitish_srivastava@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
							<email>huang@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
							<email>hanlin@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
							<email>ruixiang_zhang2@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
							<email>jsusskind@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Apple Inc</orgName>
								<address>
									<settlement>MILA</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Attention Free Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-28">28 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.14103v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Attention Free Transformer (AFT), an efficient variant of Transformers [1] that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self attention mechanisms, represented by Transformers <ref type="bibr" target="#b0">[1]</ref>, have driven the advancement of various machine learning problems, including language understanding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and computer vision applications <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Different from classic model architectures such as Convolutional Neural Nets (CNNs) or Recurrent Neural Nets (RNNs), Transformers enable direct interaction between every pair of elements within a sequence, which makes them especially powerful at capturing long term dependencies.</p><p>However, Transformers require high computational costs. The cause of this challenge is the need to perform attention operations that have quadratic time and space complexity w.r.t. the context size. This makes it difficult for Transformers to scale to inputs with large context sizes. A number of recent works have been dedicated to addressing the scalability issue of Transformers <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. The common idea here is to approximate the full attention operation, with the techniques ranging from sparsity, locality sensitive hashing, low rank decomposition, kernel approximation, etc.. In this paper, we propose a computational module that does not use or approximate the standard dot product attention. We hence name our model the attention free transformer (AFT). Similar to dot product attention, AFT is composed of the interaction of three quantities, namely the query, key and value (Q, K, V ). The difference is that, in AFT the key and value (context) are first combined Figure <ref type="figure" target="#fig_2">1</ref>: Left: average relative 2d attention maps from a pretrained 12 layer 6 head ViT <ref type="bibr" target="#b4">[5]</ref>. Right: relative position biases learned by a AFT-conv with comparable size. Each row represents a layer (with layer index ranging from {0, 2, 4, 6, 8, 10}); Each column represents a head. See the Appendix for a more complete version.</p><p>Table <ref type="table">1</ref>: Complexity comparison with different Transformers: Reformer <ref type="bibr" target="#b7">[8]</ref>, Linear Transformer <ref type="bibr" target="#b10">[11]</ref>, Performer <ref type="bibr" target="#b12">[13]</ref> (only variants that support the causal mode are shown). Here T, d denote the sequence length and feature dimension, respectively. together with a set of learned position biases. The query is then combined with the reduced context with element-wise multiplication. See Figure <ref type="figure" target="#fig_0">2</ref> for an illustration.</p><p>AFT maintains direct interaction between any two points in the context, which is a major advantage of dot product attention. In fact, AFT can be interpreted as performing attention where the number of attention heads is the same as the model's feature dimension, whereas the attention maps do not need to be explicitly computed (see Sec. 3.1 for details). This results in a memory complexity linear w.r.t. both the input and model sizes.</p><p>The rearranged computational ordering of Q, K, V is also found in recent "linearized attention" works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. The difference is that AFT combines k and v in an element-wise fashion, while all the linear attention papers rely on matrix dot products. The latter approach results in an complexity quadratic to the model's feature dimension, which is unfriendly to large model sizes. See Table <ref type="table">1</ref> for the complexity analysis of AFT in comparison to other variants. Empirically, we observed that trained Transformers tend to demonstrate extensive local patterns (see Fig. <ref type="figure" target="#fig_2">1</ref>). This motivates us to propose two variants of AFT: AFT-local and AFT-conv. In AFT-local, the learned position biases are constrained to a local region, while global connectivity is maintained. AFT-conv further extends this design by imposing spatial weight sharing, effectively making it a variant of CNN with global receptive field. We show that the locality constraint not only provides better parameter and computational efficiency, but also greatly improves model's performance in all tasks.</p><p>We perform experiments with AFT on image auto-regressive modeling, character level language modeling, and image classification tasks. We show that AFT provides competitive performance, often matching or beating standard Transformers and other variants, while providing excellent efficiency. We also provide extensive ablation studies to several design choices of AFT, and discuss its unique properties such as compatibility with Transformers, sparsity and variable sized inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-Head Attention</head><p>At the core of Transformers is the Multi-Head Attention (MHA) operation. In the mode of self attention, given an input sequence X ∈ R T ×d , and the number of heads h, MHA performs a scaled dot product attention for each head i, defined as:</p><formula xml:id="formula_0">f i (X) = σ( Q i (K i ) T √ d k )V i , s.t. Q i = XW Q i , K i = XW K i , V i = XW V i ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">W Q i ∈ R d×d k , W K i ∈ R d×d k , W V i ∈ R d×dv</formula><p>are linear transformations for head i, and σ is the non-linearity by default set as the sof tmax function (applied to each row of a matrix). d k , d v are dimensions for key and value, respectively. MHA concatenates the output of h attention heads along the channel dimension, resulting in feature dimension hd v . Unless otherwise mentioned, we assume</p><formula xml:id="formula_2">d k = d v and h = d d k .</formula><p>This means the query, key and value are the same dimension within each head, and the output dimension matches that of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Free Transformer</head><p>We now define Attention free transformer (AFT), which is a plugin replacement of MHA without the need of changing other architectural aspects of Transformers. Given the input X, AFT first linearly transforms them into</p><formula xml:id="formula_3">Q = XW Q , K = XW K , V = XW V , then performs following operation 2 : Y = f (X); Y t = σ q (Q t ) T t =1 exp(K t + w t,t ) V t T t =1 exp(K t + w t,t )<label>(2)</label></formula><p>where is the element-wise product; σ q is the nonlinearity applied to the query with default being sigmoid; w ∈ R T ×T is the learned pair-wise position biases (see Figure <ref type="figure" target="#fig_0">2</ref> for an illustration).</p><p>Explained in words, for each target position t, AFT performs a weighted average of values, the result of which is combined with the query with element-wise multiplication. In particular, the weighting is simply composed of the keys and a set of learned pair-wise position biases. This provides the immediate advantage of not needing to compute and store the expensive attention matrix, while maintaining the global interactions between query and values as MHA does.</p><p>In order to further see AFT's relationship to MHA, we can rewrite Equation 2 as:</p><formula xml:id="formula_4">Y i t = a i t V i , s.t. a i t = σ q (Q i t ) exp(K i + w t ) T t =1 exp(K i t + w t,t ) , i = 1, 2, ..., d, t = 1, 2, ..., T.<label>(3)</label></formula><p>Here we use the superscript i to index the feature dimension of a matrix. In this rearranged form, we are able to express AFT in terms of attention again. Specifically, for each position, we have an attention vector a i t ∈ R T for each dimension, composed of Q, K, w. In other words, AFT can be interpreted as performing implicit attention with as many heads as feature dimensions, where the attention matrices take a factorized form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AFT variants: locality, weight sharing and parameterization</head><p>AFT-full. We denote the basic version of AFT defined in Equation 2 as AFT-full.</p><p>AFT-local. In many applications, locality is an important inductive bias, which has been exploited by CNNs and recent works in Transformers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. In addition, we found that trained standard Transformers tend to demonstrate extensive local attention patterns. To be concrete, we visualized an ImagenetNet pretrained Vision Transformer (ViT) <ref type="bibr" target="#b4">[5]</ref>, which consists of 12 layers each with 6 heads. For the sake of visualization, we ignore the classification tokens, and reshape each layer's attention tensors to shape 6 × 196 × 196 (the spatial size of the ViT's feature maps is 14 × 14). We then sampled 256 images from the ImageNet validation set. For each layer and each head, we compute the average relative 2d attentions, averaged across query positions and images. This results in a set of attention maps of size 12 × 6 × 27 × 27 <ref type="foot" target="#foot_1">3</ref> . The result is shown in Figure <ref type="figure" target="#fig_2">1</ref> (left), where we show the attentions for every 2 layers (see the Appendix for the full visualization). We see that the relative attention maps demonstrate strong local patterns (as indicated by the sharpness), especially in the lower layers. This motivates a variant of AFT, dubbed AFT-local, where we only apply a learned set of relative position biases locally:</p><formula xml:id="formula_5">∑ T t′ =1 ∑ T t′ =1 ⊙ Q t exp exp K V K = ] [ ⊙ ) ( ( ) ( σ q w t + ) w t + Y t</formula><formula xml:id="formula_6">w t,t = w t,t , if |t − t | &lt; s 0, otherwise.<label>(4)</label></formula><p>Here s ≤ T is a local window size. AFT-local provides further computational savings, both wrt the number of parameters and time/space complexity. Note that different from local Transformers (e.g., <ref type="bibr" target="#b6">[7]</ref>), AFT-local maintains global connectivity regardless of the window size s. In the experiments we verify the effectiveness of this design choice.</p><p>AFT-simple. An extreme form of AFT-local is when s = 0, i.e., no position bias is learned. This gives rise to an extremely simple version of AFT, where we have:</p><formula xml:id="formula_7">Y t = σ q (Q t ) T t =1 exp(K t ) V t T t =1 exp(K t ) = σ q (Q t ) T t =1 (softmax(K) V ) t .<label>(5)</label></formula><p>In this version, the context reduction is further simplified to element-wise operations and global pooling. AFT-simple is similar to linearized attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, which is formulated as</p><formula xml:id="formula_8">Y t = φ(Qt) T t =1 φ(K t ) T V t φ(Qt) T t =1 φ(Kt) T</formula><p>. However, it is easy to see that AFT-simple completely gets rid of the need for dot products operations, which results in a complexity of O(T d) rather than O(T d 2 ).</p><p>AFT-conv. We can also further extend the idea of locality to incorporate spatial weight sharing, i.e., convolution. This variant is especially relevant to vision tasks, as it is often desirable to extend a pretrained model to variable sized inputs. Specifically, we let the value of w t,t to be dependent only on the relative positions of t and t , w.r.t. to a given spatial grid (1d or 2d). Similar to CNNs, we can also learn multiple sets of position biases (we reuse the notion of heads for reference). To account for the growth of #parameters as #heads increases, we adopt a design choice to tie the dimensionality of K with #heads. This makes AFT-conv amendable to an implementation relying on depth-wise separable convolutions, global pooling and element-wise operations.</p><p>As a concrete example, we denote a model configuration as AFT-conv-h-s, where h is the number of heads and s × s is the 2d local window size. We now have</p><formula xml:id="formula_9">w ∈ R h×s×s , Q, V ∈ R T ×h× d h , K ∈ R T ×h .</formula><p>For each head i = 1, 2, ..., h, we have:</p><formula xml:id="formula_10">Y i t = σ q (Q i t ) conv2d(K i V i , exp(w i ) − 1) + T t =1 exp(K i t ) V i t conv2d(K i , exp(w i ) − 1) + T t =1 exp(K i t ) .<label>(6)</label></formula><p>Here,</p><formula xml:id="formula_11">Q i , V i ∈ R T × d h , K i ∈ R T , w i ∈ R s×s ; conv2d(x, w</formula><p>) is depth-wise separable 2d convolution operation with broadcasting along channel dimension when needed <ref type="foot" target="#foot_2">4</ref> . Note that Equation 6 can be readily interpreted as a specialized convolutional layer with 1) global connectivity, 2) non-negative convolutional weights and 3) sophisticated divisive/multiplicative gating mechanism. We show experimentally that all of the three aspects contribute significantly to AFT-conv's performance.</p><p>Parameterization. Empirically, we find that it is important to parameterize the position biases w properly. For AFT-full and AFT-local, we adopt a factorized form of w as:</p><formula xml:id="formula_12">w t,t = u T t v t , u ∈ R T ×d , v ∈ R T ×d ,<label>(7)</label></formula><p>where d is a small embedding dimension (e.g., 128). This simple factorization not only greatly reduces the parameter counts (2T d vs T 2 ), but also empirically improves model's performance in both training and testing.</p><p>For AFT-conv, the factorization trick is non-applicable. We instead adopt a simple re-parameterization, where for each head i, we let</p><formula xml:id="formula_13">w i = γ i w i − mean(w i ) std(w i ) + β i ,<label>(8)</label></formula><p>where γ ∈ R h , β ∈ R h are learnable gain and bias parameters, both initialized as 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Since the Transformer was introduced, there have been numerous attempts to address the major source of inefficiency in the architecture, the quadratic cost of the attention operation. Improving this operation can enable larger context sizes and more efficient implementations. For a comprehensive, recent survey of efficient transformers, see <ref type="bibr" target="#b15">[16]</ref>.</p><p>Approximating the dot product. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> propose to approximate the exponential kernel with inner product of projections, which leads to a linearized attention operation of complexity O(T d 2 ). The d 2 term of these models however makes it difficult to scale with model size, which is not a problem for AFT. Reformers <ref type="bibr" target="#b7">[8]</ref> apply LSH as an approximation to the dot product, where AFT completely gets rid of it.</p><p>Sparse, local attention. Sparse Transformers <ref type="bibr" target="#b6">[7]</ref> and Image Transformer <ref type="bibr" target="#b16">[17]</ref> proposes to use fixed sparse or local context patterns. Attention models in vision tasks (often combined with convolutions) use image structure to help handcraft relevant spatial patterns to attend <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. AFT-local also borrows the locality idea, but we put it as a bias rather than hard constraint. This allows AFTlocal/AFT-conv to take advantage of the full context, rather than relying only on a subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context compression.</head><p>Other approaches try to learn context patterns. Adaptive-Span Transformers <ref type="bibr" target="#b22">[23]</ref> learn a range for each attention head within which to attend. Routing transformers <ref type="bibr" target="#b23">[24]</ref> use clustering to compute dot-product attention only over a subset of elements within the same cluster.</p><p>The Linformer <ref type="bibr" target="#b9">[10]</ref> reduces the length of the context by compressing the keys and values with a linear layer. Compressive Transformers <ref type="bibr" target="#b8">[9]</ref> compute and update reduced representations of the input that are far enough back in the input sequence, and attend to those compressed representations. AFT is largely complementary to these approaches, as our focus is to improve the complexity of any given sequence from the operation level.</p><p>Eliminating dot product attention. Instead of limiting the number of comparisons, other methods change the operation used to compute attention. The Synthesizer <ref type="bibr" target="#b11">[12]</ref> uses attention weights predicted from inputs, rather than derived from dot-product interactions. The LightConv module introduced in <ref type="bibr" target="#b24">[25]</ref> proposes to replace the dot product self-attention with dynamic lightweight depthwise convolution, where the weights are normalized across temporal dimension. The Sinkhorn Transformer <ref type="bibr" target="#b25">[26]</ref> uses a differentiable sorting operation to identify relevant comparisons that may not be local in the original sequence order. AFT offers a different approach along this line, while highlighting strong empirical performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLPs for vision.</head><p>Concurrent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> explore the use of MLP inplace of the attention operation for vision tasks. While AFT can be viewed in a similar way, it is also equipped with a more sophisticated gating mechanism. In particular, the weighting of values are composed of both the key and position biases, which are normalized to non-negative values (similar to attention). This allows AFT to be a plugin module to existing Transformers without any architectural changes and extra tuning. Besides, AFT-conv inherits the valuable properties of CNNs, allowing it to achieve excellent parameter efficiency, strong performance as well as ability to handle variable sized inputs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on three tasks: image autoregressive modeling (Sec. 5.1), character level language modeling (Sec. 5.2) and image classification (Sec. 5.3). The first two benchmarks use the causal model (or decoder model) of AFT, while the last one uses the encoding model. All the experiments are designed in the plug and play fashion, where we obtain a baseline Transformer architecture for the specific task and replace the attention module with an AFT module. Hyperparameters such as initialization, learning rate scheduling are also directly inherited from the Transformer counterparts. Unless otherwise mentioned, all experiments are conducted on 8×V100 GPU machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Autoregressive Modeling</head><p>In our first set of experiments, we consider the problem of image autoregressive modeling by minimizing the negative log likelihood (NLL). Similar to <ref type="bibr" target="#b16">[17]</ref>, we represent an RGB image as a sequence of length H × W × 3, with H, W being the height and width, respectively. Each sub-pixel is represented as a 256-way discrete variable. We use CIFAR10 as the benchmarking dataset.</p><p>Our reference Transformer design largely follows that of <ref type="bibr" target="#b3">[4]</ref>, where a transformer block consists of an attention layer (AFT layer in our case) with residual connection and a 2 layer MLP with residual connections (with the feedforward dimension multiplier set to 4). Layer Normalization (LN) <ref type="bibr" target="#b28">[29]</ref> is applied in a "pre-act" fashion. We adopt learned position embeddings, and use a set of shared token embeddings and prediction heads across RGB. We use AFT-local with the factorized parameterization for this experiment. The hidden dimension for the factorization is 64, with u, v initialized with N (0, 10 −2 ); the local (1d) window size s is 256.</p><p>We use AdamW <ref type="bibr" target="#b29">[30]</ref>, and follow a standard warmup learning rate schedule as in <ref type="bibr" target="#b0">[1]</ref>. We use an initial learning rate of 3 × 10 −3 a weight decay of 0.1 applied to all linear transformations weights, and a dropout rate of 0. Comparing with the state of the art. CIFAR10 is a crowded benchmark for image autoregressive modeling, and we compare with a few competitive baselines, as shown in Table <ref type="table" target="#tab_0">2</ref>. Note that CIFAR10 has an unrolled sequence length of 3072, which is already prohibitive to train a full Transformer with reasonable size. For the standard Transformer model, we adopt two configurations (L=12, d=512, h=4 and L=24, d=256, h=2), with batch size 32 which is the largest one we can fit on a 8xV100 GPU Table <ref type="table">4</ref>: Enwik8 results, measured in bits per character (bpc), the lower the better. Baselines compared are Reformer <ref type="bibr" target="#b7">[8]</ref>, Synthesizer <ref type="bibr" target="#b11">[12]</ref> (its best performing dense version), Linear Transformer <ref type="bibr" target="#b10">[11]</ref> and Performer <ref type="bibr" target="#b12">[13]</ref> node. Another baseline is Image Transformer <ref type="bibr" target="#b16">[17]</ref>, which restricts attention to local2d windows of size of 256. We also compare to Sparse Transformers <ref type="bibr" target="#b6">[7]</ref>, which restrains attention to pre-specified sparse subsets of context elements.</p><p>From Table2, we see that AFT-local outperforms all the Transformer baselines. We also observe that the deeper but narrower architecture is more effective than the shallow but wide baseline. Our best model also achieves the state-of-the-art result on CIFAR10 in this setting, outperforming a much larger Sparse Transformer model. Efficiency wise, we benchmarked the Transformer variants against AFT on a 8 V100 GPU node <ref type="foot" target="#foot_3">5</ref> . All our variants are faster than standard Transformer and Image Transformer, while consuming only half of the memory <ref type="foot" target="#foot_4">6</ref> . Perhaps surprisingly, AFT-simple also achieves very competitive performance, even outperforming the Image Transformer, while offering excellent speed and memory efficiency.</p><p>The effect of factorization. We also provide ablations on the role of the factorized parameterization of AFT. To do this, we retrained the best performing model from Table <ref type="table" target="#tab_0">2</ref> ( i.e., AFT-local-256, L=24, d=256) with a naively parameterized w, initialized with N (0, 10 −2 ). From Table <ref type="table" target="#tab_1">3</ref>, we see that the factorized version not only provides significant parameter savings, but also improves the model's performance both on training and testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language Modeling</head><p>We apply AFT to character level language modeling on Enwik8 <ref type="bibr" target="#b30">[31]</ref>, which is another popular benchmark for auto-regressive modeling. We follow the standard preprocessing procedures and training/validation/test splits as in <ref type="bibr" target="#b31">[32]</ref>. Our base Transformer reference is a 12 layer 512 dimensional 8 head architecture with 2048 feed forward dimensions. For the first set of experiments, we use sequence length of 1024. Our training protocol is largely the same as the previous experiment, except that we increase the weight decay to 0.5 and train for 100 epochs with batch size 128. We evaluate the AFT-local with a window size of 32 and d = 256. We also compare to several efficient Transformer baselines, namely Reformer <ref type="bibr" target="#b7">[8]</ref>, Synthesizer <ref type="bibr" target="#b11">[12]</ref> , Linear Transformer <ref type="bibr" target="#b10">[11]</ref> and Performer <ref type="bibr" target="#b12">[13]</ref>.</p><p>From Table <ref type="table">4</ref>, we see that with the base L = 12, d = 512 architecture, AFT achieves the lowest training bits per character (bpc), which is an indicator for high model capacity. Its test performance is slightly worse than that of the basic Transformer, but outperforms all other Transformer variants. The deeper and narrower architecture of AFT strikes the best balance across parameter, speed, memory and performance. Its test bpc is only 0.024 away from the full Transformer's, while only consuming a third of the memory and provides a 44% speedup. AFT-simple again demonstrates competitive performance and excellent efficiency.</p><p>On the local window size. In order to validate the effect of local window size, we performed additional experiments with the L = 24, d = 256 architecture, fixing everything but varying the local window size s. We show the results in Table <ref type="table" target="#tab_3">5</ref>, where we see that both the training and testing bpc forms a U-shape w.r.t. the window size, with 32 achieving the best performance. This further confirms that locality is indeed an effective inductive bias across tasks.</p><p>Longer sequence size. We are also interested in AFT's ability to adapt to longer sequence sizes. Due to its simplicity, one might even expect a degradation of performance as T increases. To this end, we trained the AFT-local-32, L=24, d=256 model with T increased to 2048 and 4096. The results are shown in Table <ref type="table" target="#tab_4">6</ref>. We see that AFT is able to take advantage of larger sequence sizes and yield consistently lower training and testing loss as T increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Image Classification</head><p>We then test the non-causal version of AFT, focusing on an image classification task. We adopt the Vision Transformer architecture <ref type="bibr" target="#b4">[5]</ref>, and perform experiments on the Imagent 1K classification dataset. We adopt training setting and hyper parameters (batch size, data augmentation, regularization and learning rate scheduling) from DeiT <ref type="bibr" target="#b5">[6]</ref>.</p><p>In a nutshell, A ViT splits an image into 16 × 16 non-overlapping patches, then linearly projects each patch with shared weights to the equivalence of token embeddings. A learned class token is appended to the resulting representation, resulting in a sequence of length T = 1 + H/16 W/16 . A linear classification head is attached to the final layer's class token to obtain the final outputs. See <ref type="bibr" target="#b4">[5]</ref> for more details of the model configuration. All the experiments are conducted on the ImageNet-1K dataset, without using extra data.</p><p>Since the sequence size is relatively small in this task (T = 197 for input sizes of 224 × 224), we first experiment with AFT-full. The hidden dimension of factorized position bias is set as d = 128. Besides, we also experiment with AFT-conv. In this setting, we also remove the use of position embedding and class token, and apply global average pooling after the final layer's output, which is then fed into the classification linear layer. This modification not only simplifies the model design, but also makes AFT-conv fully convolutional, which is absent from Transformer and its variants.</p><p>We compare against two baseline Transformer configurations, with the "tiny" (L=12, d=192, h=3) and "small" (L=12, d=384, h=6) configurations, respectively. We also consider Lambda Networks <ref type="bibr" target="#b14">[15]</ref>, which is closely related to the linearized attention line of work. Similar to AFT-conv, we remove the class token and apply global average pooling instead. We use a publicly available implementation <ref type="foot" target="#foot_5">7</ref> , and apply the full context mode with the key projection dimension |k| = 16 (this setting invokes the faster linear implementation). We also apply BatchNorm to the query, key projections as recommended by <ref type="bibr" target="#b14">[15]</ref>.</p><p>Our result is shown in Table <ref type="table" target="#tab_5">7</ref>. We first see that AFT-full achieves comparable performance with the baseline Transformer DeiT in both configurations, while with better memory footprint and similar speed. AFT-conv significantly improves the top-1 accuracy of both configurations (2.%6, 1.1% absolute improvement for "tiny" and "small", respectively), with similar or smaller parameter counts. Compared to Lambda Networks, all AFT variants achieve comparable or better accuracy, with comparable speed and much smaller memory footprints.  Visualization. We also tried to visualize the position biases (exp(w) − 1 to be precise) learned by AFT-conv, as shown in Figure <ref type="figure" target="#fig_2">1</ref> (right). Note that interesting local, symmetric sparse patterns emerge. We show in the Appendix that we can regularize the position biases to achieve more sparsity. We also show an extreme version of AFT-conv, where each head is assigned one non-zero context points, while still keep good accuracy. This effectively transforms convolution into indexing.</p><p>Variable size inputs. AFT-conv is fully convolutional, which means that it can handle an input size different from that in training. We tested an AFT-conv model (last row of Table <ref type="table" target="#tab_5">7</ref>, trained with crop size 224) on a larger crop size of 384. This results in an improved accuracy of 81.6, compared with the original 81.0. This makes AFT-conv well suited for the pretraining finetuning workflows, as often seen in Vision tasks.</p><p>Compatibility with Transformers. Although AFT is not designed to directly approximate MHA, they do share considerable similarity in that the value vectors are aggregated with learned nonnegative weighting in both models. We hypothesize that representations learned by one model can be transferred to another. To test this, we obtain a pretrained "DeiT base" model with crop size 384. We then train an AFT-conv by initializing its weights with that of the DeiT model, excluding the position embeddings, the class token, key and query projections. We use a batch size of 64 and train the model for 100 epochs. As a control, we also train a randomly initialized AFT-conv for the same number of epochs. The results are shown in Table <ref type="table" target="#tab_6">8</ref>. Interestingly, we see that the finetuned version of AFT-conv achieves significantly higher accuracy than that randomly initialized version. The resulting model is also more accurate, faster and memory efficient than the original DeiT model.</p><p>Global connectivity. AFT-conv (as well as AFT-local) maintains global connectivity regardless of the local kernel size, which is distinctive from sparse and local attention works. To see the benefit of this design, we trained a degenerate variant of AFT-conv, where we modify Equation <ref type="formula" target="#formula_6">4</ref>to assign −∞ values to w t,t outside the local window (zero weights after exponentiation). When evaluating this baseline with kernel size 7, it gives a Top 1 accuracy of 79.9, compared to the default AFT-conv's 80.8 with the same setting, which is a 0.9% drop (we observe the same trend consistently in various configurations). We hypothesize that this technique can also be extended to local and sparse Transformers, but will leave it as future work.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Additional Ablations</head><p>We conducted more experiments on the ImageNet-1K classification settings.</p><p>Factorization of w. We first verify the importance of the factorized parameterization of AFT-full.</p><p>As shown in Tab 9, the non factorized parameterization of AFT-full achieves worse training and test performance than the factorized version.</p><p>Reparameterization of w. For AFT-conv, we by default apply the reprameterization as described in Sec. 3.2. We verify that this design effectively improves the model's performance, as shown in Table <ref type="table" target="#tab_9">10</ref>.</p><p>Kernel size. We also experimented with varying the local window size based on AFT-conv small (384 heads). The results are shown in Tab 11. Note that AFT-conv achieves comparable performance to the Deit reference even with a very small kernel size of 3 × 3.  Table <ref type="table" target="#tab_0">12</ref>: Top 1 accuracy of AFT-conv without the query term (w/o q). This results in significant performance drops. Contribution of the query. The query term contributes a small fraction to the computation of AFT, but it contributes significantly to AFT's performance. We conducted an additional experiment with AFT-conv (384 heads, kernel size in 11 × 11 and 15 × 15), where we remove the query term. The result is shown in Tab 12.</p><p>Visualizing the key. The keys play a central role in AFT, as they provide content dependent reweighting for effective context reduction. In order to understand their behavior, we visualized the feature maps for a AFT-conv model on randomly sampled images from the validation set of ImageNet-1K, as shown in Fig. <ref type="figure" target="#fig_10">9</ref>, 10, 11, 12. Interestingly, we see that the keys gradually evolve to "object detectors" as the layer level goes up.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 +</head><label>2</label><figDesc>T d) Reformer O(T log T d) O(T log T + T d) Linear Transformer O(T d 2 ) O(T d + d 2 ) Performer O(T d 2 log d) O(T d log d + d 2 log d) AFT-simple O(Td) O(Td) AFT-full O(T 2 d) O(Td) AFT-local (AFT-conv) O(T sd), s &lt; T O(Td)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of AFT defined in Equation 2, with T = 3, d = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>We adopt simple data augmentation. During training, we first randomly flip each image horizontally, then add or subtract a value in the range [−10, 10] from all its subpixels, and clip resulting pixel values to [0, 255]. We use cross entropy loss, and a default batch size of 128 for 200 training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Exponentiated position biases learned by AFT-full, trained on ImageNet-1K, shown from layer 1, 2, ..., 12, arranged from top left to bottom right. Each image is of size 197 × 197, where the first element corresponds to the class token, and the remaining 196 correspond to the 14 × 14 positions. We see that local, sparse patterns are learned without explicit supervision.</figDesc><graphic url="image-3.png" coords="12,120.53,72.00,370.94,279.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Image completion with the AFT-local trained on CIFAR10 autoregressive modeling task. Top: masked images from the test set. Bottom: completed images.</figDesc><graphic url="image-5.png" coords="13,115.74,261.57,380.52,188.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The full set of average relative 2d attention maps learned by a pretrained ViT model (with 12 layers and 6 heads) on ImageNet-1K. Each row corresponds to a layer and each column corresponds to a head. Each attention map is of size 27 × 27, with the class token excluded.</figDesc><graphic url="image-6.png" coords="14,165.46,102.05,281.09,546.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Exponentiated position biases learned by AFT-conv, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a head (the first 16 are shown). This model has top 1 accuracy of 80.8%.</figDesc><graphic url="image-7.png" coords="15,123.99,74.56,364.03,273.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Exponentiated position biases learned by AFT-conv (kernel size 11 × 11) with sparsity regularization, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a head (the first 16 are shown). This model has top 1 accuracy of 80.9%.</figDesc><graphic url="image-8.png" coords="15,123.99,402.55,364.03,273.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Exponentiated position biases learned AFT-conv (kernel size 11×11) with Gumbel softmax sampling, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a head (the first 16 are shown). This model has top 1 accuracy of 79.9%.</figDesc><graphic url="image-9.png" coords="16,123.99,72.00,364.03,273.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the keys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.</figDesc><graphic url="image-10.png" coords="17,164.16,118.90,283.68,283.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the keys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.</figDesc><graphic url="image-12.png" coords="18,164.16,118.90,283.68,283.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the keys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.</figDesc><graphic url="image-14.png" coords="19,164.16,118.90,283.68,283.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the keys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.</figDesc><graphic url="image-16.png" coords="20,164.16,118.90,283.68,283.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>NLL results on CIFAR10, evaluated by bits/dim, the lower the better. Speed and memory are measured during training time, with a batch size of 32 across 8 V100 GPUs. AFT achieve the state-of-the-art result in this setting, with significant improvements wrt speed and memory over standard Transformer, Sparse Transformer<ref type="bibr" target="#b6">[7]</ref> and Image Transformer<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell>Method</cell><cell>L</cell><cell>d</cell><cell cols="4">h Train loss Test loss Iters/Sec GB/GPU</cell></row><row><cell>PixelCNN</cell><cell>-</cell><cell>-</cell><cell>-3.08</cell><cell>3.14</cell><cell></cell></row><row><cell>PixelCNN++</cell><cell>-</cell><cell>-</cell><cell>--</cell><cell>2.92</cell><cell></cell></row><row><cell>PixelSNAIL</cell><cell>-</cell><cell>-</cell><cell>--</cell><cell>2.85</cell><cell></cell></row><row><cell cols="4">Sparse Transformer strided 128 256 2 -</cell><cell>2.80</cell><cell></cell></row><row><cell cols="2">Image Transformer local2d 12</cell><cell cols="2">512 4 -</cell><cell>2.90</cell><cell>1.61</cell><cell>22.3</cell></row><row><cell>Transformer</cell><cell>12</cell><cell cols="2">512 4 2.90</cell><cell>2.88</cell><cell>1.35</cell><cell>30.6</cell></row><row><cell>Transformer</cell><cell>24</cell><cell cols="2">256 2 2.90</cell><cell>2.86</cell><cell>1.36</cell><cell>30.4</cell></row><row><cell>AFT-local-256</cell><cell>12</cell><cell cols="2">512 1 2.78</cell><cell>2.80</cell><cell>1.68</cell><cell>11.4</cell></row><row><cell>AFT-local-256</cell><cell>24</cell><cell cols="2">256 1 2.75</cell><cell>2.74</cell><cell>1.67</cell><cell>12.8</cell></row><row><cell>AFT-simple</cell><cell>24</cell><cell cols="2">256 1 2.82</cell><cell>2.89</cell><cell>2.15</cell><cell>9.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>The effect of factorized parameterization of the position bias, evaluated by autoregressive modeling on CIFAR10.</figDesc><table><row><cell></cell><cell cols="3">#params/layer Train loss Test loss</cell></row><row><cell>Non Factorized</cell><cell>9.6M</cell><cell>2.82</cell><cell>2.84</cell></row><row><cell cols="2">Factorized (default) 0.6M</cell><cell>2.75</cell><cell>2.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. L, d, h, T denote number of blocks (depth), dimension of features, number of heads, and sequence length, respectively. Speed and memory are measured during training time, with a batch size of 128 on a 8 V100 GPU node. Both Linear Transformer and Performer are implemented with customized CUDA kernels (github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch.</figDesc><table><row><cell>Method</cell><cell>L</cell><cell>d</cell><cell>h T</cell><cell cols="2">Train bpc Test bpc Iters/Sec GB/GPU</cell></row><row><cell>Transformer</cell><cell cols="4">12 512 8 1024 0.977</cell><cell>1.137</cell><cell>1.42</cell><cell>29.4</cell></row><row><cell>Transformer</cell><cell cols="4">24 256 4 1024 1.039</cell><cell>1.130</cell><cell>1.57</cell><cell>28.3</cell></row><row><cell>Reformer</cell><cell cols="4">12 512 8 1024 1.04</cell><cell>1.195</cell><cell>1.05</cell><cell>20.9</cell></row><row><cell>Synthesizer</cell><cell cols="4">12 512 8 1024 0.994</cell><cell>1.298</cell><cell>1.49</cell><cell>29.9</cell></row><row><cell cols="5">Linear Transformer 12 512 8 1024 0.981</cell><cell>1.207</cell><cell>1.46</cell><cell>10.6</cell></row><row><cell>Performer</cell><cell cols="4">12 512 8 1024 1.002</cell><cell>1.199</cell><cell>1.44</cell><cell>10.1</cell></row><row><cell>AFT-local-32</cell><cell cols="4">12 512 1 1024 0.854</cell><cell>1.180</cell><cell>1.85</cell><cell>11.3</cell></row><row><cell>AFT-local-32</cell><cell cols="4">24 256 1 1024 0.972</cell><cell>1.154</cell><cell>2.04</cell><cell>11.2</cell></row><row><cell>AFT-simple</cell><cell cols="4">24 256 1 1024 1.046</cell><cell>1.209</cell><cell>2.61</cell><cell>9.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Training and testing bpc w.r.t. the local window size for AFT-local.</figDesc><table><row><cell>Win size</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell></row><row><cell cols="12">Train bpc 1.046 1.043 1.009 0.990 0.983 0.972 0.981 0.985 0.986 0.988 0.991</cell></row><row><cell>Test bpc</cell><cell cols="11">1.209 1.205 1.176 1.165 1.162 1.154 1.160 1.165 1.164 1.171 1.173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Increasing T on Enwik8. Both training and testing loss are improved as T increases.</figDesc><table><row><cell>T</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell></row><row><cell cols="4">Train bpc 0.972 0.951 0.945</cell></row><row><cell>Test bpc</cell><cell cols="3">1.154 1.135 1.134</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Imagenet 1K classification results with the Transformer architecture from DeiT<ref type="bibr" target="#b5">[6]</ref>, cropsize is 224. Speed and memory consumption are measured in inference mode on a V100 GPU, batch size is 256.</figDesc><table><row><cell>Model</cell><cell cols="6">Kernel Heads Top1 Acc #Params (MB) Images/Sec Mem (GB)</cell></row><row><cell>ResNet50 [33]</cell><cell>3</cell><cell>-</cell><cell>76.9</cell><cell>25.6</cell><cell>1257</cell><cell>6.5</cell></row><row><cell>DeiT tiny [6]</cell><cell>-</cell><cell>3</cell><cell>72.2</cell><cell>5.7</cell><cell>2507</cell><cell>1.9</cell></row><row><cell>DeiT small [6]</cell><cell>-</cell><cell>6</cell><cell>79.9</cell><cell>22.1</cell><cell>1010</cell><cell>2.9</cell></row><row><cell>Lambda tiny [15]</cell><cell>-</cell><cell>3</cell><cell>72.4</cell><cell>4.8</cell><cell>2157</cell><cell>2.7</cell></row><row><cell cols="2">Lambda small [15] -</cell><cell>6</cell><cell>80.0</cell><cell>17.7</cell><cell>1057</cell><cell>5.8</cell></row><row><cell>AFT-full tiny</cell><cell>-</cell><cell>1</cell><cell>72.4</cell><cell>6.3</cell><cell>2523</cell><cell>1.8</cell></row><row><cell>AFT-full small</cell><cell>-</cell><cell>1</cell><cell>79.8</cell><cell>22.6</cell><cell>1011</cell><cell>2.6</cell></row><row><cell>AFT-conv tiny</cell><cell>11</cell><cell>32</cell><cell>73.9</cell><cell>5.4</cell><cell>2359</cell><cell>1.8</cell></row><row><cell>AFT-conv tiny</cell><cell>11</cell><cell>192</cell><cell>74.8</cell><cell>5.9</cell><cell>2365</cell><cell>2.2</cell></row><row><cell>AFT-conv small</cell><cell>11</cell><cell>16</cell><cell>80.2</cell><cell>20.3</cell><cell>989</cell><cell>2.5</cell></row><row><cell>AFT-conv small</cell><cell>11</cell><cell>384</cell><cell>80.8</cell><cell>22.5</cell><cell>936</cell><cell>3.2</cell></row><row><cell>AFT-conv small</cell><cell>15</cell><cell>384</cell><cell>81.0</cell><cell>23.0</cell><cell>936</cell><cell>3.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Finetuning AFT-conv for 100 epochs from a pretrained "DeiT base" on 384 × 384 crops. "ft" and "rand" stand for finetuning and random initialization, respectively.</figDesc><table><row><cell>Model</cell><cell cols="6">Kernel Heads Top1 Acc #Params (MB) Images/Sec Mem (GB)</cell></row><row><cell>Deit base [33]</cell><cell>-</cell><cell>12</cell><cell>82.9</cell><cell>86.9</cell><cell>89.6</cell><cell>13.6</cell></row><row><cell>AFT-conv ft</cell><cell>25</cell><cell>32</cell><cell>83.4</cell><cell>79.7</cell><cell>98.5</cell><cell>8.9</cell></row><row><cell cols="2">AFT-conv rand 25</cell><cell>32</cell><cell>81.6</cell><cell>79.7</cell><cell>98.5</cell><cell>8.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>The effect of factorized parameterization of AFT-full.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>The effect of reprameterization of AFTconv (kernel size 7 × 7).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Varying kernel size for AFT-conv. Train loss 3.02 2.94 2.94 2.93 2.93 2.94 3.01 Top 1 Acc 79.9 80.8 80.8 81.0 80.7 81.0 79.9</figDesc><table><row><cell>Kernel</cell><cell>3</cell><cell>7</cell><cell>11</cell><cell>15</cell><cell>25</cell><cell>27</cell><cell>DeiT small</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">we use the non-masked mode for illustration, and the masked/causal mode can be constructed by limiting the range of the summation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">12 is #layers, 6 is #heads, 27 × 27 is relative 2d attention size from feature map 14 × 14</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"> Equation 6  can also be implemented with fully connected operations, e.g., einsum, which might yield better efficiency in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We use a batch size of 32 which is the largest batch size Image Transformer can fit</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">Fair speed/memory comparison against Sparse Transformer is infeasible, as it relies on a set of advanced implementation tricks such as mixed precision and gradient checkpointing, whereas AFT is implemented with standard Pytorch utilities ran in full precision.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">github.com/lucidrains/lambda-networks, released under MIT License</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced the Attention Free Transformer that replaces dot product attention with an efficient new operation. We have demonstrated strong results on a set of standard benchmarks with excellent efficiency. We believe that our model opens a new design space for Transformer-like models, and will see impact in various areas where self attention are needed.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Sparsity</head><p>The position biases learned by AFT-conv (kernel size 11 × 11) as shown in Figure <ref type="figure">6</ref> demonstrates interesting sparsity patterns, which suggests and pruning. To this end, we experimented with a simple sparsity promoting regularization term:</p><p>Where we simply minimize the entropy for each head, with the softmax distribution using w i as the logits. We combining reg(w) with the cross entropy loss with a small weighting (0.001) and train with the AFT-conv with kernel size 11 and 384 heads. This results in a slight improvement in accuracy (due to its regularization effect) of 80.9 vs 80.8, as well as sparser looking position biases. The visualization is shown in Fig. <ref type="figure">7</ref>. We see that the position biases are much more sparsely distributed as expected.</p><p>Encouraged by this, we continued to push the sparsity to an extreme form. Now for each head, we only assign a learned relative position bias for a single position. To do this, during training, we multiply the position biases w for each layer and each head with a sample from its corresponding Gumbel softmax distribution <ref type="bibr" target="#b33">[34]</ref>:</p><p>where τ is the temperature term for Gumbel softmax, and we set it as 0.5; gumbel(w i ; τ ) produces a (sparse) sample with the same shape as w i . During inference, the Gumbel softmax is replaced with hard max, i.e., a one hot vector is returned. This results in a model with top 1 accuracy 79.9, with less than 1 point drop compared with the unregularized model. The position biases are visualized in Fig. <ref type="figure">8</ref>. This extreme model variant makes it possible to implement the context reduction of K, V with a combination of global average pooling and indexing, which has the same complexity as AFT-simple but maintains strong performance (comparable to that of the standard Transformer).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<imprint>
			<publisher>Generative pretraining from pixels</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno>ArXiv, abs/2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.05507</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.07853</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.12273</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>ArXiv, abs/1906.05909</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sainbayar Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.05997</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>ArXiv, abs/1901.10430</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<idno>ArXiv, abs/2002.11296</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<title level="m">Mlp-mixer: An all-mlp architecture for vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pay attention to mlps</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>ArXiv, abs/1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
