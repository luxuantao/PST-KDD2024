<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified 2D and 3D Pre-Training of Molecular Representations</title>
				<funder ref="#_nvGAAd3">
					<orgName type="full">Youth Innovation Promotion Association CAS</orgName>
				</funder>
				<funder ref="#_DDxm7Kb">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-14">14 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Yingce Xia ?</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Lijun Wu</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Shufang Xie</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Tao Qin</orgName>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">Wengang Zhou</orgName>
							</affiliation>
							<affiliation key="aff10">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country>China Houqiang Li</country>
								</address>
							</affiliation>
							<affiliation key="aff11">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui, Tie-Yan Liu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff13">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unified 2D and 3D Pre-Training of Molecular Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-14">14 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539368</idno>
					<idno type="arXiv">arXiv:2207.08806v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Molecule pre-training</term>
					<term>molecular property prediction</term>
					<term>conformation generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Molecular representation learning has attracted much attention recently. A molecule can be viewed as a 2D graph with nodes/atoms connected by edges/bonds, and can also be represented by a 3D conformation with 3-dimensional coordinates of all atoms. We note that most previous work handles 2D and 3D information separately, while jointly leveraging these two sources may foster a more informative representation. In this work, we explore this appealing idea and propose a new representation learning method based on a unified 2D and 3D pre-training. Atom coordinates and interatomic distances are encoded and then fused with atomic representations through graph neural networks. The model is pre-trained on three tasks: reconstruction of masked atoms and coordinates, 3D conformation generation conditioned on 2D graph, and 2D graph generation conditioned on 3D conformation. We evaluate our method on 11 downstream molecular property prediction tasks: 7 with 2D information only and 4 with both 2D and 3D information. Our method achieves state-of-the-art results on 10 tasks, and the average improvement on 2D-only tasks is 8.3%. Our method also achieves significant improvement on two 3D conformation generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Applied computing ? Molecular structural biology; Bioinformatics; ? Computing methodologies ? Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning techniques have attracted more and more attention recently in drug discovery <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>, bioinformatics <ref type="bibr" target="#b31">[32]</ref> and cheminformatics <ref type="bibr" target="#b38">[39]</ref>. Obtaining effective molecular representations, usually high dimensional vectors that are friendly to neural network models, is a key prerequisite for deep learning based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Molecules can be expressed in different forms, including 2D molecular graphs and 3D molecular conformations. A 2D molecular graph describes the 2D topological structure of a molecule, where the nodes/atoms are connected by edges/bonds respectively. Many methods have been proposed to deal with molecular graphs such as Graphormer <ref type="bibr" target="#b47">[48]</ref>, virtual node <ref type="bibr" target="#b10">[11]</ref>, etc. A 3D molecular conformation is represented by a set of coordinates for all the atoms of a molecule. People also propose various models based on 3D conformations, such as SchNet <ref type="bibr" target="#b30">[31]</ref>, DimeNet <ref type="bibr" target="#b19">[20]</ref>, PAiNN <ref type="bibr" target="#b29">[30]</ref>, etc.</p><p>While the two types of representations are complementary, i.e., 2D graphs focus on topological connections of atoms and 3D conformations focus on spatial arranges of atoms, only limited works leverage them together. Chen et al. <ref type="bibr" target="#b3">[4]</ref> use the multiscale weighted colored algebraic graphs (AG) to encode 3D conformation and obtain corresponding 3D representations. Besides, they use a bidirectional Transformer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref> to encode the molecular SMILES <ref type="bibr" target="#b42">[43]</ref> (obtained by traversing a 2D molecular graph using depth-firstsearch) and obtain another representation. The two representations are fused together for downstream tasks. Liu et al. <ref type="bibr" target="#b21">[22]</ref> and St?rk et al. <ref type="bibr" target="#b35">[36]</ref> are two recent works that use both 2D information and 3D information for molecule pre-training. Their common training objective is to maximize the mutual information between the 2D and 3D views of a molecule, where the 2D view and 3D view are encoded using two different modules.</p><p>Different from above methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>, in this work, we propose a unified method that processes both 2D and 3D information of molecules in a single model, inspired by the recent trend and success of multi-modality modeling in deep learning research <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. For examples, DALL-E <ref type="bibr" target="#b27">[28]</ref> is a unified model that can encode images and texts together, and demonstrates its great power in text-to-image generation; Kaiser et al. <ref type="bibr" target="#b17">[18]</ref> use one unified model for image, speech and text processing, and show that the tasks with less data benefit largely from the joint training.</p><p>Our model is pre-trained on PCQM4Mv2 <ref type="bibr" target="#b12">[13]</ref>, where molecules are represented by both 2D graphs and corresponding 3D conformations. In our model, atomic coordinates and interatomic distances are encoded by a feed-forward network and then fused by a graph neural network. To effectively unify 2D and 3D information, we design several pre-training tasks:</p><p>(1) Reconstruction of masked atoms and coordinates, which is to reconstruct randomly masked atoms and coordinates based on unmasked ones;</p><p>(2) 3D conformation generation conditioned on 2D graph, which is to generate 3D conformation based on the 2D graph of a molecule;</p><p>(3) 2D graph generation conditioned on 3D conformation, which is to generate 2D graph based on the 3D conformation of a molecule.</p><p>We use masked language modeling loss <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> to reconstruct the masked atoms. Besides, we adopt a permutation invariant loss function of symmetric substructures so that the training process is more effective. This is because the coordinates of atoms in symmetric substructures can be swapped, and conventional loss function cannot maintain permutation invariance. We also adopt the rototranslation invariance loss to 3D conformation generation conditioned on the 2D graph. Note that our model is compatible with molecules with 2D information only, where their coordinates can be randomly initialized.</p><p>We test our method on the following tasks:</p><p>(1) Six molecular property prediction tasks from MoleculeNet <ref type="bibr" target="#b44">[45]</ref> and one molecular prediction tasks from OGB benchmark<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b13">[14]</ref>. The 3D conformations of these molecules are not accessible.</p><p>(2) Four toxicity prediction tasks from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44]</ref> where both 2D and 3D information are available.</p><p>(3) Two 3D molecular conformation tasks where the data is sampled from the Geometric Ensemble Of Molecules dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>We achieve state-of-the-art results on 10 out of the 11 molecular property prediction tasks. Specifically, on the 2D-only tasks, we improve the previous best method by 8.3% on average. For the toxicity prediction with 3D conformations, our method outperforms the previous deep learning based methods and the manually designed 3D molecular fingerprints. For the two conformation generation tasks on GEOM-QM9 and GEOM-Drugs, in terms of the mean matching score, we improve the previous best results by 7.7% and 3.6%.</p><p>Our contributions can be summarized as follows:</p><p>(1) We propose a new method that jointly encodes both 2D and 3D information of molecules in a unified model. The learnt model can be used for both molecular prediction and conformation generation tasks.</p><p>(2) We propose several new training objective functions to fully utilize the data, which shows new directions of molecule pre-training.</p><p>(3) We achieve state-of-the-art results on 10 molecular prediction tasks and 2 conformation generation tasks.</p><p>(4) We release our code and pre-trained models at the Github repository https://github.com/teslacool/UnifiedMolPretrain for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Molecular Pre-training with 2D Information</head><p>Inspired by its success in natural language processing and computer vision, pre-training has been introduced into molecular representation learning recently. In these works, a molecule is represented by either a SMILES <ref type="bibr" target="#b42">[43]</ref> sequence or an undirected graph where the nodes and edges are atoms and bonds, respectively, By using SMILES sequence, Chithrananda et al. <ref type="bibr" target="#b4">[5]</ref>, Wang et al. <ref type="bibr" target="#b40">[41]</ref> use the masked language modeling objective <ref type="bibr" target="#b5">[6]</ref> for pre-training. Honda et al. <ref type="bibr" target="#b11">[12]</ref> learn molecular representation by reconstructing the input SMILES with a Transformer based on a sequence-to-sequence model. By regarding a molecule as a graph, Hu et al. <ref type="bibr" target="#b14">[15]</ref> perform node-level and graph-level pre-training, which are about to reconstruct the masked attributes and preserve the consistency of similar subgraphs. Wang et al. <ref type="bibr" target="#b41">[42]</ref> use contrastive learning, where the representation of a molecule ? should be similar to the augmented version of ? while dissimilar to others. GraphCL <ref type="bibr" target="#b48">[49]</ref> uses graph augmentation and contrastive learning for pre-training. Rong et al. <ref type="bibr" target="#b28">[29]</ref> design GNN Transformer for pre-training, which extends the attention blocks to graph data. They also use two new training objective functions, one is to predict the motifs in a graph, and the other is to predict the subgraph property represented by a well-designed string. Zhu et al. <ref type="bibr">[52]</ref> propose to maximize the consistency between SMILES representation and graph representation, and achieve remarkable performance on several downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D Molecular Representation</head><p>Encoding 3D spatial structure into molecular representation is important to determine molecular property. Anderson et al. <ref type="bibr" target="#b1">[2]</ref>, Lu et al. <ref type="bibr" target="#b23">[24]</ref>, Sch?tt et al. <ref type="bibr" target="#b30">[31]</ref> take the atomic distance into consideration and design a set of novel architecture to deal with atomic positions. Klicpera et al. <ref type="bibr" target="#b19">[20]</ref>, Shui and Karypis <ref type="bibr" target="#b33">[34]</ref> further involve bond angle into their methods and achieve better performance. Recently, Fuchs et al. <ref type="bibr" target="#b8">[9]</ref>, Sch?tt et al. <ref type="bibr" target="#b29">[30]</ref> introduce equivariant networks to ensure the equivariance of molecular representation under continuous 3D roto-translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Molecular Pre-training with 3D Information</head><p>Recently, there emerge several works for molecule pre-training with 3D spatial structure. Chen et al. <ref type="bibr" target="#b3">[4]</ref>   <ref type="bibr" target="#b15">[16]</ref>. The input of FF(? ? ? ) is the concatenation of all input tensors, and the output is a ?-dimensional vector, where ? is the dimension of the network hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objective</head><p>The overall training objective functions consist of three parts: reconstruction of masked atoms and coordinates; 3D conformation generation conditioned on 2D graph and 2D graph generation conditioned on 3D conformation. The illustration is in Figure <ref type="figure" target="#fig_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Reconstruction of masked atoms and coordinates.</head><p>Given a molecule ? with |? | nodes and the corresponding conformation ?, with probability ? ? (0, 1), each atom ? ? is independently masked. Also, the coordinate of each atom ? ? , i.e., ? ? , is independently masked. Denote the indices of the unmasked atoms and coordinates as ? ? and ? ? , respectively. The task is to reconstruct the masked tokens (i.e., ? \? ? and ? \? ? ) based on unmasked ones (i.e., ? ? and ? ? ). The training objective for reconstructing the masked atoms is:</p><formula xml:id="formula_0">? atom = ?? ? ?? \? ? log ? (? ? |{? ? } ? ?? ? ; {? ? } ? ?? ? ),<label>(1)</label></formula><p>which is similar to that in natural language processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> and computer vision <ref type="bibr" target="#b6">[7]</ref>. Assume the coordinate of atom ? ? is masked and let R? denote the predicted coordinate of atom ? ? . To measure the difference between the groundtruth coordinate ? ? and generated coordinate R? ?? ? ? \? ? , the most straightforward way is to use</p><formula xml:id="formula_1">l = ?? ? ?? \? ? ?? ? -R? ? 2 .<label>(2)</label></formula><p>However, for 3D conformation, this is not the best choice because there might be symmetric molecules, where the coordinates of atoms can be swapped. An example is shown in Figure <ref type="figure" target="#fig_3">2(a)</ref>, where the molecule is symmetric along the bond between atom 4 and 5. If we swap the coordinates of atom 3, 2, 1, 0 with 6, 7, 8, 9, the conformation remains the same, but the loss in Eqn.( <ref type="formula" target="#formula_1">2</ref>) changes. In addition, although some molecules are not symmetric, they still have symmetric substructures. As shown in Figure <ref type="figure" target="#fig_3">2</ref> When we randomly mask the coordinates, it is possible to mask the symmetric substructures. To tackle this challenge, we follow <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">51]</ref>  The loss function that is permutation invariant to symmetric substructures is defined as follows:</p><formula xml:id="formula_2">? coord = min ? ? P ?? ? ?? \? ? ?? ? -? ( R? )? 2 ,<label>(3)</label></formula><p>where ? ? ? R 3 and ? ( R) ? ? R 3 denote the coordinate of the ?-th atom in ? and ? ( R), respectively. Zhu et al.</p><p>[51] provide an efficient implementation to find the P based on graph automorphism and we follow their method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">3D conformation generation conditioned on 2D graph.</head><p>We mask all the coordinates of a conformation and reconstruct them based on the 2D molecular graph only. Molecular conformation generation is an important topic for both bioinformatics and machine learning, and various methods have been proposed, including the distance-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47]</ref> (which generate the interatomic distances or their gradients first and then reconstruct the conformation) or the direct approach (which directly outputs the 3D conformation). Recently, Zhu et al. <ref type="bibr">[51]</ref> proposed a method named directly molecular conformation generation (DMCG) that outputs the coordinates directly and achieved state-of-the-art results. We use DMCG for conformation generation in our unified pre-training framework.</p><p>The conformation should maintain roto-translation invariance <ref type="bibr" target="#b25">[26]</ref> and permutation invariance <ref type="bibr" target="#b26">[27]</ref>. Roto-translation invariance means that if we rotate and translate the generated 3D conformation</p><formula xml:id="formula_3">? 1 ? 2 ??? ? ? ? ??? ? 2 ? 3 ? ? ? Model ? 1 ? 2 ? 3 ? ? ? ??? ??? ? Model ??? ??? ? 1 ? 2 ? 3 ? ? Model ??? ??? ??? ??? ? 1 ? 2 ? 3 ? ? ? ? 1 ? 2 ? 3 ? ? ? ? ? 3 ? 1 (a) (b) (c)</formula><p>?  R rigidly, the distance between R and the groundtruth conformation ? remains unchanged. The permutation invariance has been introduced in Section 3.2.1. Mathematically, the loss function is defined as follows:</p><formula xml:id="formula_4">? 2D?3D = 1 |? | min ?,? ? P ?? ? ?? ?? ? -? (? ( R? ))? 2 .<label>(4)</label></formula><p>In Eqn.(4), R? is obtained based on the 2D graph ? without touching the 3D information; ? is the roto-translational operation. ? can be written as ? (?) = ?? + ?, where ? is a 3 ? 3 rotation matrix, and ? ? ? 1?3 is the transition vector. Eqn.( <ref type="formula" target="#formula_4">4</ref>) is roto-translation invariant and permutation invariant to the symmetric substructures. According to <ref type="bibr" target="#b18">[19]</ref>, calculating the minimum value over roto-translation operations can be transformed into calculating the minimum eigenvalues of some 4 ? 4 matrix, and we use their method. More details are available in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">2D graph generation conditioned on 3D conformation.</head><p>We mask all atoms and reconstruct them based on the 3D conformation. The 2D structure of the molecule is kept, i.e., we know which two atoms are connected, but we do not know the bond type between them. The training objective function is defined as follows:</p><formula xml:id="formula_5">? 3D?2D = 1 |? | ?? ? ?? log ? (? ? |{? ? } ? ?? ).<label>(5)</label></formula><p>Note that in Eqn. <ref type="bibr" target="#b0">(1)</ref>, only partial atoms are masked. We can reconstruct them based on the remaining atoms and coordinates. In comparison, Eqn.( <ref type="formula" target="#formula_5">5</ref>) is an extreme case where all atoms are masked, and we need to reconstruct them purely by coordinates. Both of the two loss functions are helpful for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Discussion.</head><p>Compared with the pre-training in natural language processing, ? atom and ? 3D?2D are similar to the masked language modeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. However, for ? coord and ? 2D?3D , we use the permutation invariant and roto-translation invariant version. In Eqn.(3), since only partial coordinates are masked, we just need to consider the permutation invariance of symmetric atoms and do not need the roto-translational operation on all coordinates. This is because the unmasked coordinates can help reduce the roto-translation freedom. In comparison, in Eqn.( <ref type="formula" target="#formula_4">4</ref>), considering all the atoms are masked, we should consider both permutation and roto-translation invariance since we have no unmasked coordinates to align.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture</head><p>As mentioned in Section 1, we propose a method that can encode the 2D and 3D information using one model and output a unified representation. We use the GN block module proposed by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">51]</ref> as our backbone due to their superior performance on molecule classification and molecular conformation generation. Denote the model as net(?, ?), where the inputs include the 2D graph ? and 3D conformation ?. The input atoms, bonds are first mapped into ?-dimensional representation using the corresponding embedding layers. All the masked atoms are represented by a special embedding ? ? and all the related bond embeddings are represented by another special embedding ? ? , both of which are learned during pre-training. If the coordinates ? ? ? R 3 of atom ? is masked, each element in ? ? is replaced with a number uniformly sampled from</p><formula xml:id="formula_6">[-1, 1].</formula><p>Compared with previous molecule pre-training with 2D information only, how to encode the 3D information is an important problem. Inspired by pointed Transformer [50] and the equivalent network <ref type="bibr" target="#b8">[9]</ref>, we encode both the coordinates and interatomic distances. Let ? ? and x? denote the representations of atom ? before and after combining with 3D information. Let ? ? ? and x? ? denote the representations of bond ? ? ? before and after combining with 3D information. We have that  Note that some ? ? could be the ? ? , some ? ? ? could be ? ? , and some coordinates ? ? 's are not the real coordinates but randomly sampled. The x? 's and x? ? 's are then fed into the main backbone for further processing. Briefly, net(? ? ? ) stacks ? identical blocks. As shown in Figure <ref type="figure" target="#fig_4">3</ref>, in each block, we first update the bond representations (the red edge) using the related atom representations (the green nodes) and a global representation of the molecule (the yellow node). After that, the atom representations (the red node) are updated in an attentive way using the related edge representations (the green edges) and the global representation (the yellow node). Finally, we update the global representation by averaging the updated atom and bond representations. The network net(?, ?) eventually outputs a representation for each atom, i.e.,</p><formula xml:id="formula_7">x? = ? ? + FF(? ? ), x? ? = ? ? ? + FF(?? ? -? ? ?).<label>(6</label></formula><formula xml:id="formula_8">(? * 1 , ? * 2 ? ? ? , ? * |? | ) = net(?, ?).<label>(7)</label></formula><p>The mathematical formulation of net(?, ?) is summarized in Appendix A. We use two different MLP layers to reconstruct the atoms and coordinates. Both of them take ? * ? as input: (1) FF atom , which outputs the masked atoms. It will be used in Eqn.(1) and Eqn. <ref type="bibr" target="#b4">(5)</ref>.</p><p>(2) FF coord , which outputs the masked coordinates. It will be used in Eqn.(3) and Eqn.(4).</p><p>For the example in Figure <ref type="figure" target="#fig_4">3</ref>, we mask the atom information of node ? 1 and mask the coordinates of node ? 3 . After obtaining their representations ? * 1 and ? * 3 , they are reconstructed by FF atom (? * 1 ) and FF coord (? *</p><p>3 ) We summarize the pre-training procedure in Algorithm 1. Please note that [51] is for 2D molecular graph to 3D conformation generation, and their loss function is specially designed for conformation generation. In this work, our focus is to jointly use 2D/3D information and obtain effective molecular representations. Our method can be used for molecule modeling and generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATION TO PROPERTY PREDICTION</head><p>We work on 11 tasks to verify the effectiveness of our method: (1) 7 molecular property prediction tasks with 2D information only;</p><p>(2) 4 molecular property prediction tasks with both 2D and 3D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Dataset. We use the PCQM4Mv2 dataset <ref type="bibr" target="#b12">[13]</ref> for pre-training, which has 3.38M data. In PCQM4Mv2, both the 2D information and Algorithm 1 Workflow of the pre-training. For molecular property prediction with 2D information only, following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, we choose six tasks on MoleculeNet <ref type="bibr" target="#b44">[45]</ref>: BBBP, Tox21, ClinTox, HIV, BACE and SIDER. Most of them are with limited data. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, for each task, we split the dataset into training, validation and test sets by 8:1:1 according to their molecular scaffolds. We also conduct experiments on ogb-molpcba <ref type="bibr" target="#b13">[14]</ref>, which is a larger dataset with 438K data. All these tasks are classification tasks. For property prediction with both 2D and 3D information, following Chen et al. <ref type="bibr" target="#b3">[4]</ref>, we work on four toxicity prediction tasks: LD50, IGC50, LC50 and LC50DM, all of which are regression tasks. They measure the toxicity from different aspects. More details of the above datasets are summarized in Table <ref type="table" target="#tab_11">7</ref> of the Appendix.</p><p>Training configuration. The pre-trained model has 12 layers, and the hidden dimension of each block is 256. The masked ratio ? is 0.25. The model is pre-trained for 100 epochs using Adam optimizer with initial learning rate 2 ? 10 -4 , batch size 128 and is trained on four P40 GPUs.</p><p>For the molecular property prediction tasks with 2D information only, all the coordinates are randomly sampled from uniform distribution [-1, 1]. For the tasks on MoleculeNet, we use grid search to determine the learning rate, dropout and batch size, which are summarized in Appendix B. For ogb-molpcba, the learning rate is fixed as 10 -4 and we train the model for 100 epochs.</p><p>For the four toxicity prediction tasks with 3D information, following <ref type="bibr" target="#b3">[4]</ref>, we use multitask learning to jointly tune the four tasks. We also use grid search to find the hyper-parameters, which are left in Appendix B.</p><p>Evaluation. For MoleculeNet, we use area under the receiver operating characteristic curve (ROC-AUC). Each experiment is independently run for three times with different seeds, and the mean and standard derivation are reported. Ogb-molpcba has 128 sub-tasks and we use average precision (briefly, AP) as suggested by Hu et al. <ref type="bibr" target="#b13">[14]</ref>. For the four toxicity prediction, following <ref type="bibr" target="#b3">[4]</ref>, we use squared Pearson correlation coefficient (? 2 ) between the groundtruth and predicted values, rooted mean square error (RMSE) and mean absolute error (MAE) as the evaluation metrics.</p><p>Baselines. For MoleculeNet, we compare with three 2D pre-training baselines: AttrMask &amp; ContextPred <ref type="bibr" target="#b14">[15]</ref>, GROVER <ref type="bibr" target="#b28">[29]</ref> and GraphCL <ref type="bibr" target="#b48">[49]</ref>. They are all introduced in related work. We also compare with two 3D pre-training baselines: St?rk et al. <ref type="bibr" target="#b35">[36]</ref> and GraphMVP <ref type="bibr" target="#b21">[22]</ref>, which have been discussed comprehensively. For ogb-molpcba, we mainly compare with Graphormer <ref type="bibr" target="#b47">[48]</ref>, which is also pre-trained on PCQM4M and achieves the best result on this dataset. For the four toxicity prediction tasks where 3D information is available, we mainly compare with AGBT <ref type="bibr" target="#b3">[4]</ref>. Since the Github repository of GraphMVP is empty by the end of the submission, we leave the comparison with it on the four toxicity prediction tasks in the future. We also compare with some classical baselines, which mainly use various molecular fingerprints and multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The results on MoleculeNet are reported in Table <ref type="table" target="#tab_2">1</ref>. We have the following observations:</p><p>(1) Compared with AttrMask &amp; ContextPred <ref type="bibr" target="#b14">[15]</ref>, GROVER <ref type="bibr" target="#b28">[29]</ref> and GraphCL <ref type="bibr" target="#b48">[49]</ref> which only leverage the 2D molecular graphs for pre-training, our method outperforms these baselines by large margins. Specifically, on average, our method improves those baselines by 11.81%, 10.50% and 14.08%. This shows the effectiveness of using 3D information in pre-training.</p><p>(2) Compared with St?rk et al. <ref type="bibr" target="#b35">[36]</ref> and GraphMVP <ref type="bibr" target="#b21">[22]</ref>, our method still outperforms those baselines. We can improve the average ROC-AUC from 70.37 and 74.65 to 80.85. St?rk et al. <ref type="bibr" target="#b35">[36]</ref> and Liu et al. <ref type="bibr" target="#b21">[22]</ref> use two separated modules to encode the 2D and 3D information, and they use consistency loss to build their connection. In comparison, we use a unified module to deeply fuse them. The improvement shows that using unified representation is a promising direction for molecule modeling.</p><p>The results on ogb-molpcba are summarized in Table <ref type="table" target="#tab_4">2</ref>. We report the results on both validation and test sets following the guidelines <ref type="bibr" target="#b13">[14]</ref>. GN <ref type="bibr" target="#b0">[1]</ref> and "GIN + virtual node" <ref type="bibr" target="#b20">[21]</ref> are two baselines without pre-training, where "GN" is the backbone of our model, and "GIN + virtual node" denotes applying a virtual node, which aggregates the global information of a molecule, into the GIN model <ref type="bibr" target="#b45">[46]</ref>. The Graphormer <ref type="bibr" target="#b47">[48]</ref> here uses an adversarial augmentation technique named FLAG <ref type="bibr" target="#b20">[21]</ref> and obtains good improvements to the non-FLAG version. We also combine FLAG with our method in this work.</p><p>We can see that, although ogb-molpcba has more data, it still benefits from pre-training. Compared with GN block, which is the non-pretraining version of network architecture, we can improve the test score from 0.2650 to 0.3125. In addition, by using FLAG, our method outperforms Graphormer by 2.4% and 1.1% on validation and test sets, setting new records for this task.</p><p>Table <ref type="table">3</ref>: Results of toxicity prediction where both 2D and 3D information is available.  The results of toxicity prediction with 3D information are reported in Table <ref type="table">3</ref>. Compared with AGBT <ref type="bibr" target="#b3">[4]</ref>, our method outperforms this strong baseline on all the four tasks and evaluation metrics. Chen et al. <ref type="bibr" target="#b3">[4]</ref> separately deal with the 2D graph and 3D conformation, while we use a unified model to extract representations of the input molecule that can more consistently utilize the two types of information. Our method also outperforms the nonpretraining methods, including the 2D fingerprint baselines like MACCS <ref type="bibr" target="#b7">[8]</ref>, Daylight(reported by <ref type="bibr" target="#b9">[10]</ref>), and the 3D based methods like 3D MT-DNN <ref type="bibr" target="#b43">[44]</ref>. This shows that the molecular representation obtained by pre-training outperforms the manually designed fingerprints. These results demonstrate the effectiveness of our proposed method for molecular property prediction.</p><formula xml:id="formula_9">LD50 LC50 IGC50 LC50DM ? 2 (?) RMSE (?) MAE (?) ? 2 (?) RMSE (?) MAE (?) ? 2 (?) RMSE (?) MAE (?) ? 2 (?) RMSE (?) MAE (?) MACCS 0.643 N/A N/A 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We mainly focus on the following questions: (Q1) What is the contribution of each training objective function? (Q2) What is the effect of different mask ratios?</p><p>To answer (Q1), we independently pre-train the models with ? atom + ? coord , ? 2D?3D and ? 3D?2D , respectively. After that, we finetune the obtained models on the MoleculeNet datasets and ogbmolpcba. Part of the results are summarized in Figure <ref type="figure" target="#fig_5">4</ref> and the remaining ones are left in Appendix C. We can see that:</p><p>(1) If we independently use any one of ? atom + ? coord , ? 2D?3D and ? 3D?2D , the performance is not as good as using them together. This shows that each of them contribute to the unified pre-training and we should combine them together.</p><p>(2) Reconstructing the masked atoms and coordinates is relatively more important for the unified pre-training. However, it is ignored by previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, which mainly focus on preserving the consistency of different views.</p><p>(3) According to our statistics, the avarage ROC-AUC scores on MoleculeNet of the above three loss functions are 79.15, 78.57 and 79.21. All of them are better than <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, which demonstrate the power of unified representations.  To answer (Q2), we try 4 different mask ratios, 0.15, 0.25, 0.35 and 0.45. We conduct experiments on the BBBP and Clintox from MoleculeNet. The results are shown in Figure <ref type="figure" target="#fig_7">5</ref>. In general, our pre-training method is robust to the choice of the mask ratios. For example, on BBBP, the maximum performance gap among the four ratios is 0.5. Empirically, setting ? as 0.25 achieves the best results.</p><p>When only 2D information is available, people might be curious about the initialization of the 3D coordinates. By default, we uniformly sample the coordinates from [-1, 1]. To verify its robustness, on ogb-molpcba, we randomly sample ten groups of initial coordinates and evaluate them. We also set all the initial coordinates as zero and check the inference result. The differences of the above 11 trials are less than 10 -4 , which shows that our model is robust to the initial coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION TO CONFORMATION GENERATION</head><p>In this section, we conduct on 3D conformation generation, which is to map the 2D molecular graph to 3D conformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>Following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47]</ref>, we use a subset of GEOM-QM9 and GEOM-Drugs <ref type="bibr" target="#b2">[3]</ref> to evaluate our method. The numbers of training, validation and test (molecule, conformation) pairs of GEOM-QM9 are 200K, 2.5K and 22408 respectively, while those numbers for GEOM-Drugs are 200K, 2.5K and 14324. On average, GEOM-QM9 and GEOM-Drugs have 8.8 and 24.9 heavy atoms respectively. Considering each molecular corresponds to multiple diverse conformations, we adopt the variational auto-encoder framework and generate multiple conformations. We choose the model proposed by et al. <ref type="bibr">[51]</ref> for conformation generation, and the parameters are initialized by our pre-trained unified model. The model is a 12-layer network with hidden dimension 256. We also use Adam optimizer for training with learning rate 10 -4 and the model is finetuned for 100 epochs.</p><p>We evaluate the diversity and accuracy of generated conformations. Let RMSD(?, R) denote the root mean square deviations of two conformations ? and R:</p><formula xml:id="formula_10">RMSD(?, R) = min ? 1 |? | |? | ?? ?=1 ?? ? -?( R? ) ? 2 1 2 , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where ? is the alignment function that aligns two conformations by roto-translation operations. We use the coverage score (COV) and matching score (MAT) for evaluation. Let ? ? and ? ? denote the collections of the generated conformations and the groundtruth conformations. Assume in the test set, the molecule ? has ? ? conformations, following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref>, we generate 2? ? conformations for it. Mathematically, COV(? ? , ? ? ) and MAT(? ? , ? ? ) are defined as follows:</p><formula xml:id="formula_12">COV(? ? , ? ? ) = 1 |? ? | {? ? ? ? | RMSD(?, R) &lt; ?, ? R ? ? ? } ; MAT(? ? , ? ? ) = 1 |? ? | ?? ???? min R??? RMSD(?, R). (<label>9</label></formula><formula xml:id="formula_13">) ?atom + ?coord ?2D ? 3D</formula><p>?3D ? 2D Ours 0.15 0.17   Following previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref>, ?'s are set as 0.5 and 1.25 for GEOM-QM9 and GEOM-Drugs datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The results are reported in Table <ref type="table" target="#tab_6">4</ref>. We can see that our method outperforms all previous baselines, no matter for distance-based methods like ConfGF <ref type="bibr" target="#b32">[33]</ref>, DGSM <ref type="bibr" target="#b24">[25]</ref>, or the recently proposed direct approach [51]. This shows the effectiveness of our pre-training in conformation generation. Similar to the first question in Section 4.3, we also conduct ablation study of different training objective functions. The results are shown in Figure <ref type="figure" target="#fig_10">6</ref>. For the MAT scores, the values of QM9 correspond to the left axis, and those of Drugs correspond ot the right axis. We get the same conclusion as Section 4.3: Using all the three types of loss function together achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we propose the unified 2D and 3D pre-training, that jointly leverages the 2D graph structure of the molecule and the 3D conformation. We design three loss functions, including reconstructing the masked atoms and coordinates, 2D graph to 3D conformation generation and the 3D conformation to 2D graph generation. The permutation invariance and roto-translation invariance are considered. We conduct experiments on 11 molecular property prediction tasks and 2 conformation generation tasks, and achieve state-of-the-art results on 12 of them.</p><p>For future work, first, we will explore how to leverage the large amount of molecules without 3D structures. We could use them to enhance the ? atom and use cycle consistency loss, where we map the 2D graph ? to 3D conformation R and then map R back to 2D graph ?. ? and ? should preserve enough similarity. Second, the combination of our method with contrastive learning is another topic. Third, we will apply our pre-trained model to more scenarios like drug-drug interaction and drug-target interaction prediction.</p><p>[50] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS ABOUT THE NETWORK</head><p>As introduced in Section 3.3, the model is a stack of ? identical blocks, each block takes the output of the previous one as input. The (? -1)-th block will output the representations of atom ? ? (denoted as ? (1) Incorporate geometric information:</p><formula xml:id="formula_14">x (?) ? = ? (?-1) ? + FF( R (?-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>),</p><formula xml:id="formula_15">x (?) ? ? = ? (?-1) ? ? + FF(? R (?-1) ? - R (?-1) ? ?).<label>(10)</label></formula><p>(2) Update bond representations: For each ? ? ? ? ?, ?</p><formula xml:id="formula_16">? ? = ? (?-1) ? ? + FF( x (?-1) ? , x (?-1) ? , x (?) ? ? , ? (?-1) ).<label>(?)</label></formula><p>(3) Update atom representations: For any ? ? ? ,</p><p>x </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B FINETUNING HYPERPARAMETERS</head><p>We summarize the finetuning hyperparameters in Table <ref type="table" target="#tab_9">5</ref>. The hyperparameters for the six tasks from MoleculeNet are in the "MoleculeNet" column, while those for the four toxicity prediction tasks are in the "Toxicity" column. The details of the dataset are in Table <ref type="table" target="#tab_11">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper parameters MoleculeNet Toxicity</head><p>Learning Rate {2e-4, 5e-4, 8e-4, 1e-3} {5e-4, 8e-4, 1e-  As introduced in Section 4.3, we conduct ablation study of pretraining with different objective functions, and finetuning on Molecu-leNet. The results are summarized in Table <ref type="table" target="#tab_10">6</ref>. We can see that independently using any objective is not as good as combining them together. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b), atoms 12, 13 are symmetric to 16, 15 along the C-C bond (atom 10 and 11). Another symmetric substructure is the piperidine at the right side of Figure 2(b). If we swap the coordinates of the symmetric substructures, the 3D conformation should remain unchanged. Such permutation invariance cannot be maintained by Eqn.(2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and use the permutation invariant loss. Given a molecule ?, let ? denote a bijective mapping from {1, 2, ? ? ? , |? |} to itself. Let P denote the collection of atom mappings on symmetric substructures. For the picture in Figure 2(b), P has four mappings. (1) ? (?) = ? ?? ? ? ; (2) ? (12) = 16, ? (16) = 12, ? (13) = 15, ? (15) = 13 and ? (?) = ? for remaining atoms; (3) ? (1) = 17, ? (17) = 1, ? (0) = 18, ? (18) = 0 and ? (?) = ? for remaining atoms; (4) ? (12) = 16, ? (16) = 12, ? (13) = 15, ? (15) = 13, ? (1) = 17, ? (17) = 1, ? (0) = 18, ? (18) = 0, ? (?) = ? for remaining atoms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The training objective functions. (a) reconstruction of masked atoms and coordinates; (b) 3D conformation generation conditioned on 2D graph; (c) 2D graph generation conditioned on 3D conformation. ??? denotes ths masked atoms and coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the symmetry in molecules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A brief workflow of the network architecture. For the input molecule, we mask the atom information of atom 1 and mask the coordinates of atom 3. It is then processed by a stack of ? blocks, where the yellow node refers to the global representation of the molecule. We eventually obtain a representation ? * ? atom, based on which we can reconstruct the masked atoms and coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of different pre-training objective functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of different masked ratios ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>MAT scores (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>COV scores (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of different pre-training objective functions on conformation generation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(?- 1 )</head><label>1</label><figDesc>? ? R ? ), the representations of bond ? ? ? (denoted as ? (?-1) ? ? ? R ? ), a predicted conformation R (?-1) , and a global representation of the molecule ? (?-1) . Let N (?) denote the neighbors of atom ?, i.e., N (?) = { ? | ? ? ? ? ?}.The workflow of the ?-th block is shown as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>? 4 )</head><label>4</label><figDesc>? ? exp(? ? ? (? ? x (?-1)<ref type="bibr" target="#b11">(12)</ref>, ?, ? ? , ? ? and ? ? are the parameters to be learned, [?; ?] is the concatenation of two vectors ? and ?, and ? is the leaky ReLU activation. (Update global representations: The global representations are updated by aggregating the new bond representations and atom representations, i.e.,? (?) = ? (?-1) + FF ? ? , ? (?-1) .(13)After obtaining the updated representations, the ?-th block predicts a new 3D conformation. For any ? ? {1, 2, ? ? ? , |? |},</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Let ? = (? , ?) denote a 2D molecular graph, where? = {? 1 , ? 2 , ? ? ? , ? |? | }is a collection of atoms, and ? is a collection of bonds. Let ? ? ? denote the bond between atom ? ? and ? ? . Let ? ? R |? |?3 denote the 3D conformation of molecule ?, where the ?-th row ? is the coordinate of atom ? ? . For ease of reference, when the context is clear, we use ? to denote the indices of atoms, i.e., ? = {1, 2, ? ? ? , |? |}.</figDesc><table><row><cell>use the element-specific multi-</cell></row><row><cell>scale weighted colored algebraic graph (AG) to embed the chemical</cell></row><row><cell>and physical interactions into graph invariants and capture 3D</cell></row><row><cell>molecular structural information, which is then fused with the</cell></row><row><cell>complementary bi-directional Transformer representation. St?rk</cell></row><row><cell>et al. [36] propose to implicitly encode the 3D information into</cell></row><row><cell>molecular representation by maximizing the mutual information</cell></row><row><cell>between a 2D graph representation and a 3D representation which</cell></row><row><cell>are produced by two separate networks. Liu et al. [22] propose</cell></row><row><cell>the Graph Multi-View Pre-training (GraphMVP) framework, which</cell></row><row><cell>leverages constrastive learning and molecule reconstruction (under</cell></row><row><cell>the variational auto-encoder framework) for pre-training. To our</cell></row><row><cell>best knowledge, almost all previous works encode 2D graph and</cell></row><row><cell>3D structure with different backbone models, and we are the first</cell></row><row><cell>to encode them using a unified model.</cell></row><row><cell>3 OUR METHOD</cell></row><row><cell>3.1 Notations</cell></row><row><cell>Let FF(? ? ? ) denote a two-layer feed-forward network with ReLU</cell></row><row><cell>activation and Batch Normalization</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>)</figDesc><table><row><cell>? 2 input</cell><cell>? 5 ? 1 (atom mask)</cell><cell></cell><cell>net(?, ?)</cell><cell>? ?</cell><cell>? 2  *  output</cell><cell>? 5  *   *  : FF atom ? ? ? 1  *  ? atom</cell></row><row><cell></cell><cell>? 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? 4  *</cell></row><row><cell></cell><cell>? 3 (coord mask)</cell><cell>update bond</cell><cell>update atom</cell><cell>update global</cell><cell cols="2">? 3  *  : FF coord ? 3  *  ? coord</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1 :</head><label>1</label><figDesc>Input: Training data (?, ?); optimizer opt; mask probability ?; ? Calculate ? atom and ? coord 2: With probability ?, we independently mask the atoms in ? and obtain ? ? . The unmasked atom indices are ? ? . 3: With probability ?, we independently replace the coordinates in ? with some coordinates uniformly sampled from [-1, 1] to obtain ? ? . The indices of the original coordinates are ? ? . 3D?2D in Eqn.(5) using {v ? } ? ?? . ? Update the model parameter ? . 16: Denote the parameter of net as ? . Update the model by ? ? opt(?, ? ? (? atom + ? coord + ? 2D?3D + ? 3D?2D )).</figDesc><table><row><cell>3D information are available. In this dataset, each 2D molecular</cell></row><row><cell>graph corresponds to one 3D conformation calculate by density</cell></row><row><cell>function theory. We randomly split the dataset into training and</cell></row><row><cell>validation sets by 95%:5%.</cell></row></table><note><p>4: ( x * 1 , x * 2 , ? ? ? , x * |? | ) = net(? ? , ? ? ); 5: If atom ? ? is masked, i.e., ? ? ? \? ? , v? = FF atom ( x * ? ); 6: If coordinate ? ? is replaced, i.e., ? ? ? \? ? , R? ? = FF coord ( x * ? ); 7: Calculate the ? atom in Eqn.(1) and the ? coord in Eqn.(3). ? Calculate ? 2D?3D 8: Replace all coordinates in ? with a random matrix ? ?? ? R |? |?3 , where each element is uniformly sampled from [-1, 1]; 9: ( x * 1 , x * 2 , ? ? ? , x * |? | ) = net(?, ? ?? ). 10: ?? ? ? , R? = FF coord ( x * ? ). 11: Calculate the loss ? 2D?3D in Eqn.(4) using { R? } ? ?? . ? Calculate ? 3D?2D 12: Mask all atoms and bonds in ? and obtain ? ?? . The connection of atoms (i.e., which two atoms are connected) is kept. 13: ( x * 1 , x * 2 , ? ? ? , x * |? | ) = net(? ?? , ?). 14: ?? ? ? , v? = FF atom ( x * ? ). 15: Calculate the loss ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results on MoleculeNet, where only 2D information is available.</figDesc><table><row><cell>Dataset</cell><cell>BBBP</cell><cell>Tox21</cell><cell>ClinTox</cell><cell>HIV</cell><cell>BACE</cell><cell>SIDER</cell><cell>Avg</cell></row><row><cell cols="8">AttrMask &amp; ContextPred [15] 71.2 ? 0.9 74.2 ? 0.8 73.7 ? 4.0 75.8 ? 1.1 78.6 ? 1.4 60.4 ? 0.6 72.31</cell></row><row><cell>GROVER [29]</cell><cell cols="7">70.3 ? 1.6 75.2 ? 0.3 77.8 ? 2.0 75.9 ? 0.9 79.2 ? 0.3 60.6 ? 1.1 73.17</cell></row><row><cell>GraphCL [49]</cell><cell cols="7">67.5 ? 3.3 75.0 ? 0.3 78.9 ? 4.2 75.0 ? 0.4 68.7 ? 7.8 60.1 ? 1.3 70.87</cell></row><row><cell>St?rk et al. [36]</cell><cell cols="7">71.1 ? 2.0 78.9 ? 0.6 59.4 ? 3.2 76.1 ? 1.1 79.4 ? 2.0 57.3 ? 5.0 70.37</cell></row><row><cell>GraphMVP [22]</cell><cell cols="7">72.4 ? 1.6 75.9 ? 0.5 77.5 ? 4.2 77.0 ? 1.2 81.2 ? 0.9 63.9 ? 1.2 74.65</cell></row><row><cell></cell><cell cols="7">? 0.6 75.9 ? 0.3 95.4 ? 1.1 82.2 ? 1.0 86.8 ? 0.6 67.4 ? 0.5 80.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on ogb-molpcba dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Valid AP Test AP</cell></row><row><cell>GN block [1]</cell><cell>0.2745</cell><cell>0.2650</cell></row><row><cell>GIN + virtual node [21]</cell><cell>0.2798</cell><cell>0.2703</cell></row><row><cell>Graphormer [48]</cell><cell>0.3227</cell><cell>0.3140</cell></row><row><cell>Ours</cell><cell>0.3225</cell><cell>0.3125</cell></row><row><cell>Ours + FLAG</cell><cell>0.3304</cell><cell>0.3174</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on conformation generation.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>QM9</cell><cell></cell><cell></cell><cell cols="2">Drugs</cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">COV(%)? Mean Median Mean Median MAT (?)?</cell><cell cols="4">COV(%)? Mean Median Mean Median MAT (?)?</cell></row><row><cell>RDKit</cell><cell>83.26</cell><cell>90.78</cell><cell>0.3447</cell><cell>0.2935</cell><cell>60.91</cell><cell>65.70</cell><cell>1.2026</cell><cell>1.1252</cell></row><row><cell cols="2">GraphDG [35] 73.33</cell><cell>84.21</cell><cell>0.4245</cell><cell>0.3973</cell><cell>8.27</cell><cell>0.00</cell><cell>1.9722</cell><cell>1.9845</cell></row><row><cell>ConfGF [33]</cell><cell>88.49</cell><cell>94.13</cell><cell>0.2673</cell><cell>0.2685</cell><cell>62.15</cell><cell>70.93</cell><cell>1.1629</cell><cell>1.1596</cell></row><row><cell>DGSM [25]</cell><cell>91.49</cell><cell>95.92</cell><cell>0.2139</cell><cell>0.2137</cell><cell>78.73</cell><cell>94.39</cell><cell>1.0154</cell><cell>0.9980</cell></row><row><cell>GeoDiff [47]</cell><cell>91.68</cell><cell>95.82</cell><cell>0.2099</cell><cell>0.2021</cell><cell>89.13</cell><cell>97.88</cell><cell>0.8629</cell><cell>0.8529</cell></row><row><cell>DMCG [52]</cell><cell>96.34</cell><cell>99.53</cell><cell cols="2">0.2065 0.2003</cell><cell>96.69</cell><cell cols="3">100.00 0.7223 0.7236</cell></row><row><cell>Ours</cell><cell>96.93</cell><cell>100.00</cell><cell>0.1958</cell><cell>0.1849</cell><cell>97.05</cell><cell>100.00</cell><cell>0.7056</cell><cell>0.6973</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Torr, and Vladlen Koltun. 2021. Point transformer. In ICCV. 16259-16268. [51] Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong Wang, Yusong Wang, Wengang Zhou, Tao Qin, Houqiang Li, and Tie-Yan Liu. 2022. Direct molecular conformation generation. arXiv preprint arXiv:2202.01356 (2022). [52] Jinhua Zhu, Yingce Xia, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2021. Dual-view molecule pre-training. arXiv preprint arXiv:2106.10234 (2021).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Finetuning hyperparameters. atom + ? coord 76.0 ? 0.5 75.5 ? 0.4 91.3 ? 2.5 ? 2D?3D 75.8 ? 0.8 74.7 ? 0.6 89.4 ? 3.4 ? 3D?2D 75.5 ? 0.7 75.0 ? 0.4 93.4 ? 1.6 Ours 77.3 ? 0.4 75.9 ? 0.3 95.0 ? 1.1 atom + ? coord 81.2 ? 1.4 85.1 ? 0.5 65.8 ? 1.3 ? 2D?3D 81.0 ? 1.3 85.3 ? 1.3 65.2 ? 0.8 ? 3D?2D 80.9 ? 1.2 85.4 ? 1.5 65.1 ? 1.5 Ours 82.2 ? 1.0 86.8 ? 0.6 67.4 ? 0.5</figDesc><table><row><cell cols="4">C PRE-TRAINING WITH DIFFERENT LOSS</cell></row><row><cell>FUNCTION</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>BBBP</cell><cell>Tox21</cell><cell>ClinTox</cell></row><row><cell>? Dataset</cell><cell>HIV</cell><cell>BACE</cell><cell>SIDER</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of different training objective functions on MoleculeNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Details about the datasets for molecular property prediction. test chemicals in water that causes 50% Daphnia Magna to die after 48h. ? 2 /RMSE/MAE</figDesc><table><row><cell>Dataset</cell><cell>3D #Instance Description</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://ogb.stanford.edu/docs/leader_graphprop/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Contract <rs type="grantNumber">61836011</rs> and in part by the <rs type="funder">Youth Innovation Promotion Association CAS</rs> under Grant <rs type="grantNumber">2018497</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DDxm7Kb">
					<idno type="grant-number">61836011</idno>
				</org>
				<org type="funding" xml:id="_nvGAAd3">
					<idno type="grant-number">2018497</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale graph representation learning with very deep GNNs and self-supervision</title>
		<author>
			<persName><forename type="first">Ravichandra</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><surname>Lok Sibon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacklynn</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Stott</surname></persName>
		</author>
		<author>
			<persName><surname>Thakoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09422</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Geom: Energy-annotated molecular conformations for property prediction and molecular generation</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05531</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algebraic graph-assisted bidirectional transformers for molecular property prediction</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duc</forename><surname>Duy Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Chemberta: Large-scale self-supervised pretraining for molecular property prediction</title>
		<author>
			<persName><forename type="first">Seyone</forename><surname>Chithrananda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09885</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reoptimization of MDL Keys for Use in Drug Discovery</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Durant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burton</forename><forename type="middle">A</forename><surname>Leland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Nourse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1273" to="1280" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are 2D fingerprints still valuable for drug discovery?</title>
		<author>
			<persName><forename type="first">Kaifu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duc</forename><surname>Duy Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishnu</forename><surname>Sresht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Mathiowetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meihua</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Wei</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Chem. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="8373" to="8390" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Issue 16</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery</title>
		<author>
			<persName><forename type="first">Shion</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki R</forename><surname>Ueda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04738</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Round 2</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arxiv abs/2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning identifies synergistic drug combinations for treating COVID-19</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">T</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zina</forename><surname>Itkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><forename type="middle">V</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">One Model To Learn Them All. CoRR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quaternions in molecular modeling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><surname>Karney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Graphics and Modelling</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="595" to="604" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Directional Message Passing for Molecular Graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">FLAG: Adversarial Data Augmentation for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>CoRR abs/2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pre-training Molecular Graph Representation with 3D Geometry</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Molecular property prediction: A multilevel quantum interactions modeling perspective</title>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting Molecular Conformation via Dynamic Graph Score Matching</title>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Molecular Geometry Prediction using a Deep Generative Graph Neural Network</title>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20381</biblScope>
			<date type="published" when="2019-12-31">2019. 31 Dec 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">spyrmsd: symmetry-corrected RMSD calculations in Python</title>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Meli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Biggin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2020-08-31">2020. 31 Aug 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12559" to="12571" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gastegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SchNet: A Continuous-Filter Convolutional Neural Network for Modeling Quantum Interactions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Long Beach, California, USA; Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Artificial Intelligence in Bioinformatics</title>
		<author>
			<persName><forename type="first">Hari</forename><surname>Om</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-981-33-6191-1_21</idno>
		<ptr target="https://doi.org/10.1007/978-981-33-6191-1_21" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="395" to="403" />
			<pubPlace>Singapore, Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9558" to="9568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Heterogeneous molecular graph neural networks for predicting molecule properties</title>
		<author>
			<persName><forename type="first">Zeren</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="492" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Generative Model for Molecular Distance Geometry</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Miguel Hernandez-Lobato</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="8949" to="8958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04126</idno>
		<title level="m">Stephan G?nnemann, and Pietro Li?. 2021. 3D Infomax improves GNNs for Molecular Property Prediction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">2021. 3D Infomax improves GNNs for Molecular Property Prediction</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<idno>CoRR abs/2110.04126</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><forename type="middle">M</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Craig R Macnair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><forename type="middle">A</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName><surname>Bloom-Ackermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From Big Data to Artificial Intelligence: chemoinformatics meets new challenges</title>
		<author>
			<persName><forename type="first">Igor</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-020-00475-y</idno>
		<ptr target="https://doi.org/10.1186/s13321-020-00475-y" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2020-12-18">2020. 18 Dec 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Smiles-bert: large scale unsupervised pre-training for molecular property prediction</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics</title>
		<meeting>the 10th ACM international conference on bioinformatics, computational biology and health informatics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Barati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farimani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10056</idno>
		<title level="m">MolCLR: molecular contrastive learning of representations via graph neural networks</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</title>
		<author>
			<persName><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci00057a005</idno>
		<ptr target="https://doi.org/10.1021/ci00057a005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quantitative Toxicity Prediction Using Topology Based Multitask Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Kedi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Wei</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="520" to="531" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation</title>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Badly for Graph Representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
