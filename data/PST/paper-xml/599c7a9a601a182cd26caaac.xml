<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>junweihan2010@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">)</forename><forename type="middle">X</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Foundation of Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05D4E8AF9A308CFAA287DE7FDD02945F</idno>
					<idno type="DOI">10.1109/TIP.2017.2694222</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2694222, IEEE Transactions on Image Processing received June 22, 2016; revised November 29, 2016; accepted March 31, 2017. This work was supported in part by the National Science Foundation of China under Grants 61522207, in part by the Excellent Doctorate</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Co-saliency</term>
					<term>co-clustering</term>
					<term>multi-class salient object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the goal of discovering the common and salient objects from the given image group, co-saliency detection has received tremendous research interest in recent years. However, as most of the existing co-saliency detection methods are performed based on the assumption that all the images in the given image group should contain co-salient objects in only one category, they can hardly be applied in practice, particularly for the large-scale image set obtained from the internet. To address this problem, this paper revisits the co-saliency detection task and advances its development into a new phase, where the problem setting is generalized to allow the image group to contain objects in arbitrary number of categories and the algorithms need to simultaneously detect multi-class co-salient objects from such complex data. To solve this new challenge, we decompose it into two sub-problems, i.e., how to identify subgroups of relevant images and how to discover relevant co-salient objects from each subgroup, and propose a novel co-saliency detection framework to correspondingly address the two sub-problems via two-stage multi-view spectral rotation co-clustering. Comprehensive experiments on two publically available benchmarks demonstrate the effectiveness of the proposed approach. Notably, it can even outperform the state-of-the-art co-saliency detection methods which are performed based on the image subgroups carefully separated by the human labor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ith the goal of discovering the common and salient objects from the given image groups, co-saliency detection has received growing attention in recent years. Compared with the traditional saliency detection (Fig. <ref type="figure" target="#fig_1">1 (a)</ref>), co-saliency detection (Fig. <ref type="figure" target="#fig_1">1 (b)</ref>) needs to additionally consider the group-level information among multiple relevant images. Thus, it is more desirable yet challenging than the traditional saliency detection task. Compared with image co-segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> which considers not only common salient foreground regions but also similar non-salient background areas in images, co-saliency detection focuses on exploring the most important information, i.e., the common foreground regions, among the image group with a reduced computational demand by implying priorities based on human visual attention. Thus, co-saliency tends to be a promising preprocessing step for many high-level visual information understanding tasks such as video foreground extraction <ref type="bibr" target="#b2">[3]</ref>, image retrieval <ref type="bibr" target="#b3">[4]</ref>, and object detection <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, most of the existing co-saliency detection methods are performed based on a strong assumption that all the images in the given image group should contain co-salient objects in only one category such as pyramid, cheetah, building as shown in Fig. <ref type="figure" target="#fig_1">1(b)</ref>. They can hardly be applied in practice, particularly for the large-scale image set obtained from the internet, as such "clean" data can hardly be obtained unless expensive human labor is devoted to manually grouping those relevant images. For example, in the recently established real-world image recognition systems <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, one of the biggest challenges, i.e., how to leverage massive image data obtained from the social media portals is still under-addressed. This problem is mainly caused by the fact that such unconstrained image collection is so complex that it always contains diverse objects, e.g., "basket", "basketball", "player", "course", that are relevant to the searched entity, e.g., "basketball". Thus, if one can decompose such complex image collection into multiple more compact subgroups and subsequently learn the foreground model of the co-salient objects in each subgroup, it would become much easier to understand the image collection and learn object models for the desirable categories more precisely.</p><p>For the sake of the above analysis, this paper revisits the co-saliency detection task and propels its development into a new phase, where the problem setting is generalized to allow the image group to contain objects in arbitrary number of categories and the algorithms need to simultaneously detect multi-class co-salient objects from such complex data (see Fig. <ref type="figure" target="#fig_1">1 (c)</ref>). As can be seen, co-saliency detection under this weak assumption can better serve for the real-world multimedia and vision tasks whereas it encounters much more challenges. Based on our understanding, these challenges can be summarized into two folds: 1) how to identify subgroups of  A naïve method that performs clustering in two successive stages may solve these two key problems. At the first stage, image scenes with similar appearances are clustered into the same subgroup by using conventional clustering methods such as k-means or spectral clustering. Then, the same clustering method is further employed on superpixels to separate co-salient objects from backgrounds in each subgroup. However, at the first stage, the naïve method may incorrectly group images with similar background but different foreground objects into the same subgroup. As for the second stage, similar superpixels coming from foreground objects and backgrounds also can be detrimental to discover co-salient objects in images of the same subgroup.</p><p>Through a closer look at the benchmark datasets as shown in Fig. <ref type="figure" target="#fig_1">1</ref>, we find that exploring the co-occurring relationships of object proposals (OP) with image scenes and superpixels in the two successive stages can mitigate the limitations of the naïve method. By introducing object proposals, we hope that 1) in the same cluster of image scenes, the inner object proposals can be grouped together, and 2) superpixels are clustered on the basis of the object proposals in which they co-occur. Unlike one-side clustering used in the naïve method which just groups similar images or superpixels, we here resort to co-clustering which makes use of relationships of OP with images and superpixels to simultaneously group them into image clusters, OP clusters, and superpixel clusters. As far as we know, we make the earliest effort to introduce co-clustering to model useful co-occurring relationship in multi-class co-salient object detection task. We also demonstrate that co-clustering is an effective way for solving the aforementioned two challenging problems.</p><p>Although co-clustering has shown impressive performance improvement over traditional one-side clustering on document analysis <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, the traditional co-clustering often suffers from a severe result deviation from the true discrete solution. The main reason is that the traditional co-clustering method <ref type="bibr" target="#b8">[9]</ref> adopts k-means to obtain the final cluster indicator matrix (each column of this matrix gives the cluster to which each item was assigned) from the co-occurring matrix and the potential flaw of k-means objective function cannot guarantee accurate transform from continuous co-clustering solution to discrete cluster indicator matrix. To address this problem, we impose an additional orthonormal constraint to the objective function and thus obtain a spectral rotation invariant property, which can guarantee that the final cluster indicator matrix could best approximate the continuous co-clustering solution. Moreover, instead of using a single feature type to perform co-clustering in those two stages, it is of great interests to combine multiple complementary feature modalities in co-clustering to further improve the clustering result. In this paper, we refer to the modified co-clustering method as multi-view spectral rotation co-clustering method (MV-SRCC) and show that it can effectively solve the problems in each of the co-clustering stage in our framework.</p><p>The concrete framework of the proposed algorithm is shown in Fig. <ref type="figure" target="#fig_2">2</ref>. In the first stage, given a cluttered image set consisting of images from diverse categories, we first extract OPs from each image via objectness measure method <ref type="bibr" target="#b11">[12]</ref>. Then, co-occurring matrix is constructed based on the relationship between OPs and the image scenes. After performing MV-SRCC with this matrix, we can obtain the subgroups of the relevant images in this stage. In the second stage, we first extract superpixel regions from each image in the obtained same subgroup by using SLIC method <ref type="bibr" target="#b12">[13]</ref> and then construct another co-occurring matrix based on the relationship between superpixel regions and previous extracted OPs. Afterwards, MV-SRCC is performed on this co-occurring matrix to derive clusters of superpixels. To compute the co-saliency of each superpixel cluster, an efficient computation method is proposed by jointly considering the cluster's repetitiveness among multiple images and compactness in an image. The key motivation is based on the observation that superpixels belonging to co-salient foreground cluster are not only widely distributed over the majority of images in the subgroup but also reveals a more compact distribution within an individual image compared with superpixels of background cluster. Finally, the pixel-level co-saliency map is obtained by applying spatial refinement on the raw cluster-level saliency to promote gathering salient pixels together within an image.</p><p>To sum up, this paper mainly has four-fold contributions: 1) We revisit the co-saliency detection task and propel its development into a new phase, where the problem setting is generalized to allow the image group to contain objects in arbitrary number of categories and the algorithms need to simultaneously detect multi-class co-salient objects from the unconstrained image collection.</p><p>2) We naturally formulate the multi-class co-salient object detection as a problem of exploring the co-occurring relationship among different levels of image data such as image scene, object proposals and superpixel regions, and then propose an effective solution using the novel co-clustering algorithm as described in contribution 3).</p><p>3) A novel co-clustering algorithm named multi-view spectral rotation co-clustering (MV-SRCC) is proposed, which takes advantage of spectral rotation invariant property and multiple complementary feature modalities to guarantee good clustering results.</p><p>4) Comprehensive experiments on two widely used benchmark datasets are conducted to demonstrate the effectiveness of the proposed co-clustering algorithm as well as the entire framework for the revisited co-saliency detection task. Notably, the performance of our approach can be even better than the conventional methods which need human labor to separate the image subgroup manually.</p><p>The rest of this paper is organized as follows. The next section gives a brief review of the related works. Section III describes the proposed MV-SRCC algorithm and Section IV gives the details of our multi-class co-salient object detection framework. Comprehensive experimental results are provided in Section V and Section VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>This section provides a brief review of recent works on co-saliency detection and co-clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Co-saliency Detection</head><p>Originally, co-saliency detection approaches <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> focused on detecting common salient objects from image pairs. For example, the earliest co-saliency method <ref type="bibr" target="#b14">[15]</ref> was designed to detect common objects in a pair of images with similar backgrounds. However, the requirement of highly similar backgrounds limits its applicability as different backgrounds could be a more general case. In <ref type="bibr" target="#b13">[14]</ref>, a simple but efficient progressive algorithm which explored the joint information provided by the image pairs was proposed to detect co-salient objects. Tan et al. <ref type="bibr" target="#b16">[17]</ref> first computed affinity matrix for superpixels of image pairs and then performed propagation on the affinity matrix using SimRank algorithm to derive co-saliency maps. In <ref type="bibr" target="#b15">[16]</ref>, co-saliency maps were obtained by linearly combining single-image saliency and multi-image saliency based on a co-multilayer graph model.</p><p>The aforementioned methods can only deal with the image groups containing two relevant images. In order to extend the co-saliency detection to work on image groups consisting of more than two relevant images, a number of co-saliency detection methods are proposed recently <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Some existing methods concentrate on fusing intra-image saliency and inter-image saliency to generate the final co-saliency maps. The intra-image saliency is used to describe the saliency for individual image in the group and the inter-image saliency is defined as the correspondence among multiple images. In <ref type="bibr" target="#b20">[21]</ref>, a multi-scale segmentation voting scheme was exploited to compute intra-image saliency and a pairwise similarity ranking algorithm was proposed to measure inter-image saliency. Then the obtained intra-and inter-image saliency were linearly combined to produce the final co-saliency maps. To mitigate the limitation of method <ref type="bibr" target="#b20">[21]</ref> that exploits a fusion of intra-and inter-image saliency with fixed weights, several methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref> attempted to self-adaptively combine multiple saliency cues via rank constraint. The key idea is that the feature representation of the co-salient regions should be both similar and consistent and thus the feature matrix appears low rank. The main limitation of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref> is that the performance relies on the power of the elementary saliency maps. Additionally, the hand-designed metrics used in <ref type="bibr" target="#b22">[23]</ref> also limited its ability to handle more complex scenes. In <ref type="bibr" target="#b29">[30]</ref>, intra-image contrast was integrated with inter-image consistency in a principled Bayesian framework to compute co-saliency map. In <ref type="bibr" target="#b25">[26]</ref>, an unsupervised random forest was used to extract the rough contours of common objects, and then intra-and inter-image saliency map were fused in a way that takes advantage of multiplication. Instead of fusing intra-and inter-image saliency, methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> only computed intra-image saliency for each image. The work in <ref type="bibr" target="#b17">[18]</ref> considered the salient regions frequently occurring in most images as co-salient. The work in <ref type="bibr" target="#b23">[24]</ref> adjusted the intra-image saliency by exploiting the dissimilarities and global consistency of regions to obtain the co-saliency maps. The work in <ref type="bibr" target="#b26">[27]</ref> fully exploited the intra-image saliency under a two-stage query scheme to guide co-saliency detection. However, these methods may suffer from accuracy degrade if the intra-image saliency is invalid. For method <ref type="bibr" target="#b21">[22]</ref>, a candidate set of co-salient regions was first discovered and further employed as reconstruction bases, and then the co-saliency map was obtained by computing the reconstruction error. Along this line of consideration, the candidate co-salient regions were defined as exemplars in <ref type="bibr" target="#b28">[29]</ref> and their exemplar saliency was propagated to other matched regions to generate co-saliency maps. To capture more meaningful regions for effective saliency detection, the work in <ref type="bibr" target="#b24">[25]</ref> proposed a hierarchical segmentation based co-saliency detection model. Intra-image saliency was measured on the fine segmentation and object prior saliency map was measured on the coarse segmentation, and the two maps were finally integrated to generate the co-saliency map.</p><p>Another group of related works are cluster-based co-saliency methods proposed in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>, which obtain global correspondence among multiple images by performing clustering. In <ref type="bibr" target="#b19">[20]</ref>, pixels were first clustered into different clusters and three bottom-up saliency cues (i.e., contrast, spatial, and corresponding) were exploited to measure the cluster-level saliency. Then, the cluster-level saliency was fused with intra-image saliency to yield the co-saliency map. The method <ref type="bibr" target="#b19">[20]</ref> has been treated as a very simple and effective pre-processing step for many applications. However, as stated in <ref type="bibr" target="#b27">[28]</ref>, this kind of co-saliency detection method may suffer from the curse of dimensionality. To address this problem, in <ref type="bibr" target="#b27">[28]</ref>, a structured sparse PCA with feature selection scheme was proposed to improve the performance of cluster-based co-saliency detection. Although our work proposed in this paper is also a kind of cluster-based method, our work differs in the way of using more effective and novel clustering method and performing under more practical and challenging settings. Specially, we naturally cast the co-saliency task as a co-clustering problem by introducing co-occurring relationships among images, object proposals. and superpixels into clustering, which can achieve more reliable correspondence among multiple images. Moreover, we attempt to simultaneously detect multi-class co-salient objects from cluttered image sets instead of a given well-organized image group containing single-class co-saliency objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Co-clustering</head><p>Compared with traditional one-side clustering algorithms such as popular k-means or spectral clustering, which only consider the information of the to-be-clustered data to perform clustering. Co-clustering algorithm also leverages co-occurring relationship to realize data division. This co-occurring relationship is established based on two different types of data, where one is affiliated to the other. By leveraging the co-occurring relationship from these two types of data, better clustering results can be obtained than the traditional ways. Co-clustering was first proposed in <ref type="bibr" target="#b8">[9]</ref> to cluster a collection of unlabeled documents under the guidance of the co-occurring relationship between the documents and the words in them. Recently, co-clustering has shown impressive clustering results in various applications, such as image segmentation <ref type="bibr" target="#b30">[31]</ref>, video co-summarization <ref type="bibr" target="#b31">[32]</ref> and hyperspectral image clustering <ref type="bibr" target="#b32">[33]</ref>. The work in <ref type="bibr" target="#b30">[31]</ref> constructed a co-occurring matrix to model the relationship between pixels and superpixels and this matrix was further decomposed by transfer cut method to perform clustering. In <ref type="bibr" target="#b31">[32]</ref>, a video was summarized by exploiting co-clustering to find shots that co-occur most frequently across videos. In <ref type="bibr" target="#b32">[33]</ref>, an unsupervised co-clustering framework was proposed to incorporate both the pixel spectral and spatial information to improve the clustering performance on hyperspectral image data. In this paper, we attempt to exploit co-clustering to tackle the co-saliency detection problem and introduce the relationship between image scene and object proposals, and the relationship between object proposals and superpixels to better identify subgroup image scenes constraining the same class of co-salient objects and detect the co-salient objects from the identified subgroup images.</p><p>However, the above existing co-clustering methods usually formulate the co-clustering task as a bipartite graph partitioning problem and obtain the final clusters by using k-means, which has a potential flaw that the obtained cluster indicator matrix might severely deviate from the real solutions. To remedy this problem, this paper proposes an efficient spectral rotation co-clustering method by taking advantage of the rotation invariant property to establish a better cluster indicator matrix. Moreover, complementary information of multiple feature modalities is also exploited to further improve co-clustering performance. Finally, the proposed co-clustering algorithm can be used in our framework to detect multi-class co-salient object effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTI-VIEW SPECTRAL ROTATION CO-CLUSTERING ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation of MV-SRCC</head><p>As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, the proposed MV-SRCC algorithm will be used in two stages of our co-saliency detection framework. In the first stage, MV-SRCC is performed with two multi-view data sets of image scenes and their inside object proposals to identify subgroups of relevant images. In the second stage, two multi-view data sets of object proposals and their inside superpixels are fed to MV-SRCC to obtain superpixels clusters. Next, we will give the detailed formulation about the proposed MV-SRCC algorithm.</p><p>We firstly define a view as a feature representation extracted from one convolutional layer of convolutional neural networks (CNNs) and can obtain two multi-view data sets</p><formula xml:id="formula_0">(1) ( ) { ,..., } V = X X  and (1) ( ) { ,..., } V = Y Y  from V views in each stage. The th i sample of th v view from data  is denoted as 1 ( ) v n v i × ∈  x</formula><p>. Similarly, the th j sample of th v view from data  is denoted as</p><formula xml:id="formula_1">1 ( ) v n v j × ∈  y . All the N samples of th v view is denoted as ( ) ( ) ( ) ( ) 1 2 [ , ,..., ] v n N v v v v N × = ∈ X  x x</formula><p>x and all the M samples of</p><formula xml:id="formula_2">th v view is denoted as ( ) ( ) ( ) ( ) 1 2 [ , ,..., ] v n M v v v v M × = ∈ Y  y y y</formula><p>The goal of the proposed MV-SRCC algorithm is to simultaneously group  and  into K co-clusters by fully making use of inter data relationships as well as intra data relationships. The co-clustering result is represented by cluster indicator matrix</p><formula xml:id="formula_3">( ) {0,1} N M K + × ∈ G</formula><p>, where 1 ik = G if the th i sample is assigned to the th k cluster and 0 otherwise. Given view v , we model a pair of ( ) v X and ( ) v Y as a weighted bipartite graph</p><formula xml:id="formula_4">( ) ( ) ( ) ( ) ( , , ) v v v v = W    , where ( ) ( ) ( ) v v v = X Y   is the vertex, ( ) v  is the edge set, and ( ) v W</formula><p>is the adjacency matrix.</p><p>( ) v W is constructed under the guidance of enforcing similarity smoothness of individual data and co-occurring interaction between different data. The construction is as follows:</p><formula xml:id="formula_5">( ) ( ) ( ) v v T v   =     X Y W C W C W ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_6">( ) v N N × ∈ X W  and ( ) v M M × ∈ Y W  are similarity matrices of ( ) v X and ( ) v Y</formula><p>, respectively, and</p><formula xml:id="formula_7">N M × ∈ C  is co-occurring matrix to measure the relationship between ( ) v X and ( ) v Y . In these matrices, ( ) v X W is defined as: ( ) ( ) ( ) ( ) ( ) ( ) ( ) exp( ( , )), (<label>)</label></formula><formula xml:id="formula_8">( ) 0, v v v v i j i K j v v v j K i d if or otherwise ρ  - ∈  = ∈    X W x x x x x x   ,<label>(2)</label></formula><p>where ( , ) d ⋅ ⋅ is the Euclidean distance measure function, ρ is the bandwidth value which is set to the median of all distance values in our paper, and</p><formula xml:id="formula_9">( ) K x  represents the K nearest neighbors of x in the given distance measure function. Similarly, ( ) v Y W is defined analogously to ( ) v X W .</formula><p>In <ref type="bibr" target="#b30">[31]</ref>, the entry ij C of co-occurring matrix is assigned with a fixed value if ( )</p><formula xml:id="formula_10">v i</formula><p>x is included in ( ) v j y and 0 otherwise. In other words, the elements in ( ) v j y with the same weights are considered as equally important. In this paper, we attempt to assign the relationship between ( ) v i</p><p>x and ( ) v j y with different values by further analyzing the specific characteristics of ( ) v i x and ( ) v j y . Thus, the co-occurring matrix C is defined as:</p><formula xml:id="formula_11">( ) ( ) ( ) ( ) ( , ), 0, v v v v i j i j f if otherwise  ∈ =   C x y x y ,<label>(3) where ( ) ( ) ( , )</label></formula><formula xml:id="formula_12">v v i j f x y is a function to measure the importance of ( ) v i</formula><p>x for ( ) v j y . We will give more details about this function in next section.</p><p>To get the co-clustering result matrix ( ) v G in view v , we firstly construct the Laplacian matrix ( )</p><formula xml:id="formula_13">( ) ( ) v v v = - L D W , where ( ) ( ) ( ) v N M N M + × + ∈ D  is the degree matrix with ( ) ( ) (( ) ) v v T N M diag e + = D W</formula><p>. Then, similar to spectral clustering, the single-view co-clustering problem can be transformed to solve the following objective function:</p><formula xml:id="formula_14">( ) ( ) ( ) ( ) ( ) ( ) min (( ) ) v T v v T v v Tr = G G I G L G . (<label>4</label></formula><formula xml:id="formula_15">)</formula><p>To naturally integrate multiple features, we further propose a unified objective function to simultaneously minimize the co-clustering error of each view and the differences between the multi-view co-clustering result and each single-view co-clustering result. The objective function is defined as follows:</p><formula xml:id="formula_16">( ) ( ) ( )<label>( ) ( ) ( ) , 1 min (( ) ) (( ) ( )</label></formula><formula xml:id="formula_17">) T v V v T v v v T v v Tr Tr α = = + - - ∑ G G I G G L G G G G G ,<label>(5)</label></formula><p>where α is the penalty parameter. Thus, given the Laplacian matrix of each view, we try to learn the cluster indicator matrix for each view and cluster indicator matrix for the multiple views simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Solution of MV-SRCC</head><p>Eq. ( <ref type="formula" target="#formula_17">5</ref>) can be equivalent to minimizing the following optimization problem in Eq. <ref type="bibr" target="#b5">(6)</ref>. For clarity, the detailed derivation process is provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>min ( )</head><formula xml:id="formula_18">T T Tr = G G I G PG ,<label>(6)</label></formula><p>where multi-view Laplacian matrix P is set to be </p><p>where R is an arbitrary orthonormal matrix. Based on the rotation invariant property, HR is also the demanded continuous solution for any solution H . As can be seen, this function aims to find a G which can best approximate HR among all discrete cluster indicator matrices under the orthonormal constraint T = R R I . Compared with directly applying K-Means, the spectral rotation technique can lead to an improvement in clustering accuracy, which motivates us to use it to obtain the co-clustering indicator matrix G in this paper. We resort to the alternative optimization method proposed in <ref type="bibr" target="#b33">[34]</ref> to obtain G and R . The overall procedure of the proposed co-clustering algorithm is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MULTI-CLASS CO-SALIENT OBJECT DETECTION</head><p>In this section, we will give the detailed descriptions about applying the proposed MV-SRCC algorithm to perform co-salient object detection. Firstly, how to extract multi-view features for image scenes, object proposals and superpixels by using CNN model is introduced. Then, two stages of the proposed co-salient objects detection framework are described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-view Features</head><p>Recently, CNN has shown significant performance improvement for many computer vision tasks compared with the traditional hand-designed features. Moreover, combination of multiple layers of convolutional neural networks has proven beneficial in vision tasks such as pedestrian detection <ref type="bibr" target="#b34">[35]</ref>, fine-grained localization <ref type="bibr" target="#b35">[36]</ref> and semantic segmentation <ref type="bibr" target="#b36">[37]</ref>. The main reason is that multi-level representation can be obtained from different layers of CNN. For example, the bottom convolutional layers of CNN mainly illustrate the saliency from low-level image representation (e.g. edge information) whereas the middle layers can capture coarse-grained shape information and the top layers can show high-level semantic object information. Inspired by this, in this paper, we consider the multiple convolutional layers as multi-view features and try to exploit the complementary information across multiple levels to boost clustering performance.</p><p>The structure and model parameters of the network used in our paper are identical to the CNN-S model in <ref type="bibr" target="#b37">[38]</ref>, which contains five convolutional layers and three fully-connected layers. Here, we use conv1 (96 channels), conv2 (256 channels), conv3 (512 channels), conv4 (512 channels), conv5 (512 channels) to denote the convolutional layers, respectively. Of these layers, conv1, conv3 and conv5 are employed to extract multi-view features of the images and the corresponding object proposals and superpixels. Next, we present the details about how to extract multi-view features for image scenes, object proposals and superpixels, respectively. As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, when applying the CNN model to an image scene, each convolutional layer can result in a number of feature maps with one corresponding to each channel filter. The size (i.e. height and width) of the feature maps varies with the convolutional layers. We upsampled all the feature maps to the size of the image by using bilinear interpolation.</p><p>Given the resized feature maps produced by a certain convolutional layer, we define a compact robust representation to describe image scenes by encoding the global spatial relations between the underlying parts of image scenes. Specifically, a 2×2 spatial grid cell partitioning scheme is employed on the resized feature maps. Then, a channel-wise max pooling is performed on all pixels in each grid cell of feature maps. Afterwards, the max-pooled features in each grid cell are concatenated together to serve as the image-level features of the current convolutional layer. By repeating the above process on the resized feature maps of other convolutional layers, we can finally obtain multiple types of features for an image scene.</p><p>As for extracting multi-view features of object proposals and superpixels, the only difference is that we need to perform the above procedure on the masked feature maps of object proposals and superpixels. The masked feature maps are obtained as follows. Firstly, binary maps are produced according to the position of each object proposal and suprepixel in the image scenes. Then, these binary maps are used to mask the feature maps of image scenes to obtain a set of masked feature maps for object proposals and superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subgroup Identification</head><p>In this subsection, MV-SRCC is performed to group relevant images from a cluttered image set into the same subgroup by using the co-occurring relationship between image scenes and object proposals.</p><p>The first step is to extract a number of object proposals from each image by adopting objectness measure method <ref type="bibr" target="#b11">[12]</ref>, which measures the probability of any image window containing a generic object. The objectness measure is designed based on the general properties of an object as shown in an image, such as well-defined closed boundary, unique and salient appearance. We randomly sample 10000 windows over each image, and assign each window w with a probability score ( ) p w to indicate its objectness score. Thereafter, we select the top largest op N windows to serve as object proposals.</p><p>The second step is to extract multi-view features data of image scenes  and object proposals  as described in Section III.A. For images feature sets in all V views</p><formula xml:id="formula_20">(1) ( ) { ,..., } V = X X  , ( ) ( ) ( ) ( ) 1 2 [ , ,..., ] v n N v v v v N × = ∈ X  x x</formula><p>x denotes the feature matrix that contains the th v view feature data of all N images, where</p><formula xml:id="formula_21">1 ( ) v n v i × ∈  x</formula><p>is the feature vector of the th i image from th v view. For object proposals feature data sets</p><formula xml:id="formula_22">(1) ( ) { ,..., } V = Y Y  , the th v view feature matrix ( ) ( ) ( ) ( ) 1 2 [ , ,..., ] v n M v v v v M × = ∈ Y  y y y contains the feature data of all op M N N = × object proposals in the th v view, where 1 ( ) v n v j × ∈  y</formula><p>denotes the feature vector of the th j object proposal in the th v view. Given the two types of feature matrixes  and  in V views, the next step is to construct a bipartite graph for a pair of ( ) v X and ( ) v Y in each view as formulated in Eq. ( <ref type="formula" target="#formula_5">1</ref>) by jointly considering the co-occurrence relationships between images and their inside object proposals as well as the inter-image relationships and inter-OP relationships. In the construction, the inter-image and inter-OP relationships are represented in the form of similarity matrix ( ) v X W and ( ) v Y W as defined in Eq. ( <ref type="formula" target="#formula_8">2</ref>). For co-occurring matrix C , instead of assigning the relationship between the image and its inside OPs with a fixed value as in <ref type="bibr" target="#b30">[31]</ref> to model the co-occurring relationship, we design a function ( ) f ⋅ in Eq. ( <ref type="formula" target="#formula_11">3</ref>) to measure the importance of each object proposal for their image. Intuitively, the higher the objectness score is, the more important the object proposal is for its image. Thus, in this stage, we simply exploit the objectness score calculated by <ref type="bibr" target="#b11">[12]</ref> to serve as the output of the importance measure function.</p><p>Given the constructed bipartite graph in each view, we can obtain the cluster indicator matrix</p><formula xml:id="formula_23">( 1 ) {0,1} N M K + × ∈ G</formula><p>for both images and object proposals by following steps 2-5 as listed in Algorithm 1. For this stage, the cluster label for each image is listed in the top N rows of 1 G .Thus, the cluttered image set can be divided into subgroups of relevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Co-salient Object Detection 1) Group Superpixels</head><p>In this subsection, MV-SRCC is performed to jointly cluster superpixels and object proposals in each subgroup.</p><p>Firstly, for image q I in the subgroup</p><formula xml:id="formula_24">1 { } Q q q I = =  of Q</formula><p>images, we adopt SLIC (Simple Linear Iterative Clustering) method <ref type="bibr" target="#b12">[13]</ref> to obtain q N superpixels and use the same object proposals extracted in the first stage. Secondly, multi-view feature data  and  are extracted from superpixels and object proposals of each image in the subgroup  as described in Section III.A. Here  denotes the feature sets of superpixels and  still denotes the feature sets of object proposals. For superpixels feature sets in all views</p><formula xml:id="formula_25">(1) ( ) { ,..., } V = X X  , ( ) ( ) ( ) ( ) 1 2 [ , ,..., ] v n N v v v v N × = ∈ X  x x</formula><p>x contains the th v view feature data of all</p><formula xml:id="formula_26">q N N Q = × superpixels in subgroup  , where 1 ( ) v n v i × ∈  x</formula><p>is the th v view feature data of the th i superpixel. Refer to Section III.B for the descriptions of  and the only difference is the number of object proposals</p><formula xml:id="formula_27">op M Q N = × .</formula><p>Thirdly, the bipartite graph for a pair of ( ) v X and ( ) v Y in each view is constructed by using Eq. ( <ref type="formula" target="#formula_5">1</ref>). In the construction, we can obtain similarity matrices</p><formula xml:id="formula_28">( ) v X W and ( ) v Y</formula><p>W according to the definitions in Eq.( <ref type="formula" target="#formula_8">2</ref>). As for co-occurring matrix C measuring the importance of superpixels for their object proposals, the importance measure function ( ) f ⋅ is obtained as follows. The first step is to compute the objectness score ( ) p x for each pixel x by summing the scores of all the object proposals that contain pixel x . Then, the objectness score ( ) p s for each superpixel s is computed by averaging the scores of all the pixels that are included in superpixel s . Afterwards, the objectness scores of superpixels in the same object proposal are normalized to make the sum to be 1. Then, the normalized ( ) p s is used to measure the importance value of superpixel s for its object proposal. Finally, given the constructed multiple bipartite graphs, we can use Algorithm 1 to obtain the cluster indicator matrix</p><formula xml:id="formula_29">( 2 ) {0,1} N M K + × ∈ G</formula><p>for both superpixels and object proposals.</p><p>From the top N rows of 2 G , we can get K superpixel</p><formula xml:id="formula_30">clusters 1 { } K k k c = ,</formula><p>where k c is the cluster center vector of the th k cluster. Next, we describe how to measure the co-saliency for every superpixel cluster in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Cluster-level Co-saliency</head><p>In this subsection, we will detail the computation of co-saliency at cluster-level based on contrast cue. Contrast cue is widely used for saliency computation at pixel/region-level in a single image. The work in <ref type="bibr" target="#b19">[20]</ref> further extended the traditional contrast computation to cluster-level and obtained satisfactory results. To enhance the cluster-level contrast computation, we introduce a weighted term by jointly considering the cluster's repetitiveness among multiple images and compactness in an image. The motivation is based on the observation that superpixels belonging to co-salient foreground cluster are not only widely distributed over the majority images in the subgroup but also compactly distributed within an individual image compared with superpixels of background cluster.</p><p>For a given cluster k c , let , k q c be the set of superpixels that come from image q I . We utilize the entropy to measure the repetitiveness, which is defined as:</p><formula xml:id="formula_31">1 ( ) ( )log ( ) Q k q H c q q π π = = -∑<label>(8)</label></formula><p>where ( ) q π is the proportion of ,</p><formula xml:id="formula_32">k q c , i.e., , ( ) | | / | | k q k q c c π =<label>.</label></formula><p>( ) k H c is maximized when cluster k c is uniformly distributed and is 0 if its superpixels come from the same image. Larger ( ) k H c indicates that the cluster's superpixels appeared in more images in the group, which should be assigned with larger co-saliency value. In a single image, superpixels of foreground cluster tend to be compactly distributed with a lower spatial variance than background superpixels. Hence we define the compactness by using its spatial variance:</p><formula xml:id="formula_33">, 2 , 1 || || i k q Q k i k q q x c c ζ µ = ∈ ϕ( ) = - ∑ ∑ (<label>9</label></formula><formula xml:id="formula_34">)</formula><p>where i ζ is the position of superpixel i</p><p>x and , k q µ is the mean position of cluster k c in image q I .</p><p>By jointly considering the cluster's repetitiveness and compactness, the cluster's co-saliency can be computed from exp( ) ( ) , ( ) ( )</p><formula xml:id="formula_35">i k i k i k c c i H c c c c c S d ≠ ϕ( = ) ∑<label>(10)</label></formula><p>where ( ) ,  <ref type="formula" target="#formula_35">10</ref>), clusters containing salient objects appeared in majority of images are assigned with high weights. And the contrast of these clusters is enhanced. On the contrary, the contrast of background clusters is attenuated. Thus, the contrast difference between foreground and background clusters is effectively enlarged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Pixel-level Co-saliency</head><p>Inspired by <ref type="bibr" target="#b38">[39]</ref>, we obtain the final pixel-level co-saliency map from the cluster-level map based on Gestalt laws, which suggests that the pixels which are close to the foci of attention should be more salient than far away pixels. Accordingly, we first obtain the foci of attention area q Θ in image q I by thresholding the normalized cluster-level co-saliency map S with 0.3 ( ) max S × as suggested by <ref type="bibr" target="#b22">[23]</ref>. Then, the final co-saliency for each pixel p in image q I is defined as</p><formula xml:id="formula_36">* ( ) ( ) exp( min ( )) , q p j j S p S p d ω τ τ ∈Θ = ⋅ -⋅<label>(11)</label></formula><p>where p τ is the spatial position of pixel p , ( ) , p j d τ τ defines the spatial positional distance between pixel p and pixel j in q Θ , and the scale parameter ω is set to 6 according to [23].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In this section, we firstly introduce the experimental settings including the descriptions of two widely used benchmark datasets, the adopted evaluation metrics and the implementation details. Then, we compare the proposed framework with 8 state-of-the-art methods on two benchmark datasets. Finally, detailed evaluation of the proposed MV-SRCC in those two stages is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings 1) Datasets</head><p>In order to evaluate the performance of the proposed method, we conducted a set of qualitative and quantitative experiments on two benchmark datasets: the iCoseg dataset <ref type="bibr" target="#b39">[40]</ref> and the MSRC dataset <ref type="bibr" target="#b40">[41]</ref>. Specifically, the iCoseg dataset includes 38 image groups of totally 643 images, which is the largest public dataset for co-saliency detection until now. These images cover many semantic concepts such as air shows, pandas and hot balloons, etc. Manually labeled pixel-wise ground truth is available per image. The MSRC dataset includes 240 images belonging to 7 groups. Compared with iCoseg dataset, MSRC dataset appears to be more challenging as it contains co-salient objects with different colors or shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Evaluation Metrics</head><p>To evaluate the clustering performance of the first stage, three standard measures consisting of Accuracy (ACC), Normalized Mutual Information (NMI), and Purity were exploited to measure the clustering performance <ref type="bibr" target="#b41">[42]</ref>. ACC is defined as the number of correct clustering results divided by the number of all images. NMI assesses the level of agreement between two affinity matrices formed by the clustering labels and the ground truth. Purity is a simple evaluation measure. To compute this criterion, we first count the number of most frequent ground truth appearing in each cluster. Then this 1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. number is divided by the number of all image number to obtain the Purity.</p><p>To evaluate the performance of the generated final co-saliency maps, we adopted three widely used criteria: 1) the Precision-Recall (PR) curve which is drawn by using the precision rate versus the true positive rate (or the recall rate) at each threshold from 0 to 255 of co-saliency maps; 2) the Average Precision (AP) score which is obtained by calculating the area under the PR curve; 3) the F-measure which is defined by:</p><formula xml:id="formula_37">2 2 1 Precision Recall F Precision Recall β β β × = × + ( + )<label>(12)</label></formula><p>where we set 2 0.3 β = as suggested in <ref type="bibr" target="#b20">[21]</ref>. The Precision and Recall are calculated under the a self-adaptive threshold T µ ε = + that was also recommended in <ref type="bibr" target="#b42">[43]</ref>, where µ and ε are the mean value and the standard deviation of the co-saliency map, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Implementation Details</head><p>In the experiments, our method is implemented in MATLAB on a computer with an Intel i5-4590 CPU (3.3 GHz) and 16 GB RAM. For each image, the object proposals extraction was performed by objectness method <ref type="bibr" target="#b11">[12]</ref> with the object proposals number op N being 100 and the superpixel over segmentation was carried out by the SLIC method <ref type="bibr" target="#b12">[13]</ref> with the superpixel number q N being 200. To extract the multi-view features of the image and corresponding object proposals and superpixels, three convolutional layers (i.e., conv1, conv3, conv5) of CNN-S model were employed. To integrate multiple features, we chose 10</p><formula xml:id="formula_38">α =</formula><p>as the weight in Eq. ( <ref type="formula" target="#formula_17">5</ref>), which shows good performance in our empirical study.</p><p>Considering the problem investigated in our paper, the knowledge of which images belong to and how many images contained in a certain image group are unknown whereas the number of categories in image sets is known. For example, there are 38 categories in iCoseg dataset including stonehenge, bear, elephants, etc. And the MSRC dataset includes 7 categories such as car, book, face, etc. This setting is more suitable in practice particularly for the large-scale image set obtained from the internet by inputting a keyword in social media portals such as Google and Flickr. In this context, for the first stage the cluster number is set to be the number of categories in image sets. For the second stage, we follow <ref type="bibr" target="#b19">[20]</ref> to set the cluster number of superpixels as min{3 ,18} Q (where Q is the image number in each subgroup) because <ref type="bibr" target="#b19">[20]</ref> has demonstrated the effectiveness of addressing co-saliency detection in large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with Co-saliency Detection Methods</head><p>To evaluate the performance of our framework for co-saliency detection, we compared our method with 8 state-of-the-art algorithms, i.e., HDCT <ref type="bibr" target="#b43">[44]</ref>, RBD <ref type="bibr" target="#b44">[45]</ref>, CSHS <ref type="bibr" target="#b24">[25]</ref>, CBCS <ref type="bibr" target="#b19">[20]</ref>, CBCS-S <ref type="bibr" target="#b19">[20]</ref>, ESMG <ref type="bibr" target="#b26">[27]</ref>, ESMG-S <ref type="bibr" target="#b26">[27]</ref>, and SACS <ref type="bibr" target="#b22">[23]</ref>, where the first two algorithms were proposed by the traditional saliency detection approaches while the last six algorithms were proposed by state-of-the-art co-saliency detection approaches. Moreover, CBCS-S and ESMG-S are the single image saliency detection method in CBCS <ref type="bibr" target="#b19">[20]</ref> and ESMG <ref type="bibr" target="#b26">[27]</ref>. It needs to further point out that our method was performed on the whole image dataset for simultaneously detecting multi-class co-salient objects whereas other co-saliency detection methods were performed on each   <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> may be pre-classified by human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Quantitative Performance Comparison</head><p>For quantitative evaluation, we compared the proposed approach with above 8 state-of-the-art methods. Fig. <ref type="figure" target="#fig_8">4 (a)</ref> and<ref type="figure">(b)</ref> show the corresponding PR curve performance of all the competing approaches on the iCoseg dataset and MSRC dataset separately. As can been seen, the proposed approach (OURS) performs the best at recall rates among [0, 0.8] on the iCoseg dataset and performs much better than other algorithms on the MSRC dataset. In addition, we also present the corresponding AP and F-measure scores of all state-of-the-art methods on the two benchmark datasets in Table <ref type="table" target="#tab_2">1</ref> and<ref type="table" target="#tab_3">2</ref>. Clearly, our method can achieve the highest AP and F-measure values compared with other 8 state-of-the-art methods. On the iCoseg dataset, the second best co-saliency method (SACS) can obtain quite competitive performance. Our method only has a slight advantage compared with SACS, which achieves about 0.3% and 1.7% improvement in terms of AP and F-measure. Notice that the problem setting investigated in our paper is much complex than that investigated in other state-of-the-art methods.</p><p>Our problem setting is to detect multi-class co-salient objects from "cluttered" image sets consisting of arbitrary number of categories whereas other state-of-the-art methods only perform detection from one "clean" image group containing common objects in one category. Even in this case, our method still achieves better performance than all other state-of-the-art methods, which clearly demonstrates the effectiveness of the proposed method. In addition, as shown in Table <ref type="table" target="#tab_3">2</ref>, our method provides a significant improvement for the more challenging MSRC dataset. On average, our method performs better than SACS by 5.4% and 7.3% in terms of AP and F-measure, as well as 2.9% and 6.4% compared with the second best method (HDCT) on the MSRC dataset. It is notable that our proposed multi-class co-salient object detection method even achieves much better performance than the best single-class co-salient object detection method on MRSC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Qualitative Performance Comparison</head><p>For an intuitive illustration, Fig. <ref type="figure" target="#fig_9">5</ref> shows the co-saliency detection results of some examples in four image groups, i.e., Hot air balloon group and Cheetah group from iCoseg dataset, Car group and Face group from MSRC dataset. As can be seen, our method can obtain more accurate results than other co-saliency methods. For example, as shown in the Face group, our method still works well in the cases of complex backgrounds and can robustly suppress the background regions. The examples in the Cheetah group indicate that our method can accurately detect the co-salient objects even if they are in different viewpoints, scales and shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of MV-SRCC</head><p>In this subsection, we present the detailed evaluation of the proposed MV-SRCC algorithm for the two stages on iCoseg dataset. Two traditional clustering methods (i.e., k-means <ref type="bibr" target="#b45">[46]</ref> and spectral clustering <ref type="bibr" target="#b46">[47]</ref> ) and SRCC algorithm proposed in this paper were employed as baseline clustering methods for performance comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Evaluation of MV-SRCC in Stage 1</head><p>To evaluate the effectiveness of the proposed MV-SRCC in the first stage for identifying subgroups, all these three clustering methods were used for clustering images based on the image-level features extracted from three convolutional layers (i.e., conv1, conv3, conv5) . Table <ref type="table" target="#tab_4">3</ref> gives the corresponding ACC, NMI and Purity for each clustering method on different features. As can be seen, the proposed MV-SRCC obtains the best clustering performance in all metric terms. By combining multiple features, MV-SRCC can improve about 4.6%, 1.8% and 3.1% in terms of ACC, NMI and Purity compared with SRCC. Furthermore, SRCC achieves much better clustering performance than k-means and spectral clustering, which indicates the superiority of incorporating the information from object proposals for clustering images in the first stage. Besides, clustering based on image-level features extracted from conv5 performs better than conv1 and conv3 for all baseline methods. This indicates that higher level convolution layer can capture more semantic object information which forms more robust image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Evaluation of MV-SRCC in Stage 2</head><p>We firstly evaluated the final co-saliency performance differences of the proposed MV-SRCC algorithm performed on well-organized subgroups and automatic-divided subgroups produced by the first stage. In Table <ref type="table" target="#tab_5">4</ref>, we denote the MV-SRCC running on well-organized subgroups as MV-SRCC0 for clarity. As can be seen, performing co-salient object detection on our automatic-divided subgroups achieves competitive results with only a decrease of 1.1% and 0.1% in terms of AP and F-measure, which clearly demonstrates the effectiveness of our proposed co-saliency detection framework.</p><p>We also investigated the co-saliency performance of k-means, spectral clustering and SRCC on different features. Note that for fair comparison, all the baseline clustering methods are performed in each well-organized subgroup to cluster superpixels. Fig. <ref type="figure" target="#fig_10">6</ref> and Table <ref type="table" target="#tab_5">4</ref> present the co-saliency results in terms of PR curve, AP and F-measure. Clearly, MV-SRCC0 achieves the best performance and provides a significant performance improvement over SRCC, which shows that multiple features are beneficial to improve   performance. Generally, better performance can be achieved by using features from higher level convolution layer for all clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Other Multi-view Features</head><p>In this subsection, we investigated how much performance improvement can be gained by integrating multiple features from multiple convolutional layers of CNN model compared with other multi-view features on iCoseg dataset.</p><p>For this comparison experiment, three types of features such as color histogram, LBP and SIFT are employed as the multi-view features. Specifically, we firstly use a fixed-size patch (16×16 pixels) with the spacing step to be 8 pixels to extract all the above features in each patch of the image. For color histogram, each channel of the RGB color space is quantized into 64 bins to form a total histogram feature length of 192 by simply concatenation of the three channels. For LBP feature, we use the common used 8 neighbors to get the binary values and convert the 8-bit binary values into a decimal value for each pixel in the patch, which result in a 256 dimensional LBP feature. For SIFT, the histograms of gradients is computed over a 4×4 grid and the gradients are then quantized into eight bins, which result in a 128 dimensional SIFT feature. Then, the 2×2 spatial grid cell partitioning scheme and max-pooling method are adopted as described in Section IV.A to obtain the final features of image, object proposals and superpixels.</p><p>Table <ref type="table" target="#tab_6">5</ref> lists the ACC, NMI and Purity for the performance of the first stage. Fig. <ref type="figure">7</ref> presents the precision-recall curve of MV-SRCC with the two different multi-view features and Table <ref type="table" target="#tab_7">6</ref> gives the corresponding AP and F-measure. For the first stage, multi-view features from CNN achieves significantly better clustering performance in all metric terms as shown in Table <ref type="table" target="#tab_6">5</ref>. From Table <ref type="table" target="#tab_7">6</ref> and Fig. <ref type="figure">7</ref> for the performance of the second stage, we can observe that combination of features extracted from conv1,3,5 layers can improve 8.1% and 9.8% in term of AP and F-measure, and achieves much higher precision for all recall than multi-features of color histogram, LBP and SIFT. The results for both the two stages clearly demonstrate the superior of multi-view features extracted from CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>This paper has proposed a novel multi-class co-salient detection framework which is among the earliest efforts to simultaneously detect multi-class co-salient objects from complex and practical image sets. Firstly, we formulated multi-class co-salient object detection as a co-clustering problem. Then, we proposed a novel MV-SRCC algorithm and developed the multi-class co-salient detection framework based on two stages of MV-SRCC. Even without the strong assumption to restrict the given image group to contain only one class of co-salient objects, the proposed approach was demonstrated to have even better performance than the traditional state-of-the-art co-salient object detection methods.</p><p>Since our proposed method attempts to perform co-salient object detection directly from complex image sets based on two-stage co-clustering algorithm, the final co-saliency performance heavily depends on the effectiveness of co-clustering in two stages. Although the information of image scenes and the inside object proposals are considered to identify subgroups of relevant images, the proposed MV-SRCC algorithm could not well separate similar image scenes such as the scenes of playing soccer with man and woman players both wearing red jersey.</p><p>In the future, we plan to use more powerful CNN model to extract robust features for improving the clustering performance. In addition, we intend to design an effective clustering-based contrast measure method that can handle the noise in the subgroup. Also, it is desirable to exploit the extra information such as ranking information obtained from social media portals such as Google and Flickr to improve the co-salient object detection performance. Moreover, it is interesting to leverage the proposed MV-SRCC algorithm to facilitate other tasks, e.g., object detection and annotation from high-resolution remote sensing images <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> and video saliency detection <ref type="bibr" target="#b49">[50]</ref>.   Thus, Eq. ( <ref type="formula" target="#formula_17">5</ref>) is equivalent to minimizing the following objective function  </p><formula xml:id="formula_39">) v T v T v v v v T v v v v T v v v T v v v T v v</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2694222, IEEE Transactions on Image Processing TIP-15725-2016 2 relevant images, which contain co-salient objects of the same class, from the cluttered image set; and 2) how to discover co-salient objects from the image scenes in each subgroup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the difference between the revisited co-saliency detection task and the conventional saliency detection and co-saliency detection tasks.</figDesc><graphic coords="2,77.65,50.05,465.15,210.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of the proposed co-saliency detection approach, where "OPs" denotes the object proposals.</figDesc><graphic coords="3,53.20,57.85,238.75,445.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>corresponding to the top smallest K eigenvalues of P . Since H is in relaxed continuous form, it is a common practice to apply K-Means to H to obtain the final discrete solution G . However, the potential flaw of such common practice is that the obtained cluster indicator matrix might severely deviate from the real solutions, which would lead to the sub-optimal clustering results of images or superpixels in the two stages. Inspired by<ref type="bibr" target="#b33">[34]</ref>, we take advantage of the spectral solution invariance property to obtain the final discrete indicator matrix G from continuous solution H by minimizing the following objective function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of multi-view feature extraction for an image scene.</figDesc><graphic coords="6,313.20,402.90,251.75,176.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2694222, IEEE Transactions on Image Processing TIP-15725-2016 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>defines the Euclidean distance between cluster i c and k c . The co-saliency ( ) k S c is normalized to [0,1] . According to Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2694222, IEEE Transactions on Image Processing TIP-15725-2016 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Quantitative experimental results (in terms of the precision-recall curve) of the proposed approach and other state-of-the-art methods on the iCoseg (a) and MSRC (b) datasets.</figDesc><graphic coords="9,110.05,52.20,394.55,164.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Some examples of co-saliency detection on iCoseg and MSRC datasets. Hot air balloon and Cheetah in the first row belong to the iCoseg dataset. Car and Face in the second row belong to the MSRC dataset.</figDesc><graphic coords="10,58.50,67.55,493.45,384.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Precision-recall curve of applying the baseline clustering methods used different features: (a) Conv1, (b) Conv3 and (c) Conv5.</figDesc><graphic coords="11,58.00,163.30,492.30,140.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Revisiting Co-Saliency Detection: A Novel Approach based on Two-stage Multi-view Spectral Rotation Co-clustering Xiwen Yao, Junwei Han, Senior Member, IEEE, Dingwen Zhang, and Feiping Nie</figDesc><table /><note><p>W 1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2694222, IEEE Transactions on Image Processing</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TIP-15725-2016</cell><cell>7</cell></row><row><cell cols="11">Algorithm 1 Multi-View Spectral Rotation</cell></row><row><cell cols="11">Co-clustering Algorithm</cell></row><row><cell cols="11">Input: two data sets</cell><cell></cell><cell>(1) { ,..., = X</cell><cell>X</cell><cell>( ) V</cell><cell>}</cell><cell>and</cell></row><row><cell cols="2"></cell><cell cols="7">(1) { ,..., = Y</cell><cell cols="2">Y</cell><cell>( ) V</cell><cell>}</cell><cell>from V views</cell></row><row><cell cols="11">Output: Cluster indicator matrix G</cell></row><row><cell cols="11">1: Construct the adjacency matrix ( ) v W for each view</cell></row><row><cell>v</cell><cell cols="2">∈</cell><cell cols="2">1,...,</cell><cell cols="2">V</cell><cell cols="4">by using Eq.(1)</cell></row><row><cell cols="11">2: Calculate the Laplacian matrix ( ) v L for each view</cell></row><row><cell>v</cell><cell cols="2">∈</cell><cell cols="2">1,...,</cell><cell cols="2">V</cell><cell cols="4">by ( ) v = L</cell><cell>D</cell><cell>( ) v</cell><cell>-</cell><cell>W</cell><cell>( ) v</cell></row><row><cell cols="11">3: Obtain multi-view Laplacian matrix P by</cell></row><row><cell cols="2">P</cell><cell cols="2">= ∑</cell><cell cols="2">v</cell><cell>I</cell><cell>-</cell><cell>α</cell><cell>(</cell><cell>( ) v L</cell><cell>+</cell><cell>1 α -) I</cell></row><row><cell cols="11">4: Calculate the K smallest eigenvectors of P and</cell></row><row><cell cols="11">obtain the continuous solution H</cell></row><row><cell cols="11">5: Obtain G by minimizing Eq.(7) via the alternative</cell></row><row><cell cols="11">optimization method proposed in [34]</cell></row><row><cell cols="7">return G</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : AP and F-measure scores on iCoseg dataset</head><label>1</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>HDCT[43]</cell><cell>RBD[44]</cell><cell>CSHS[25]</cell><cell>CBCS[20]</cell><cell>CSBS-S[20]</cell><cell>ESMG-S[27]</cell><cell>ESMG[27]</cell><cell>SACS[23]</cell><cell>OURS</cell></row><row><cell>AP</cell><cell>0.804</cell><cell>0.798</cell><cell>0.839</cell><cell>0.805</cell><cell>0.769</cell><cell>0.767</cell><cell>0.849</cell><cell>0.865</cell><cell>0.868</cell></row><row><cell>F-measure</cell><cell>0.770</cell><cell>0.780</cell><cell>0.754</cell><cell>0.740</cell><cell>0.715</cell><cell>0.752</cell><cell>0.801</cell><cell>0.793</cell><cell>0.810</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : AP and F-measure scores on MSRC dataset</head><label>2</label><figDesc>subgroup of relevant images to detect each single class one by one. Those subgroups of images in methods</figDesc><table><row><cell>Methods</cell><cell>HDCT[43]</cell><cell>RBD[44]</cell><cell>CSHS[25]</cell><cell>CBCS[20]</cell><cell>CSBS-S[20]</cell><cell>ESMG-S[27]</cell><cell>ESMG[27]</cell><cell>SACS[23]</cell><cell>OURS</cell></row><row><cell>AP</cell><cell>0.824</cell><cell>0.787</cell><cell>0.783</cell><cell>0.713</cell><cell>0.699</cell><cell>0.766</cell><cell>0.679</cell><cell>0.799</cell><cell>0.853</cell></row><row><cell>F-measure</cell><cell>0.720</cell><cell>0.712</cell><cell>0.711</cell><cell>0.588</cell><cell>0.620</cell><cell>0.706</cell><cell>0.616</cell><cell>0.711</cell><cell>0.784</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Evaluation of MV-SRCC in stage 1 on iCoseg dataset Methods K-Means Spectral Clustering SRCC MV-SRCC</head><label>3</label><figDesc></figDesc><table><row><cell>Features</cell><cell>Conv1</cell><cell>Conv3</cell><cell>Conv5</cell><cell>Conv1</cell><cell>Conv3</cell><cell>Conv5</cell><cell>Conv1</cell><cell>Conv3</cell><cell>Conv5</cell><cell>Conv1,3,5</cell></row><row><cell>ACC</cell><cell>0.622</cell><cell>0.717</cell><cell>0.753</cell><cell>0.745</cell><cell>0.795</cell><cell>0.824</cell><cell>0.785</cell><cell>0.879</cell><cell>0.903</cell><cell>0.949</cell></row><row><cell>NMI</cell><cell>0.802</cell><cell>0.818</cell><cell>0.846</cell><cell>0.838</cell><cell>0.871</cell><cell>0.892</cell><cell>0.886</cell><cell>0.924</cell><cell>0.963</cell><cell>0.981</cell></row><row><cell>Purity</cell><cell>0.701</cell><cell>0.779</cell><cell>0.806</cell><cell>0.765</cell><cell>0.841</cell><cell>0.866</cell><cell>0.857</cell><cell>0.919</cell><cell>0.941</cell><cell>0.972</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : Evaluation of MV-SRCC in stage 2 on iCoseg dataset</head><label>4</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2694222, IEEE Transactions on Image Processing</figDesc><table><row><cell>Methods</cell><cell></cell><cell>K-Means</cell><cell></cell><cell cols="3">Spectral Clustering</cell><cell></cell><cell>SRCC</cell><cell></cell><cell>MV-SRCC0</cell><cell>MV-SRCC</cell></row><row><cell>Features</cell><cell>Conv1</cell><cell>Conv3</cell><cell>Conv5</cell><cell>Conv1</cell><cell>Conv3</cell><cell>Conv5</cell><cell>Conv1</cell><cell>Conv3</cell><cell>Conv5</cell><cell>Conv1,3,5</cell><cell>Conv1,3,5</cell></row><row><cell>AP</cell><cell>0.698</cell><cell>0.722</cell><cell>0.773</cell><cell>0.737</cell><cell>0.763</cell><cell>0.802</cell><cell>0.753</cell><cell>0.790</cell><cell>0.832</cell><cell>0.879</cell><cell>0.868</cell></row><row><cell>F-measure</cell><cell>0.537</cell><cell>0.625</cell><cell>0.743</cell><cell>0.593</cell><cell>0.665</cell><cell>0.699</cell><cell>0.651</cell><cell>0.711</cell><cell>0.757</cell><cell>0.810</cell><cell>0.809</cell></row></table><note><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Performance of different multi-view features in Stage 1 on iCoseg dataset</head><label>5</label><figDesc></figDesc><table><row><cell>Features</cell><cell>ACC</cell><cell>NMI</cell><cell>Purity</cell></row><row><cell>Color, LBP, SIFT</cell><cell>0.809</cell><cell>0.882</cell><cell>0.869</cell></row><row><cell>Conv1,3,5</cell><cell>0.949</cell><cell>0.981</cell><cell>0.972</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 : Performance of different multi-view features in Stage 2 on iCoseg dataset</head><label>6</label><figDesc></figDesc><table><row><cell>Features</cell><cell>AP</cell><cell>F-measure</cell></row><row><cell>Color, LBP, SIFT</cell><cell>0.787</cell><cell>0.702</cell></row><row><cell>Conv1,3,5</cell><cell>0.868</cell><cell>0.810</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Figure 7: Precision-recall curve of different multi-view features.</head><label></label><figDesc></figDesc><table><row><cell>)</cell><cell>((</cell><cell>)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In order to solve this objective function in Eq. ( <ref type="formula">5</ref>), firstly, we set the derivative of Eq. ( <ref type="formula">5</ref>) with respect to ( )   v G to zero. Then, we get</p><p>Further, we have</p><p>( ) </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video object discovery and co-segmentation with extremely weak supervision</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient algorithm for co-segmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="269" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object-based Multiple Foreground Video Co-segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object retrieval using visual query context</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1295" to="1307" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust object co-detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3206" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Everything about Anything: Webly-Supervised Visual Concept Learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3270" to="3277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NEIL: Extracting Visual Knowledge from Web Data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prajna: Towards Recognizing Whatever You Want from Images without Image Labeling</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-clustering documents and words using bipartite spectral graph partitioning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Conf. Knowl. Disc. Data Min</title>
		<meeting>ACM SIGKDD Conf. Knowl. Disc. Data Min</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Locally discriminative coclustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Engineer</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1025" to="1035" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint Local and Global Consistency on Interdocument and Interword Relationships for Co-Clustering</title>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring the Objectness of Image Windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Preattentive co-saliency detection</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Image Process</title>
		<meeting>IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cosaliency: Where people look when comparing images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp. User Inter. Software Tech</title>
		<meeting>ACM Symp. User Inter. Software Tech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image co-saliency detection by propagating superpixel affinities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Acous. Speech Signal Process</title>
		<meeting>IEEE Conf. Acous. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2114" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From co-saliency to co-segmentation: An efficient and fully unsupervised energy minimization model</title>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2129" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Saliency map fusion based on rank-one constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Multimedia Expo</title>
		<meeting>IEEE Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Co-salient object detection from multiple images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1896" to="1909" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-Saliency Detection via Base Reconstruction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Multimedia</title>
		<meeting>ACM Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-adaptively Weighted Co-saliency Detection via Rank Constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on region-level fusion and pixel-level refinement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Multimedia Expo</title>
		<meeting>IEEE Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on hierarchical segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting Co-Salient Objects in Large Image Sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="148" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient Saliency-Model-Guided Visual Co-Saliency Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="588" to="592" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved structured sparse PCA for cluster-based co-saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ningmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Inter. Multimedia Comput. Service</title>
		<meeting>ACM Conf. Inter. Multimedia Comput. Service</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-Saliency Detection via Co-Salient Object Discovery and Recovery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2073" to="2077" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Co-saliency Detection via Looking Deep and Wide</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2994" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmentation using superpixels: A bipartite graph partitioning approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="789" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video Co-summarization: Video Summarization by Visual Co-occurrence</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3584" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral-spatial co-clustering of hyperspectral image data based on bipartite graph</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral Rotation versus K-Means in Spectral Clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="431" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pedestrian Detection with Unsupervised Multi-stage Feature Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context-Aware Saliency Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">icoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Introduction to information retrieval</title>
		<imprint>
			<publisher>Cambridge university press Cambridge</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1761" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Salient Region Detection via High-Dimensional Color Transform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="883" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency Optimization from Robust Background Detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An efficient k-means clustering algorithm: Analysis and implementation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="881" to="892" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic annotation of high-resolution satellite images via weakly supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3660" to="3671" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revealing event saliency in unconstrained video collection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1746" to="1758" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
