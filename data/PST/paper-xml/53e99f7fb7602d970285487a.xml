<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
							<email>qsliu@seu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Wang</surname></persName>
							<email>jwang@mae.cuhk.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical and Automation Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Control Science and Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">526F7410C5F8CD4158229D7FA0EA7921</idno>
					<idno type="DOI">10.1109/TNNLS.2013.2244908</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A One-Layer Projection Neural Network for Nonsmooth Optimization Subject to Linear Equalities and Bound Constraints Qingshan Liu, Member, IEEE, and Jun Wang, Fellow, IEEE Abstract-This paper presents a one-layer projection neural network for solving nonsmooth optimization problems with generalized convex objective functions and subject to linear equalities and bound constraints. The proposed neural network is designed based on two projection operators: linear equality constraints, and bound constraints. The objective function in the optimization problem can be any nonsmooth function which is not restricted to be convex but is required to be convex (pseudoconvex) on a set defined by the constraints. Compared with existing recurrent neural networks for nonsmooth optimization, the proposed model does not have any design parameter, which is more convenient for design and implementation. It is proved that the output variables of the proposed neural network are globally convergent to the optimal solutions provided that the objective function is at least pseudoconvex. Simulation results of numerical examples are discussed to demonstrate the effectiveness and characteristics of the proposed neural network.</p><p>Index Terms-Differential inclusion, global convergence, Lyapunov function, nonsmooth optimization, projection neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ONSIDER THE following general nonsmooth optimiza- tion problem:</p><formula xml:id="formula_0">minimize f (x), subject to Ax = b, x ∈<label>(1)</label></formula><p>where x = (x 1 , x 2 , . . . , x n ) T ∈ R n , f : R n → R is an objective function which is not necessarily convex or smooth;</p><p>A ∈ R m×n is a full row-rank matrix (i.e., rank</p><formula xml:id="formula_1">(A) = m ≤ n); b = (b 1 , b 2 , . . . , b m ) ∈ R m ;</formula><p>and is a nonempty and closed convex set in R n . Constrained optimization arises in a broad variety of scientific and engineering applications where the real-time solutions of the optimization problems are often required. One of the possible and very promising approaches for real-time optimization is to apply recurrent neural networks based on circuit implementation. As parallel computational models for solving constrained optimization problems, recurrent neural networks have received a great deal of attention and brought a wide range of applications over the past few decades (e.g., <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, and references therein). In 1986, Tank and Hopfield <ref type="bibr" target="#b36">[37]</ref> proposed a recurrent neural network for solving linear programming problems which inspired many researchers to develop other neural networks for optimization. In 1988, the dynamical canonical nonlinear programming circuit (NPC) was introduced by Kennedy and Chua <ref type="bibr" target="#b20">[21]</ref> for nonlinear programming by utilizing a finite penalty parameter, which can generate the approximate optimal solutions. From then on, the research on NPCs has been well developed and many neural network models have been designed for optimization problems problems <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Among them, the Lagrangian network (based on the Lagrangian method) was proposed by Zhang and Constantinides <ref type="bibr" target="#b48">[49]</ref> for solving convex nonlinear programming problems. The deterministic annealing neural network by Wang <ref type="bibr" target="#b39">[40]</ref> was developed for linear and convex programming. Moreover, the projection method was introduced to design recurrent neural networks for solving smooth convex (pseudoconvex) optimization problems, such as the projection networks proposed by Xia et al. <ref type="bibr" target="#b41">[42]</ref> and Hu and Wang <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In order to reduce the model complexity, the dual and simplified dual neural networks were introduced for solving convex programming problems with dual variables only <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>, we proposed several one-layer recurrent neural networks with lower model complexity for solving linear and quadratic programming problems.</p><p>In recent years, recurrent neural networks based on penalty method were widely investigated for solving nonsmooth optimization problems. In <ref type="bibr" target="#b13">[14]</ref>, the generalized NPC (G-NPC) proposed by Forti et al. <ref type="bibr" target="#b13">[14]</ref> can be considered as a natural extension of NPC for solving nonsmooth optimization problems with inequality constraints. Xue and Bian <ref type="bibr" target="#b47">[48]</ref> proposed a recurrent neural network modeled by a differential inclusion for nonsmooth convex optimization based on the subgradient and penalty parameter method. In <ref type="bibr" target="#b27">[28]</ref>, we proposed a onelayer recurrent neural network to solve problem (1) with only equality constraints. More recently, some one-layer recurrent neural networks were proposed for solving some locally convex and pseudoconvex nonsmooth optimization problems <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In this paper, we are concerned with the nonsmooth optimization problem <ref type="bibr" target="#b0">(1)</ref>, and a one-layer projection neural network model with easy implementation structure is developed. Compared with the existing neural networks for nonsmooth optimization, the proposed neural network here has several merits. The objective function of problem <ref type="bibr" target="#b0">(1)</ref> does not need to be convex everywhere, and only needs to be convex (pseudoconvex) on a set defined by the constraints; the neural network does not have any design parameter; and some conditions in <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b47">[48]</ref> are removed, such as the existence of a feasible point in the interior of .</p><p>The remainder of this paper is organized as follows. Section II discusses some preliminaries. In Section III, the proposed projection neural network model is described. The optimality and global convergence of the proposed neural network are analyzed in Section IV. Next, in Section V, four illustrative examples are given to show the effectiveness and performance of the proposed neural network. Finally, Section VI concludes this paper and presents some future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>In this section, we present some definitions and properties concerning set-valued map, nonsmooth analysis, and convex analysis, which are needed for the theoretical analysis in the paper. We refer the reader to <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and <ref type="bibr" target="#b14">[15]</ref> for more thorough discussions.</p><p>Definition 1:</p><formula xml:id="formula_2">Suppose E ⊂ R n . F : x → F(x) is called a set-valued function from E → R n if, to each point x of a set E, there corresponds to a nonempty closed set F(x) ⊂ R n . Definition 2: A function ϕ : R n → R is said to be Lipschitz near x ∈ R n if there exist ε, δ &gt; 0, such that for any x , x ∈ R n satisfies x -x &lt; δ and x -x &lt; δ, we have |ϕ(x ) - ϕ(x )| ≤ ε x -x . If ϕ is Lipschitz near any point x ∈ R n , then ϕ is also said to be locally Lipschitz in R n .</formula><p>Assume that ϕ is Lipschitz near x. The generalized directional derivative of ϕ at x in the direction v ∈ R n is given by</p><formula xml:id="formula_3">ϕ 0 (x; v) = lim sup y→x s→0 + ϕ(y + sv) -ϕ(y)</formula><p>s .</p><p>The Clarke's generalized gradient of f is defined as</p><formula xml:id="formula_4">∂ϕ(x) = {y ∈ R n : ϕ 0 (x; v) ≥ y T v, ∀v ∈ R n }.</formula><p>When ϕ is locally Lipschitz in R n , ϕ is differentiable for almost all (a.a.) x ∈ R n (in the sense of Lebesgue measure). Then, the Clarke's generalized gradient of ϕ at x ∈ R n is equivalent to</p><formula xml:id="formula_5">∂ϕ(x) = K lim n→∞ ∂ϕ(x n ) : x n → x, x n / ∈ N , x n / ∈ E</formula><p>where K (•) denotes the closure of the convex hull, N ⊂ R n is an arbitrary set with measure zero, and E ⊂ R n is the set of points where ϕ is not differentiable. Definition 3: A function ϕ : R n → R, which is locally Lipschitz near x ∈ R n , is said to be regular at x if there exists the one-sided directional derivative for any direction v ∈ R n which is given by</p><formula xml:id="formula_6">ϕ (x; v) = lim ξ →0 + ϕ(x + ξv) -ϕ(x) ξ</formula><p>and we have ϕ 0 (x; v) = ϕ (x; v). The function ϕ is said to be regular in R n if it is regular for any x ∈ R n . Consider the following ordinary differential equation (ODE):</p><formula xml:id="formula_7">dx dt = ψ(x), x(t 0 ) = x 0 . (<label>2</label></formula><formula xml:id="formula_8">)</formula><p>A set-valued is map defined as</p><formula xml:id="formula_9">φ(x) = ε&gt;0 μ(N )=0 K [ψ(B(x, ε) -N )]</formula><p>where μ(N ) is the Lebesgue measure of set N , B(x, ε) = {y : yx ≤ ε}. A solution of ( <ref type="formula" target="#formula_7">2</ref>) is an absolutely continuous function x(t) defined in an interval [t 0 , t 1 ](t 0 ≤ t 1 ≤ + ∞), which satisfies x(t 0 ) = x 0 and the differential inclusion</p><formula xml:id="formula_10">dx dt ∈ φ(x), a.a. t ∈ [t 0 , t 1 ].</formula><p>Definition 4: Let E ⊂ R n be a nonempty convex set. Function ϕ : E → R is said to be convex on E if, for any x , x ∈ E and 0 ≤ α ≤ 1, we have</p><formula xml:id="formula_11">ϕ(αx + (1 -α)x ) ≤ αϕ(x ) + (1 -α)ϕ(x ).</formula><p>Moreover, ϕ is said to be strictly convex on E if strict inequality holds whenever x = x and 0 &lt; α &lt; 1. ϕ is said to be strongly convex on E if</p><formula xml:id="formula_12">ϕ(αx + (1 -α)x ) ≤ αϕ(x ) + (1 -α)ϕ(x ) -σ α(1 -α) x -x 2</formula><p>where σ is a positive constant. Definition 5: Let E ⊂ R n be a nonempty convex set. A set-valued map F : E → R m is said to be monotone on E if, for any x , x ∈ E, we have</p><formula xml:id="formula_13">(ξ -ξ ) T (x -x ) ≥ 0, ∀ ξ ∈ F(x ), ξ ∈ F(x ).</formula><p>Moreover, F is said to be strictly monotone on E if strict inequality holds whenever x = x . F is said to be strongly monotone on E if</p><formula xml:id="formula_14">(ξ -ξ ) T (x -x ) ≥ σ x -x 2 , ∀ ξ ∈ F(x ), ξ ∈ F(x )</formula><p>where σ is a positive constant.</p><p>A continuous function ϕ(x) is convex (strictly convex, strongly convex) if and only if its generalized gradient ∂ϕ(x) is a monotone (strictly monotone, strongly monotone) mapping.</p><p>Definition 6 ( <ref type="bibr" target="#b35">[36]</ref>): Let E ⊂ R n be a nonempty convex set. A function ϕ : E → R is said to be pseudoconvex on E if, for any x , x ∈ E, we have</p><formula xml:id="formula_15">∃ η ∈ ∂ϕ(x ) : η T (x -x ) ≥ 0 ⇒ ϕ(x ) ≥ ϕ(x ).</formula><p>It is noted that a convex function is obviously pseudoconvex. Definition 7 ( <ref type="bibr" target="#b35">[36]</ref>): Let E ⊂ R n be a nonempty convex set. A set-valued map F : E → R m is said to be pseudomonotone on E if, for any x , x ∈ E, we have</p><formula xml:id="formula_16">∃ η x ∈ F(x ) : η T x (x -x ) ≥ 0 ⇒∀ η x ∈ F(x ) : η T x (x -x ) ≥ 0. Moreover, F is said to be strongly pseudomonotone on E if ∃ η x ∈ F(x ) : η T x (x -x ) ≥ 0 ⇒∀ η x ∈ F(x ) : η T x (x -x ) ≥ σ x -x 2 where σ is a positive constant.</formula><p>It is shown in <ref type="bibr" target="#b35">[36]</ref> that a continuous function ϕ(x) is pseudoconvex if and only if its generalized gradient ∂ϕ(x) is a pseudomonotone mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL DESCRIPTION</head><p>This section describes the one-layer projection model for solving the nonsmooth optimization problem <ref type="bibr" target="#b0">(1)</ref>. First, some notations are introduced. In this paper, • denotes the Euclidean norm. The region where the constraints of problem (1) are satisfied (feasible region) is defined as S = {x ∈ R n : Ax = b, x ∈ }. The region where the equality constraints of problem (1) are satisfied is defined as E = {x ∈ R n : Ax = b}; then, clearly S = E . The optimal solution set of problem (1) is denoted as M. Throughout this paper, we assume that the optimal solution set of problem (1) is nonempty; i.e., M = ∅.</p><p>The dynamic equations of the proposed projection neural network model are described as follows for solving problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>1) State equation dy dt ∈ -Pg(y)-(I -P)(y -g(y)+∂ f ((I -P)g(y)+q))+q.</p><p>(3) 2) Output equation</p><formula xml:id="formula_17">x = g(y) (4)</formula><p>where is a positive scaling constant, I is the identity matrix,</p><formula xml:id="formula_18">P = A T (A A T ) -1 A, q = A T (A A T ) -1 b, ∂ f is the generalized gradient of f , and g : R n → is a projection operator defined by g(u) = arg min v∈ u -v .</formula><p>In general, the calculation of projection of a point onto a convex set is nontrivial. However, if is a box set or a sphere set, the calculation is straightforward. For example, if</p><formula xml:id="formula_19">= {u ∈ R n : l i ≤ u i ≤ h i , i = 1, 2, . . . , n}, then g(u) = [g(u 1 ), g(u 2 ), . . . , g(u n )] T and g(u i ) = ⎧ ⎪ ⎨ ⎪ ⎩ h i u i &gt; h i u i l i ≤ u i ≤ h i l i u i &lt; l i . If = {u ∈ R n : u -s ≤ r, s ∈ R n , r &gt; 0}, then g(u) = u u -s ≤ r s + r(u-s) u-s u -s &gt; r.</formula><p>The matrix P, called the projection matrix, has several desirable properties, such as symmetry, P 2 = P, (I -P) 2 = I -P, P(I -P) = 0, and P = 1, which can be derived directly from the definition of P. <ref type="bibr">Lemma 1 ([22]</ref>): For the projection operator g(x), the following inequality holds:</p><formula xml:id="formula_20">(u -g(u)) T (g(u) -v) ≥ 0, ∀ u ∈ R n , v ∈ .</formula><p>Furthermore, according to Lemma 1, we have the following result.</p><p>Lemma 2 ( <ref type="bibr" target="#b23">[24]</ref>): For the projection operator g(x), the following inequality holds:</p><formula xml:id="formula_21">(u -v) T (g(u) -g(v)) ≥ g(u) -g(v) 2 , ∀ u, v ∈ R n .</formula><p>Remark 1: In <ref type="bibr" target="#b26">[27]</ref>, we have investigated the quadratic programming problems in which the objective functions are only needed to be convex on the equality constraints. However, the method used in <ref type="bibr" target="#b26">[27]</ref> is not suitable for general constrained nonlinear programming problems. Moreover, to obtain the global convergence of the proposed neural network in <ref type="bibr" target="#b26">[27]</ref>, the parameter in the model needs to be large enough than an estimated lower bound. For a more general nonlinear nonsmooth optimization, the proposed neural network in ( <ref type="formula">3</ref>) and ( <ref type="formula">4</ref>) is capable of solving problem (1) and its global convergence can be guaranteed, which will be proved in next section.</p><p>Remark 2: Recurrent neural networks for solving convex optimization problems with convex objective functions have been well studied in the literature <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Recently, some recurrent neural networks based on the penalty parameter method have been proposed for nonsmooth convex and nonconvex optimization problems <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and <ref type="bibr" target="#b29">[30]</ref>. However, to get the optimality and convergence of the neural networks, the penalty parameters need to be larger than the estimated lower bounds. To avoid the inconvenience of estimation of the penalty parameters, here the proposed neural network in (3) and ( <ref type="formula">4</ref>) does not contain any penalty parameter. Thus the proposed neural network in this paper is more convenient for solving <ref type="bibr" target="#b0">(1)</ref>.</p><p>Problem <ref type="bibr" target="#b0">(1)</ref> has two special cases. The first one is with only equality constraints described as follows:</p><formula xml:id="formula_22">minimize f (x), subject to Ax = b. (<label>5</label></formula><formula xml:id="formula_23">)</formula><p>The second one is with only bound constraints described as follows:</p><formula xml:id="formula_24">minimize f (x), subject to x ∈ . (<label>6</label></formula><formula xml:id="formula_25">)</formula><p>For problem <ref type="bibr" target="#b4">(5)</ref>, the proposed neural network can be described as the following system:</p><formula xml:id="formula_26">dx dt ∈ -Px -(I -P)∂ f ((I -P)x + q) + q. (<label>7</label></formula><formula xml:id="formula_27">)</formula><p>It is considered as an improved version of the model investigated in <ref type="bibr" target="#b27">[28]</ref>.</p><p>For problem <ref type="bibr" target="#b5">(6)</ref>, the proposed neural network can be described as follows. </p><formula xml:id="formula_28">dy dt ∈ -y + g(y) -∂ f (g(y)). (<label>8</label></formula><formula xml:id="formula_29">)</formula><p>2) Output equation</p><formula xml:id="formula_30">x = g(y). (<label>9</label></formula><formula xml:id="formula_31">)</formula><p>It is considered as an extension of the model investigated in <ref type="bibr" target="#b43">[44]</ref> for nonsmooth optimization. Throughout this paper, we denote X = {v ∈ R n : v = (I -P)x +q, x ∈ } and assume that the generalized gradient of the objective function f (x) of problem ( <ref type="formula" target="#formula_0">1</ref>) is bounded on X . It is clear that X ⊂ E from the definitions of P and q. It is easy to show that S ⊂ X . Consequently, S ⊂ X ⊂ E.</p><p>A comparison of the proposed neural network with several other neural networks for solving problem ( <ref type="formula" target="#formula_0">1</ref>) is shown in Table <ref type="table" target="#tab_0">I</ref>. We can see that the neural network proposed here has the least number of neurons among the existing ones. The other neural network models in Table <ref type="table" target="#tab_0">I</ref> require the objective function f (x) to be convex in R n or include a penalty parameter. However, here the objective function f (x) may be nonconvex for the global convergence of the proposed neural network. Moreover, there is no penalty parameter in the proposed neural network here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THEORETICAL ANALYSIS</head><p>To obtain the optimal solutions of problem (1) using recurrent neural networks, two main issues need to be addressed. The first one is the optimality of the neural networks; i.e., we need to find the relationship between the optimal solutions and the outputs (or equilibrium points) of the neural networks. In general, the outputs (or equilibrium points) correspond to the optimal solutions. The second one is the convergence of the neural network. To show the performance of the neural networks for solving optimization problems, the convergence is most important. In this section, these two issues of the proposed neural network will be investigated in detail.</p><p>Definition 8: ȳ ∈ R n is said to be an equilibrium point of system (3) if 0 ∈ Pg( ȳ)+(I -P)( ȳ-g( ȳ)+∂ f ((I -P)g( ȳ)+q))-q <ref type="bibr" target="#b9">(10)</ref> i.e., if there exists γ ∈ ∂ f ((I -P)g( ȳ) + q) such that</p><formula xml:id="formula_32">Pg( ȳ) + (I -P)( ȳ -g( ȳ) + γ ) -q = 0. (<label>11</label></formula><formula xml:id="formula_33">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimality Analysis</head><p>From the definitions of P and q, it is easily to obtain the following lemma.</p><p>Lemma 3: Assume A is full row-rank. For any x ∈ R n , Ax = b if and only if Px = q, where P and q are defined in <ref type="bibr" target="#b2">(3)</ref>.</p><p>The optimality of the proposed neural network can be described as the following theorem.</p><p>Theorem 1: Assume that the objective function f (x) in problem ( <ref type="formula" target="#formula_0">1</ref>) is pseudoconvex on the feasible region S. Then x * ∈ R n is an optimal solution of problem (1) if and only if there exists an equilibrium point y * ∈ R n for system (3) such that x * = g(y * ).</p><p>Proof: Assume x * ∈ R n to be an optimal solution of problem <ref type="bibr" target="#b0">(1)</ref>. According to the Karush-Kuhn-Tucker conditions <ref type="bibr" target="#b2">[3]</ref> for problem <ref type="bibr" target="#b0">(1)</ref>, there exist v * ∈ R m , w * ∈ R n , and</p><formula xml:id="formula_34">γ * ∈ ∂ f (x * ) such that γ * + A T v * + w * = 0 (<label>12</label></formula><formula xml:id="formula_35">)</formula><formula xml:id="formula_36">Ax * = b (13) x * = g(x * + w * ). (<label>14</label></formula><formula xml:id="formula_37">)</formula><p>According to <ref type="bibr" target="#b13">(14)</ref>, let y * = x * + w * , then x * = g(y * ). It follows that</p><formula xml:id="formula_38">y * = g(y * ) + w * . (<label>15</label></formula><formula xml:id="formula_39">)</formula><p>From <ref type="bibr" target="#b11">(12)</ref>, w * = -γ * -A T v * and substituting it into (15), we have</p><formula xml:id="formula_40">y * = g(y * ) -γ * -A T v * . (<label>16</label></formula><formula xml:id="formula_41">)</formula><p>Multiplying the both sides of ( <ref type="formula" target="#formula_40">16</ref>) with A results in</p><formula xml:id="formula_42">v * = (A A T ) -1 A(g(y * ) -y * -γ * ). (<label>17</label></formula><formula xml:id="formula_43">)</formula><p>Substituting ( <ref type="formula" target="#formula_42">17</ref>) into ( <ref type="formula" target="#formula_40">16</ref>), it follows that</p><formula xml:id="formula_44">(I -P)(y * -g(y * ) + γ * ) = 0. (<label>18</label></formula><formula xml:id="formula_45">)</formula><p>According to <ref type="bibr" target="#b12">(13)</ref> and Lemma 3, we have Px * = q; i.e., Pg(y * ) = q. Combining with <ref type="bibr" target="#b17">(18)</ref>, one gets</p><formula xml:id="formula_46">Pg(y * ) + (I -P)(y * -g(y * ) + γ * ) -q = 0.</formula><p>Thus y * is an equilibrium point of system <ref type="bibr" target="#b2">(3)</ref>.</p><p>Next, we prove that the opposite side of the theorem is true. Assume ȳ ∈ R n to be an equilibrium point of system (3) and x = g( ȳ). According to Definition 8, there exists γ ∈ ∂ f ((I -P) x + q) such that</p><formula xml:id="formula_47">P x + (I -P)( ȳ -x + γ ) -q = 0 (<label>19</label></formula><formula xml:id="formula_48">)</formula><p>where x = g( ȳ).</p><p>Multiplying both sides of <ref type="bibr" target="#b18">(19)</ref> with P, since P 2 = P and Pq = q, it follows that P x = q and</p><formula xml:id="formula_49">(I -P)( ȳ -x + γ ) = 0. (<label>20</label></formula><formula xml:id="formula_50">)</formula><p>Thus according to Lemma 3, x = g( ȳ) is a feasible solution of problem (1). For any x ∈ S, from (20), we have</p><formula xml:id="formula_51">(x -x) T (I -P)( ȳ -x + γ ) = 0.</formula><p>Because Px = P x = q for x ∈ S, it follows that</p><formula xml:id="formula_52">(x -x) T ( ȳ -x + γ ) = 0. (<label>21</label></formula><formula xml:id="formula_53">)</formula><p>According to Lemma 1, for</p><formula xml:id="formula_54">x ∈ S ⊂ , (x -x) T ( x - ȳ) = (x -g( ȳ)) T (g( ȳ) -ȳ) ≥ 0. Then, (21) implies (x - x) T γ = (x -x) T ( x -ȳ) ≥ 0. Since f (x) is pseudoconvex on S, we have f (x) ≥ f ( x).</formula><p>Thus x is an optimal solution of problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>According to the proof of Theorem 1, it is easy to get the following corollary.</p><p>Corollary 1: An optimal solution of problem ( <ref type="formula" target="#formula_0">1</ref>) is x = g( ȳ) if the objective function f (x) is pseudoconvex at x, where ȳ is an equilibrium point of system <ref type="bibr" target="#b2">(3)</ref>.</p><p>Remark 3: From the above analysis, we know that if the state vector y(t) of the neural network in (3) and ( <ref type="formula">4</ref>) converges to an equilibrium point ȳ, and f (x) is pseudoconvex at x = g( ȳ), then the output vector x(t) of the neural network converges to an optimal solution of problem (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence Analysis</head><p>In this subsection, the convergence property of the proposed neural network in (3) and ( <ref type="formula">4</ref>) is discussed by using the Lyapunov method and nonsmooth analysis <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>Definition 9: The output vector of the neural network in (3) and ( <ref type="formula">4</ref>) is said to be globally convergent to the optimal solution set M of problem (1) if, for any initial value</p><formula xml:id="formula_55">y 0 = y(t 0 ) ∈ R n lim t →∞ dist(x(t), M) = 0</formula><p>where dist(x(t), M) is the distance between x(t) and M, which is defined as</p><formula xml:id="formula_56">dist(x, M) = min u∈M x -u .</formula><p>To prove the global convergence of the proposed neural network, we first define</p><formula xml:id="formula_57">V 0 (y) = y -g( ȳ) 2 -y -g(y) 2 (<label>22</label></formula><formula xml:id="formula_58">)</formula><p>where ȳ is an equilibrium point of system (3). Then V 0 (y) has some properties as the following lemma. Lemma 4: For any y ∈ R n , we have (i) V 0 (y) ≥ g(y)g( ȳ) 2 ;</p><p>(ii) V 0 (y) is differentiable and its gradient is ∇V 0 (y) = 2(g(y)g( ȳ)). Proof: (i) By simple calculation, we have</p><formula xml:id="formula_59">V 0 (y) -g(y) -g( ȳ) 2 = 2(y -g(y)) T (g(y) -g( ȳ)).</formula><p>According to Lemma 1, V 0 (y)g(y)g( ȳ) 2 ≥ 0. Thus the result holds.</p><p>(ii) Assume ϕ(y) = yg(y) 2 . Then</p><formula xml:id="formula_60">ϕ(y) = min z∈ y -z 2 . (<label>23</label></formula><formula xml:id="formula_61">)</formula><p>Since the minimum on the right-hand side of ( <ref type="formula" target="#formula_60">23</ref>) is uniquely attained at z = g(y), it follows from [2, Ch. 4, Th. 1.7] that ϕ(y) is differentiable and</p><formula xml:id="formula_62">∇ϕ(y) = ∇ y y -z 2 z=g(y) = 2(y -g(y)). (<label>24</label></formula><formula xml:id="formula_63">)</formula><p>Furthermore, it is easily to get that ∇V 0 (y) = 2(g(y)g( ȳ)). Now, we present the global convergence of the proposed neural network.</p><p>Theorem 2: Assume that the objective function f (x) in problem ( <ref type="formula" target="#formula_0">1</ref>) is convex on X . For any initial value y 0 = y(t 0 ) ∈ R n , the output vector of the neural network in (3) and ( <ref type="formula">4</ref>) is globally convergent to the optimal solution set M if y(t) is bounded and</p><formula xml:id="formula_64">∀x ∈ S, (x -x * ) T (γ -γ * ) = 0 ⇐⇒ x ∈ M where x * ∈ M, γ ∈ ∂ f (x), and γ * ∈ ∂ f (x * ).</formula><p>Proof: Since f (x) is convex on X , it is convex on the feasible region S. According to Theorem 1, x = g( ȳ) is an optimal solution of problem <ref type="bibr" target="#b0">(1)</ref>, where ȳ is an equilibrium point of system (3).</p><p>Since ȳ is an equilibrium point of system (3), there exists γ ∈ ∂ f ((I -P) x + q) such that</p><formula xml:id="formula_65">P x + (I -P)( ȳ -x + γ ) -q = 0. (<label>25</label></formula><formula xml:id="formula_66">)</formula><p>By substituting ( <ref type="formula" target="#formula_65">25</ref>) into (3), it follows that dy dt ∈ -P(x -x)-(I -P)(y-x +∂ f ((I -P)x +q)-ȳ+ x-γ )</p><p>where x = g(y).</p><p>Consider the Lyapunov function</p><formula xml:id="formula_67">V (y) = 2 V 0 (y) + (y -ȳ) T P(y -ȳ)<label>(26)</label></formula><p>where V 0 (y) is defined in <ref type="bibr" target="#b21">(22)</ref>. From Lemma 4(ii), we have</p><formula xml:id="formula_68">∇V (y) = [x -x + P(y -ȳ)] .</formula><p>According to the chain rule, it follows that V (y(t)) is differentiable for almost all (a.a.) t ≥ t 0 and it results in</p><formula xml:id="formula_69">V (y(t)) = (∇V (y)) T ẏ(t) ≤ sup γ ∈∂ f (z) [x -x + P(y -ȳ)] T [-P(x -x) -(I -P)(y -x + γ -ȳ + x -γ )] = -(x -x) T P(x -x) -(x -x) T (I -P) × (y -ȳ) + (x -x) T (I -P)(x -x) -inf γ ∈∂ f (z) (x -x) T (I -P)(γ -γ ) -(y -ȳ) T P(x -x) = -(x -x) T P(x -x) -(x -x) T (y -ȳ) + (x -x) T (I -P)(x -x) -inf γ ∈∂ f (z) (x -x) T (I -P)(γ -γ )</formula><p>where z = (I -P)x + q.</p><p>Since I -P ≤ 1, combining with Lemma 2, we have</p><formula xml:id="formula_70">(x -x)(I -P)(x -x) ≤ x -x 2 = g(y) -g( ȳ) 2 ≤ (y -ȳ) T (g(y) -g( ȳ)) = (x -x) T (y -ȳ).</formula><p>Then, since P x = q, we have</p><formula xml:id="formula_71">V (y(t)) ≤ -(x -x) T P(x -x) -inf γ ∈∂ f (z) (x -x) T (I -P)(γ -γ ) = -Px -q 2 -inf γ ∈∂ f (z) (z -z) T (γ -γ )</formula><p>where z = (I -P)</p><formula xml:id="formula_72">x + q = x. Since f is convex on X , ∂ f is monotone on X . Then V (y(t)) ≤ 0. Define H (y) = Px -q 2 + inf γ ∈∂ f (z) (z -z) T (γ -γ ).</formula><p>Assume there exists y ∈ R n such that x = g( y) is an optimal solution of problem (1). Thus P x = q. From the condition, (ž -z) T ( γ -γ ) = ( x -x) T ( γ -γ ) = 0, where ž = (I -P) x + q = x and γ ∈ ∂ f ((I -P) x + q). Thus H ( y) = 0. Conversely, if there exists ŷ ∈ R n such that H ( ŷ) = 0, we have P x = q, where x = g( ŷ). Combining with ∂ f (ẑ) being a compact convex subset in R n , there exists γ ∈ ∂ f (ẑ) such that (ẑ -z) T ( γ -γ ) = 0, where ẑ = (I -P) x + q = x. From the condition, x = ẑ is an optimal solution of problem (1). Consequently, there exists y ∈ R n such that H (y) = 0 if and only if x = g(y) is an optimal solution of problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>From the boundedness of y(t) and (3), we infer that ẏ(t) is also bounded, denoted by M. Then, there exists an increasing sequence {t k } with lim k→∞ t k = ∞ and a limit point ỹ such that lim k→∞ y(t k ) = ỹ. Next, inspired by this paper in <ref type="bibr" target="#b22">[23]</ref>, we prove that H ( ỹ) = 0. If it does not hold, that is, H ( ỹ) &gt; 0, from the definition of H (y), it is lower semicontinuous, then there exist δ &gt; 0 and ε &gt; 0, such that H (y) &gt; ε for all y ∈ B( ỹ, δ), where B( ỹ, δ) = {y ∈ R n : yỹ ≤ δ} is the δ neighborhood of ỹ. Since lim k→∞ y(t k ) = ỹ, there exists a positive integer N, such that for all k ≥ N,</p><formula xml:id="formula_73">y(t k ) -ỹ ≤ δ/2. When t ∈ [t k -δ/(4M), t k + δ/(4M)] and k ≥ N, we have y(t) -ỹ ≤ y(t) -y(t k ) + y(t k ) -ỹ ≤ M|t -t k | + δ 2 ≤ δ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It follows that H (y(t)) &gt; ε for all</head><formula xml:id="formula_74">t ∈ [t k -δ/(4M), t k + δ/(4M)]. Since the Lebesgue measure of the set t ∈ k≥N [t k -δ/(4M), t k + δ/(4M)] is infinite, then we have ∞ t 0 H (y(t))dt = ∞.<label>(27)</label></formula><p>Since V (y(t)) ≤ 0, V (y(t)) is monotonically nonincreasing. Combining with V (y(t)) ≥ 0, there exists a constant V c such that lim t →∞ V (y(t)) = V c . We have</p><formula xml:id="formula_75">∞ t 0 H (y(t))dt = lim s→∞ s t 0 H (y(t))dt ≤ -lim s→∞ s t 0 V (y(t))dt = -lim s→∞ V (y(s)) -V (y(t 0 )) = -V c + V (y(t 0 ))</formula><p>which contradicts <ref type="bibr" target="#b26">(27)</ref>. Therefore, we have H ( ỹ) = 0, and then x = g( ỹ) is an optimal solution of problem (1). That is, any limit point of the output vector is an optimal solution of problem <ref type="bibr" target="#b0">(1)</ref>. Then lim</p><formula xml:id="formula_76">t →∞ dist(x(t), M) = 0.</formula><p>Consequently, any trajectory of output vector of the neural network is globally convergent to the optimal solution set M of problem <ref type="bibr" target="#b0">(1)</ref>.</p><p>As a special case of Theorem 2, if the objective function in problem ( <ref type="formula" target="#formula_0">1</ref>) is strictly convex on X , the condition in Theorem 2 is obviously true since ∂ f is strictly monotone. Then one gets the following corollary:</p><p>Corollary 2: Assume that the objective function f (x) in problem ( <ref type="formula" target="#formula_0">1</ref>) is strictly convex on X . For any initial value y 0 = y(t 0 ) ∈ R n , the output vector of the neural network in (3) and ( <ref type="formula">4</ref>) is globally convergent to the unique optimal solution of problem (1) if y(t) is bounded.</p><p>Next, we investigate the models of the two special cases in Section III.</p><p>Corollary 3: Assume that the objective function f (x) in problem ( <ref type="formula" target="#formula_22">5</ref>) is pseudoconvex on the equality constraint set E. For any initial value x 0 = x(t 0 ) ∈ R n , the state vector of the neural network in <ref type="bibr" target="#b6">(7)</ref> is globally convergent to the optimal solution set M 1 lim t →∞ dist(x(t), M 1 ) = 0 where M 1 is the optimal solution set of problem <ref type="bibr" target="#b4">(5)</ref>.</p><p>Proof: Let x be an equilibrium point of system <ref type="bibr" target="#b6">(7)</ref>. According to Theorem 1, x is an optimal solution of problem <ref type="bibr" target="#b4">(5)</ref>.</p><p>Consider the Lyapunov function</p><formula xml:id="formula_77">V 1 (x) = 2 (x -x) T (I + P)(x -x).</formula><p>By using the chain rule, it follows that V (x(t)) is differentiable for a.a. t ≥ t 0 and it results in</p><formula xml:id="formula_78">V1 (x(t)) = (∇V 1 (x)) T ẋ(t) ≤ sup γ ∈∂ f (z) (x -x) T (I + P)(-Px -(I -P)γ + q) = sup γ ∈∂ f (z) (x -x) T (I + P)(-P(x -x) -(I -P)γ )</formula><p>where z = (I -P)x + q and the last equality holds since q = P x. Furthermore, it follows that</p><formula xml:id="formula_79">V1 (x(t)) ≤ sup γ ∈∂ f (z) (x -x) T (I + P)(-P(x -x) -(I -P)γ ) = -2(x -x) T P(x -x) -inf γ ∈∂ f (z) (x -x) T (I -P)γ = -2(x -x) T P(x -x) -inf γ ∈∂ f (z) (z -z) T γ</formula><p>where z = (I -P) x + q = x.</p><p>Then,</p><formula xml:id="formula_80">V1 (x(t)) ≤ -2(x -x) T P(x -x) -inf γ ∈∂ f (z) (z -z) T γ = -2 Px -q 2 -inf γ ∈∂ f (z) (z -z) T γ .</formula><p>Since x is an optimal solution of problem <ref type="bibr" target="#b4">(5)</ref>, from ( <ref type="formula" target="#formula_44">18</ref>), we have (x -x) T (I -P) γ = 0; i.e., (z -z) T γ = 0, where γ ∈</p><formula xml:id="formula_81">∂ f ( x). Since f is pseudoconvex on E, it is pseudomonotone on E. Then (z -z) T γ ≥ 0 for any γ ∈ ∂ f (z) and it results in V1 (x(t)) ≤ 0. Furthermore, from the definition of V 1 (x), V 1 (x) ≥ x -x 2 /2. Consequently, x(t) is bounded. Define H 1 (x) = 2 Px -q 2 +inf γ ∈∂ f (z) (z -z) T γ .</formula><p>Assume there exists x ∈ R n to be an optimal solution of problem <ref type="bibr" target="#b4">(5)</ref>. We have P x = q and ( x -x) T (I -P) γ = 0 from <ref type="bibr" target="#b17">(18)</ref>, where γ ∈ ∂ f ( x). Then it results in H 1 ( x) = 0. Conversely, if there exists x ∈ R n such that H 1 ( x) = 0, we have P x = q. Combining with the fact that ∂ f (ẑ) is a compact convex subset in R n , there exists γ ∈ ∂ f (ẑ) such that (ẑ -z) T γ = 0, where ẑ = (I -P) x + q = x. Since f is pseudoconvex on E, it follows that f (z) ≥ f (ẑ). Thus x = ẑ is also an optimal solution of problem (1) as x = z is an optimal solution. Therefore, there exists x ∈ R n such that H 1 (x) = 0 if and only if x is an optimal solution of problem <ref type="bibr" target="#b4">(5)</ref>.</p><p>As the remainder of the proof is similar to that of Theorem 2, it is omitted here.</p><p>Corollary 4: Assume that the objective function f (x) in problem is pseudoconvex on the bound constraint set . For any initial value y 0 = y(t 0 ) ∈ R n , the output vector of the neural network in ( <ref type="formula" target="#formula_28">8</ref>) and ( <ref type="formula" target="#formula_30">9</ref>) is globally convergent to the optimal solution set M 2 if y(t) is bounded lim</p><formula xml:id="formula_82">t →∞ dist(x(t), M 2 ) = 0</formula><p>where M 2 is the optimal solution set of problem <ref type="bibr" target="#b5">(6)</ref>.</p><p>Proof: Let ȳ be an equilibrium point of system <ref type="bibr" target="#b7">(8)</ref>. According to Theorem 1, x = g( ȳ) is an optimal solution of problem <ref type="bibr" target="#b5">(6)</ref>.</p><p>Consider the Lyapunov function</p><formula xml:id="formula_83">V 2 (y) = 2 V 0 (y)</formula><p>where V 0 (y) is defined in <ref type="bibr" target="#b21">(22)</ref>. According to Lemma 4(ii), we have</p><formula xml:id="formula_84">∇V 2 (y) = (x -x)</formula><p>where x = g(y) and x = g( ȳ). By using the chain rule, it follows that V (x(t)) is differentiable for a.a. t ≥ t 0 and it results in</p><formula xml:id="formula_85">V2 (y(t)) = (∇V 2 (y)) T ẏ(t) ≤ sup γ ∈∂ f (x) (x -x) T (-y + x -γ ) = -(x -x) T (y -x) -inf γ ∈∂ f (x) (x -x) T γ .</formula><p>According to Lemma 1, we have</p><formula xml:id="formula_86">(x -x) T (y -x) = (g(y)- g( ȳ)) T (y -g(y)) ≥ 0. Then V2 (y(t)) ≤ -inf γ ∈∂ f (x) (x -x) T γ .</formula><p>Since x is an optimal solution of problem <ref type="bibr" target="#b5">(6)</ref>, from <ref type="bibr" target="#b17">(18)</ref>, we have (x -x) T ( ȳ -g( ȳ)+ γ ) = 0, where γ ∈ ∂ f ( x). According to Lemma 1, we have</p><formula xml:id="formula_87">( x -x) T ( ȳ -g( ȳ)) = (g( ȳ) -g(y)) T ( ȳ -g( ȳ)) ≥ 0. Then (x -x) T γ ≥ 0. Since f is pseudoconvex on , ∂ f is pseudomonotone on E. Then (x -x) T γ ≥ 0 for any γ ∈ ∂ f (x) and it results in V2 (x(t)) ≤ 0.</formula><p>Define H 2 (y) = inf γ ∈∂ f (x) (x -x) T γ . Assume there exists x as an optimal solution of problem <ref type="bibr" target="#b5">(6)</ref>. On one hand, according to Theorem 1, there exists an equilibrium point y ∈ R n for system <ref type="bibr" target="#b7">(8)</ref> such that x = g( y). From ( <ref type="formula" target="#formula_44">18</ref>), ( x -x) T ( y -g( y)+ γ ) = 0, where γ ∈ ∂ f ( x). According to Lemma 1, we have ( x -x) T ( yg( y)) = (g( y)g( ȳ)) T ( yg( y)) ≥ 0. Then ( x -x) T γ ≤ 0. On the other hand, from <ref type="bibr" target="#b17">(18)</ref>, since x is an optimal solution of problem ( <ref type="formula" target="#formula_24">6</ref>),</p><formula xml:id="formula_88">( x -x) T ( ȳ -g( ȳ) + γ ) = 0, where γ ∈ ∂ f ( x). According to Lemma 1, we have ( x -x) T ( ȳ -g( ȳ)) = (g( ȳ) -g( y)) T ( ȳ -g( ȳ)) ≥ 0. Then ( x -x) T γ ≥ 0. Since f is pseudoconvex on , ∂ f is pseudomonotone on . Then ( x -x) T γ ≥ 0. Hence ( x -x) T γ = 0. Then H 2 ( y) = 0. Conversely, if there exists ŷ ∈ R n such that H 2 ( ŷ) = 0, combining with the fact that ∂ f ( x) is a compact convex subset in R n , there exists γ ∈ ∂ f ( x) such that ( x -x) T γ = 0, where x = g( ŷ). Since f is pseudoconvex on , it follows that f ( x) ≥ f ( x). Thus</formula><p>x is also an optimal solution of problem (6) as x is an optimal solution. Therefore, there exists y ∈ R n such that H 2 (y) = 0 if and only if x = g(y) is an optimal solution of problem <ref type="bibr" target="#b5">(6)</ref>.</p><p>As the remainder of the proof is similar to that of Theorem 2, it is omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence Rate Analysis</head><p>It is well known that studying the convergence rate of neural networks is useful for real applications. This subsection studies the convergence rate of the proposed neural network. The results are stated as the following theorem.</p><p>Theorem 3: Assume that the objective function f (x) in problem ( <ref type="formula" target="#formula_0">1</ref>) is strongly convex on X . For any initial value y 0 = y(t 0 ) ∈ R n , the output vector of the neural network in (3) and ( <ref type="formula">4</ref>) is globally convergent to the unique optimal solution of problem (1) if y(t) is bounded, and the convergence rate of the output vector is described by</p><formula xml:id="formula_89">x(t) -x 2 ≤ V 0 (y(t 0 )) + (y(t 0 ) -ȳ)P(y(t 0 ) -ȳ) - 2 t t 0 Px(s) -q 2 + σ (I -P)(x(s) -x) 2 ds (<label>28</label></formula><formula xml:id="formula_90">)</formula><p>where V 0 (y) is defined in <ref type="bibr" target="#b21">(22)</ref> and σ is a positive constant. Proof: Since f (x) is strongly convex on X , for any z = (I -P)x + q ∈ X , we have</p><formula xml:id="formula_91">(z -z) T (γ -γ ) ≥ σ z -z) 2 = σ (I -P)(x -x) 2</formula><p>where σ is a positive constant, z = (I -P) x + q, γ ∈ ∂ f (z), and γ ∈ ∂ f (z).</p><p>According the proof of Theorem (2), by the same Lyapunov function in <ref type="bibr" target="#b25">(26)</ref>, it follows that where σ is a positive constant. Then</p><formula xml:id="formula_92">V (y(t)) ≤ -Px -q 2 -inf γ ∈∂ f (z) (z -z) T (γ -γ ) ≤ -Px -q 2 -σ (I -P)(x -x) 2<label>0 1 2 x 10 -4 -3 -2.5 -2 -1.5 -1</label></formula><formula xml:id="formula_93">V (y(t)) ≤ V (y(t 0 )) - t t 0 Px(s) -q 2 + σ (I -P)(x(s) -x) 2 ds. (<label>29</label></formula><formula xml:id="formula_94">)</formula><p>According to Corollary 2, the output vector of the neural network in (3) and ( <ref type="formula">4</ref>) is globally convergent to the unique optimal solution of problem <ref type="bibr" target="#b0">(1)</ref>. Furthermore, the convergence rate can be derived from <ref type="bibr" target="#b28">(29)</ref>.</p><p>In fact, from Lemma 4(i), V (y(t)) ≥ x -x 2 /2. Combining with <ref type="bibr" target="#b28">(29)</ref> and V (y(t 0 )) = [V 0 (y(t 0 )) + (y(t 0 ) -ȳ) P(y(t 0 ) -ȳ)]/2, it follows <ref type="bibr" target="#b27">(28)</ref>.</p><p>Remark 4: According to the formula in (28), we have</p><formula xml:id="formula_95">t t 0 Px(s) -q 2 + σ (I -P)(x(s) -x) 2 ds ≤ 2 V 0 (y(t 0 )) + (y(t 0 ) -ȳ)P(y(t 0 ) -ȳ).</formula><p>Thus for a fixed initial value y(t 0 ), the smaller the , the smaller will be Px(s)q 2 + σ (I -P)(x(s) -x) 2 , i.e., the faster x(t) converges to the unique optimal solution. Similar to the proof of Theorem 3, we can derive the following two corollaries.</p><p>Corollary 5: Assume that ∂ f (x) is strongly pseudomonotone on the equality constraint set E. For any initial value x 0 = x(t 0 ) ∈ R n , the state vector of the neural network in <ref type="bibr" target="#b6">(7)</ref> is globally convergent to the unique optimal solution of problem <ref type="bibr" target="#b4">(5)</ref>, and the convergence rate of the state vector is described by</p><formula xml:id="formula_96">x(t) -x 2 ≤ (x(t 0 ) -x)(I + P)(x(t 0 ) -x) - 2 t t 0 2 Px(s) -q 2 + σ (I -P)(x(s) -x) 2 ds. Corollary 6: Assume that ∂ f (x)</formula><p>is strongly pseudomonotone on the bound constraint set . For any initial value y 0 = y(t 0 ) ∈ R n , the output vector of the neural network in ( <ref type="formula" target="#formula_28">8</ref>) and ( <ref type="formula" target="#formula_30">9</ref>) is globally convergent to the unique optimal solution of problem (6) if y(t) is bounded, and the convergence rate of the output vector is described by</p><formula xml:id="formula_97">x(t) -x 2 ≤ V 0 (y(t 0 )) - 2σ t t 0 x(s) -x 2 ds</formula><p>where V 0 (y) is defined in <ref type="bibr" target="#b21">(22)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION RESULTS</head><p>In the following, four examples of optimization problems are solved using the proposed neural network.</p><p>Example 1: Consider the following nonlinear optimization problem:</p><formula xml:id="formula_98">minimize f (x) = -3x 2 1 + x 2 2 + 2x 1 x 2 + 6x 1 -2x 2 -e x 1 + e x 2 +2 , subject to x 1 -x 2 = 1, -2 ≤ x 1 , x 2 ≤ 2. (<label>30</label></formula><formula xml:id="formula_99">)</formula><p>It is obvious that the objective function f (x) is nonconvex. However, if we substitute x 2 = x 1 -1 into the objective function, then f (x 1 ) = e x 1 +1e x 1 + 3 is strictly convex. Consequently, the objective function of the problem is strictly convex on the equality constraint. Here, we use this simple example with simulations to show the performance of the proposed neural network compared with some other models in the literature. Assume = 10 -5 in the neural network (3). Figs. 1 and 2 respectively, depict the transient behaviors of state vector y(t) and output vector x(t) with 10 random initial values. Fig. <ref type="figure">3</ref> depicts the two-dimensional phase plot of (x 1 , x 2 ) T from 10 random initial points. The simulation results show that the output variables are globally convergent to the unique optimal solution x * = (-1, -2) T in the bound constraints, in which the dashed line and rectangles indicate the equality and bound constraints, respectively.</p><p>However, many other neural networks proposed in the literature may not be used for solving this problem. We give some simulations shown in Figs. <ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure">7</ref>. Among them, Fig. <ref type="figure">4</ref> depicts the transient behaviors of the neural network proposed in <ref type="bibr" target="#b37">[38]</ref> from 10 random initial points, which shows that the state variables are oscillatory in the bound constraints. Fig. <ref type="figure">5</ref> depicts the transient behaviors of the neural network proposed in <ref type="bibr" target="#b46">[47]</ref> from 10 random initial points. We can observe that the state variables are not convergent. Moreover, if the bound constraints are unbounded, the neural network in <ref type="bibr" target="#b46">[47]</ref> is divergent. In Fig. <ref type="figure">6</ref>, the neural network proposed in <ref type="bibr" target="#b15">[16]</ref> is used for solving this problem. However, we can observe that the state variables of the neural network are not convergent due to the nonconvexity of the objective function in problem <ref type="bibr" target="#b29">(30)</ref>. The simulations are further given for the more recent neural network model in <ref type="bibr" target="#b40">[41]</ref> as shown in Fig. <ref type="figure">7</ref>. Fig. <ref type="figure">6</ref>. Transient behaviors of the neural network in <ref type="bibr" target="#b15">[16]</ref> for solving <ref type="bibr" target="#b29">(30)</ref> in Example 1.</p><p>The simulation results shown in Figs. <ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure">7</ref>illustrate that the neural networks in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref> [41], and <ref type="bibr" target="#b46">[47]</ref> are not capable of solving some optimization problems with nonconvex objective functions.</p><p>Example 2: Consider a nonsmooth optimization problem as follows:</p><formula xml:id="formula_100">minimize f (x) = |x 1 -x 2 + 2x 3 + 1| + |x 2 -x 3 + x 4 -2| -|x 1 + x 3 -2|, subject to x 1 + x 2 + x 3 + x 4 = 8, x 1 + x 2 -x 3 -x 4 = 2, x 1 , x 4 ≥ 0, x 2 , x 3 ∈ R.<label>(31)</label></formula><p>In this problem, the objective function f (x) is nonsmooth and nonconvex. If we substitute the equality constraints into the objective function f (x), we can get that f (x) is convex with respect to x 1 and x 3 . Thus the objective function of the problem is convex on the equality constraint. Consequently, the proposed neural network in (3) and (4) is capable of solving this problem. Let = 10 -5 in the simulations. Figs. <ref type="figure">8</ref> and<ref type="figure">9</ref>, respectively, depict the transient behaviors of state vector y(t) and output vector x(t) with 10 random initial values, which shows that the output variables are globally convergent to the unique optimal solution x * = (0, 5, 3, 0) T . However, for the neural networks proposed in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b29">[30]</ref>, in order to estimate the lower bounds of the penalty parameters, the feasible regions of optimization problems need to be bounded. Then the proposed neural networks in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b29">[30]</ref> are not suitable for solving the problem in <ref type="bibr" target="#b30">(31)</ref> due to the unboundedness of the feasible region. Recently, a recurrent neural network with a hardlimiting activation function has been proposed for solving constrained optimization problems with piecewise-linear objective functions <ref type="bibr" target="#b28">[29]</ref>. Compared with the neural network in <ref type="bibr" target="#b28">[29]</ref> for solving problem (31), here we avoid using any penalty parameter for the design of the projection neural network in (3) and ( <ref type="formula">4</ref>).</p><p>Example 3: Consider a quadratic fractional optimization problem as follows <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_101">minimize f (x) = x T Qx + a T x + a 0 c T x + c 0 , subject to x 1 + x 2 -x 3 = 3, x 1 -2x 2 + x 4 = 0, 2 ≤ x 1 , x 2 , x 3 , x 4 ≤ 4<label>(32)</label></formula><p>where As stated in <ref type="bibr" target="#b24">[25]</ref>, the objective function is pseudoconvex on the feasible region. Let = 10 -5 in the simulations. Figs. <ref type="figure">10</ref> and<ref type="figure">11</ref>, respectively, depict the transient behaviors of state vector y(t) and output vector x(t) with 10 random initial values. Fig. <ref type="figure">10</ref> shows that the state vector y(t) of the neural network is convergent to an equilibrium point. Thus according to Corollary 1, the output vector x(t) is convergent to the unique optimal solution x * = (8/3, 7/3, 2, 2) T , which is shown in Fig. <ref type="figure">11</ref>. Compared with the neural network in <ref type="bibr" target="#b24">[25]</ref> for solving this problem, here no penalty parameter is included in the proposed neural network, which provides a potential application for pseudoconvex optimization.</p><formula xml:id="formula_102">Q = ⎛ ⎜ ⎜ ⎝ -1 0.5 1 0 0.5 5.5 -1 -0.5 1 -1 1 0 0 -0.5 0 0 ⎞ ⎟ ⎟ ⎠ , a = ⎛ ⎜ ⎜ ⎝ 1 -1 -1 1 ⎞ ⎟ ⎟ ⎠ , a 0 = -2, c = (1, 1, 1, -1) T , c 0 = 6.</formula><p>Example 4: Consider minimizing the condition number of a nonzero matrix A as follows: minimize κ(A), subject to A ∈ 0 <ref type="bibr" target="#b32">(33)</ref> where A ∈ R n×n is a symmetric matrix, 0 is a compact convex set in R n×n , and the condition number κ(A) is defined as κ(A) = λ max (A) λ min (A) in which λ max and λ min denote the maximum and minimum eigenvalues of matrix A, respectively.</p><p>The optimization of condition number has been recently investigated in the literature (e.g., <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b34">[35]</ref>, and the references therein). Consider a diagonal matrix The neural network in ( <ref type="formula" target="#formula_28">8</ref>) and ( <ref type="formula" target="#formula_30">9</ref>) can be used for optimizing the condition number. Compared with the methods used in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b34">[35]</ref> for the optimization of the condition number, the recurrent neural network in ( <ref type="formula" target="#formula_28">8</ref>) and ( <ref type="formula" target="#formula_30">9</ref>) is a parallel computational model which is easy for real-time optimization based on circuit implementation. Fig. <ref type="figure" target="#fig_7">12</ref> depicts the convergence of the condition number κ(A) for solving problem <ref type="bibr" target="#b32">(33)</ref>, which shows that it converges to 1 along the output of the neural network in <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref>, where = 10 -5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presented a one-layer projection neural network with piecewise-linear activation functions for solving nonsmooth optimization problems with linear equalities and bound constraints. The global convergence conditions of the neural network were derived by means of the Lyapunov method and nonsmooth analysis. Compared with the existing recurrent neural networks for solving constrained optimization problems, the proposed projection neural network has several salient features. The objective functions in the investigated optimization problems are not required to be convex everywhere, but only need to be convex (pseudoconvex) on a set defined by the constraints. Moreover, the proposed neural network does not have any design parameter. In addition, some extra conditions of many theoretical analyses in the literature are relaxed. Furthermore, numerical examples with simulation results were given to illustrate the effectiveness and performance of the proposed neural network.</p><p>As future research directions, we propose to investigate the projection method used in this paper for solving some generalized convex optimization problems. Moreover, the convergence of the proposed neural network in this paper will be another interesting future research direction, such as the finite-time convergence to feasible region. Furthermore, it is interesting to investigate the possibility of how the proposed neural network in this paper can be realized for real-time optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Fig. 1 .</head><label>21</label><figDesc>Fig. 1. Transient behaviors of the state variables of the neural network in (3) and (4) for solving (30) in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Fig. 2 .</head><label>22</label><figDesc>Fig. 2. Transient behaviors of the output variables of the neural network in (3) and (4) for solving (30) in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Fig. 3 . 2 Fig. 4 .</head><label>2324</label><figDesc>Fig. 3. Two-dimensional phase plot of output variables (x 1 , x 2 ) T of the neural network in (3) and (4) for solving (30) in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 Fig. 5 .</head><label>25</label><figDesc>Fig.5. Transient behaviors of the neural network in<ref type="bibr" target="#b46">[47]</ref> for solving<ref type="bibr" target="#b29">(30)</ref> in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 7 .Fig. 8 .</head><label>278</label><figDesc>Fig.7. Transient behaviors of the neural network in<ref type="bibr" target="#b40">[41]</ref> for solving<ref type="bibr" target="#b29">(30)</ref> in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Transient behaviors of the output variables of the neural network in (3) and (4) for solving (31) in Example 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 Fig. 11 .</head><label>411</label><figDesc>Fig. 11. Transient behaviors of the output variables of the neural network in (3) and (4) for solving (32) in Example 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Transient behaviors of the condition number along the output of the neural network in (8) and (9) for solving<ref type="bibr" target="#b32">(33)</ref> in Example 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF RELATED NEURAL NETWORKS IN TERMS OF MODEL COMPLEXITY AND CONVERGENCE CRITERIA</figDesc><table><row><cell>Model Type</cell><cell>Layer(s)</cell><cell>Neurons</cell><cell>Penalty Parameter(s)</cell><cell>Convergence Condition</cell><cell>Reference(s)</cell></row><row><cell>Lagrangian network</cell><cell>2</cell><cell>3n + m</cell><cell>No</cell><cell>f (x) is strictly convex</cell><cell>[49]</cell></row><row><cell>Primal-dual network</cell><cell>2</cell><cell>3n + m</cell><cell>No</cell><cell>f (x) is convex</cell><cell>[43]</cell></row><row><cell>Projection network</cell><cell>2</cell><cell>n + m</cell><cell>No</cell><cell>f (x) is (strictly) convex</cell><cell>[16], [38], [47]</cell></row><row><cell>Dual network</cell><cell>1</cell><cell>n + m</cell><cell>No</cell><cell>f (x) is strictly convex and quadratic</cell><cell>[45]</cell></row><row><cell>Simplified dual network</cell><cell>1</cell><cell>n</cell><cell>No</cell><cell>f (x) is strictly convex and quadratic</cell><cell>[31]</cell></row><row><cell>One-layer network</cell><cell>1</cell><cell>n</cell><cell>Yes</cell><cell>f (x) is convex on S</cell><cell>[30]</cell></row><row><cell>One-layer projection network</cell><cell>1</cell><cell>n</cell><cell>No</cell><cell>f (x) is convex (pseudoconvex) on X</cell><cell>This paper</cell></row><row><cell>1) State equation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of Q. Liu was supported in part by the National Natural Science Foundation of China under Grant 61105060, the Program for New Century Excellent Talents in University under Grant NCET-12-0114, the Natural Science Foundation of Jiangsu Province of China under Grant BK2011594, Fundamental Research Funds for the Central Universities Teachers. The work of J. Wang was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region, China, under Grant CUHK416811E and Grant CUHK416812E, and the National Natural Science Foundation of China under Grant 61273307.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Aubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cellina</surname></persName>
		</author>
		<title level="m">Differential Inclusions: Set-Valued Maps and Viability Theory</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimisation: Méthodes Numériques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auslender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Masson</publisher>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shetty</surname></persName>
		</author>
		<title level="m">Nonlinear Programming: Theory and Algorithms</title>
		<meeting><address><addrLine>Hoboken, New Jersey; New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Subgradient-based neural networks for nonsmooth nonconvex optimization problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1024" to="1038" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural network for quadratic optimization with bound constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pattison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="304" />
			<date type="published" when="1993-03">Mar. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Minimizing the condition number of a Gram matrix</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Womersley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="148" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network for non-smooth convex optimization problems with application to the identification of genetic regulatory networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="714" to="726" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of a class of neural networks for solving linear programming problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1995" to="2006" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Clarke</surname></persName>
		</author>
		<title level="m">Optimization and Nonsmooth Analysis</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Filippov</surname></persName>
		</author>
		<title level="m">Differential Equations with Discontinuous Righthand Sides, Mathematics and its applications (Soviet series)</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized lyapunov approach for convergence of neural networks with discontinuous or nonlipschitz activations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grazzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pancioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="99" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global convergence of neural networks with discontinuous neuron activations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst.-I</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1421" to="1435" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global exponential stability and global convergence in finite time of delayed neural networks with infinite gain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netwo</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1449" to="1463" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized neural network for nonsmooth nonlinear programming problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quincampoix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst.-I</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1741" to="1754" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convergence of neural networks for programming problems via a nonsmooth Łojasiewicz inequality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quincampoix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1471" to="1486" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel neural network for nonlinear convex programming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="613" to="621" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving pseudomonotone variational inequalities and pseudoconvex optimization problems using the projection neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1487" to="1499" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design of general projection neural networks for solving monotone linear variational inequalities and linear and quadratic optimization problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sys., Man Cybern.-B</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1414" to="1421" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An improved dual neural network for solving a class of quadratic programming problems and its k-winners-take-all application</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2022" to="2031" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new recurrent neural network for solving convex quadratic programming problems with an application to the k-winnerstake-all problem</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="654" to="664" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural networks for nonlinear programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="554" to="562" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An Introduction to Variational Inequalities and Their Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stampacchia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural network model for nonsmooth optimization over a compact convex subset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Symp. Neural Netw</title>
		<meeting>3rd Int. Symp. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="344" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A delayed neural network for solving linear projection equations and its analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="843" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for constrained pseudoconvex optimization and its application for portfolio optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network with a discontinuous activation function for linear programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1366" to="1383" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network with a discontinuous hard-limiting activation function for quadratic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="558" to="570" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for nonsmooth convex optimization subject to linear equality constraints</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf. Neural Inf. Proc</title>
		<meeting>15th Int. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finite-time convergent recurrent neural network with a hard-limiting activation function for constrained optimization with piecewise-linear objective functions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="601" to="613" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for constrained nonsmooth optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern.-B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1323" to="1333" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simplified dual neural network for quadratic programming with its kwta application</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1500" to="1510" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamical behaviors of delayed neural network systems with discontinuous activation functions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="683" to="708" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linear and quadratic programming neural network analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Maa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="580" to="594" />
			<date type="published" when="1992-07">Jul. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recurrent Neural Networks For Prediction: Learning Algorithms, Architectures, and Stability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimizing condition numbers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maréchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="935" to="947" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized convexity of functions and generalized monotonicity of set-valued maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Penot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Quang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="356" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple neural optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="1986-05">May 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A high performance neural network for solving nonlinear programming problems with hybrid constraints</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analysis and design of a recurrent neural network for linear programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst.-I</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="613" to="618" />
			<date type="published" when="1993-09">Sep. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deterministic annealing neural network for convex programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="641" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A novel recurrent neural network for solving nonlinear optimization problems with inequality constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1340" to="1353" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A projection neural network and its application to constrained optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst.-I</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="458" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A general methodology for designing globally convergent optimization neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1331" to="1343" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Global exponential stability of recurrent neural networks for solving optimization and related problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1017" to="1022" />
			<date type="published" when="2000-07">Jul. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A dual neural network for kinematic control of redundant robot manipulators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern.-B</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="154" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A general projection neural network for solving monotone variational inequalities and related optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="328" />
			<date type="published" when="2004-03">Mar. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A recurrent neural network for solving nonlinear convex programs subject to linear constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="386" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Subgradient-based neural networks for nonsmooth convex optimization problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst.-I</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2391" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lagrange programming neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Constantinides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst.-II</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="441" to="452" />
			<date type="published" when="1992-07">Jul. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
