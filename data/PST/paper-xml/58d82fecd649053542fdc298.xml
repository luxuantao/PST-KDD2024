<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Li</forename><surname>Chongyi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jichang</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE, Yanwei Pang, Senior Member, IEEE</roleName><forename type="first">Runmin</forename><surname>Cong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D78FE662EE2FEB6451E8B8D4B6CCE60B</idno>
					<idno type="DOI">10.1109/TIP.2016.2612882</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2612882, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2612882, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2612882, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Underwater image enhancement</term>
					<term>underwater image dehazing</term>
					<term>contrast enhancement</term>
					<term>scattering removal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images captured under water are usually degraded due to the effects of absorption and scattering. Degraded underwater images show some limitations when they are used for display and analysis. For example, underwater images with low contrast and color cast decrease the accuracy rate of underwater object detection and marine biology recognition. To overcome those limitations, a systematic underwater image enhancement method which includes an underwater image dehazing algorithm and a contrast enhancement algorithm is proposed. Built on a minimum information loss principle, an effective underwater image dehazing algorithm is proposed to restore the visibility, color, and natural appearance of underwater images. A simple yet effective contrast enhancement algorithm is proposed based on a kind of histogram distribution prior, which increases the contrast and brightness of underwater images. The proposed method can yield two versions of enhanced output. One version with relatively genuine color and natural appearance is suitable for display. The other version with high contrast and brightness can be used for extracting more valuable information and unveiling more details. Simulation experiment, qualitative and quantitative comparisons as well as color accuracy and application tests are conducted, respectively, to evaluate the performance of the proposed method. Extensive experiments demonstrate that the proposed method achieves better visual quality, more valuable information, and more accurate color restoration when compared with several state-of-the-art methods, even for underwater images taken under several challenging scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U NDERWATER image processing is challenging due to the physical properties of underwater environment. In many cases, captured underwater images are degraded by absorption and scattering. Fig. <ref type="figure" target="#fig_1">1</ref> shows a schematic diagram of underwater optical imaging. In an underwater scenario, the light received by a camera is mainly generated by three components: a direct component that reflects light from the objects; a forward scattering component that randomly deviates light on its ways to the camera; and a back scattering component that reflects light towards the camera before the light actually Manuscript received January 13, 2016; revised <ref type="bibr">April 18, 2016</ref>  The authors are with the school of Electronic Information Engineering, Tianjin University, Weijing Road 92, Tianjin, China (e-mail: lichongyi@tju.edu.cn; jcguo@tju.edu.cn; rmcong@tju.edu.cn; pyw@tju.edu.cn; neuwb@tju.edu.cn).</p><p>reaches the objects <ref type="bibr" target="#b0">[1]</ref>. An underwater image can be represented as a linear superposition of the above-discussed three components <ref type="bibr" target="#b1">[2]</ref>. The forward scattering component causes the blurring of image whereas the back scattering component masks details of the scenario. Additionally, the marine snow (i.e., macroscopic floating particles) brings unwanted noise and increases the effects of scattering. Degraded underwater images show some limitations when being used for display and extracting valuable information for further processing, such as marine biology and archaeology <ref type="bibr" target="#b2">[3]</ref>, marine ecological research <ref type="bibr" target="#b3">[4]</ref>, and aquatic robot inspection <ref type="bibr" target="#b4">[5]</ref>. Therefore, an effective method which can enhance underwater images for both display and analysis is meaningful, and thus desired. In this paper, we present a systematic method for single underwater image enhancement which can produce two versions of enhanced output. One version with natural appearance and relatively genuine color is suitable for display. The other version with increased contrast and brightness can be used for further analysis. First, we present an effective underwater image dehazing algorithm based on a minimum information loss principle and the optical properties of underwater imaging. The proposed underwater image dehazing algorithm can restore the visibility and color of degraded underwater images. Then, a simple yet effective histogram distribution prior is proposed to increase the contrast and brightness of the obtained haze-free underwater images. In this way, a pair of enhanced images is produced and can be used for different applications. Simulation experiment, qualitative and quantitative comparisons, and color accuracy and application tests are conducted, respectively, to assess the performance of the proposed method. Experimental results show that our haze-free output version is characterized by relatively genuine color, natural appearance, and good visibility. Moreover, our contrast enhanced output version with increased contrast and brightness is suitable for extracting more valuable information and unveiling more details. Furthermore, several images taken under challenging underwater scenes (e.g., deep water scene, turbid underwater scene, artificial lighting scene, low-light scene, and noise scene) are used to demonstrate that the proposed method achieves competitive results on both common underwater images and underwater images taken from the challenging scenes. This paper introduces the following main contributions:</p><p>• A new underwater image dehazing algorithm is proposed based on the minimum information loss principle and optical properties of underwater imaging. Compared with previous algorithms, our underwater image dehazing algorithm reduces the information loss of output images, takes different attenuation rates of different color light into account, and builds the relationship of medium transmission maps of three color channels by deriving the optical properties of underwater imaging. • A simple yet effective contrast enhancement algorithm is proposed based on an interesting observation on the histogram distributions of outdoor nature-scene images.</p><p>Using this histogram distribution prior with a haze-free output version, we can directly yield another output version with high contrast and brightness. Compared with previous contrast enhancement algorithms, our contrast enhancement algorithm is simple and time-saving. Besides, the results of our contrast enhancement algorithm has fewer artifacts and clearer details. • To the best of our knowledge, we are the first that enhance underwater images according to different applications (i.e., display and analysis). • Multiple images taken for the same scenario and complex information about the underwater environment are no longer required with the proposed method. The rest of the paper is organized as follows. In Section II, the previous work related to underwater image enhancement and restoration is presented. In Section III, the proposed methods are presented; In Section IV, the experimental results are evaluated and discussed. In Section V, a conclusion is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Numerous underwater image enhancement and restoration methods have emerged in the last few years. Traditional image enhancement methods (e.g., Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLA-HE) <ref type="bibr" target="#b5">[6]</ref>, Generalized Unsharp Masking (GUM) <ref type="bibr" target="#b6">[7]</ref>, and Probability-based method (PB) <ref type="bibr" target="#b7">[8]</ref>) are effective for common images. However, traditional image enhancement methods can not adaptively compensate the contrast degradation of underwater images. We found that direct applying traditional image enhancement methods to degraded underwater images ignores the fact that contrast degradation of underwater images is proportional to the distance of object-camera. Recently, many single image dehazing methods have been proposed. Tan <ref type="bibr" target="#b8">[9]</ref> improved the visibility of images captured in bad weather based on the observation that clear images have better contrast than images degraded by bad weather. Fattal <ref type="bibr" target="#b9">[10]</ref> exploited the fact that the surface shading and transmission functions are locally statistically uncorrelated. He et al. <ref type="bibr" target="#b10">[11]</ref> estimated the medium transmission based on dark channel prior that haze-free images have at least one color channel with a very low intensity. Several methods (e.g., <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref>) are based on the extension and modification of He's method. Ancuti et al. <ref type="bibr" target="#b13">[14]</ref> employed a multi-scale fusion strategy to improve the visual quality of images taken in the haze weather. Fattal <ref type="bibr" target="#b14">[15]</ref> proposed a novel method for single image dehazing, which takes advantage of a color-lines pixel regularity. While these single image dehazing methods have shown their effectiveness in terrestrial images, there is still room for improvement when these methods are used to enhance or restore underwater images. Generally, if these single image dehazing methods are directly applied to degraded underwater images, the results show little improvement because of the special properties of underwater imaging and lighting conditions. For underwater scenes, the assumption and priors used in these single dehazing methods may not always hold.</p><p>In recent years, we have witnessed significant advances in underwater image enhancement and restoration techniques. Iqbal et al. <ref type="bibr" target="#b15">[16]</ref> proposed an unsupervised color correction method (UCM) to achieve underwater image enhancement based on color balance and histogram stretching. Carlevaris-Bianco et al. <ref type="bibr" target="#b16">[17]</ref> proposed a simple prior that exploits the significant difference in attenuation among the three color channels of an underwater image to estimate the depth of a scene. As a result, the effects of light scattering can be removed. Chiang and Chen <ref type="bibr" target="#b17">[18]</ref> restored underwater images by combining a dehazing algorithm with a wavelength compensation. The effects of the haze from scattering can be reduced by the classical dark channel prior algorithm. According to the amount of attenuation of each wavelength, a reverse compensation is conducted to restore the distortion from color cast. Ancuti et al. <ref type="bibr" target="#b18">[19]</ref> proposed a novel strategy to enhance the visual quality of underwater images and videos based on the fusion principle. Serikawa and Lu <ref type="bibr" target="#b19">[20]</ref> solved the problems of scattering and color deviation for underwater images by compensating the attenuation discrepancy along the propagation path. Galdran et al. <ref type="bibr" target="#b20">[21]</ref> proposed a red channel method that can be considered as a variant of the dark channel prior algorithm, where color associated with short wavelengths is recovered and leads to a restoration of the lost contrast. Ghani and Isa <ref type="bibr" target="#b21">[22]</ref> modified and extended the UCM method. Chani and Isa's method significantly enhances contrast and minimizes under-enhanced and over-enhanced areas. Zhao et al. <ref type="bibr" target="#b22">[23]</ref> derived the inherent optical properties of water from background color and enhanced underwater images based on an underwater image formation model. Lu et al. <ref type="bibr" target="#b23">[24]</ref> developed a color lines-based ambient light estimator and a locally adaptive filtering algorithm to enhance underwater images. However, some assumptions made in the above-mentioned methods are not suitable for many underwater situations.</p><p>Additionally, some existing techniques use multiple images <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b25">[26]</ref> or specialized hardware devices <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref> to improve the quality of underwater images. Despite their effectiveness for underwater image enhancement and restoration, those existing techniques have several problems that potentially reduce their practical applicability. For instance, hardware devices used in those techniques may be relatively expensive and complex. In another instance, the techniques of using multiple images may be hard to obtain multiple images of the same scene.</p><p>In contrast to those traditional image enhancement methods and single image dehzaing methods which process images by non-adaptively compensating or apply the same equations to three channels of an image, the proposed method takes the optical properties of underwater imaging into account and adaptively compensates three degraded color channels of an underwater image using different strategies. Compared with the existing underwater image enhancement and restoration methods, the proposed method can produce two output versions according to different applications (i.e., display and analysis). Our underwater image dehazing algorithm can reduce information loss of the restored images and build the relationship of medium transmission maps of three color channels by deriving the optical properties from the background light. Our contrast enhancement algorithm based on a kind of statistics of outdoor nature-scene images is simple yet effective. Additionally, the proposed method can achieve better visual quality than the existing methods in terms of visibility, contrast, and color. Different from the existing global background light estimation algorithms, our global background light estimation algorithm which combines hierarchical searching with the properties of light travelling in the water is more effective and robust because it removes the effects of bright objects and suspended particles. Compared with existing techniques which use multiple images or specialized hardware devices, the proposed method does not require multiple images or complex information about underwater scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The proposed method is composed of two main parts: underwater image dehazing and underwater image contrast enhancement. A flowchart of the proposed method is shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Underwater Image Dehazing</head><p>Following the previous research <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref>, the simplified underwater optical imaging model can be described as:</p><formula xml:id="formula_0">I c (x) = J c (x)t c (x) + A c (1 -t c (x)), c ∈ {r, g, b},<label>(1)</label></formula><p>where x denotes a pixel, I(x) is the observed image, J(x) is the restored image, A is the global background light, and t(x)∈ [0,1] is the medium transmission map which represents the percentage of the scene radiance reaching the camera. The purpose of dehazing is to recover J(x), A, and t(x) from I(x). To solve this kind of ill-posed problem, the proposed underwater image dehazing algorithm includes three main processing steps: global background light estimation, medium transmission map estimation, and adaptive exposure map estimation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Global background light estimation:</head><p>The global background light A in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is often estimated as the brightest color in an underwater image. However, the assumption is not suitable in a scheme where objects are brighter than the global background light. To robustly estimate the global background light, we first use a hierarchical searching technique based on quad-tree subdivision <ref type="bibr" target="#b28">[29]</ref>, then remove the effects of suspended particles via the dark channel prior algorithm <ref type="bibr" target="#b10">[11]</ref>, and finally remove the disturbance of bright objects and determine the global background light according to the properties of light travelling in the water. An example to illustrate the global background light estimation algorithm is shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>First, an underwater image is divided into four rectangular regions <ref type="bibr" target="#b28">[29]</ref>. To select a flat background region, the score of each region is defined as the average pixel value subtracted by the standard deviation of the pixel values within the region. The candidate region is selected via searching for the region with the highest score. Then, to avoid the effects of suspended particles, we pick the top 0.1 percent brightest pixels in the dark channel of the candidate region inspired by He's research <ref type="bibr" target="#b10">[11]</ref>. According to the characteristic of underwater optical imaging, the blue light travels the longest distance in the water because of its shortest wavelength, followed by the green light and then the red light. Finally, among those brightest pixels, one of the pixels with the maximum bluered difference in the input image is selected as the global 1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. background light in order to remove the effects of bright objects. This kind of selection guarantees the robustness of the proposed global background light estimation algorithm.</p><p>2) Medium transmission map estimation: After estimating the global background light, the restored image depends on the selection of the medium transmission map. Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be rewritten as a medium transmission function: Observing Eq. ( <ref type="formula" target="#formula_1">2</ref>), we found that the medium transmission function maps an input pixel value I(x) to an output value J(x). As shown in Fig. <ref type="figure" target="#fig_4">4</ref> The truncated pixel values represent information loss and can be seen in the red regions in Fig. <ref type="figure" target="#fig_4">4</ref>  <ref type="bibr" target="#b28">[29]</ref>. Furthermore, the amount of information loss is proportional to the size of the red regions, which are in turn proportional to the slope 1 t . To reduce information loss, we attempt to select an optimal medium transmission map t. In order to minimize information loss, we first estimate the medium transmission map of the most degraded channel (i.e., red channel) of an underwater image. Besides, the medium transmission map of red channel is more representative for the underwater image. In fact, the medium transmission maps of green and blue channels are similar with those of image captured in outdoor haze scenes. Therefore, starting with estimating the medium transmission map of red channel is more suitable for restoring degraded underwater images. For simplicity, we define an information loss cost Infor loss in a local block B for red channel as the squared sum of truncated values, and can be expressed as:</p><formula xml:id="formula_1">J c (x) = 1 t c (x) (I c (x) -A c ) + A c . (<label>2</label></formula><formula xml:id="formula_2">Inf or loss = ∑ x∈B {(min(0, J r )) 2 +(max(0, J r -255)) 2 }. (3)</formula><p>Assuming that the medium transmission map has the same pixel value in the local block B. Replacing J r with Eq. ( <ref type="formula" target="#formula_1">2</ref>), Eq. ( <ref type="formula">3</ref>) can be rewritten as:</p><formula xml:id="formula_3">Inf or loss = ∑ x∈B {(min(0, I r -A r t r + A r )) 2 + (max(0, I r -A r t r + A r -255)) 2 }.<label>(4)</label></formula><p>In order not to yield any information loss, Eq. ( <ref type="formula" target="#formula_3">4</ref>) should satisfy the constraint as follows:</p><formula xml:id="formula_4">   min x∈B ( I r (x)-A r t r + A r ) ≥ 0 max x∈B ( I r (x)-A r t r + A r -255) ≤ 0.<label>(5)</label></formula><p>Eq. ( <ref type="formula" target="#formula_4">5</ref>) can be rewritten as:</p><formula xml:id="formula_5">   t r (x) ≥ min x∈B ( I r (x)-A r -A r ) t r (x) ≥ max x∈B ( I r (x)-A r 255-A r ).<label>(6)</label></formula><p>Accordingly, the estimated medium transmission map t r * can be expressed as:</p><formula xml:id="formula_6">t r * ≥ max[min x∈B ( I r (x) -A r -A r ), max x∈B ( I r (x) -A r 255 -A r )].<label>(7)</label></formula><p>In this paper, we select the minimum value which satisfies the constraint in the Eq. ( <ref type="formula" target="#formula_6">7</ref>) based on the experimental results that the large medium transmission map values may cause contrast cost. Observing Eq. ( <ref type="formula" target="#formula_6">7</ref>), we found that the estimated medium transmission map is determined by the min x∈B (I r (x)), max x∈B (I r (x)), and the global background light A r . Furthermore, according to the Lambert-Beer empirical law <ref type="bibr" target="#b29">[30]</ref>, the medium transmission map t also can be expressed as:</p><formula xml:id="formula_7">t c (x) = exp(-p c d(x)), c ∈ {r, g, b},<label>(8)</label></formula><p>where p c is the total attenuation coefficient, which can be decomposed as a linear superposition of absorption coefficient a c ab and scattering coefficient b c sc . The medium transmission map in Eq. ( <ref type="formula" target="#formula_7">8</ref>) only has correlation with the attenuation coefficient p and the distance of object-camera d. However, there is abundant texture information and edge information  <ref type="bibr" target="#b30">[31]</ref> proposed an effective method for extracting the structure from texture via relative total variation (RTV). To verify the availability of this RTV method, we present an example of comparisons between this RTV method and other texture removing methods in Fig. <ref type="figure">5</ref>. Moreover, the smoothed results of the min the estimated global background light A r . After obtaining the medium transmission map using Eq. ( <ref type="formula" target="#formula_6">7</ref>) block by block, we incorporate the guided filter <ref type="bibr" target="#b33">[34]</ref> to refine the medium transmission map because the block-based medium transmission map usually yields blocking artifacts. The refined transmission map t r * is a linear transform of the guidance image I r in a window η x (41 × 41) centered at pixel x:</p><formula xml:id="formula_8">t r * (x) = κ T x I r (x) + ν x , ∀x ∈ η x , (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>where κ x and ν x are linear coefficients in the window η x . In order to select the optimal parameters κ x and ν x , we minimize the difference between the refined medium transmission map t r * and the coarse medium transmission map t r * using the following cost function in the window η x :</p><formula xml:id="formula_10">E(κ x , ν x ) = ∑ x∈ηx {[κ T x I r (x) + ν x -t r * (x)] 2 }. (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>By substituting the solution to Eq. ( <ref type="formula" target="#formula_10">10</ref>) into Eq. ( <ref type="formula" target="#formula_8">9</ref>), the refined medium transmission map of red channel can be achieved. Some results of the refined medium transmission map of red channel are shown in Fig. <ref type="figure" target="#fig_8">7</ref>. Next, we estimate the medium transmission maps of green and blue channels. Note that we attempted to estimate the medium transmission maps of green and blue channels using the same constraint as that for the red channel. Unfortunately, the results are not good enough, even with some artifacts. Some results of using the same constraint to estimate the medium transmission maps of three color channels are shown in Fig. <ref type="figure" target="#fig_9">8</ref>. There are obvious artifacts in Fig. <ref type="figure" target="#fig_9">8</ref>, such as the regions around the fish and the background. Our further experiments show that the artifacts appear when the three restored channels obtained by the same constraint get together to form an color underwater image. The reason is that the pixels at the same position in different channels are uncorrelated. For this reason, we explore the correlation of the medium transmission maps of three color channels via deriving the optical properties of underwater imaging.</p><p>Inspired by <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b34">[35]</ref>, the total background light from the scene point to the camera can be obtained by integrating the attenuation from distance l=0 to l=d and casting up all scattering components at all directions. The total background light can be expressed as: <ref type="bibr" target="#b10">(11)</ref> where A c (d) is the total background light of the path-length d from the scene point to the camera, θ denotes all possible scattering angles for a certain scattering volume, ξ c (θ) is the volume scattering function, E a is the intensity of the ambient light that is simplified to a constant, p c is the total attenuation coefficient, k l represents the properties of the camera system and is a constant within a single image, and</p><formula xml:id="formula_12">A c (d) = ∫ d 0 ∫ θ ξ c (φ)E a exp(-p c l)k l dl dφ = A c (∞)[1 -exp(-p c d)],</formula><formula xml:id="formula_13">A c (∞) = k l E a p c ∫ θ ξ c (φ) dφ,<label>(12)</label></formula><p>where A c (∞) is the global background light (i.e., the background light from the infinity distance to the camera), ∫ θ ξ c (φ) dφ denotes all scattering events toward the camera's line of sight from all directions and is essentially identical with the definition of scattering coefficient b c sc . Since k l and E a are constants, we substitute b c sc for ∫ θ ξ c (φ) dφ, and then Eq. ( <ref type="formula" target="#formula_13">12</ref>) can be rewritten as:</p><formula xml:id="formula_14">A c (∞) ∝ b c sc p c . (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>Eq. ( <ref type="formula" target="#formula_14">13</ref>) indicates that the global background light is proportional to the scattering coefficient and inversely proportional to the total attenuation coefficient. Gould et al. <ref type="bibr" target="#b35">[36]</ref> found that the scattering coefficient has an approximately linear relationship with wavelengths of light in general water, and can be expressed as:</p><formula xml:id="formula_16">b c sc = (-0.00113λ c + 1.62517)τ (λ c ),<label>(14)</label></formula><p>where λ c is the wavelength of different color channels, and τ (λ c ) can be simplified to an identical value in different color channels. According to <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b36">[37]</ref>, λ r , λ g , and λ b are 620nm, 540nm, and 450nm, respectively, in general water. According to Eq. ( <ref type="formula" target="#formula_14">13</ref>) and Eq. ( <ref type="formula" target="#formula_16">14</ref>), ratios of the total attenuation coefficients between different color channels can be expressed as:</p><formula xml:id="formula_17">p g p r = b g sc A r (∞) b r sc A g (∞) = (-0.00113λ g + 1.62517)A r (∞) (-0.00113λ r + 1.62517)A g (∞) , (<label>15</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">p b p r = b b sc A r (∞) b r sc A b (∞) = (-0.00113λ b + 1.62517)A r (∞) (-0.00113λ r + 1.62517)A b (∞) ,<label>(16)</label></formula><p>where p g p r and p b p r are the green-red and blue-red total attenuation coefficient ratios, respectively.</p><p>According to Eq. ( <ref type="formula" target="#formula_7">8</ref>), the medium transmission map t c has an exponential correlation with the total attenuation coefficient p c . Thus, the medium transmission maps of the green and blue channels can be estimated as:</p><formula xml:id="formula_20">t g (x) = (t r (x)) p g p r , (<label>17</label></formula><formula xml:id="formula_21">)</formula><formula xml:id="formula_22">t b (x) = (t r (x)) p b p r . (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>Finally, the degraded underwater image can be restored using Eq. (2).</p><p>3) Adaptive exposure map estimation: Based on the observation that the dark and bright regions of underwater images become too dark or too bright after being restored by our underwater image dehazing algorithm, an adaptive exposure map <ref type="bibr" target="#b37">[38]</ref> is employed to adjust our results for better visual quality. The adaptive exposure map s(x) can be obtained by solving the following optimization problem:</p><formula xml:id="formula_24">min s ∑ x {[1 -s(x) Y J(x) Y I(x) ] 2 + σ[s(x) -1] 2 } + Φ(s),<label>(19)</label></formula><p>where s(x) is the adaptive exposure map, Y J is the illumination intensity of the restored image, Y I is the illumination intensity of the input image, σ = 0.3 is a constant, and Φ(•) is a smoothness regularization. This optimization problem can be approximately solved using a two-step approach. First, solve s(x) without the smoothness regularization, which has a closedform solution. Second, apply guided filter GF I <ref type="bibr" target="#b33">[34]</ref> to smooth this solution. Thus, we can obtain a fast approximate solution:</p><formula xml:id="formula_25">s(x) = GF I [ Y J(x) Y I(x) + σY 2 I(x) Y 2 J(x) + σY 2 I(x)</formula><p>].</p><p>The exposed output can be written as:</p><formula xml:id="formula_27">Output = J c (x). * s(x), c ∈ {r, g, b}, (<label>21</label></formula><formula xml:id="formula_28">)</formula><p>where J c is the restored image, and s(x) is the estimated adaptive exposure map. Fig. <ref type="figure" target="#fig_10">9</ref> shows the results of our underwater image dehazing algorithm. It is hard to distinguish the medium transmission maps of three color channels by color image form. Thus, we just present the medium transmission map of red channel. As shown in Fig. <ref type="figure" target="#fig_10">9</ref>, the estimated medium transmission map denotes the distance of objects-camera in a relatively accurate manner. Our underwater image dehazing algorithm can restore the visibility and color of the degraded underwater image. Additionally, the result with the adaptive exposure map can balance the too dark and too bright regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Underwater Image Contrast Enhancement</head><p>In some cases, we not only require visible and natural underwater images, but also require high contrast and brightness for underwater images. These underwater images with high contrast and brightness play a significant role in underwater object detection, fish classification, and so forth. In this part, a new algorithm is proposed to enhance contrast and brightness of the restored images produced by our underwater image dehazing algorithm. The proposed contrast enhancement algorithm is based on the statistics of histogram distribution of visually appealing natural-scene images. Our work is motivated by an interesting observation on the natural-scene images which have good contrast and brightness. We conducted a series of experiments to explore the relationship between the characteristics of natural-scene images and their histogram distributions. In our experiments, the histogram distribution prior is counted from five natural-scene image datasets. Some examples of the natural-scene images are shown in Fig. <ref type="figure" target="#fig_11">10</ref>. The five datasets are listed in Table <ref type="table" target="#tab_1">I</ref>.</p><p>The histogram distributions of natural-scene images and underwater images are shown in Fig. <ref type="figure" target="#fig_12">11</ref>. Intuitively, the histogram distributions of natural-scene images are wider and more consistent while the histogram distribution of each color channel of the underwater image is shifted in a horizontal  direction (e.g., the histogram of blue component concentrates on a brightest side, followed by the green component and then the red component) due to the effects of the absorption and scattering as well as the floating particles. The histogram distributions of natural-scene images inspire us to adjust the histogram distributions of underwater images to achieve better contrast and brightness. The average histogram distributions of natural-scene images are regarded as a template. Then, the approximate histogram matching is employed to adjust the histogram distributions of haze-free underwater images obtained by our underwater dehazing algorithm. Fig. <ref type="figure" target="#fig_13">12</ref> shows the results of our contrast enhancement algorithm based on a histogram distribution prior.</p><p>As shown in Fig. <ref type="figure" target="#fig_13">12</ref>, the histogram distributions of the contrast enhanced underwater image become wider and more consistent than those of the raw underwater image. Additionally, the contrast enhanced result preserves the genuine color and visibility of our dehazing result, and achieves higher contrast and clearer details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>To evaluate the performance of the proposed method, simulation experiment, qualitative comparison, quantitative comparison, color accuracy test, and application test are carried In the histogram distributions, the x-axis represents the gray levels, the y-axis represents the normalized frequency.</p><p>out, respectively. The methods used for comparisons include Single Image Dehazing (SID) method <ref type="bibr" target="#b9">[10]</ref>, Dark Channel Prior (DCP) method <ref type="bibr" target="#b10">[11]</ref>, Chromatism-based (CB) underwater image dehazing method <ref type="bibr" target="#b16">[17]</ref>, Wavelength Compensation and Dehazing (WCID) underwater image enhancement method <ref type="bibr" target="#b17">[18]</ref>, Histogram Equalization (HE) method, and Probability-based (PB) image enhancement method <ref type="bibr" target="#b7">[8]</ref>. The code of SID, HE, and PB methods are available. The results of DCP, CB, and WCID methods are obtained from the related papers and our re-produced code. SID and DCP methods are representative single image dehazing methods which employ the same imaging formation model as our method. Moreover, the classical DCP method is modified and extended to underwater image processing (e.g., CB and WCID). The proposed method is compared with the SID, DCP, CB, and WCID methods in order to demonstrate that our results are better than the results from a direct application of outdoor image dehazing methods to underwater images and the results of those state-of-theart underwater image enhancement methods. Additionally, the proposed method is compared with traditional image enhancement methods (e.g., HE and PB) in order to demonstrate that our method outperforms the traditional methods that were specially designed for common image enhancement. Since our method includes an underwater image dehazing part and a contrast enhancement post-processing part, we attempt to add contrast enhancement post-processing to outdoor image dehazing method (e.g., DCP) and underwater image enhancement methods (e.g., CB and WCID) for fair comparisons. The enhanced results of underwater images obtained by PB method not good enough although it is the state-of-the-art method for common image enhancement. Accordingly, HE method is used as a post-processing for the DCP, CB, and WCID methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Experiment</head><p>To verify the effectiveness and accuracy of our underwater image dehazing method, several underwater images are simulated from clear images with known medium transmission maps, global background light, and noise. However, these simulated underwater images still have some differences from real underwater images. In our simulation experiment, the values of medium transmission map of red channel and global background light are assigned. Then, the values of medium transmission maps of green and blue channels are generated according to Eq. ( <ref type="formula" target="#formula_20">17</ref>) and Eq. ( <ref type="formula" target="#formula_22">18</ref>). According to Eq. ( <ref type="formula" target="#formula_0">1</ref> As shown in Fig. <ref type="figure" target="#fig_14">13</ref>, our underwater image dehazing method can effectively remove the effect of haze and improve visibility and brightness. Furthermore, in order to further analyze the restored results, we treat the original image as ground truth and compute the mean square error (MSE) and peak signal-to-noise ratio (PSNR) between the restored results and the corresponding original images. The MSE and PSNR values are presented in Table <ref type="table" target="#tab_4">III</ref>. The results presented in Table <ref type="table" target="#tab_4">III</ref> demonstrate that our underwater image dehazing method can restore these simulated underwater images to some degree despite of the variation of noise, medium transmission map, and global background light. For underwater images Type I, the MSE becomes higher (PSNR becomes lower) when the variation of noise becomes higher, because our underwater image dehazing method can not address the problem of noise. For underwater images Type II and Type III, simulation parameters significantly affect accuracy of the restored results. Results show that smaller values of medium transmission map are more difficult to be accurately estimated. This also leads to the conclusion that seriously degraded underwater images are difficult to be restored to original images. Additionally, for underwater image Type II, the original image is captured in outdoor scene. The simulated images of underwater image Type II are affected by natural lighting and simulated illumination. This affects the accuracy of our global background light estimation algorithm. Thus, the restored results of underwater image Type II are not good enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Comparison</head><p>As shown in Fig. <ref type="figure" target="#fig_16">14</ref>(k), our dehazing method can successfully remove the effects of scattering and restore visibility because of the accurate estimation of the global background light and medium transmission maps. Furthermore, the results of our dehazing method are neither over-enhanced nor underenhanced, and even more natural than the results of other methods. Our global background light estimation algorithm plays a significant role in our dehazing method. The estimated global background light affects the estimation of the medium transmission maps of the three color channels. Assuming that the estimation of light is wrong (e.g., light is often estimated as the brightest color in an underwater image, A c is approximate to 255), the estimated medium transmission map of red channel is prone to 1 according to Eq. <ref type="bibr" target="#b6">(7)</ref>. At the same time, the estimated medium transmission maps of green and blue channels are also prone to 1 according to Eq. ( <ref type="formula" target="#formula_20">17</ref>) and Eq. ( <ref type="formula" target="#formula_22">18</ref>). Thus, the restored image is close to the input degraded image according to Eq. ( <ref type="formula" target="#formula_1">2</ref>). It means that 1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.     <ref type="bibr" target="#b43">[44]</ref> which was taken at Bali, and the depths of where those images were taken range from 18-22m, (b) SID method, (c) DCP method, (d) CB method, (e) WCID method, (f) HE method, (g) PB method, (h) DCP+HE method, (i) CB+HE method, (j) WCID+HE method, (k) our dehazing method, and (l) our contrast enhancement method (the results of our contrast enhancement method are achieved by post-processing the results of our dehazing method using histogram distribution prior). (Best viewed on high-resolution display with zoom-in.)</p><formula xml:id="formula_29">(i) (j) (k)<label>(d) (e) (f) (g) (h)</label></formula><p>our dehazing method has little effect on the input underwater image. Fortunately, our global background light estimation algorithm is effective and robust since it can remove the effects of bright objects and suspended particles. As shown in Fig. <ref type="figure" target="#fig_16">14(b</ref>)-(d), although the haze in the raw underwater images are removed by SID, DCP, and CB methods, the visibility, color, and details are not good enough because the attenuated energy is not compensated individually according to different wavelengths. For example, the seaweed and fish in the results of SID, DCP, and CB methods are too dark. As shown in Fig. <ref type="figure" target="#fig_16">14</ref>(e), although WCID method is able to increase the contrast and unveil color of the raw underwater images, its results are not as natural as our dehazing results. Fig. <ref type="figure" target="#fig_16">14</ref>(f) shows that HE method's results contain over-enhanced regions and artifacts. Fig. <ref type="figure" target="#fig_16">14</ref>(g) shows that PB method can improve the brightness of the raw underwater images, but it can not remove the effects of scattering. The results of the traditional image enhancement methods are not satisfactory because those methods do not take the underwater imaging formation model into consideration. Therefore, the qualitative comparison demonstrates that our dehazing results are the most visually appealing among the compared results. The visual appeal is significant when underwater images are used for display. As shown in Fig. <ref type="figure" target="#fig_16">14</ref>(h)-(j), the results produced by the methods with post-processing (i.e., DCP+HE, CB+HE, and WCID+HE) have improved the brightness and contrast. However, the raw underwater images are over-enhanced and brought artifacts and noise by the post-processing, especially for the bright background regions. Fig. <ref type="figure" target="#fig_16">14</ref>(l) shows that our contrast enhancement method can effectively increase contrast and brightness, unveil more details, and preserve the natural appearance of our dehazing results. Meanwhile, compared with the methods with post-processing, our contrast enhancement method brings fewer artifacts and the background regions are clearer. Thus, the qualitative comparison demonstrates that the results of our contrast enhancement method have the better contrast and brightness, fewer artifacts, and clearer details.</p><p>Additionally, we further tested the proposed method using underwater images captured under challenging underwater scenes (i.e., turbid scene, deep water scene, artificial lighting scene, low-light scene, and noise scene) in order to demonstrate the effectiveness and robustness of the proposed method. In Fig. <ref type="figure" target="#fig_16">14</ref>, we have demonstrated that the methods with postprocessing can not achieve visually appealing results, and brings artifacts and noise, even in the common underwater scenes. Due to the limited space, we do not compare the proposed method with those methods with post-processing in the challenging scenes comparison. The compared results can be seen in Fig. <ref type="figure" target="#fig_17">15</ref>.</p><p>For the underwater image taken under turbid scene, SID, DCP, CB, WCID, and PB methods have little effect on this image because it is seriously degraded by low contrast and color cast. HE method can improve the contrast of this image, but it introduces considerable artifacts. Our dehazing and contrast enhancement methods increase the contrast and visibility. Our contrast enhancement method even unveils more color details of this image. For the underwater image captured at the depths of 26 meters, SID, DCP, CB, WCID, and our dehazing and contrast enhancement methods produce relatively clear results, especially for the front ground. HE method overenhances this image while PB method aggravates the effect of scattering. For the underwater image with artificial lighting, physics-based methods (i.e., SID, DCP, CB, WCID, and our dehazing) produce natural and visible results. Our contrast  <ref type="bibr" target="#b34">[35]</ref>, artificial lighting scene <ref type="bibr" target="#b43">[44]</ref>, low-light scene (image obtained from the Internet), and noise scene (adding gaussian random noise of zero mean noise with 0.01 variance) <ref type="bibr" target="#b43">[44]</ref>, respectively, (b) SID method, (c) DCP method, (d) CB method, (e) WCID method, (f) HE method, (g) PB method, (h) our dehazing method, and (l) our contrast enhancement method. (Best viewed on high-resolution display with zoom-in.) enhancement method improves contrast and unveil details of this images via enlarging the artificial lighting. However, the results of HE and PB methods are not satisfactory. In terms of the physics-based methods, the most challenging scene is the low-light scene because they often saturate pixels that interfere with the estimation of the global background light and medium transmission maps. Taking our dehazing method for example, in such cases, the estimated medium transmission map is approximate to 1 according to Eq. ( <ref type="formula" target="#formula_6">7</ref>) because the pixel values of the input image are approximate to 0 (i.e., the input image almost is dark). According to Eq. ( <ref type="formula" target="#formula_1">2</ref>), if the medium transmission maps are approximate to 1, the dehazing method has little effect on the input image. Therefore, compared with HE, PB, and our contrast enhancement methods, physics-based methods show limitations when they are used for processing underwater images captured under low-light conditions. Processing image captured in a dark environment is beyond the discussion scope of this paper. In general, it needs the help of special equipments or strategies. For the underwater image with random noise, the noise does not affect the performance of the physics-based methods because those methods estimate a medium transmission map based on image blocks. However, HE method brings artifacts due to enlarged noise. Our contrast method is prone to enlarging noise although it can improve the contrast and brightness. For all of the challenging scenes, PB method shows unsatisfactory results because it is specially designed for common images and does not take the complicated underwater imaging and lighting conditions into consideration. In sum, for the above-mentioned challenging scenes, our dehazing and contrast enhancement methods are more robust than the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Comparison</head><p>To our knowledge, there is no underwater image databases that has been acknowledged. Hence, different researchers utilize different strategies to evaluate their results. We consider the main goal of image enhancement as to emphasize the features, contrast, color, and visibility for display and analysis. In this part, we quantify the performance of the proposed method using several evaluation metrics. Following other researchers, MSE, PSNR, entropy, patch-based contrast quality index (PCQI) <ref type="bibr" target="#b44">[45]</ref>, underwater color image quality evaluation (UCIQE) <ref type="bibr" target="#b45">[46]</ref>, and processing time (PT) are employed.</p><p>The MSE and PSNR are normally used to assess the image noise. Since there is no available ground truth for real underwater images, we calculate the MSE and PSNR values between the raw underwater images and the corresponding resultant images. Despite the values of the MSE and PSNR are not very objective in this case, those values still have some guiding significance for evaluating the performance of different methods. The lower MSE and higher PSNR values indicate less noise and more valuable information of the resultant images. Generally, the entropy is used to measure the image information content. Higher entropy values of an image indicate more information contained in that image. Wang et al. <ref type="bibr" target="#b44">[45]</ref> proposed a method based on an adaptive representation of local patch structure for providing accurate predictions on the human perception of contrast variations. The higher PCQI values indicate the image has better contrast. Recently, an underwater color image quality evaluation metric (UCIQE) <ref type="bibr" target="#b45">[46]</ref> has been proposed by Yang and Sowmya for quantifying the non-uniform color cast, blurring, and low contrast of underwater images. The higher UCIQE values indicate the image has better balance among the chroma, saturation, and contrast. To compare the processing time, the abovementioned methods are implemented on a Windows 7 PC with Inter(R) Core(TM) i5-4260U CPU@1.40GHz, 4.00GB RAM, running Matlab2014. The processing time is the average time of processing an underwater image with size 720×1280.</p><p>To assess the robustness and effectiveness of the proposed method, we extract 500 underwater images from a video <ref type="bibr" target="#b43">[44]</ref> which is also used by other researchers for comparing their results, and then compare the results of different methods on those extracted images. Those underwater images were taken at depths of 18-22m near Bali, and some of them have been shown in Fig. <ref type="figure" target="#fig_9">8</ref>, Fig. <ref type="figure" target="#fig_10">9</ref>, and Fig. The highest UCIQE score means that our dehazing method can effectively balance the chroma, saturation, and contrast of the restored underwater images, and then produce most visually appealing results. Our contrast enhancement method outperforms the other methods in terms of entropy and PCQI values, which means that our contrast enhancement method can effectively increase the valuable information and change the most of contrast. For processing time, HE method achieves best value owing to its low complexity. The average processing time of our dehazing method is 0.7639s. The most time-consuming processing in our dehazing method is the guided filtering. Moreover, the guided filter is used twice (refining medium transmission map of red channel and smoothing the adaptive exposure map). The processing time of our contrast enhancement method includes the time of dehazing and contrast enhancement. In practice, the processing time spent on our contrast enhancement is just 0.2s. As shown in Table <ref type="table" target="#tab_6">V</ref>, even for the underwater images taken under challenging scenes, the quantitative results have the similar trend as the those of common underwater images. The quantitative values of the images of challenging scenes further demonstrate the robustness and effectiveness of the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Color Accuracy Test</head><p>To verify the color accuracy of the proposed method, we took a ColorChecker 24 X-Rite Chart (21 cm×29.7 cm) image at the depths of 8m under the Pacific Ocean near Taiwan. Due to the limited space, we compare the proposed method only with the physics-based methods (i.e., SID, DCP, CB, and WCID). Furthermore, the MSE and PSNR evaluation metrics are used to quantify the color accuracy of different methods since the standard ColorChecker Chart image (ground truth) is available. First, we extract the color block part in the results. Then, the size and orientation of the color block part are adjusted to match those of the the standard ColorChecker Chart image. Finally, the values of MSE and PSNR are     <ref type="table" target="#tab_8">VI</ref>, our contrast enhancement method achieves the best performance among the compared methods in terms of the values of MSE and PSNR. Moreover, our dehazing method ranks the second. The color accuracy test demonstrates that our method can restore underwater images to a relatively genuine color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Application Test</head><p>In this part, we attempt to utilize an application (i.e., SIFT local feature points matching <ref type="bibr" target="#b46">[47]</ref>) to further evaluate the performance of the proposed method. SIFT local feature points matching plays an important role in many computer vision applications, such as object recognition, object detection, and so forth. SIFT local feature points matching evaluation for underwater images was performed for the first time by Ancuti et al. <ref type="bibr" target="#b18">[19]</ref>. We simply show an example of this application test due to the limited space. The results can be seen in Fig. <ref type="figure" target="#fig_22">17</ref>.</p><p>Fig. <ref type="figure" target="#fig_22">17</ref> shows that SID, DCP, CB, WCID, and PB methods have little effect on this raw underwater image pair. HE, DCP+HE, CB+HE, WCID+HE, and our dehazing and contrast enhancement methods improve the contrast of this underwater image pair. Our contrast enhancement method unveils more color and brightness. The methods with HE post-processing bring artifacts in the background regions of resultant images due to the effects of noise. From Fig. <ref type="figure" target="#fig_22">17</ref>  l), the good SIFT local feature matching points are 5, 9, 9, 10, 12, 7, 0, 11, 10, 8, 12, and 12, respectively. HE and the methods with HE post-processing produce many mismatching points because the enlarged noise is treated as a feature point. The application test demonstrates that the proposed method performs well when it is applied to computer vision application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we introduced an underwater image enhancement method which can produce a pair of output versions. The proposed method includes an underwater image dehazing algorithm and a contrast enhancement algorithm. The dehazing algorithm can minimize the information loss of the enhanced underwater images based on the minimum information loss principle and optical properties of underwater imaging. The contrast enhancement algorithm based on histogram distribution prior can effectively increase contrast and brightness. Moreover, the proposed method introduces fewer artifacts and 1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  noise. Extensive experiments demonstrate that our dehazing results are characterized by relatively genuine color, natural appearance, and improved visibility. Meanwhile, our contrast enhancement results with increased contrast and brightness can be used for unveiling more details and valuable information.</p><p>Despite of the good performance, our method nevertheless has some limitations. First, our dehazing algorithm unveils little detail and color of the underwater images taken under several challenging scenes (e.g., low-light condition) due to the limited visible light. Second, we only take the distance between the object and the camera into account in our dehazing model. However, the distance from the object to the water surface has significant effects on underwater imaging, which is yet to be considered. Third, the proposed method can not remove the effects of noise. For future work, we intend to address the above-mentioned problems. Additionally, we will establish an underwater image database for the development of underwater image and video analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and June 21, 2016; accepted September 11, 2016. This work was supported in part by the National Key Basic Research Program of China (2014CB340403) and the Natural Science Foundation of Qinghai Province of China (2015-ZJ-721). (Corresponding author: Jichang Guo.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of underwater optical imaging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example to illustrate the global background light estimation algorithm. (a) Raw underwater image, (b) the result of the quad-tree subdivision, where the red rectangle represents the candidate region, (c) the result of searching for the brightest pixels in the dark channel, where red pixels are the brightest pixels in the dark channel, and (d) the result of searching for the pixels with the maximum blue-red difference, where red pixels in the red rectangle are the candidates of the global background light.</figDesc><graphic coords="4,80.01,132.80,84.92,56.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of the medium transmission function. Input pixel values are mapped to output pixel values according to the medium transmission function, depicted by the black line. The red regions denote the information loss due to the truncation of output pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, input values in [α, β] are mapped to output values in the full dynamic range [0, 255], where the medium transmission map t determines the valid input range [α, β]. When some input values lie outside of the range [α, β] , the mapped output values do not belong to the valid output range [0, 255]. In such cases, the underflow or overflow occurs in some pixel values, which are truncated to 0 or 255.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>in the min x∈B (I r (x)) and max x∈B (I r (x)), which interfere with the accuracy of the estimation of medium transmission map. Hence, we first remove the texture and edge in the min x∈B (I r (x)) and max x∈B (I r (x)), and preserve the depth information. Xu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig.5. An example to illustrate the effectiveness of the RTV method. (a) Raw texture image, (b) the result of TV method<ref type="bibr" target="#b31">[32]</ref>, (c) the result of L 0 smoothing method<ref type="bibr" target="#b32">[33]</ref>, and (d) the result of the RTV method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The medium transmission map of red channel. (a) Raw underwater image, (b) the coarse medium transmission map, and (c) the refined medium transmission map. In the medium transmission map, the color bar represents different pixel values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Restored results via the medium transmission maps of three color channels obtained by the same constraint. (Best viewed on high-resolution display with zoom-in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Results of our underwater image dehazing algorithm. (a) Raw underwater image, (b) the refined medium transmission map of red channel, (c) the dehazing result, and (d) the dehazing result with the adaptive exposure map. (Best viewed on high-resolution display with zoom-in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Examples of natural-scene images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Histogram distributions. (a) The average histogram distributions obtained from five natural-scene image datasets, and (b) a representative example of the histogram distributions of underwater images. In the histogram distributions, the x-axis represents the gray levels, the y-axis represents the normalized frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Results of our underwater image contrast enhancement algorithm. (a) From left to right: raw underwater image, our dehazing result, and our contrast enhancement result, (b)-(d) histogram distributions of raw underwater image, our dehazing result, and our contrast enhancement result, respectively.In the histogram distributions, the x-axis represents the gray levels, the y-axis represents the normalized frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Simulated underwater images (up) and the results generated by the proposed underwater image dehazing method (bottom). (a) Type I, (b) Type II, and (c) Type III.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>70,0.81,0.82)/(0.80,0.92,0.94)/(0.90,1.00,1.00) Type III Noise variance Medium transmission map (red channle) Global background light (R,G,B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Qualitative comparison. (a) Raw underwater images with size 720×1280 extracted from a video<ref type="bibr" target="#b43">[44]</ref> which was taken at Bali, and the depths of where those images were taken range from 18-22m, (b) SID method, (c) DCP method, (d) CB method, (e) WCID method, (f) HE method, (g) PB method, (h) DCP+HE method, (i) CB+HE method, (j) WCID+HE method, (k) our dehazing method, and (l) our contrast enhancement method (the results of our contrast enhancement method are achieved by post-processing the results of our dehazing method using histogram distribution prior). (Best viewed on high-resolution display with zoom-in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Comparison on challenging underwater scenes. (a) Underwater images captured under challenging scenes, from left to right: turbid scene [19], deep water scene (26 meters depths)<ref type="bibr" target="#b34">[35]</ref>, artificial lighting scene<ref type="bibr" target="#b43">[44]</ref>, low-light scene (image obtained from the Internet), and noise scene (adding gaussian random noise of zero mean noise with 0.01 variance)<ref type="bibr" target="#b43">[44]</ref>, respectively, (b) SID method, (c) DCP method, (d) CB method, (e) WCID method, (f) HE method, (g) PB method, (h) our dehazing method, and (l) our contrast enhancement method. (Best viewed on high-resolution display with zoom-in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Color accuracy test. (a) The standard ColorChecker 24 X-Rite Chart image, (b) the ColorChecker Chart image taken at the depth of 8 meters, (c) SID method, (d) DCP method, (e) CB method, (f) WCID, (g) our dehazing method, and (h) our contrast enhancement method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Comparing Fig. 16 (</head><label>16</label><figDesc>a) with Fig. 16(b), some color changes appear in the underwater ColorChecker Chart image due to the effects of light scattering and absorbtion. Fig. 16(c)-(e) show that SID, DCP, and CB methods have little effect on the color of the ColorChecker Chart image. As shown in Fig. 16(f)-(h), WCID and our dehazing and contrast enhancement methods unveil more color details of the underwater Col-orChecker Chart image than the other methods. Unfortunately, the solutions of SID, DCP, CB, and WCID cause color cast, such as the color in the third row and third column in the underwater ColorChecker Chart image. In Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>(a) to Fig.17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. Application test. (a) Raw underwater image pair<ref type="bibr" target="#b18">[19]</ref>, (b) SID method, (c) DCP method, (d) CB method, (e) WCID method, (f) HE method, (g) PB method, (h) DCP+HE, (i) CB+HE method, (j) WCID+HE method, (k) our dehazing method, and (l) our contrast enhancement method. (Best viewed on high-resolution display with zoom-in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Fig. 17. Application test. (a) Raw underwater image pair<ref type="bibr" target="#b18">[19]</ref>, (b) SID method, (c) DCP method, (d) CB method, (e) WCID method, (f) HE method, (g) PB method, (h) DCP+HE, (i) CB+HE method, (j) WCID+HE method, (k) our dehazing method, and (l) our contrast enhancement method. (Best viewed on high-resolution display with zoom-in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Input Image Global Background Light Estimation Medium Transmission Map Estimation Adaptive Exposure Map Estimation Quad-tree Subdivision Red Pixels Denote Candidates of Background Light Eestimated Background Light Dark Channel Light Channel Remove Texture Estimated Transmission Map of Red Channel Refined Transmission Map of Red Channel Refined Transmission Map of Green Channel Refined Transmission Map of Blue Channel Recovered Output Image Dehazing Version Adaptive Exposure Map Histogram Distribution Prior Contrast Enhancement Version</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I NATURAL</head><label>I</label><figDesc>-SCENE IMAGE DATASETS: SOURCES AND SIZES.</figDesc><table><row><cell>Dataset</cell><cell>[39]</cell><cell>[40]</cell><cell>[41]</cell><cell>[42]</cell><cell>[43]</cell><cell>Total</cell></row><row><cell>Sizes</cell><cell>1005</cell><cell>832</cell><cell>6033</cell><cell>1204</cell><cell>2688</cell><cell>11762</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Citation information: DOI 10.1109/TIP.2016.2612882, IEEE Transactions on Image Processing</figDesc><table /><note><p>IEEE TRANSACTIONS ON IMAGE PROCESSING</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II SIMULATION</head><label>II</label><figDesc>PARAMETERS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>14. Table IV summarizes the average MSE, PSNR, entropy, PCQI, UCIQE, and PT results on the 500 tested underwater images with a size of 720×1280. Furthermore, Table V presents the average evaluation values of the challenging results shown in Fig. 15. In the Tables, the values in bold represent the best results. For convenience, our dehazing method and our contrast enhancement method are represented by ODM and OCM in Tables. As shown in Table IV, our dehazing method stands out among the compared methods in terms of MSE, PSNR, and UCIQE values. The best MSE and PSNR values indicate that our dehazing method introduces fewer artifacts and noise.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V QUANTITATIVE</head><label>V</label><figDesc>RESULTS FOR THE IMAGES OF CHALLENGING SCENES.</figDesc><table><row><cell>Method</cell><cell>MSE</cell><cell cols="2">PSNR (dB) Entropy</cell><cell>PCQI</cell><cell>UCIQE</cell></row><row><cell>SID</cell><cell>1690.7</cell><cell>15.851</cell><cell>5.6293</cell><cell>0.6555</cell><cell>0.4249</cell></row><row><cell>DCP</cell><cell>638.06</cell><cell>20.082</cell><cell>6.8610</cell><cell>0.8956</cell><cell>0.4549</cell></row><row><cell>CB</cell><cell>1168.5</cell><cell>17.455</cell><cell>6.4079</cell><cell>0.8314</cell><cell>04543</cell></row><row><cell>WCID</cell><cell>1013.3</cell><cell>18.073</cell><cell>6.7445</cell><cell>0.7535</cell><cell>0.4391</cell></row><row><cell>HE</cell><cell>1556.3</cell><cell>16.209</cell><cell>7.1564</cell><cell>1.0022</cell><cell>0.4474</cell></row><row><cell>PB</cell><cell>1130.6</cell><cell>17.598</cell><cell>6.5846</cell><cell>0.7447</cell><cell>0.3828</cell></row><row><cell>ODM</cell><cell>551.44</cell><cell>20.716</cell><cell>6.9249</cell><cell>0.8252</cell><cell>0.4809</cell></row><row><cell>OCM</cell><cell>2984.8</cell><cell>13.382</cell><cell>7.7986</cell><cell>0.9052</cell><cell>0.5939</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>RESULTS IN TERMS OF THE AVERAGE VALUES OF MSE, PSNR, ENTROPY, PCQI, UCIQE, AND PT.</figDesc><table><row><cell>Method</cell><cell>MSE</cell><cell cols="2">PSNR (dB) Entropy</cell><cell>PCQI</cell><cell>UCIQE</cell><cell>PT (s)</cell></row><row><cell>SID</cell><cell>2409.5</cell><cell>14.312</cell><cell>7.2721</cell><cell>0.7821</cell><cell>0.5752</cell><cell>0.7555</cell></row><row><cell>DCP</cell><cell>2749.3</cell><cell>13.739</cell><cell>6.7641</cell><cell>0.9213</cell><cell>0.4610</cell><cell>0.4042</cell></row><row><cell>CB</cell><cell>2932.9</cell><cell>13.458</cell><cell>6.3434</cell><cell>0.5839</cell><cell>0.5198</cell><cell>0.5547</cell></row><row><cell>WCID</cell><cell>1458.6</cell><cell>16.491</cell><cell>7.6347</cell><cell>0.5215</cell><cell>0.5098</cell><cell>3.4589</cell></row><row><cell>HE</cell><cell>2654.0</cell><cell>14.677</cell><cell>7.6807</cell><cell>1.0491</cell><cell>0.5161</cell><cell>0.0119</cell></row><row><cell>PB</cell><cell>1431.0</cell><cell>16.575</cell><cell>7.0453</cell><cell>0.8853</cell><cell>0.5075</cell><cell>4.5847</cell></row><row><cell>DCP+HE</cell><cell>1330.2</cell><cell>16.892</cell><cell>7.4019</cell><cell>0.8360</cell><cell>0.5784</cell><cell>0.7674</cell></row><row><cell>CB+HE</cell><cell>2283.5</cell><cell>14.545</cell><cell>7.2770</cell><cell>0.9116</cell><cell>0.6245</cell><cell>0.5669</cell></row><row><cell cols="2">WCID+HE 1167.5</cell><cell>17.458</cell><cell>7.6928</cell><cell>0.8145</cell><cell>0.5998</cell><cell>3.4708</cell></row><row><cell>ODM</cell><cell>658.91</cell><cell>19.941</cell><cell>7.4900</cell><cell>0.9787</cell><cell>0.6451</cell><cell>0.7639</cell></row><row><cell>OCM</cell><cell>742.58</cell><cell>19.423</cell><cell>7.7763</cell><cell>1.1126</cell><cell>0.5237</cell><cell>0.9675</cell></row><row><cell cols="3">computed using the adjusted color block part and the standard</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ColorChecker Chart image.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI QUANTITATIVE</head><label>VI</label><figDesc>COLOR ACCURACY TEST IN TERMS OF THE VALUES OF MSE AND PSNR.</figDesc><table><row><cell>Method</cell><cell>MSE</cell><cell>PSNR (dB)</cell></row><row><cell>SID</cell><cell>1326.5</cell><cell>16.904</cell></row><row><cell>DCP</cell><cell>1159.3</cell><cell>17.489</cell></row><row><cell>CB</cell><cell>1066.2</cell><cell>17.852</cell></row><row><cell>WCID</cell><cell>826.33</cell><cell>18.959</cell></row><row><cell>ODM</cell><cell>804.59</cell><cell>19.075</cell></row><row><cell>OCM</cell><cell>425.68</cell><cell>21.840</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Miao Yang and Arocot Sowmya for providing the source code of the UCIQ evaluation metric. The authors would like to thank the anonymous reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A computer model for underwater camera systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcglamery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Underwater image processing: state of the art of restoration and image enhancement methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Corchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="volume">746052</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Applications of georeferenced underwater photo mosaics in marine biology and archaeology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ludvigsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sortland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Johnsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oceanography</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="140" to="149" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition of fish species by colour and shape</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J C</forename><surname>Strachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2" to="10" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Color correction of underwater images for aquatic robot inspection</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Torres-Mndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMMCVPR, 2005</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>EMMCVPR, 2005</meeting>
		<imprint>
			<biblScope unit="volume">3757</biblScope>
			<biblScope unit="page" from="60" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Academic</publisher>
			<biblScope unit="page" from="474" to="485" />
		</imprint>
	</monogr>
	<note>Graphics Gems IV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A generalized unsharp masking algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1249" to="1261" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4965" to="4977" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast algorithm for dark channel prior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="1826" to="1828" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image dehazing with a physical model and dark channel prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image dehazing by multiscale fusion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3271" to="3282" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph., SIG-GRAPH</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing the low quality images using unsupervised colour correction method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Odetayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Systems Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1703" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Initial results in underwater single image dehazing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlevaris-Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE OCEANS</title>
		<meeting>IEEE OCEANS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by wavelength compensation and dehazing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1756" to="1769" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing underwater images and videos by fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underwater image dehazing using joint trilateral filter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Serikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic red-channel underwater image restoration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Picn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image R</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="132" to="145" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Underwater image quality enhancement through integrated color model with Rayleigh distribution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A M</forename><surname>Isa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deriving inherent optical properties from background color and underwater image enhancement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ocean Eng</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contrast enhancement for images in turbid water</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized image recovery in scattering media</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Averbuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1655" to="1660" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Divergent-beam lidar imaging in turbid water</title>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Laser in Eng</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualization and image enhancement for multistatic underwater laser line scan system using image-based rendering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dalgleish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vuorenkoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean. Eng</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="566" to="580" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimized contrast enhancement for real-time image and video dehazing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image R</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="410" to="425" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Can the Lambert Beer law be applied to the diffuse attenuation coefficient of ocean water?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Lim. Oceanography</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1389" to="1409" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phy. D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="269" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image smoothing via L0 gradient minimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph., SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clear underwater vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral dependence of the scattering coefficient in case 1 and case 2 waters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arnone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martinolich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2377" to="2383" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Physics-based approach to color image enhancement in poor visibility conditions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oakley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2460" to="2467" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Investigating haze-relevant features in a learning framework for image dehazing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2995" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.vision.ee.ethz.ch/showroom/zubud/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning models for object recognition from natural language descriptions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<ptr target="http://www.comp.leeds.ac.uk/scs6jwks/dataset/leedsbutterfly/" />
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<ptr target="http://www.vision.caltech.edu/visipedia/CUB-200.html" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Statistics for optimal point prediction in natural images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perry</surname></persName>
		</author>
		<ptr target="http://natural-scenes.cps.utexas.edu/db.shtml" />
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://groups.csail.mit.edu/vision/SUN/" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<ptr target="http://www.youtube.com/user/bubblevision" />
		<title level="m">Bubblevision underwater image</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A patch-structure representation method for quality assessment of contrast changed images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yeganeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2387" to="2390" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An underwater color image quality evaluation metric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6062" to="6067" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chong-yi Li is pursuing his Ph.D. degree with the school of Electronic Information Engineering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
			<pubPlace>Tianjin, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Tianjin University</orgName>
		</respStmt>
	</monogr>
	<note>His current research focuses on image processing</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">He is a Professor at Tianjin University. His current research interests include image processing, video coding, and computer vision. Run-min Cong is pursuing the Ph.D. degree with the School of Electronic Information Engineering</title>
	</analytic>
	<monogr>
		<title level="s">His research interests include image processing</title>
		<imprint>
			<date type="published" when="1993">1993 and 2006</date>
			<publisher>Ji-chang Guo received his M.S. and Ph</publisher>
			<pubPlace>Tianjin, China; Tianjin, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Tianjin University ; Tianjin University</orgName>
		</respStmt>
	</monogr>
	<note>D. degrees from the School of Electronic Information Engineering. computer vision, and 3D imaging</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">His research interests include computer vision, pattern recognition, and digital image processing</title>
		<author>
			<persName><forename type="first">Yan-Wei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">which he has published more than 60 scientific papers including more than 10 IEEE TRANSACTIONS. He is an Associate Editor of the International Journal of Images and Graphics</title>
		<meeting><address><addrLine>Tianjin, China; China; Tianjin, China</addrLine></address></meeting>
		<imprint>
			<publisher>Tianjin University</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Science and Technology of China ; Tianjin University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include image processing and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
