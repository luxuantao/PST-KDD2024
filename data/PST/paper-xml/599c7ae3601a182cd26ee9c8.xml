<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A joint cascaded framework for simultaneous eye detection and eye state estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02-01">1 February 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Gou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Qingdao Academy of Intelligent Industries</orgName>
								<address>
									<postCode>266109</postCode>
									<settlement>Qingdao</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>10 0 049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical, Computer, and Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical, Computer, and Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei-Yue</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Qingdao Academy of Intelligent Industries</orgName>
								<address>
									<postCode>266109</postCode>
									<settlement>Qingdao</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical, Computer, and Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A joint cascaded framework for simultaneous eye detection and eye state estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02-01">1 February 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">C38D061BFD10B0D3636050640F0F8D81</idno>
					<idno type="DOI">10.1016/j.patcog.2017.01.023</idno>
					<note type="submission">Received 15 September 2016 Revised 3 December 2016 Accepted 15 January 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Eye detection Eye state estimation Learning-by-synthesis Cascade regression framework</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Eye detection and eye state (close/open) estimation are important for a wide range of applications, including iris recognition, visual interaction and driver fatigue detection. Current work typically performs eye detection first, followed by eye state estimation by a separate classifier. Such an approach fails to capture the interactions between eye location and its state. In this paper, we propose a method for simultaneous eye detection and eye state estimation. Based on a cascade regression framework, our method iteratively estimates the location of the eye and the probability of the eye being occluded by eyelid. At each iteration of cascaded regression, image features from the eye center as well as contextual image features from eyelid and eye corners are jointly used to estimate the eye position and openness probability. Using the eye openness probability, the most likely eye state can be estimated. Since it requires large number of facial images with labeled eye related landmarks, we propose to combine the real and synthetic images for training. It further improves the performance by utilizing this learning-by-synthesis method. Evaluations of our method on benchmark databases such as BioID and Gi4E database as well as on real world driving videos demonstrate its superior performance comparing to state-of-the-art methods for both eye detection and eye state estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Eye detection aims to estimate the pupil location in a image. Eye state prediction aims to estimate the binary state(open/close) of eye. Eye detection is becoming an increasingly important research topic due to its various applications such as iris recognition, eye gaze estimation and human-robot interaction. Eye state estimation is critical to detect the individual's affective state, and the corresponding pupil location is essential to reflect the individual's focus attention. Eye state estimation also has extensive applications in real world including diagnosing neurological disorders, sleep studies and driver drowsiness detection.</p><p>Although much work has been done for eye detection and eye state estimation, they still are challenging tasks due to variations in appearance, illumination and occlusion. In addition, most of the existing works only perform eye detection and eye state estimation separately and independently. In this paper, we propose a method for simultaneous eye localization and eye state estimation, on the basis of a joint cascaded regression framework. In cascaded regression framework, eye states and eye locations are updated simultaneously. Since it is time-consuming to collect large number of eye images with accurate eye related landmark labels for training, we propose to learn from the combination of synthetic and real images. Our main contributions are highlighted as follows:</p><p>• Simultaneity: On the basis of the cascade regression framework, the eye openness and eye locations are updated in each iteration simultaneously. Different from the conventional sequential eye detection and eye state estimation methods, our method is the first work that performs eye detection and eye state prediction at the same time.</p><p>• Robustness: The proposed framework relaxes the binary eye state to be a continuous probability, which measures the degree of openness of eyes. By setting flexible threshold, the eye states can be robustly predicted. In addition, it can estimate the location of eyes even when the eyes are closed. • Learning-by-synthesis: For learning-based methods, it is timeconsuming to collect various eye images and annotate them with ground truth. We propose to learn the regression mod-els from generated synthetic photorealistic eye images and it improves the result. • Effectiveness and efficiency: By exploiting the cascade regression and the interactions between eye location and eye state, our method performs significantly better than other state-ofthe-art methods and can achieve nearly real time.</p><p>The remainder of this paper is arranged as follows. Section 2 reviews the related work on eye detection and eye state estimation (open/close). The proposed method is described in Section 3 . Experimental results are discussed in Section 4 . The conclusion is drawn in Section 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Eye localization</head><p>Eye detection has been studied for decades and numerous methods have been proposed. In this section, we focus on reviewing most of the recent works. A detailed review of earlier techniques devoted to this topic can be found in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> . Generally, On the basis of captured information, we summarize the representative eye localization methods into five categories: (i) shape-based, (ii) appearance-based, (iii) context-based, (iv) structure-based, and (v) others.</p><p>Shape-based models generally capture the geometric information of an iris. Yuille et al. <ref type="bibr" target="#b2">[3]</ref> build a parameterized deformable model formulated by geometric shape with 11 parameters. Their model considers peaks, edges and valleys by using energy functions. To fit the model to a testing image, it has to optimize in a large continuous parameter space which covers shape variations. Based on the elliptical shape of an iris, Hansen and Pece <ref type="bibr" target="#b3">[4]</ref> propose a likelihood model which incorporates neighboring information for iris detection and tracking. By using EM and RANSAC methods, the ellipse is locally fitted to the image. In <ref type="bibr" target="#b4">[5]</ref> , the authors use curvature of isophotes in the intensity image to design a voting-based method for pupil localization.</p><p>Appearance-based methods are based on the photometric appearance, which is characterized by filter responses and color distribution. In <ref type="bibr" target="#b5">[6]</ref> , the authors propose a method for eye localization based on an ensemble of randomized regression trees, which are trained by using the pixel intensity differences around pupils. Araujo et al. <ref type="bibr" target="#b6">[7]</ref> describe an Inner Product Detector for eye localization based on the correlation filters. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> use local linear SVM for eye center detection, and ASEF-based filters are applied to select the candidate centers. Wu and Ji <ref type="bibr" target="#b8">[9]</ref> propose to learn deep features to capture the appearance variations of eyes in uncontrolled conditions. In <ref type="bibr" target="#b9">[10]</ref> , the authors apply a discriminative feature extraction method to 2D Haar wavelet transformation <ref type="bibr" target="#b10">[11]</ref> and use an efficient SVM for fast classification. Support Vector Regressor (SVR) is used to estimate the distance of patch center to the pupil center by extracted HoG features in <ref type="bibr" target="#b11">[12]</ref> .</p><p>Since eye centers have a stable relationship with other facial parts in terms of both appearance and shape, it is important to capture the contextual information to detect the eyes. Yang et al. <ref type="bibr" target="#b12">[13]</ref> propose to detect the pupil by using different Gabor kernels to convolute with the image, which highlights the eye-and-brow regions. By employing a coarse to fine strategy for robust initialization, Zhou et al. <ref type="bibr" target="#b13">[14]</ref> propose multi-scale nonlinear feature mapping based on the Supervised Decent Method (SDM) <ref type="bibr" target="#b14">[15]</ref> for eye detection. They use 14 eye related key points to capture the contextual information.</p><p>The structural locations information related to nose, mouth, etc. is helpful for the eye localization. Pictorial Structure <ref type="bibr" target="#b15">[16]</ref> and enhanced Pictorial Structure <ref type="bibr" target="#b16">[17]</ref> provide a powerful framework to model the face in terms of its appearance and geometrical rela-tionship between parts. The pupil is part of face and this model allows for accurate eye detection by capturing the structural information.</p><p>It is a challenge to organize some other methods into a specific aforementioned types. Based on the intensity information, Chen and Liu <ref type="bibr" target="#b17">[18]</ref> propose to extract eye regions by image enhancement, Gabor transformation, cluster analysis, and neighborhood operation with similarity measures in eye regions for final eye detection. Some researchers also propose combined models to overcome the shortcomings of separate model. Timm and Barth <ref type="bibr" target="#b18">[19]</ref> propose to use image gradients and squared dot products to detect the pupils. The aforementioned model in <ref type="bibr" target="#b16">[17]</ref> also combines shape and appearance features in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Eye state estimation</head><p>After accurate eye detection, eye state estimation can be achieved. Since pupil is frequently occluded by eyelids, hair and sunglasses, it is crucial to recognize the eye states to decide whether the pupil is occluded. Although the eye states estimation has received increasing research attention, eye state estimation is still an unsolved problem in uncontrolled scenes. Plenty of eye state estimation methods have been proposed. Generally, these methods are classified into three categories: (i) shape based, (ii) template based, and (iii) learning based.</p><p>Shape-based approaches aim to recognize the eye states based on geometric relationships or circular shape of visible iris. Kurylyak et al. <ref type="bibr" target="#b19">[20]</ref> set some thresholds for the difference between video frames in eye region pixel level to detect the eyelid movement. Then differences of vertical and horizontal projections are used to detect the degree of eye openness. Another simple and direct way for eye state estimation is template matching. Feng et al. <ref type="bibr" target="#b20">[21]</ref> use template matching for coarse driver eye state estimation followed by capturing the upper eyelids curvature for fine recognition. By setting flexible thresholds for the combined model, it can achieve a reasonable performance on warning system for a driver. Gonzalez et al. <ref type="bibr" target="#b21">[22]</ref> use projection operation to produce three templates: open, nearly closed and closed eyes. Both pair of eye state classifier and individual eye state classifier measure similarities between the templates and the test image.</p><p>Since eye state estimation is a binary classification problem, machine learning techniques are widely used to tackle this problem and they significantly improve the performance compared with the aforementioned methods. Song et al. <ref type="bibr" target="#b22">[23]</ref> propose Mul-tiHPOG features to recognize the eye states and made a comparison using other features in different datasets. Extensive experimental results show that MultiHPOG are effective and robust. The authors also released the Closed eyes in the Wild (CEW) database which contains 2423 images with open and closed eyes. In <ref type="bibr" target="#b23">[24]</ref> , minimum intensity projection is adapted. After histogram equalization, the pixels in vertical and horizontal direction with minimum intensity value are chosen to combine the feature vector. Through their experiment, a Random Forest classifier performs better than other tree based classifiers. Another projection operation based work is presented in <ref type="bibr" target="#b24">[25]</ref> . The authors introduce a discriminative feature by projecting the gray value distribution in x and y direction. In addition, a brightness adjustment based on the mean value of color image is proposed to overcome the variation of illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Learning-by-synthesis</head><p>Learning based cascaded regression framework requires large scale training data. In our case, there is limited public available dataset with eye center location labeled under various illuminations and head pose. In addition, it is time-consuming and also can be unreliable for accurate manual annotation of eye related landmarks. Motivated by recent work of learning-by-synthesis for appearance-based eye gaze estimation and eye detection <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> , we propose to learn from the synthetic eye images for accurate eye detection.</p><p>In summary, we realize that utilizing multi types of features is important on eye detection. In addition, capturing appearance features and learning classifiers significantly improve eye state estimation performance. Hence, we propose to capture the shape, appearance, structural and contextual information for eye detection and eye state estimation. Moreover, we learn from the combination of real and synthetic images to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>The conventional framework for eye state estimation is to locate the eyes first and then perform the binary classification. Since pupils are very likely to be occluded, general methods can not deal with this problem. To the best of our knowledge, there is little research focusing on eye localization and eye state estimation at the same time. In this paper, motivated by our previous work for facial landmark detection <ref type="bibr" target="#b29">[30]</ref> which uses a robust cascaded regression method for facial landmark detection under occlusions and large head poses, we propose to simultaneously detect eyes and recognize eye state (open/close) using a joint cascaded regression framework, on the basis of eye-related shape, appearance, structural and contextual information.</p><p>Before we introduce our proposed method, we first review general cascaded regression framework which has been successfully applied to facial landmark detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> . The overall algorithm is shown in Algorithm 1 . The facial landmark coordinates are denoted as</p><formula xml:id="formula_0">x t = x t 1 , x t 2 , . . . , x t D ,</formula><p>where D denotes the number of landmarks and t denotes the iteration in cascaded regression framework. It iteratively predict the location updates x t based on the extracted appearance features with regression model g t and then adds the current estimated updates x t to the previous locations x t-1 to acquire new landmark locations x t . It repeats until convergence.</p><p>The coarse-to-fine joint cascaded framework of our proposed method for simultaneous eye detection and eye state estimation is summarized in Fig. <ref type="figure" target="#fig_1">1</ref> and Algorithm 2 . It should be noted that we perform eye detection and eye state estimation for left eye and right eye separately during training and testing. In the following description, we take left eye as example. To capture eyerelated shape, structural, appearance and contextual information, we consider five eye-related key points (see green and red points in Fig. <ref type="figure" target="#fig_1">1 (b)</ref>), consisting of two eye corners, two eyelid points and one pupil for cascade regression. Since we focus on eye state estimation, we introduce openness probability p ∈ [0 , 1] related to the landmark of eye center. Before doing cascade regression, the eye Algorithm 1 General cascaded regression framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Give and estimated eye openness probability p t . In the following, we discuss the major components of the proposed method in the cascade regression framework including initialization, updating eye openness probability and key point locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initializaiton</head><p>In general regression based landmark detection framework, it makes sense to initialize the landmarks by mean face. In our case, directly initializing the eye related points by mean face is not adequate because it is not likely to converge to the global optimum if the initialization is far away from the ground truth. Hence, it is reasonable to focus on eye regions for accurate eye center localization and eye state estimation. We firstly extract eye regions based on 51 landmarks detection using method in <ref type="bibr" target="#b30">[31]</ref> . In this paper, we normalize the width of two eye outer corners to 25 in pixel. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Update probability of eye openness</head><p>Even though eye state estimation is a binary classification problem, there are large variations of appearance for different individuals, especially for individual with glasses or nearly half closed eyes. Extracting features from whole eye regions is limited to represent the closed and open eyes. To use more class-specific information for robust eye state estimation, we propose to relax binary eye state to be a continuous eye openness probability, which can be inferred from pupil appearance features and the related contextual information. Since it is not easy to accurately locate the pupil, we The eye openness probability p ∈ [0 , 1] is updated at each iteration. To capture the pupil appearance and its related contextual information, SIFT features of local patches around the pupils, eye corners and eyelids are used. To capture shape information, the differences for pairwise points are calculated as shape features. Then the shape and appearance features are combined to generate a concatenated feature vector denoted as Ψ (I, x t-1 ) ∈ 5 •128+5 •4 , where I and t denote the input image and iteration index, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Learn the eye state prediction model</head><p>To estimate the eye openness probability, we use a linear regression model. For training at each iteration, linear model parameters β t and bias b t are estimated by the standard least-square formulation with closed form solution:</p><formula xml:id="formula_1">β t * , b t * = arg min β t , b t K i =1 p t i -β t Ψ (I i , x t-1 i ) -b t i 2 (1)</formula><p>where K is the number of training samples. Given the training images with estimated key point locations x t-1 , the feature</p><formula xml:id="formula_2">Ψ (I i , x t-1 i</formula><p>) of i th image can be calculated. The probability update p t i can be acquired by subtracting the current probability p t-1 i from the ground truth. It is noted that, the eye openness probability is labeled as 1 when eye open and 0 when closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Estimate the eye openness probability</head><p>After learning the parameters β and b t for each iteration, given the current key point locations, we can estimate the update probability p t for next iteration by:</p><formula xml:id="formula_3">p t = β t Ψ (I, x t-1 ) + b t (2)</formula><p>Then eye openness probability can be acquired though:</p><formula xml:id="formula_4">p t = p t-1 + p t sub ject to : 0 ≤ p t ≤ 1</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Update point locations</head><p>After estimating the eye openness probability, the 5 key point locations can be updated. We also use a linear regression model for point location prediction. Intuitively, when the pupil is occluded with a low visibility probability, the local appearance features are less reliable for the localization of pupil. When the eyes are totally closed, the pupil related SIFT features should be discarded. Based on this intuition, we modify the regression method with the visibility probability as in Eq. (4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Learn the eye location prediction model</head><p>For the training, similar to Eq. ( <ref type="formula">1</ref>), we learn the weight parameters α t and bias c t by a standard least-square formulation with closed form solution:</p><formula xml:id="formula_5">α t * , c t * i = arg min α t , c t i K i =1 x t i -α t [ p t i • Ψ (I i , x t-1 i )] -c t i 2 (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where K is the number of training samples and • denotes blockwise product. It is worth nothing that, eye center related features are weighted by the corresponding openness probability after applying block-wise multiplication. Hence, when the eye is partially occluded with a low probability, the corresponding pupil features are less reliable for the eye detection. Given the current point locations x t-1 , the combined shape and appearance features can be extracted. Then the eye openness probability is acquired by Eqs.</p><p>(2) and (3 ). During training, the update x t is estimated by subtracting the current key point locations x t-1 from the ground truth locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Infer the eye location</head><p>Given the image I and corresponding key point locations x t-1 , according to Eqs. ( <ref type="formula">2</ref>) and (3) in inference, the current eye openness probability p t can be calculated. After learning the parameters α t and bias c t for each iteration, we can estimate the update location</p><p>x t for iteration t by:</p><formula xml:id="formula_7">x t = α t [ p t • Ψ (I, x t-1 )] + c t<label>(5)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_7">5</ref>) , • denotes block-wise product, and it allows for weighting the pupil related appearance features. As a result, the feature vector Ψ (I, x t-1 ) is weighted though p t such that pupil less likely to be occluded contributes more to x t . Then key point locations for next iteration can be acquired through:</p><formula xml:id="formula_8">x t = x t-1 + x t (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where the eye center location can be acquired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>In this section, we firstly describe the implementation details. Then we evaluate the proposed eye localization and eye state prediction method and compare it with the stat-of-the-art methods on two benchmark databases including BioID <ref type="bibr" target="#b32">[33]</ref> and GI4E <ref type="bibr" target="#b33">[34]</ref> . To verify the robustness of proposed method, we evaluate it on the extremely challenging real world driving videos <ref type="bibr" target="#b34">[35]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details 4.1.1. Evaluation database</head><p>We train the cascade regression model using 5274 images, which consists of 1690 eye images, 2958 face images from MUCT <ref type="bibr" target="#b35">[36]</ref> , 594 images with closed eyes from CEW <ref type="bibr" target="#b22">[23]</ref> , and 32 images with one eye open and another closed collected from the Internet. In addition, the training images are augmented by perturbing the scale, rotation angle, and position of the initial eye shape for learning.</p><p>One test dataset is GI4E <ref type="bibr" target="#b33">[34]</ref> . It contains 1236 images of 103 subjects with 12 different gaze directions. These images have a resolution of 800 × 600 in pixel and are representative for the ones that can be acquired by a normal camera. Another test set is BioID <ref type="bibr" target="#b32">[33]</ref> , which is one of the most widely used database for eye center localization. It is also widely used for eye state estimation. The BioID database contains 1521 gray images with a resolution of 384 × 286. This database is very challenging with complex backgrounds, various illuminations, different face sizes and head poses, and subjects with glasses or closed eyes. To verify robustness of our proposed method, 2220 extreme challenging frames from Strategic Highway Research Program (SHRP2) <ref type="bibr" target="#b34">[35]</ref> database are chosen and manually labeled for testing. SHRP2 database consists of 44 driving videos with a resolution of 720 × 480.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Synthetic eye</head><p>We use UnityEyes from <ref type="bibr" target="#b25">[26]</ref> to generate 1690 synthetic eye images with landmark labels for training to boost the performance. In <ref type="bibr" target="#b25">[26]</ref> , the authors adopt image-based lighting and rasterizing method to cover the various illumination conditions. By driving 3D eye region model from 3D face scans, various eyeball texture and shapes, iris width and color can be generated. In addition, different head poses can be generated by using spherical coordinates and pointing it towards the eye ball center. More details about Uni-tyEyes can be found in <ref type="bibr" target="#b25">[26]</ref> and it is public available. Some synthetic eye images are shown in Fig. <ref type="figure" target="#fig_2">2</ref> . Since it only generates left eye images, we flip them to train models for right eye detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Evaluation criteria</head><p>The maximum normalized error <ref type="bibr" target="#b32">[33]</ref> is adopted to evaluate the performance of eye center localization. It is defined as follows:</p><formula xml:id="formula_10">d eye = max (d r , d l ) C r -C l (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where d r and d l are the Euclidean distances between the estimated right and left eye centers and the ones in the ground truth, and C r and C l are the true centers of the right pupil and left pupil respectively. d eye is normalized by the inter-ocular distance. It measures the error obtained by the worst of both eye estimation. In this measure, d eye ≤ 0.25 corresponds to the distance between eye center and eye corner, d eye ≤ 0.1 corresponds to the range of iris, and d eye ≤ 0.05 corresponds to the range of pupil diameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Parameters setting</head><p>The OpenCV implementation of boosted cascade face detector proposed by Viola and Jones <ref type="bibr" target="#b36">[37]</ref> is used for face detection. The minimal face region is set to 50 × 50 and the largest detected face is chosen as the final detection result. The false negatives from the test set are discarded. As a result, the face detection rates on BioID and GI4E database are 97.5% and 99.4%, respectively. The number of iterations for the cascade regression model is set to 4. The normalized eye corner distance is 25 pixels. For binary eye state estimation, the threshold is 0.2. That means when the estimated openness probability is below 0.2, the predicted eye state is to be close.  <ref type="bibr" target="#b18">[19]</ref> 82 .5% 93 .4% 98 .0% Valenti2012 <ref type="bibr" target="#b4">[5]</ref> 86 .1% 91 .7% 97 .9% Chen2014 <ref type="bibr" target="#b17">[18]</ref> 87 .3% 94 .9% 99 .2% Araujo2014 <ref type="bibr" target="#b6">[7]</ref> 88 .3% 92 .7% 98 .9% Chen2015 <ref type="bibr" target="#b9">[10]</ref> 88 .8% 95 .2% 99 .0% Ours 91 .2 % 99 .4 % 99 .8 %</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Eye state estimation comparison on BioID database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Cheng2012 <ref type="bibr" target="#b38">[39]</ref> 94 .0% Song2014 <ref type="bibr" target="#b22">[23]</ref> 97 .1% Lin2015 <ref type="bibr" target="#b24">[25]</ref> 97 .5% Ours 98 .1 %</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results</head><p>To verify the effectiveness of learning-by-synthesis for eye detection, we firstly perform training on 3584 real images, 1690 synthetic images and their combination separately and test on BioID database. As show in Table <ref type="table" target="#tab_1">1</ref> , training on combination of real data and synthetic data improves the performance. The following experiments are based on training on combination of real and synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Test on BioID</head><p>We further compare the performance of the proposed method on most widely used BioID database in Table <ref type="table" target="#tab_2">2</ref> and Fig. <ref type="figure">4</ref> with the state-of-the-art eye localization methods. Since there is little literature focusing on both eye localization and eye state estimation, separate comparison with existing methods for eye state estimation is listed in Table <ref type="table">3</ref> . The best performance for evaluation criteria is highlighted in bold. As shown in Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table">3</ref> , the performance of our proposed method is significant better than other state-of-the-art methods both on eye localization and eye state estimation.</p><p>Fig. <ref type="figure">3</ref> shows some samples of eye localization and eye state prediction by our proposed method on BioID database, where white dot represents the manual annotation and red dot denotes the predicted eye location. Even though the eye is closed or subject with glasses, we can still predict the eye locations by the captured shape and contextual information.</p><p>Since our proposed framework for eye localization and state estimation is totally automatic given the input image, it is more reasonable for real application. Some other experiments are conducted given the face or specific eye region. In this paper, another comparison experiment with a similar work <ref type="bibr" target="#b13">[14]</ref> is conducted. Zhou et al. <ref type="bibr" target="#b13">[14]</ref> improves the basic SDM <ref type="bibr" target="#b14">[15]</ref> by extracting SIFT features at first stage and LBP at the following iterations. In <ref type="bibr" target="#b13">[14]</ref> , for testing, the authors firstly generate the basic eye bounding box by annotated landmarks. To fairly compare with our method, we extract eye regions by annotated eye outer corners, instead of using the automatically detected outer eye corners as the previous experiment. The results are shown in Table <ref type="table" target="#tab_3">4</ref> . It shows that our    a Are estimated from the accuracy curves in corresponding paper <ref type="bibr" target="#b33">[34]</ref> .</p><p>proposed method performs significantly better than the similar existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Test on GI4E</head><p>Experimental results on GI4E database are listed in Table <ref type="table" target="#tab_4">5</ref> . We only conduct comparisons of eye detection on GI4E because all the testing images are with open eyes. Our proposed approach  achieves preferable location results compared with other methods. As shown in Table <ref type="table" target="#tab_4">5</ref> , a detection rate of 94.2% can be achieved when normalized error is d eye ≤ 0.05. Compared with results in testing on challenging database BioID, results on GI4E are better since the testing images are more clear with smaller range of head pose and illumination changes. Some qualitative results are shown in Fig. <ref type="figure" target="#fig_4">5</ref> . It is worth nothing that purely learning-by-synthesis can perform better on GI4E with more synthetic eyes and using more landmarks. But it is not robust enough to handle the realistic images like ones from BioID.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Test on SHRP2</head><p>To verify the robustness of the proposed method, we evaluate it on extremely challenging SHRP2 database with the trained model in previous experiments. 2220 driving frames are chosen from different videos for our quantitative testing. Some examples are shown in Fig. <ref type="figure" target="#fig_6">7</ref> . The eye detection and state estimation result are shown in Table <ref type="table" target="#tab_5">6</ref> . Normalized error less than 0.05 corresponding to pupil diameter is not calculated since even human can not accurately annotate the pupil location in such low resolution images. The public available code from <ref type="bibr" target="#b16">[17]</ref> is used to test on this database which can only achieve 43.1% with normalized error 0.1. Experimental result show that the proposed method is robust enough to deal with these challenging images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Further analysis</head><p>Since cascade regression converges to different eye locations at different cascade level, we further investigate the convergence of cascade regression for eye detection. We take the results of BioID database for example, as shown in Fig. <ref type="figure" target="#fig_5">6</ref> , it converges fast at first two iterations and to optimal after 4 iterations. We initialize it by mean eye locations after coarsely extracted eye regions using our landmark detection method and achieve only detection rate of 82.3%. After several cascade regression, we can achieve a detection rate of 91.2%. Some eye detection and state estimation results of failure are shown in Fig. <ref type="figure" target="#fig_7">8</ref> , where white dot represents the ground truth, red dot represents the prediction and the digit is the estimated openness probability. To capture the contextual information, we combine features of 5 landmark together as the input of regression model to estimate the locations and openness probability. Hence, the feature of eye corners also have effects on final estimation. As shown in Fig. <ref type="figure" target="#fig_7">8</ref> , for the first case, it yields inaccurate eye detection and eye state estimation under strong highlights on the glasses. For the second case, we fail to estimate the state of right eye due to the various appearance of eyelashes. Large head pose also leads to inaccurate openness estimation as shown in Fig. <ref type="figure" target="#fig_7">8</ref> for the third case. In addition, false positive of face detection result in false eye detection and state estimation like the last case shown in Fig. <ref type="figure" target="#fig_7">8</ref> . It should be noted that we do not discard these images with false positives for face detection during testing.</p><p>We also use the SURF features to capture local appearance and it achieves 85.9% detection rate on Gi4E database where SIFT features can achieve 94.2% with normalized error of 0.05. All experiments are conducted with nonoptimized Matlab codes on a standard PC, which has an Intel i5 3.47 GHz CPU and 16 GB RAM. 15 frames per second can be achieved by our proposed method, which allows for near real time eye detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further discussion and future work</head><p>The experimental results demonstrate that our proposed method can achieve preferable results both on eye detection and eye state estimation on benchmark databases. Based on the cascaded framework, it simultaneously updates the eye location and eye openness probability at each iteration. By further investigation, as shown in Table <ref type="table" target="#tab_3">4</ref> , the performance of proposed frame work is sensitive to eye region detection since the initialization of 5 key landmarks is important. It can not converge to the global optimization when the initialization of key points is far away from the ground truth. Actually, if we only train on large number of synthetic data using more eye related landmark and test on clear GI4E images, it can get improvement. But it performs much worse on BioID and SHRP2 database. In addition, it can improve the performance by using a good face detector. Further work will focus on these problems.</p><p>Due to the effectiveness and efficiency of our proposed method, it can be widely used for real application like iris detection and recognition. Moreover, Our proposed method can be applied for visual analysis for driver like gaze estimation, monitoring the driver attention and calculating PERCLOSE for driver fatigue detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose an effective cascade regression method for simultaneous eye localization and eye state estimation. The binary eye state is mapped to a continuous variable denoted by eye openness probability. Both eye center position and eye openness probability are updated during regression iterations using captured shape, appearance, structural and contextual information. In addition, eye localization relies less on appearance information of pupil with low openness probability. Experimental results show that our proposed method is significantly better than other state-of-the-art methods for both eye localization and eye state estimation.</p><p>In the future, we will focus on applying the proposed simultaneous eye detection and eye state estimation method to other applications, such as eye tracking, gaze estimation and driver fatigue monitoring. In addition, we will further improve the method to robustly deal with large head poses in unconstrained scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b), 5 key point locations are initialized by mean locations in eye region from training. The eye state is initialized as open with openness probability p 0 = 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of our proposed approach for simultaneous eye detection and eye state estimation. (a) Face detection and 51 landmark detection. (b) Extracte the eye regions based on the detected 51 landmarks and initilaize eye state and 5 key point locations. (c) Output of first iterations. (d) Final estimated location and openness probability of eye. * Take the left eye as a example.</figDesc><graphic coords="4,128.03,57.21,330.48,75.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Samples of synthetic eye. (a) Origninal synthetic left eye with landmark location labels. (b) Synthetic eyes with different illuminations, head pose and gaze, various eye shape and textures.</figDesc><graphic coords="5,46.34,57.67,243.60,113.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Eye localization and eye openness estimation examples of successes on BioID database. The white dot represents the ground truth and red dot represents estimated eye locations. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="6,128.03,56.77,330.48,72.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Eye localization and eye openness estimation examples of successes on GI4E database. The white dot represents the ground truth and red dot represents esimated eye locations. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="6,44.03,436.45,229.20,90.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Eye detection results at each cascaded iteration on BioID database. Y coordinate denotes the detection rate with the normalized error less than 0.05. It converges after fourth iteration.</figDesc><graphic coords="6,325.82,174.14,213.89,122.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Samples of eye detection and eye state estimation results on extreme challenging SHRP2 driving database.The faces of the subjects are partially covered as this identity information can not be made public under the terms of a data sharing agreement.</figDesc><graphic coords="6,305.53,356.79,243.84,220.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Eye location and eye state estimation examples of failures on testing database. The white dot represents the ground truth and red dot represents estimated eye locations. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="7,144.84,56.77,316.08,81.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Joint cascaded regression framework for eye detection and eye openness estimation. Give the image I . Left/right eye openness probability is initialized as open by p 0 = 1 . Five key point locations x 0 are initialized by the coarse detected eye region and mean eye locations in normalized eye region. Update the key point locations given the current key point locations x t-1 and the calculated eye openness p t . g</figDesc><table><row><cell>Input:</cell></row><row><cell>Do cascade regression:</cell></row><row><cell>for t=1,2,… ,T do</cell></row><row><cell>Update the eye openness probability given the current key</cell></row><row><cell>point locations x t-1 .</cell></row><row><cell>f</cell></row></table><note><p><p><p><p><p><p><p><p><p>the image I . Facial landmark locations x 0 are initialized by mean face.</p>Do cascade regression:</p>for t=1,2,… ,T do Update the key point locations x t given the current key point locations x t-1 g t : I , x t-1 → x t x t = x t-1 + x t end for Output:</p>Landmark locations x T .</p>Algorithm 2 t : I , x t-1 → p t p t = p t-1 + p t t : I , x t-1 , p t → x t x t = x t-1 + x t end for Output:</p>Estimated eye states based on the eye openness probability p T and the locations x T of key points.</p>state is initialized as open with openness probability 1 as shown in Fig.</p>1 (b</p>). In addition, to capture the structural information for eye center detection, all eye related five key point locations x ∈ 2 •5 are initialized by the detected eye region and mean location from training. In cascade regression, the eye openness probability and key point locations are iteratively updated at each iteration. For updating the openness probability, a linear regression model f t is used to predict the eye openness probability update p t on the basis of current key point locations x t-1 . For updating the key point locations, another regression model g t is used to predict the key point location updates x t based on the current locations x t-1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Eye localization results on BioID( d eye ≤ 0.05) based on different training data.</figDesc><table><row><cell>Training data</cell><cell>Real</cell><cell>Synthetic</cell><cell>Combination</cell></row><row><cell>d eye ≤ 0 .05</cell><cell>90 .3%</cell><cell>88 .6%</cell><cell>91 .2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Eye localization comparison using the normalizaed error measurement on BioID database.</figDesc><table><row><cell>Method</cell><cell>d eye ≤ 0.05</cell><cell>d eye ≤ 0.1</cell><cell>d eye ≤ 0.25</cell></row><row><cell>Campadelli2009 [38]</cell><cell>80 .7%</cell><cell>93 .2%</cell><cell>99 .3%</cell></row><row><cell>Timm2011</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Eye localization comparison results with SDM-based methods.</figDesc><table><row><cell>Method</cell><cell>d eye ≤ 0.05</cell><cell>d eye ≤ 0.1</cell><cell>d eye ≤ 0.25</cell></row><row><cell>Basic-SDM [14]</cell><cell>90 .3%</cell><cell>96 .4%</cell><cell>100 .0%</cell></row><row><cell>CF-MF-SDM [14]</cell><cell>93 .8%</cell><cell>99 .8%</cell><cell>99 .9%</cell></row><row><cell>Ours</cell><cell>95 .1 %</cell><cell>99 .9 %</cell><cell>100 .0 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Eye localization comparison on GI4E database.</figDesc><table><row><cell>Method</cell><cell>d eye ≤ 0.05</cell><cell>d eye ≤ 0.1</cell><cell>d eye ≤ 0.25</cell></row><row><cell>Timm2011 [19]</cell><cell>92 .4%</cell><cell>96 .0% a</cell><cell>97 .5% a</cell></row><row><cell>Villanueva2013 [34]</cell><cell>93 .9%</cell><cell>97 .3% a</cell><cell>98 .5% a</cell></row><row><cell>Ours</cell><cell>94 .2 %</cell><cell>99 .1 %</cell><cell>99 .8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Eye localization and state estimation results on SHRP2.</figDesc><table><row><cell>d eye ≤ 0.1</cell><cell>d eye ≤ 0.25</cell><cell>Eye State Accuracy</cell></row><row><cell>83 .8%</cell><cell>98 .2%</cell><cell>91 .4%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was completed when the first author visited Rensselaer Polytechnic Institute (RPI), supported by a scholarship from University of Chinese Academy of Sciences (UCAS). The authors would like to acknowledge support from UCAS and RPI. This work was also supported in part by National Science Foundation under the grant 1145152 and by the National Natural Science Foundation of China under Grant 61304200 and 61533019 .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In the eye of the beholder: a survey of models for eyes and gaze</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Mach. Intel. IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="478" to="500" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A literature survey on robust and efficient eye localization in real-life scenarios</title>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3157" to="3173" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature extraction from faces using deformable templates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eye tracking in the wild</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Pece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="181" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate eye center location through invariant isocentric patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Mach. Intel. IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1785" to="1798" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eye pupil localization with an ensemble of randomized trees</title>
		<author>
			<persName><forename type="first">N</forename><surname>Markuš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frljak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Pandži Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="578" to="587" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast eye localization without a face model using inner product detectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1366" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Precise eye localization by fast local linear SVM</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the deep features for eye detection in uncontrolled conditions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="455" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eye detection using discriminatory haar features and a new efficient SVM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Sidney</surname></persName>
		</author>
		<title level="m">Introduction to wavelets and wavelet transforms: a primer</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regressor based estimation of the eye pupil center</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Karakoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Akgul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel pupil localization method based on gaboreye model and radial symmetry operator</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP&apos;04. 2004 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nam</surname></persName>
		</author>
		<title level="m">Precise eye localization with improved SDM, in: Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">470</biblScope>
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced pictorial structures for precise eye localization under incontrolled conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 20 09. CVPR 20 09. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">20 09</date>
			<biblScope unit="page" from="1621" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering-based discriminant analysis for eye detection, Image Process</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1629" to="1638" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Accurate eye centre localisation by means of gradients</title>
		<author>
			<persName><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>VISAPP</publisher>
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection of the eye blinks for human&apos;s fatigue monitoring</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kurylyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lamonaca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mirabelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Measurements and Applications Proceedings (MeMeA), 2012 IEEE International Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A combined eye states identification method for detection of driver fatigue</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yutian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dexuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pingqiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IET International Communication Conference on</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
	<note>Wireless Mobile and Computing</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time vision-based eye state detection for driver alertness monitoring</title>
		<author>
			<persName><forename type="first">D</forename><surname>González-Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Díaz-Pernas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antón-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martínez-Zarzuela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Díez-Higuera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="306" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eyes closeness detection from still images with multi-scale histograms of principal oriented gradients</title>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Driver Eye State Detection Based on Minimum Intensity Projection Using Tree Based Classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Punitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Geetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems Technologies and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An effective eye states detection method based on the projection of the gray interval distribution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1875" to="1879" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning an appearance-based gaze estimator from one million synthesised images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rendering of eyes for eye-shape registration and gaze estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05916</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4511" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning-by-synthesis for accurate eye detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision, 2015a</title>
		<meeting>the IEEE International Conference on Computer Vision, 2015a</meeting>
		<imprint>
			<biblScope unit="page" from="3658" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape augmented regression method for face alignment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Constrained joint cascade regression framework for simultaneous facial action unit recognition and facial landmark detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Robust face detection using the hausdorff distance, in: Audio-and Video-Based Biometric Person Authentication</title>
		<author>
			<persName><forename type="first">O</forename><surname>Jesorsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kirchberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Frischholz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hybrid method based on topography for robust detection of iris center and eye corners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Villanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ponz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sesma-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ariz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cabeza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimed. Comput. Commun. Appl. (TOMM)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="https://insight.shrp2nds.us/" />
		<title level="m">Transportation Research Board of the National Academies of Science, The 2nd Strategic Highway Research Program Naturalistic Driving Study Dataset</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The muct landmarked face database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Assoc. South Africa</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Precise eye and mouth localization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lipori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="359" to="377" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Eye state detection in facial image based on linear prediction error of wavelet coefficients</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Biomimetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1388" to="1392" />
		</imprint>
	</monogr>
	<note>20 08. ROBIO 20 08</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He is currently a Ph.D. student at the State Key Laboratory of Management and Control for Complex Systems</title>
	</analytic>
	<monogr>
		<title level="m">Institute of Automation, Chinese Academy of Sciences. Since September 2015, he has been a Visiting Student at Rensselaer Polytechnic Institute</title>
		<meeting><address><addrLine>Troy, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Chao Gou received his B.S</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>His research interest covers computer vision, pattern recognition and intelligent transportation systems</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Her areas of research include computer vision, pattern recognition, and their applications in human-computer interaction. She is a student member of the IEEE and the</title>
	</analytic>
	<monogr>
		<title level="m">Yue Wu received the B.S and M.S degrees in electrical engineering from the Southeast University of China in 2008 and 2011, respectively. She is currently pursuing the Ph.D. degree at Rensselaer Polytechnic Institute</title>
		<meeting><address><addrLine>Troy, New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m">He is currently pursuing the Ph.D. degree at Rensselaer Polytechnic Institute</title>
		<meeting><address><addrLine>Troy, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Kang Wang received his B.S degree from Department of Electronic Engineering and Information Science, University of Science and Technology of China</orgName>
		</respStmt>
	</monogr>
	<note>His main research interests include computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">He is currently an associate professor at the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences. Since January 2016, he has been a visiting scholar at the College of Computing</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include intelligent transportation systems, intelligent vision computing, and machine learning</title>
		<meeting><address><addrLine>Georgia Institute of Technology, U.S.A</addrLine></address></meeting>
		<imprint>
			<publisher>Kunfeng Wang received his Ph</publisher>
		</imprint>
	</monogr>
	<note>D. degree in control theory and control engineering from the Graduate University of Chinese Academy of Sciences in 2008</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">His current research interests include social computing, complex systems, and intelligent computing and control. He is currently the Director of the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, the Vice President of the ACM China Council and the Vice President/Secretary-General of Chinese Association of Automation. He is a member of Sigma Xi and an elected fellow of IEEE</title>
		<author>
			<persName><forename type="first">Fei-Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Journal of Intelligent Control and Systems , the World Scientific Series in Intelligent Control and Intelligent Automation , IEEE Intelligent Systems , and the IEEE Transactions on Intelligent Transportation Systems. He has served as chairs for over 20 IEEE, ACM, INFORMS, and ASME conferences</title>
		<meeting><address><addrLine>Troy, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Rensselaer Polytechnic Institute</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>He was an editor-in-chief of the. INCOSE, IFAC, ASME, and AAAS</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">He recently served as director of the Intelligent Systems Laboratory (ISL) at RPI. Prof. Ji&apos;s research interests are in computer vision, probabilistic graphical models, information fusion, and their applications in various fields. Prof. Ji is an editor on several related IEEE and international journals and he has served as a general chair, program chair, technical area chair</title>
		<imprint/>
	</monogr>
	<note>He is currently a professor with the Department of Electrical, Computer, and Systems Engineering at Rensselaer Polytechnic Institute (RPI). and program committee member in numerous international conferences/workshops. Prof. Ji is a fellow of IEEE and IAPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
