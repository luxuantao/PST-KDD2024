<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MapTask Scheduling in MapReduce With Data Locality: Throughput and Heavy-Traffic Optimality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weina</forename><surname>Wang</surname></persName>
							<email>weina.wang@asu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lei</forename><surname>Ying</surname></persName>
							<email>lei.ying.2@asu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<email>zhangli@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85281</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MapTask Scheduling in MapReduce With Data Locality: Throughput and Heavy-Traffic Optimality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3537AFC79949E25CF59AF7D2AEE4822</idno>
					<idno type="DOI">10.1109/TNET.2014.2362745</idno>
					<note type="submission">received August 07, 2013; revised April 22, 2014; accepted September 09, 2014;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Heavy-traffic analysis</term>
					<term>MapReduce</term>
					<term>queueing systems</term>
					<term>scheduling</term>
					<term>throughput optimality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MapReduce/Hadoop framework has been widely used to process large-scale datasets on computing clusters. Scheduling map tasks with data locality consideration is crucial to the performance of MapReduce. Many works have been devoted to increasing data locality for better efficiency. However, to the best of our knowledge, fundamental limits of MapReduce computing clusters with data locality, including the capacity region and theoretical bounds on the delay performance, have not been well studied. In this paper, we address these problems from a stochastic network perspective. Our focus is to strike the right balance between data locality and load balancing to simultaneously maximize throughput and minimize delay. We present a new queueing architecture and propose a map task scheduling algorithm constituted by the Join the Shortest Queue policy together with the MaxWeight policy. We identify an outer bound on the capacity region, and then prove that the proposed algorithm can stabilize any arrival rate vector strictly within this outer bound. It shows that the outer bound coincides with the actual capacity region, and the proposed algorithm is throughput-optimal. Furthermore, we study the number of backlogged tasks under the proposed algorithm, which is directly related to the delay performance based on Little's law. We prove that the proposed algorithm is heavy-traffic optimal, i.e., it asymptotically minimizes the number of backlogged tasks as the arrival rate vector approaches the boundary of the capacity region. Therefore, the proposed algorithm is also delay-optimal in the heavy-traffic regime. The proofs in this paper deal with random processing times with heterogeneous parameters and nonpreemptive task execution, which differentiate our work from many existing works on MaxWeight-type algorithms, so the proof techniques themselves for the stability analysis and the heavy-traffic analysis are also novel contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>yet powerful framework for processing large-scale datasets in a distributed and parallel fashion. Originally developed by Google, and later on popularized by its open-source implementation Hadoop <ref type="bibr" target="#b1">[2]</ref>, MapReduce has been widely used in practice by companies including Google, Yahoo!, Facebook, Amazon, and IBM.</p><p>A production MapReduce cluster may consist of tens of thousands of machines <ref type="bibr" target="#b2">[3]</ref>. The stored data are typically organized on distributed file systems (e.g., Google File System (GFS) <ref type="bibr" target="#b3">[4]</ref>, Hadoop Distributed File System (HDFS) <ref type="bibr" target="#b4">[5]</ref>), which divide a large dataset into data chunks (e.g., 128 MB) and store multiple replicas (by default 3 in GFS and HDFS) of each chunk on different machines. A data processing request under the MapReduce framework, called a job, consists of two types of tasks: map and reduce. A map task reads one data chunk and processes it to produce intermediate results (key-value pairs). Then, reduce tasks fetch the intermediate results and carry out further computations to produce the final result. Map and reduce tasks are assigned to the machines in the computing cluster by a master node that keeps track of the status of these tasks to manage the computation process. In assigning map tasks, a critical consideration is to place map tasks on or close to machines that store the input data chunks, a problem called data locality.</p><p>For each task, we call a machine a local machine for the task if the data chunk associated with the task is stored locally, and we call this task a local task on the machine; otherwise, the machine is called a remote machine for the task, and correspondingly this task is called a remote task on the machine. Locality also refers to the fraction of tasks that run on local machines. Improving locality can reduce both the processing time of map tasks and the network traffic load since fewer map tasks need to fetch data remotely. However, assigning all tasks to local machines may lead to an uneven distribution of tasks among machines, i.e., some machines may be heavily congested while others may be idle. Therefore, we need to strike the right balance between data locality and load balancing in MapReduce.</p><p>We call the algorithm that assigns map tasks to machines a map-scheduling algorithm or simply a scheduling algorithm. Most existing scheduling algorithms put great effort on increasing data locality for performance improvement <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Among these scheduling algorithms, the Hadoop Fair Scheduler <ref type="bibr" target="#b6">[7]</ref> is the de facto industry standard. It deploys a technique called delay scheduling, which delays some map tasks for a small amount of time to attain higher locality. More discussions on the related works are provided in Section II.</p><p>While the data locality issue has received a lot of attention and scheduling algorithms that improve data locality have been proposed in the literature and implemented in practice, to the best of our knowledge, none of the existing works have studied the fundamental limits of MapReduce computing clusters with data locality. Basic questions such as what is the capacity region of a MapReduce computing cluster with data locality, which scheduling algorithm can achieve the full capacity region, and how to minimize the waiting time and congestion in a MapReduce computing cluster with data locality remain open.</p><p>In this paper, we will address these basic questions from a stochastic network perspective. Trace analysis on some production MapReduce clusters shows that a large portion of jobs are mapintensive, and many of them only require map tasks <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Therefore, we focus on map-scheduling algorithms and assume that reduce tasks are either not required or not the bottleneck of the job processing. We simply use task to refer to map task in the rest of this paper. We assume that the data have been divided into chunks, and each chunk has three replicas stored at three different machines. The computing cluster is modeled as a timeslotted system, in which jobs consisting of a number of map tasks arrive at the beginning of each time-slot according to some stochastic process. Each map task processes one data chunk, and map tasks are nonpreemptive. The service (processing) time of a map task is a random variable. The cumulative distribution function (CDF) of this random variable is with mean if the task is served at a local machine, and is with mean if the task is served at a remote machine. Both CDFs have finite variances. Based on this model, we establish the following fundamental results.</p><p>• First, we present an outer bound on the capacity region of a MapReduce computing cluster with data locality, where the capacity region consists of all the arrival rate vectors for which there exists a scheduling algorithm that stabilizes the system. • We propose a new queueing architecture with one local queue for each machine, storing local tasks associated with the machine, and a common queue for all machines. Based on this new queueing architecture, we propose a two-stage scheduling algorithm under which a newly arrived task is routed to one of the three local queues associated with the three local machines or the common queue using the Join the Shortest Queue (JSQ) policy; when a machine is available, it selects a task from the associated local queue or the common queue using the MaxWeight policy. • We prove that the joint JSQ and MaxWeight scheduling algorithm is throughput-optimal, i.e., it stabilizes any arrival rate vector strictly within the outer bound of the capacity region, which also shows that the outer bound coincides with the actual capacity region. We remark that most existing results on MaxWeight-type scheduling algorithms assume deterministic service time or geometrically distributed service time with preemptive tasks. To the best of our knowledge, the stability of MaxWeight scheduling with nonpreemptive task execution and random processing time has not been established before. Thus, the proof technique itself is a novel contribution of this paper and may be extended to prove the stability of MaxWeight-type scheduling for other applications. • In addition to throughput optimality, we further study the number of backlogged tasks, which is directly related to the delay performance based on Little's law. We consider the case that the service times have geometric distributions and assume a heavy local traffic condition. Then, the joint JSQ and MaxWeight scheduling algorithm is proved to be heavy-traffic optimal, i.e., it asymptotically minimizes the number of backlogged tasks as the arrival rate vector approaches the boundary of the capacity region. Therefore, the proposed algorithm strikes the right balance between data locality and load balancing and is both throughputand delay-optimal in the heavy-traffic regime. The rest of this paper is organized as follows. Section II discusses the related work. Section III describes the system model and presents the scheduling algorithm for map tasks. Section IV characterizes the capacity region and shows the throughput optimality. Section V analyzes the delay performance under heavytraffic regime and establishes the heavy-traffic optimality. Simulation results are given in Section VI, and the paper is concluded in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There is a huge body of work (see, e.g., <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>) devoted to the scheduling problem of MapReduce. The Fair Scheduler in Hadoop is the de facto standard <ref type="bibr" target="#b6">[7]</ref> in which the delay scheduling technique is used to improve locality. When a machine requests a new task, if the job that should be scheduled next according to fairness does not have available local tasks for this machine, the job is temporarily skipped and the machine checks the next job in the list. Since machines free up quickly, more tasks are served locally. However, machine idling could be introduced since an available machine may skip all the jobs when it cannot find a local task, and the tradeoff between the idling time and locality is not clear.</p><p>Many other scheduling algorithms are also commonly used in practice. The default scheduler of Hadoop is a first-in-first-out (FIFO) scheduler <ref type="bibr" target="#b5">[6]</ref>, which schedules an available machine to serve the map task from the head-of-line job with data closest to the machine. Although some locality optimization is performed, the head-of-line blocking problem results in limited locality and throughput performance. The scheduling algorithm Quincy <ref type="bibr" target="#b10">[11]</ref> is designed for Dryad, which is a distributed computational model that allows more complicated data flow than MapReduce. Quincy uses the amount of data transfer as the measure of locality and encodes it into the price model. Then, scheduling decisions are made by solving a min-cost flow problem. Joint scheduling of multiple phases of MapReduce have also been proposed <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. In this paper, we focus on map-intensive jobs.</p><p>There are numerous works on the scheduling problem in MapReduce with data locality, and we cannot list them all. Most of these existing scheduling algorithms put great effort on increasing locality for performance improvement. However, locality is not a direct performance measure of the MapReduce cluster. More concerned performance measures are throughput and delay. To the best of our knowledge, none of the existing scheduling algorithms provide direct theoretical guarantee on the throughput performance or the delay performance of the MapReduce cluster.</p><p>Since introduced, the idea of the MaxWeight scheduling algorithm <ref type="bibr" target="#b17">[18]</ref> is widely applied in the design of scheduling algorithms. However, most existing results on MaxWeight-type scheduling algorithms assume deterministic service time or geometrically distributed service time with preemptive tasks. In the MapReduce system, the service (processing) time of a map task is not deterministic, and it depends on whether the task is executed on a local machine or not. Besides, map tasks are nonpreemptive. In our earlier work <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we proved the throughput-optimality result for geometrically distributed service time of map tasks. In this paper, we extend the throughput-optimality result to general service time distributions. We remark that recently in <ref type="bibr" target="#b20">[21]</ref>, the authors studied MaxWeight scheduling for resource allocation in clouds and independently established a similar result with general service time distributions.</p><p>In terms of heavy-traffic analysis, our proof of heavy-traffic optimality follows the Lyapunov drift-based technique recently developed in <ref type="bibr" target="#b21">[22]</ref>, where heavy-traffic optimality of JSQ only and MaxWeight only have been proved. The result has been extended to a joint JSQ and MaxWeight algorithm in <ref type="bibr" target="#b22">[23]</ref> (in a different context), where jobs are preemptive and servers are homogeneous. Again, for the map task scheduling problem in MapReduce, tasks are nonpreemptive and machines are heterogeneous due to data locality, which imposes challenges to the state-space collapse analysis and makes the proof of heavy-traffic optimality a nontrivial application of the drift-based analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM MODEL AND THE SCHEDULING ALGORITHM</head><p>MapReduce is a framework for processing large-scale datasets in a distributed and parallel fashion <ref type="bibr" target="#b0">[1]</ref>. A data processing request under the MapReduce framework, called a job, is expressed as two functions: map and reduce. The input data for a job are split into data chunks and stored distributedly across the computing cluster by the distributed file system. Users submit jobs, the MapReduce framework breaks a job into multiple map tasks and reduce tasks as follows. Each data chunk of the input data is associated with one map task, which reads the data chunk, applies the map function to the data chunk, and then produces the intermediate data. The intermediate data are in the form of key-value pairs and stored on the local disks of the machines that execute the map tasks. The set of all possible keys of the intermediate data is divided into several subsets, each of which corresponds to a reduce task. The reduce task fetches the intermediate key-value pairs with keys in its assigned subset from all map tasks and applies the reduce function. By doing these, reduce tasks combine the intermediate data and produce the final results. Now we use a toy example as shown in Fig. <ref type="figure" target="#fig_0">1</ref> to demonstrate how MapReduce works. In this example, the objective of the job is to count the number of occurrences of each word. The input data consists of a set of documents, and it is split into data chunks with contents shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Whenever the map function sees a word in the data chunk, it generates a key-value pair using the word as the key and 1 as the value. The set of all the possible keys comprises of all the words. It is divided into two subsets: one containing all the words starting with letters a-m, and another one containing all the words starting with letters n-z. Therefore, we have two reduce tasks in the job. For each key, the reduce function sums up all the values belonging to the same key and then gets the number of occurrences of each word.</p><p>In this paper, we assume reduce tasks are not the bottleneck of the job processing and consider the map task scheduling. We consider a time-slotted model for a computing cluster with machines, indexed . Let be the set of indices of all the machines. Jobs come in stochastically, and when a job comes in, it brings a random number of map tasks, which need to be served by the machines. We assume that each data chunk is replicated and placed at three different machines. Therefore, each task is associated with three local machines. On average, it takes a longer time for a machine to process a task if the required data chunk is not stored locally since the machine needs to retrieve the data first. According to the associated local machines, tasks can be classified into types denoted by <ref type="bibr" target="#b0">(1)</ref> where are the indices of the three local machines. We use the notation to indicate machine is a local machine for type tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Arrival and Service</head><p>Let denote the number of type tasks arriving at the beginning of time-slot . We assume that the arrival process is temporally i.i.d. with arrival rate . We further assume that the number of total arrivals in each time-slot is bounded by a constant and has a positive probability to be zero. Let be the arrival rate vector, where is the set of task types with arrival rates greater than zero; i.e., . The execution of tasks is nonpreemptive, and the service time of each task is a random variable with positive integer value. At each machine, we assume that the service times for local tasks have the same CDF with expectation and the service times for remote tasks have the same CDF with expectation . Then, the service rate is for local tasks and for remote tasks. We assume since processing local tasks is more efficient than processing remote tasks. By making this assumption, data locality is integrated into our service model. We also assume that and have finite variances. Note that, for now, we do not impose any requirements on the shapes of and except the assumptions on the expectations and variances. Later on in the heavy-traffic analysis, we assume and to be geometric distributions with means and , respectively. This service model is motivated by <ref type="bibr" target="#b9">[10]</ref>, where the service times for tasks follow geometric distributions with different means due to data locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Task Scheduling Algorithm</head><p>The task scheduling problem is to assign the incoming tasks to the machines. Due to data locality, the task scheduling algorithm can significantly affect the efficiency of the system. As a simple example, consider a system with two machines and two task types as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Each data chunk has one copy stored on one of the two machines. Then, there are two task types: type 1 with machine 1 as the local machine, and type 2 with machine 2 as the local machine. Suppose both the arrival processes of the two types are Bernoulli processes with parameter , and the service processes are Bernoulli processes with parameter for local tasks and parameter for remote tasks. Recall that . Algorithm 1 always assigns tasks to remote machines. Then, the system can be stable only when is less than , resulting in a throughput less than . However, Algorithm 2 always assigns tasks to local machines, under which the system can achieve a throughput close to . Furthermore, to compare the delay performance, consider the scenario when . Under both algorithms, two Geo/Geo/1 queues are formed. Under Algorithm 1, the average waiting time of a task is , while under Algorithm 2, the average waiting time of a task becomes . In this paper, we consider a task scheduling algorithm consisting of two parts: routing and scheduling. The task scheduling algorithm is performed by the master, where the data locality information is also available. We present a new queue architecture as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. For each machine , the master maintains a local queue to buffer local tasks, denoted by ; for all the machines, the master maintains a common remote queue, denoted by (or sometimes</p><p>). The queue length vector <ref type="bibr" target="#b1">(2)</ref> denotes the queue lengths at the beginning of time-slot . When a task comes in, the master routes the task to some queue in the queueing system; when a machine is available, the master schedules the machine to serve a task from the corresponding local queue or the common remote queue. These two steps are illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. We call the first step routing, and with a slight abuse of terminology, we call the second step scheduling.</p><p>It should be clear from the context whether we are referring to the whole task scheduling problem or to this service scheduling step. Based on our queue architecture, we propose the following joint routing and scheduling algorithm. • Join the Shortest Queue (JSQ) Routing: When a task comes in, the master compares the queue lengths of the three local queues of the local machines and the common remote queue, and then routes the task to the shortest one. Thus, for an incoming task with type , the routing destination is the shortest queue in . Ties are broken randomly.</p><p>• MaxWeight Scheduling: If a machine just finished a task at time-slot , we say the machine is available at time-slot , and otherwise the machine is busy since it must be serving some task. Available machines are scheduled according to the MaxWeight algorithm. Let machine be an available machine at time-slot . Then, it is scheduled to serve a task from the local queue if , and a task from the common remote queue otherwise. Note that there can be multiple available machines that are scheduled to serve the tasks from the common remote queue at the same time-slot. We let these machines fetch the tasks in the common remote queue in a random order. Busy machines are scheduled to continue to serve their unfinished tasks, i.e., the execution of tasks is nonpreemptive. The proposed algorithm has a very low computational complexity. In the routing step, the master compares four queue lengths for each arriving task. In the scheduling step, the master compares and for each available machine. Therefore, for each arriving task and each available machine, the computational complexity is a constant.</p><p>Remark: The proposed scheduling algorithm, with a slight modification, works in heterogeneous environments, where machines may process tasks at different rates <ref type="bibr" target="#b13">[14]</ref>. For each machine , let and denote the service rates for local and remote tasks, respectively. Then, we modify the MaxWeight scheduling step as follows: An available machine is scheduled to serve the local queue if , and the common remote queue otherwise. After this modification, the throughput-optimality result also holds under heterogeneity. The proof is a direct extension of the current proof, so we focus on the current model for simplicity.</p><p>Remark: Here, we note that the proposed map task scheduling algorithm can cooperate with job-level strategies. Under the proposed queue architecture, each queue can be further divided into multiple subqueues according to the job that the task comes from, i.e., into per-job subqueues. Then, in the scheduling step, after being scheduled to serve some queue, an available machine can further choose from the subqueues according to the job-level strategy. However, this change will not affect our analysis throughout this paper, so we only consider this structure in the simulation part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Queue Dynamics</head><p>In this section, we derive the dynamics of the queue length vector . We use a working status vector</p><p>(3)</p><p>to denote the working status of the machines at the beginning of time-slot , where is the amount of time that machine has spent on the currently in-service task, and it is set back to 0 if machine just finished a task at the previous time-slot. By this definition, machine is available if and only if . At the beginning of time-slot , the master checks the working status of machines and the queue length vector. Then, tasks arrive, and the master routes the incoming tasks according to the queue length vector. Let and denote the number of type task arrivals allocated to and , respectively. Then,</p><p>. The number of arrivals allocated to each queue is expressed by the following arrival vector:</p><p>(4) where <ref type="bibr" target="#b4">(5)</ref> After the routing step, the master schedules the machines according to their working status and the queue length vector. If a machine is available and is scheduled to serve the local queue, or if the machine is busy serving the local queue, we say the machine is scheduled to the local queue. Otherwise, we say the machine is scheduled to the common remote queue. For both cases, an available machine may remain available if the assigned queue is empty when the machine tries to fetch a task. If this happens, we say this machine is idling. The scheduling decisions are recorded by the following scheduling decision vector: <ref type="bibr" target="#b5">(6)</ref> where machine is scheduled to and nonidling machine is scheduled to and idling machine is scheduled to and nonidling machine is scheduled to and idling.</p><p>(7) At the end of time-slot , finished tasks depart the queueing system. We use random variables and defined as follows to describe the service performed by machine . If machine is scheduled to the local queue and finishes a task at the end of time-slot , let ; if machine is scheduled to the local queue and is idling at time-slot , let ; otherwise let . Similarly, if machine is scheduled to the common remote queue and finishes a task at the end of time-slot , let ; if machine is scheduled to the common remote queue and is idling at time-slot , let ; otherwise let . Let <ref type="bibr" target="#b7">(8)</ref> which we call the service performed to the queues. Note that the service is not the departure of the queue. To obtain the departures, we also consider the following quantities, which we call the unused service:</p><formula xml:id="formula_0">if if (9)<label>(10)</label></formula><p>where is the set of machines that are scheduled to the common remote queue and nonidling at time-slot . Then, the departures of the queues are <ref type="bibr" target="#b10">(11)</ref> Let the service vector be denoted by <ref type="bibr" target="#b11">(12)</ref> and the unused service vector be denoted by <ref type="bibr" target="#b12">(13)</ref> Then, by the above notation, the queue dynamics can be finally expressed as <ref type="bibr" target="#b13">(14)</ref> The following lemma presents a property of the unused service vector , which will be used in the analysis of the throughput performance. The proof of this lemma is provided in the supplementary material.</p><p>Lemma 1 (Approximate Orthogonality): Let be the queue length vector and be the unused service vector at time-slot . Then, for any time-slot <ref type="bibr" target="#b14">(15)</ref> where is the number of machines in the system. In this queueing system, since the execution of tasks is nonpreemptive and the distributions of service times are not assumed to follow some particular distribution, the queueing process itself is not a Markov chain. We consider the working status vector , the scheduling decision vector , and the queue length vector , and they together form a Markov chain . We assume that the initial state of this Markov chain is and the state space consists of all the states that can be reached from the initial state, where is the set of nonnegative integers.</p><p>Remark: The Markov chain is irreducible and aperiodic under our assumptions. For any state in the state space, the queue lengths in the system are finite. Let be an integer such that and . Such exists since and are CDFs. Consider the event that each task in the system is finished within time-slots and there are no arrivals for the next time-slots. If this event happens, the Markov chain reaches the initial state after time-slots. Since this event happens with a positive probability, the -step transition probability from to the initial state is positive. Therefore, any state in the state space can reach the initial state, and hence the Markov chain is irreducible. The Markov chain is also aperiodic since the transition probability from the initial state to itself is positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THROUGHPUT OPTIMALITY</head><p>In this section, the throughput optimality of the map-scheduling algorithm proposed in Section III-B is established.</p><p>Let be the arrival rate vector. Consider a scheduling algorithm, and let be the number of unfinished tasks in the system at time-slot under this scheduling algorithm. Then, we say this scheduling algorithm stabilizes the system for if <ref type="bibr" target="#b15">(16)</ref> i.e., if the number of unfinished tasks does not explode. For the proposed scheduling algorithm, the number of unfinished tasks is . Therefore, the existence of a stationary distribution of the Markov chain is a sufficient condition for the stability of the system.</p><p>The capacity region is defined to be the set of arrival rate vectors for which there exists a scheduling algorithm that stabilizes the system. We will first identify an outer bound of the capacity region, and then prove that the proposed map-scheduling algorithm stabilizes any arrival rate vector strictly within this outer bound, which means that the capacity region coincides with the outer bound, and the proposed algorithm is throughput-optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outer Bound of the Capacity Region</head><p>Recall that is the arrival rate vector. For any task type , we assume that the number of type tasks that are allocated to machine has a rate ; then . Such a rate vector is called a decomposition of the arrival rate vector , and the index range may be omitted for conciseness in the rest of this paper. A necessary condition for the arrival rate vector to be supportable is that the average arrivals allocated to each machine in one time-slot must be served within one time-slot. Therefore, for any machine needs to satisfy <ref type="bibr" target="#b16">(17)</ref> where the left-hand side is the amount of time that machine needs to serve the arrivals allocated to it in one time-slot on average since the service rate is for local tasks and for remote tasks.</p><p>Let be the set of arrival rate vectors such that each element of it has a decomposition satisfying the necessary condition above. Formally there exists such that for any and for any and for any <ref type="bibr" target="#b17">(18)</ref> Then, gives an outer bound of the capacity region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Achievability Theorem 1 (Throughput Optimality):</head><p>The map-scheduling algorithm proposed in Section III-B stabilizes any arrival rate vector strictly within . Hence, is the capacity region of the system, and the proposed algorithm is throughput-optimal.</p><p>For any arrival rate vector is an irreducible and aperiodic Markov chain, and the number of unfinished tasks . If this Markov chain is positive recurrent, then the condition ( <ref type="formula">16</ref>) is satisfied since the distribution of will converge to the stationary distribution as . Thus, the stability can be obtained by proving the positive recurrence.</p><p>However, since the execution of tasks is nonpreemptive and the service times are random, there are two aspects in our model that make the proof difficult and differentiate our proof from the standard Lyapunov analysis. First, in the Markov chain, both and can be unbounded, so they both need to be considered in the Lyapunov function. Second, scheduling decisions are made only when some tasks are just completed. Then, the expected departures at time-slot not only depend on the state of the Markov chain at time-slot , but also depend on the state at some previous time-slot, which makes it difficult to find a Lyapunov function whose one time-slot drift is negative outside a finite set. Therefore, we consider a period of time-slots and analyze the time-slot drift of a Lyapunov function. By an extension of the Foster-Lyapunov theorem <ref type="bibr" target="#b23">[24]</ref>, it suffices to find a Lyapunov function and a positive integer for each arrival rate vector such that the time-slot Lyapunov drift is bounded within a finite subset of the state space and negative outside the subset. The intuition for throughput optimality is that for large enough , the average service time of departed tasks in the same category (local or remote) concentrates on its mean value. Then, the system behaves like a system with deterministic service times ( for a local task and for a remote task), for which the MaxWeight scheduling is throughput-optimal.</p><p>Consider any arrival rate vector .</p><p>Since is an open set, there exists such that . Suppose the decomposition of that satisfies ( <ref type="formula">17</ref>) is . Then <ref type="bibr" target="#b18">(19)</ref> is a decomposition of , and for any <ref type="bibr" target="#b19">(20)</ref> Let be defined as <ref type="bibr" target="#b20">(21)</ref> Then, can be thought of as one possibility of arrival allocation so that each is the arrival rate allocated to the queue . The following two lemmas reveal more properties of and lead to the proof of the throughput-optimality theorem. The proofs of these two lemmas are provided in the supplementary material.</p><p>Lemma 2: Consider any arrival rate vector and the corresponding defined in <ref type="bibr" target="#b20">(21)</ref>. Let be the arrival vector to the queues and be the queue length vector at time-slot . Then, under the JSQ routing algorithm, for any and any <ref type="bibr" target="#b21">(22)</ref> Lemma 3: Consider any arrival rate vector and the corresponding defined in <ref type="bibr" target="#b20">(21)</ref>. Let be the service vector and be the queue length vector at time-slot . Then, under the MaxWeight scheduling algorithm, there exists such that for any and any <ref type="bibr" target="#b22">(23)</ref> where and are positive constants independent of and is the -norm. Proof of Theorem 1: Consider the following Lyapunov function <ref type="bibr" target="#b23">(24)</ref> where is the -norm and is the -norm. In the rest of the proof, we use as an abbreviation of . Then, for any arrival rate vector , it suffices to find a finite set and two constants and with such that for some positive integer and any time-slot if <ref type="bibr" target="#b24">(25)</ref> if <ref type="bibr" target="#b25">(26)</ref> Let and . Then, for any , the time-slot Lyapunov drift can be calculated as <ref type="bibr" target="#b26">(27)</ref> First let us consider</p><p>. By the queue dynamics ( <ref type="formula">14</ref>)</p><p>The summation range is usually omitted for conciseness when it is clear from the context. By Lemma 1, the second term is bounded by a constant. Meanwhile, by our assumptions, the arrival vector and the service vector are bounded, and thus the unused service vector is also bounded since each entry of is no greater than the corresponding entry of by definition. Therefore, the drift can be bounded as where is a constant independent of . Utilizing the vector defined in <ref type="bibr" target="#b20">(21)</ref> . This proves positive recurrence. Hence, is the capacity region of the system, and the proposed algorithm is throughput-optimal.</p><p>Remark: The common Lyapunov function for proving the throughput optimality of MaxWeight-type algorithms is , which does not work in our case because is not a finite set for any . Note that for a fixed , we have infinitely many , which is a result of nonpreemptive tasks and general service time distributions. We therefore have to use a modified Lyapunov function defined in <ref type="bibr" target="#b23">(24)</ref>, which includes the and guarantees that the drift is negative when either or is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. HEAVY-TRAFFIC OPTIMALITY</head><p>In this section, we analyze the performance of the proposed algorithm beyond throughput. We consider the case that the service times have geometric distributions, i.e., the CDFs and are geometric distributions with parameters and , respectively. We will show that in the heavy-traffic regime, the proposed algorithm asymptotically minimizes the number of backlogged tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simplified Markov Chain</head><p>When the service times are geometrically distributed, the Markov chain that represents the queueing system can be simplified due to the memoryless property of geometric distributions. Instead of keeping track of both the amount of time that a machine has spent on the currently in-service task and the scheduling decision, we only need to maintain the following working status vector:</p><p>(29) where is the working status of machine at time-slot , and denotes available, working on a local task, and working on a remote task, respectively.</p><p>We still use the arrival vector to denote the arrivals to each queue. In contrast to the general case, we do not need to differentiate between whether a machine is idling or not. The scheduling decisions are recorded by the following scheduling decision vector:</p><p>(30) where if machine is scheduled to if machine is scheduled to (31)</p><p>We still use random variables and to denote the service performed by machine . Let denote the Bernoulli distribution with parameter . If machine is scheduled to the local queue and finishes a task at the end of timeslot , let ; if machine is scheduled to the local queue and is idling at time-slot , let ; otherwise let . Similarly, if machine is scheduled to the common remote queue and finishes a task at the end of time-slot , let ; if machine is scheduled to the common remote queue and is idling at time-slot , let ; otherwise let . Therefore, since we assume geometrically distributed service times, is a Bernoulli random variable with parameter as long as machine is scheduled to the local queue, and is a Bernoulli random variable with parameter as long as machine is scheduled to the common remote queue. Consider</p><formula xml:id="formula_1">if if if if</formula><p>Then, and . We still consider the service vector , where and . Then, the queue dynamics can still be expressed by <ref type="bibr" target="#b13">(14)</ref>. Now the queueing system can be expressed by the simplified Markov chain . We still assume that the system starts from the state that all the queues are empty and all the machines are available, so the initial state of the Markov chain is . The state space consists of all the states that can be reached from the initial state. Using similar arguments we can see, this Markov chain is irreducible, aperiodic, and positive recurrent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Heavy-Traffic Regime</head><p>Suppose the set of existing task types is such that there are machines, each of which is considered as a local machine by some task type, and the other machines are remote machines for all the task types. Denote the set of the machines that can have local tasks as and the set of the machines that only have remote tasks as . Then s.t. and . Without loss of generality, we assume and . We consider the heavy-traffic regime such that for any subset of , the sum arrival rate of tasks with local machines in is greater than the processing capability of the machines in . Formally, consider any subset of machines , and let s.t.</p><p>be the set of task types with local machines in . Let be the arrival rate vector. Then, the considered heavy-traffic regime assumes that for any <ref type="bibr">(32)</ref> which is referred to as the heavy local traffic assumption. In this regime, the machines in cannot accommodate all the arrivals, so we assume to stabilize the system. A legitimate application scenario for this heavy-traffic regime is node leasing. It is a common practice (e.g., for IBM Platform Computing) to support node leasing for MapReduce groups. Consider a MapReduce cluster that is shared by several groups, and each group has a dedicated pool of machines. There are agreements among these groups that they can lease some nodes to other groups when needed. In this case, it is therefore very common for a busy pool to borrow nodes from another group for executing its MapReduce jobs.</p><p>When the arrival rate vector satisfies the heavy local traffic assumption, there exists a decomposition of as described in the following lemma. This lemma is a crucial step to derive the state-space collapse result, and the proof is provided in the supplementary material.</p><p>Lemma 4: Let the arrival rate vector satisfy the heavy local traffic assumption, and consider any with , where is a positive constant. Then, there exists a decomposition of and a positive constant not depending on such that we have the following.</p><p>1) For any and any . 2) For any . 3) For any . When is in the capacity region, it is easy to see . We assume that <ref type="bibr">(33)</ref> where characterizes the distance between the arrival rate vector and the capacity boundary. The superscript will be The variance of the number of overall arrivals is given by (35) Let denote the Markov chain consisting of the queue length vector and the working status vector. Later, we will let go to zero to get heavy-traffic limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lower Bound</head><p>In this section, we derive a lower bound on the expectation of the number of backlogged tasks in the system under any stabilizing scheduling algorithm.</p><p>Recall that the arrival processes are . Consider any stabilizing scheduling algorithm, and let be the number of backlogged tasks at time-slot in the system under this scheduling algorithm. Assume that the process starts from and converges in distribution to a random variable as . Consider a hypothetical single server queueing system as depicted in Fig. <ref type="figure" target="#fig_3">4</ref>. The arrival process and service process of this system are defined as follows:</p><p>(36) where all the processes and are independent and each process is temporally i.i.d. For any and any , let , and for any and any , let . The queueing process in this system is denoted by , and it starts from . For any time-slot , the overall arrivals to the map task scheduling system in MapReduce and to the hypothetical single server queueing system are the same, and the overall service in the former system is stochastically less than the overall service in the latter system. Thus, the queue length is stochastically greater than and . Since is an irreducible, aperiodic, positive recurrent Markov chain, it will converge to a random variable in distribution as . Then, . IEEE/ACM TRANSACTIONS ON NETWORKING Reference <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Lemma 4</ref>] provides a lower bound on , which is also a lower bound on . The arrival process and service process in the hypothetical system satisfy that Denote by . Then, the lower bound is given by , and the following theorem holds. Theorem 2 (Lower Bound): For the map task scheduling system in MapReduce, consider an arrival process such that the number of total arrivals at each time-slot has expectation and variance . Let be the process of the number of backlogged tasks in the system under any stabilizing scheduling algorithm. Assume that it starts from and converges in distribution to a random variable as . Then, for any with , the expectation of the number of backlogged tasks in steady state can be lower-bounded as (37) Therefore, in the heavy-traffic limit as the arrival rate approaches the service rate from below, assuming the variance converges to a constant , the lower bound becomes</p><p>Especially, assuming that the Markov chain is in steady state under the proposed map-scheduling algorithm, the lower bound becomes (39)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. State-Space Collapse</head><p>For a single-server queueing system like the one in Fig. <ref type="figure" target="#fig_3">4</ref>, the discrete-time Kingman's bound <ref type="bibr" target="#b24">[25]</ref> gives an upper bound on the expectation of the queue length in steady state, which is derived by studying the drift of an appropriate Lyapunov function in steady state. The task scheduling system in MapReduce is a more complicated queueing system, which consists of multiple queues and thus has a multidimensional state space. However, in the heavy-traffic scenario, we will show that the multidimensional state description of the system reduces to a single dimension in the sense that the deviation from a particular direction has bounded moments, independent of the heavy-traffic parameter. This behavior of the queueing system in heavy-traffic scenario is called state-space collapse <ref type="bibr" target="#b21">[22]</ref>. When the state-space collapse happens, the system can be analyzed by the similar techniques as used for the single-dimensional system. In this section, we will establish the state-space collapse for the task scheduling system in MapReduce.</p><p>For the Markov chain with state space , since our aim is to analyze the queue lengths, we focus on the subspace of consisting of the queue length vectors. In this paper, the norms are norms and denoted by unless otherwise specified. Let be a vector with unit norm, where is the set of nonnegative real numbers. Then, the corresponding parallel and perpendicular components of a queue length vector are (40) If all the moments of , which represent the deviation of the queue length vector from the direction , are bounded by constants not depending on the heavy-traffic parameter , the state space is said to collapse to the direction of . Let (41) be the direction to which we will prove the state space collapses, where the first entries are ones and the following entries are zeros. Consider the Lyapunov function We can prove that the drift of satisfies the conditions in [22, <ref type="bibr">Lemma 1]</ref> (see <ref type="bibr" target="#b25">[26]</ref> for the derivation of this lemma). Then, by this lemma, has bounded moments in steady state not depending on , i.e., the state space collapses, which gives the following theorem. The proof of this theorem is provided in the supplementary material.</p><p>Theorem 3 (State-Space Collapse): For the map-scheduling system in MapReduce, consider an arrival process such that the number of total arrivals at each time-slot has expectation and variance . Let the arrival rate vector be strictly within the capacity region and satisfy the heavy local traffic assumption. Suppose the Markov chain is in steady state under the proposed map-scheduling algorithm. Then, for any and any with (42) there exist constants not depending on such that for each positive integer where the component is w.r.t. the direction defined in (41).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Upper Bound and Heavy-Traffic Optimality</head><p>In this section, we derive an upper bound on the expectation of the sum of the queue lengths in steady state based on the Lyapunov drift-based moment bounding techniques developed in <ref type="bibr" target="#b21">[22]</ref>, and we show that this upper bound is asymptotically tight under the heavy-traffic regime.</p><p>We have established the state-space collapse for the task scheduling system in MapReduce under the proposed algorithm, so the queue length vector in steady state concentrates along a single direction. Enlightened by the way the queue length in the single-server queueing system is bounded, we view the multidimensional state space in our problem as a one-dimensional state space along the collapse direction and then set the drift of the Lyapunov function to zero in steady state to obtain an upper bound for the expected queue length along this direction.</p><p>Due to the multiservice rates in our system, the terms related to service in the Lyapunov drift cannot be bounded</p><p>The following lemma bounds . The proof is provided in the supplementary material.</p><p>Lemma 6:</p><p>where is a constant. This bound indicates that, on average, the queue length vector only has finite projection in the direction of the difference between and . Similarly as in the proof of Lemma 1, .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>. Then</p><p>To bound the term , we use the state-space collapse result. By the state-space collapse theorem, there exists a constant such that . Then, using the Cauchy-Schwartz inequality and this bound yields</p><p>The following lemma bounds in the asymptotic regime . The proof is provided in the supplementary material. By this lemma, as . Lemma 7:</p><p>. Therefore, the term (45) can be bounded as We revive the superscript now. Combining the inequalities yields where Consequently Obviously, , so . Then, the bound (47) for the heavy-traffic limit follows immediately by taking limits on both sides. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SIMULATIONS</head><p>In this section, we use simulations to compare performance of the proposed algorithm (JSQ-MaxWeight) with Hadoop's default FIFO scheduler (FIFO) and the Hadoop Fair Scheduler (HFS) <ref type="bibr" target="#b6">[7]</ref>, where HFS is the de facto standard. Beyond these comparisons, we also demonstrate the robustness of JSQ-MaxWeight to estimation errors of parameters.</p><p>We consider a computing cluster with 400 machines and a dataset distributed uniformly in 360 of the them. Each data chunk has three replicas stored on three different machines uniformly. This simulation models the scenario that the dateset is a database like the user profile database of Facebook, and each job is some manipulation of the data such as searching.</p><p>The related simulation parameters are from mimicking real workload analyzed in <ref type="bibr" target="#b26">[27]</ref>. According to this analysis, the number of tasks in a job follows a power-law distribution, so we use a truncated Pareto distribution ranging from 10 to 100 000 with shape parameter 1.9 to generate the number of tasks for each job. Each task processes one data chunk uniformly selected from the dataset. The service time of a task follows a geometric distribution with parameter at a local machine, and with parameter at a remote machine. With this processing capability, the total task arrival rate should be no larger than per time-slot. We run the simulations for from 140 to 300 per time-slot with 20 intervals to evaluate performance.</p><p>Throughput Performance: Fig. <ref type="figure" target="#fig_4">5</ref> compares the throughput region of the three algorithms. The -axis shows the total task arrival rate , and the -axis shows the average task delay. A turning point on the curve indicates that is close to the throughput region boundary of the algorithm. Fig. <ref type="figure" target="#fig_4">5</ref> shows that under FIFO, the system becomes unstable at some between 240-260, whereas under JSQ-MaxWeight and HFS, the system stays stable for all the considered arrival rates.</p><p>Delay Performance: Fig. <ref type="figure" target="#fig_5">6</ref> compares the delay performance of JSQ-MaxWeight and HFS. FIFO is not included since the job-level policy of FIFO and these two algorithms follow different principle, thus not directly comparable. Also, FIFO becomes unstable at a medium load. For each , we calculate the average delay for jobs and tasks departing in steady state. Fig. <ref type="figure" target="#fig_5">6</ref> shows that for heavy traffic, JSQ-MaxWeight outperforms HFS on both job delay and task delay, and for light to medium traffic,  the two algorithms perform similarly. Note that the average job delay can be smaller than the average task delay. Most jobs are small jobs, each of which consists of up to several hundreds of tasks, whereas a great portion of tasks come from large jobs. Therefore, the average job delay is determined by small jobs, but the average task delay is more related to the tasks in large jobs.</p><p>Robustness: The MaxWeight scheduling step of JSQ-MaxWeight weights each local queue and the common remote queue by the service rates and , respectively. Therefore, errors in estimating these two parameters may affect the performance. We conducted simulations to evaluate the performance of JSQ-MaxWeight with estimation errors. The actual service rates are and . We tested three different settings with different 's in the MaxWeight scheduling step. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, the performances under these three sets of estimates are very close, which indicates that the algorithm is robust to estimation errors. This simulation result is consistent with the intuition since, in general, the MaxWeight scheduling algorithm is still throughput-optimal when the queue lengths are replaced with some functions of the queue lengths (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">Sec. 5.5]</ref>). The theoretical analysis of the robustness of the system is one of the future research problems we would like to study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we studied the map task scheduling problem in MapReduce with data locality. We derived an outer bound on the capacity region and a lower bound on the expected number of backlogged tasks in steady state. Then, we developed a scheduling algorithm constituted by the Join the Shortest Queue policy and the MaxWeight policy. This algorithm achieves the full capacity region and minimizes the expected number of backlogged tasks in the considered heavy-traffic regime. Therefore, the proposed algorithm is both throughput-optimal and heavy-traffic optimal. The proof technique was novel since nonpreemptive task execution and random service times were involved. Simulation results were given to evaluate the throughput performance and the delay performance for a large range of total arrival rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of MapReduce: word count.</figDesc><graphic coords="3,304.98,64.14,246.00,111.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example showing the impact of task scheduling algorithms with data locality. Machine 1 is local for type-1 tasks and remote for type-2 tasks; machine 2 is local for type-2 tasks and remote for type-1 tasks.</figDesc><graphic coords="4,91.02,63.12,147.96,70.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Queue architecture: are local queues associated with machines , respectively, and is the common remote queue. The scheduling algorithm has two steps: routing and scheduling.</figDesc><graphic coords="4,306.00,69.12,246.00,169.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Lower bounding system.</figDesc><graphic coords="9,355.98,63.12,144.00,51.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Throughput performance.</figDesc><graphic coords="12,341.04,65.10,175.92,136.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Delay performance. (a) Average job delay in steady state. (b) Average task delay in steady state.</figDesc><graphic coords="13,103.02,64.14,384.00,141.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Robustness to estimation errors.</figDesc><graphic coords="13,70.98,245.16,184.98,130.98" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>approved by IEEE/ACM TRANSACTIONS ON NETWORKING Editor T. Javidi. This work was supported in part by the NSF under Grant ECCS-1255425. W. Wang, K. Zhu, and L. Ying are with the School of Electrical, Computer and Energy Engineering,</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>directly. We consider the ideal service process that makes the best use of every machine. This service process is defined by if if if which is coupled with in the following way. For each , in the case that ; in the other case that if , and if . For each . Then, it can be seen that for each is temporally i.i.d. with , and for each is temporally i.i.d. with</p><p>. Furthermore, all the processes and are independent, and they are independent from and . Utilizing this service process, the queue dynamics ( <ref type="formula">14</ref>) can be rewritten as <ref type="bibr">(43)</ref> where</p><p>. Since the distribution of is known, we will use this equivalent queue dynamics to express the Lyapunov drift. Then, setting the Lyapunov drift to zero gives the following lemma.</p><p>Lemma 5: For the map task scheduling system in MapReduce, consider any arrival process with an arrival rate vector strictly within the capacity region. Suppose the queueing process is in steady state under the proposed map-scheduling algorithm. Then, for any direction</p><p>The proof of this lemma is standard, so we omit it due to space limit. Analyzing each term gives the following upper bound, which is asymptotically tight under the heavy-traffic limit.</p><p>Theorem 4 (Upper Bound): For the map-scheduling system in MapReduce, consider an arrival process such that the number of total arrivals at each time-slot has expectation and variance . Let the arrival rate vector be strictly within the capacity region and satisfy the heavy local traffic assumption. Suppose the Markov chain is in steady state under the proposed map-scheduling algorithm. Then, for any and any with the expectation of the sum of the queue lengths in steady state can be upper-bounded as (46) where , i.e., .</p><p>Therefore, in the heavy-traffic limit as the arrival rate approaches the service rate from below, assuming the variance converges to a constant , the upper bound becomes (47)</p><p>This upper bound under heavy-traffic limit coincides with the lower bound (39), which establishes the first moment heavytraffic optimality of the proposed algorithm.</p><p>Proof: The superscript is still temporarily omitted. For any time-slot , we analyze each term in Lemma 5 with respect to the collapse direction defined in (41).</p><p>First, the term on the left side of (44) satisfies where (a) follows from the fact that the sum of the arrivals, the ideal service process, and the queue lengths are independent; (b) follows from our assumption about the arrivals and . Next, we study the two terms on the right side in (44). Recall that we use to denote the variance of in (36). In fact, since this random variable has the same distribution as . Then</p><p>Since is in steady state, . Thus, . Meanwhile Note that by the coupling of and . Therefore, , and thus </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Commun</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hadoop</title>
		<ptr target="http://hadoop.apache.org" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scarlett: Coping with skewed content popularity in mapreduce clusters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys, Salzburg, Austria</title>
		<meeting>EuroSys, Salzburg, Austria</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Google file system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SOSP</title>
		<meeting>ACM SOSP<address><addrLine>Bolton Landing, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Hadoop distributed file system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shvachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chansler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE MSST</title>
		<meeting>IEEE MSST<address><addrLine>Incline Village, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hadoop: The Definitive Guide</title>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Yahoo Press</publisher>
			<pubPlace>Sunnyvale, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys</title>
		<meeting>EuroSys<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Resource-aware adaptive scheduling for mapreduce clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Polo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IFIP/USENIX Int. Conf. Middleware</title>
		<meeting>ACM/IFIP/USENIX Int. Conf. Middleware<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bar: An efficient data locality driven task scheduling algorithm for cloud computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/ACM CCGRID</title>
		<meeting>IEEE/ACM CCGRID<address><addrLine>Newport Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Degree-guided map-reduce task assignment with data locality constraint</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ISIT</title>
		<meeting>IEEE ISIT<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="985" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quincy: Fair scheduling for distributed computing clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SOSP</title>
		<meeting>ACM SOSP<address><addrLine>Big Sky, MT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of traces from a production MapReduce cluster</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kavulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/ACM CC-GRID</title>
		<meeting>IEEE/ACM CC-GRID<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Energy efficiency for large-scale mapreduce workloads with significant interactive analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys</title>
		<meeting>EuroSys<address><addrLine>Bern, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving mapreduce performance in heterogeneous environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX OSDI</title>
		<meeting>USENIX OSDI<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint scheduling of processing and shuffle phases in MapReduce systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kodialam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Lakshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IN-FOCOM</title>
		<meeting>IEEE IN-FOCOM<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coupling task progress for MapReduce resource-aware scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="1618" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint optimization of overlapping phases in MapReduce</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP Performance</title>
		<meeting>IFIP Performance<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stability properties of constrained queueing systems and scheduling policies for maximum throughput in multihop radio networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ephremides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1936" to="1948" />
			<date type="published" when="1992-12">Dec. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Map task scheduling in MapReduce with data locality: Throughput and heavy-traffic optimality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1609" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A throughput optimal algorithm for map task scheduling in mapreduce with data locality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perform. Eval. Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scheduling jobs with unknown duration in clouds</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Maguluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM<address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1887" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asymptotically tight steady-state queue length bounds implied by drift conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eryilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queueing Syst</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="311" to="359" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heavy traffic optimal resource allocation algorithms for cloud computing clusters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Maguluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ITC</title>
		<meeting>ITC<address><addrLine>Krakow, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Communication Networks: An Optimization, Control and Stochastic Networks Perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some inequalities for the queue GI/G/1</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F C</forename><surname>Kingman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="315" to="324" />
			<date type="published" when="1962-12">Dec. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hitting-time and occupation-time bounds implied by drift analysis with applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="502" to="525" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pacman: Coordinated memory caching for parallel jobs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discrete Stochastic Processes, ser</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">The Springer International Series in Engineering and Computer Science</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m">Her research interests include resource allocation in stochastic networks and data privacy</title>
		<meeting><address><addrLine>Beijing, China; Tempe, AZ, USA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Tsinghua University ; Arizona State University</orgName>
		</respStmt>
	</monogr>
	<note>2009, and is currently pursuing the Ph.D. degree in electrical, computer, and energy engi</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m">2010, and is currently pursuing the Ph.D. degree in electrical, computer, and energy engineering at Arizona State University</title>
		<meeting><address><addrLine>Beijing, China; Tempe, AZ, USA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>His research interest is in social networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
