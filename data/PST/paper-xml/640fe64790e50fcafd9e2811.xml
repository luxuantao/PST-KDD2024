<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Twin Contrastive Learning with Noisy Labels</title>
				<funder>
					<orgName type="full">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
				</funder>
				<funder ref="#_PpHwU4V">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_AZhGyw2">
					<orgName type="full">Shanghai Municipal of Science and Technology Project</orgName>
				</funder>
				<funder ref="#_RWzpXgb #_w7Baj5m">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhizhong</forename><surname>Huang</surname></persName>
							<email>zzhuang19@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<email>jpzhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>hmshan@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Science and Technology for Brain-inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Twin Contrastive Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from noisy data is a challenging task that significantly degenerates the model performance. In this paper, we present TCL, a novel twin contrastive learning model to learn robust representations and handle noisy labels for classification. Specifically, we construct a Gaussian mixture model (GMM) over the representations by injecting the supervised model predictions into GMM to link labelfree latent variables in GMM with label-noisy annotations. Then, TCL detects the examples with wrong labels as the outof-distribution examples by another two-component GMM, taking into account the data distribution. We further propose a cross-supervision with an entropy regularization loss that bootstraps the true targets from model predictions to handle the noisy labels. As a result, TCL can learn discriminative representations aligned with estimated labels through mixup and contrastive learning. Extensive experimental results on several standard benchmarks and real-world datasets demonstrate the superior performance of TCL. In particular, TCL achieves 7.5% improvements on CIFAR-10 with 90% noisy label-an extremely noisy scenario. The source code is available at https://github.com/Hzzone/TCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have shown exciting performance for classification tasks <ref type="bibr" target="#b12">[13]</ref>. Their success largely results from the large-scale curated datasets with clean human annotations, such as CIFAR-10 <ref type="bibr" target="#b18">[19]</ref> and ImageNet <ref type="bibr" target="#b5">[6]</ref>, in which the annotation process, however, is tedious and cumbersome. In contrast, one can easily obtain datasets with some noisy annotations-from online shopping websites <ref type="bibr" target="#b39">[40]</ref>, crowdsourcing <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref>, or Wikipedia <ref type="bibr" target="#b31">[32]</ref>-for training a clas-sification neural network. Unfortunately, the mislabelled data are prone to significantly degrade the performance of deep neural networks. Therefore, there is considerable interest in training noise-robust classification networks in recent years <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>To mitigate the influence of noisy labels, most of the methods in literature propose the robust loss functions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47]</ref>, reduce the weights of noisy labels <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, or correct the noisy labels <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>. In particular, label correction methods have shown great potential for better performance on the dataset with a high noise ratio. Typically, they correct the labels by using the combination of noisy labels and model predictions <ref type="bibr" target="#b30">[31]</ref>, which usually require an essential iterative sample selection process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Arazo et al. <ref type="bibr" target="#b0">[1]</ref> uses the small-loss trick to carry out sample selection and correct labels via the weighted combination. In recent years, contrastive learning has shown promising results in handling noisy labels <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. They usually leverage contrastive learning to learn discriminative representations, and then clean the labels <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref> or construct the positive pairs by introducing the information of nearest neighbors in the embedding space. However, using the nearest neighbors only considers the label noise within a small neighborhood, which is sub-optimal and cannot handle extreme label noise scenarios, as the neighboring examples may also be mislabeled at the same time.</p><p>To address this issue, this paper presents TCL, a novel twin contrastive learning model that explores the label-free unsupervised representations and label-noisy annotations for learning from noisy labels. Specifically, we leverage contrastive learning to learn discriminative image representations in an unsupervised manner and construct a Gaussian mixture model (GMM) over its representations. Unlike unsupervised GMM, TCL links the label-free GMM and label-noisy annotations by replacing the latent variable of GMM with the model predictions for updating the parameters of GMM. Then, benefitting from the learned data This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.</p><p>Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. distribution, we propose to formulate label noise detection as an out-of-distribution (OOD) problem, utilizing another two-component GMM to model the samples with clean and wrong labels. The merit of the proposed OOD label noise detection is to take the full data distribution into account, which is robust to the neighborhood with strong label noise. Furthermore, we propose a bootstrap cross-supervision with an entropy regulation loss to reduce the impact of wrong labels, in which the true labels of the samples with wrong labels are estimated from another data augmentation. Last, to further learn robust representations, we leverage contrastive learning and Mixup techniques to inject the structural knowledge of classes into the embedding space, which helps align the representations with estimated labels.</p><p>The contributions are summarized as follows:</p><p>? We present TCL, a novel twin contrastive learning model that explores the label-free GMM for unsupervised representations and label-noisy annotations for learning from noisy labels.</p><p>? We propose a novel OOD label noise detection method by modeling the data distribution, which excels at handling extremely noisy scenarios.</p><p>? We propose an effective cross-supervision, which can bootstrap the true targets with an entropy loss to regularize the model.</p><p>? Experimental results on several benchmark datasets and real-world datasets demonstrate that our method outperforms the existing state-of-the-art methods by a significant margin. In particular, we achieve 7.5% improvements in extremely noisy scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Contrastive learning. Contrastive learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref> have shown promising results for both representation learning and downstream tasks. The popular loss function is InfoNCE loss <ref type="bibr" target="#b27">[28]</ref>, which can pull together the data augmentations from the same example and push away the other negative examples. MoCo <ref type="bibr" target="#b11">[12]</ref> uses a memory queue to store the consistent representations. SimCLR <ref type="bibr" target="#b2">[3]</ref> optimizes InfoNCE within mini-batch and has found some effective training tricks, e.g., data augmentation. However, as unsupervised learning, they mainly focus on inducing transferable representations for the downstream tasks instead of training with noisy annotations. Although supervised contrastive learning <ref type="bibr" target="#b16">[17]</ref> can improve the representations by human labels, it harms the performance when label noise exists <ref type="bibr" target="#b22">[23]</ref>.</p><p>Learning with noisy labels. Most of the methods in literature mitigate the label noise by robust loss functions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>, noise transition matrix <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, sample selection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref>, and label correction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In particular, label correction methods have shown promising results than other methods. Arazo et al. <ref type="bibr" target="#b0">[1]</ref> applied a mixture model to the losses of each sample to distinguish the noisy and clean labels, inspired by the fact that the noisy samples have a higher loss during the early epochs of training. Similarly, DivideMix <ref type="bibr" target="#b19">[20]</ref> employs two networks to perform the sample selection for each other and applies the semi-supervised learning technique where the targets are computed from the average predictions of different data augmentations. Due to the success of contrastive learning, many attempts have been made to improve the robustness of classification tasks by combining the advantages of contrastive learning. Zheltonozhskii et al. <ref type="bibr" target="#b47">[48]</ref> used contrastive learning to pre-train the classification model. MOIT <ref type="bibr" target="#b28">[29]</ref> quantifies this agreement between feature representation and original label to identify mislabeled samples by utilizing a k-nearest neighbor (k-NN) search. RRL <ref type="bibr" target="#b20">[21]</ref> performs label cleaning by two thresholds on the soft label, which is calculated from the predictions of previous epochs and its nearest neighbors. Sel-CL <ref type="bibr" target="#b22">[23]</ref> leverages the nearest neighbors to select confident pairs for supervised contrastive learning <ref type="bibr" target="#b16">[17]</ref>. Unlike existing methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref> that detect the wrong labels within the neighborhood, TCL formulates the wrong labels as the out-of-distribution examples by modeling the data distribution of representations learned by contrastive learning. In addition, we propose a cross-supervision with entropy regularization to better estimate the true labels and handle the noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed TCL</head><p>Each image in dataset D = {x i } N i=1 associates with an annotation y ? {1, 2, . . . , K}. In practice, some examples may be mislabeled. We aim to train a classification network, p ? (y|x) = g(x; ?) ? R K , that is resistant to the noisy labels in training data, and generalizes well on the clean testing data. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the framework of our proposed TCL.</p><p>Overview. In the context of our framework, f (?) and g(?) share the same backbone and have additional individual heads to output representations and class predictions from two random and one mixup data augmentations. Afterward, there are four components in TCL, including (i) modeling the data distribution via a GMM in Sec. 3.1 from the model predictions and representations; (ii) detecting the examples with wrong labels as out-of-distribution samples in Sec. 3.2; (iii) cross-supervision by bootstrapping the true targets in Sec. 3.3; and (iv) learning robust representations through contrastive learning and mixup in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modeling Data Distribution</head><p>Given the image dataset consisting of N images, we opt to model the distribution of x over its representation v = f (x) via a spherical Gaussian mixture model (GMM). After in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representations</head><p>Wrong Clean</p><p>Clean or wrong labels? troducing discrete latent variables z ? {1, 2, . . . , K} that determine the assignment of observations to mixture components, the unsupervised GMM can be defined as</p><formula xml:id="formula_0">p(v) = K k=1 p(v, z = k) = K k=1 p(z = k)N (v|? k , ? k ).<label>(1)</label></formula><p>where ? k is the mean and ? k a scalar deviation. If we assume that the latent variables z are uniform distributed, that is, p(z = k) = 1/K, we can define the posterior probability that assigns x i to k-th cluster:</p><formula xml:id="formula_1">? ik = p(z i = k|x i ) ? N (x i |? k , ? k ).<label>(2)</label></formula><p>In an ideal scenario where all the samples have clean labels y ? {1, 2, . . . , K}, the discrete latent variables z would be identical to the annotation y, and the parameters ? k , ? k and latent variable z can be solved through a standard Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b4">[5]</ref>. However, in practice, the labels are often noisy and the latent variable z, estimated in an unsupervised manner, has nothing to do with the label y. Therefore, we are interested in connecting latent variable z, estimated in an unsupervised fashion (i.e. label-free), and the available annotations y, label-noisy, for the task of learning from noisy labels.</p><p>To link them together, we propose to inject the model predictions p ? (y i = k|x i ), learned from noisy labels, into the latent variables z. Specifically, we propose to replace the unsupervised assignment p(z i = k|x i ) with noisy-supervised assignment p ? (y i = k|x i ). As a result, we can connect the latent variable z with the label y, and thus use the noisy supervision to guide the update of the parameters of GMM.</p><p>In particular, the update of the GMM parameters becomes</p><formula xml:id="formula_2">? k = norm i p ? (y i = k|x i )v i i p ? (y i = k|x i ) ,<label>(3)</label></formula><formula xml:id="formula_3">? k = i p ? (y i = k|x i )(v i -? k )(v i -? k ) T i p ? (y i = k|x i ) ,<label>(4)</label></formula><p>where norm(?) is 2 -normalization such that ? k 2 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Out-Of-Distribution Label Noise Detection</head><p>Previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref> typically detect the wrong labels within the neighborhood, that is, using the information from nearest neighbors. It is limited as the neighboring examples are usually mislabeled at the same time. To address this issue, we propose to formulate label noise detection as to detect the out-of-distribution examples.</p><p>After building the connection between the latent variables z and labels y, we are able to detect the sample with wrong labels through the posterior probability in Eq. ( <ref type="formula" target="#formula_1">2</ref>). We implement it as a normalized version to take into account the intra-cluster distance, which allows for detecting the samples with likely wrong labels:</p><formula xml:id="formula_4">? ik = exp -(v i -? k ) T (v i -? k )/2? k k exp (-(v i -? k ) T (v i -? k )/2? k ) .<label>(5)</label></formula><p>Since 2 -normalization has been applied to both embeddings v and the cluster centers</p><formula xml:id="formula_5">? k , yielding (v -? k ) T (v -? k ) = 2 -2v T ? k .</formula><p>Therefore, we can re-write Eq. ( <ref type="formula" target="#formula_4">5</ref>) as:</p><formula xml:id="formula_6">? ik = p(z i = k|x i ) = exp(v T i ? k /? k ) k exp(v T i ? k /? k ).<label>(6)</label></formula><p>Once built the GMM over the distribution of representations, we propose to formulate the conventional noisy label detection problem as out-of-distribution sample detection problem. Our idea is that the samples with clean labels should have the same cluster indices after linking the cluster index and class label. Specifically, given one particular class y = k, the samples within this class can be divided into two types: in-distribution samples with clean labels, and out-of-distribution samples with wrong labels. Therefore, we define the following conditional probability to measure the probability of one sample with clean label:</p><formula xml:id="formula_7">? y=z|i = p(y i = z i |x i ) = exp(v T i ? zi /? zi ) k exp(v T i ? k /? k ). (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>Although Eqs. ( <ref type="formula" target="#formula_6">6</ref>) and ( <ref type="formula" target="#formula_7">7</ref>) share similar calculations, they have different meanings. Eq. ( <ref type="formula" target="#formula_6">6</ref>) calculates the probability of one example belonging to k-th cluster while Eq. ( <ref type="formula" target="#formula_7">7</ref>) the probability of one example having clean label-that is, y i = z i . Therefore, the probability of one example having the wrong label can be written as</p><formula xml:id="formula_9">? y =z|i = p(y i = z i |x i ) = 1 -p(y i = z i |x i ).</formula><p>Furthermore, instead of setting a human-tuned threshold for ? y=z|i , we opt to employ another two-component GMM following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> to automatically estimate the clean probability ? y=z|i for each example. Similar to the definition of GMM in Eq. ( <ref type="formula" target="#formula_0">1</ref>), this two-components GMM is defined as follows:</p><formula xml:id="formula_10">p(? y=z|i ) = 1 c=0 p(? y=z|i , c) = 1 c=0 p(c)p(? y=z|i |c), (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where c is the new introduced latent variable: c = 1 indicates the cluster of clean labels with higher mean value and vice versus c = 0. After modeling the GMM over the probability of one example having clean labels, ? y=z|i , we are able to infer the posterior probability of one example having clean labels through the two-component GMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-supervision with Entropy Regularization</head><p>After the label noise detection, the next important step is to estimate the true targets by correcting the wrong label to reduce its impact, called label correction. Previous works usually perform label correction using the temporal ensembling <ref type="bibr" target="#b24">[25]</ref> or from the model predictions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> before mixup augmentation without back-propagation.</p><p>TCL leverages a similar idea to bootstrap the targets through the convex combination of its noisy labels and the predictions from the model itself:</p><formula xml:id="formula_12">t (1) i = w i y i + (1 -w i )g(x (1) i ) t (2) i = w i y i + (1 -w i )g(x (2) i ) ,<label>(9)</label></formula><p>where g(x</p><p>i ) and g(x</p><p>i ) are the predictions of two augmentations, y i the noisy one-hot label, and w i ? [0, 1] represents the posterior probability as p(c = 1|? y=z|i ) from the two-component GMM defined in Eq. ( <ref type="formula" target="#formula_10">8</ref>). When computing Eq. ( <ref type="formula" target="#formula_12">9</ref>), we stop the gradient from g to avoid the model predictions collapsed into a constant, inspired by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Guided by the corrected labels t i , we swap two augmentations to compute the classification loss twice, leading to the bootstrap cross supervision, formulated as:</p><formula xml:id="formula_15">L cross = g(x (1) i ), t (2) i + g(x (2) i ), t (1) i ,<label>(10)</label></formula><p>where is the cross-entropy loss. This loss makes the predictions of the model from two data augmentations close to corrected labels from each other. In a sense, if w i = 0, the model is encouraged for consistent class predictions between different data augmentations, otherwise w i = 1 it is supervised by the clean labels.</p><p>In addition, we leverage an additional entropy regularization loss on the predictions within a mini-batch B:</p><formula xml:id="formula_16">L reg = -H 1 |B| x?B g(x) + 1 |B| x?B H (g(x)) ,<label>(11)</label></formula><p>where H(?) is the entropy of predictions <ref type="bibr" target="#b32">[33]</ref>. The first term can avoid the predictions collapsing into a single class by maximizing the entropy of average predictions. The second term is the minimum entropy regularization to encourage the model to have high confidence for predictions, which was previously studied in semi-supervised learning literature <ref type="bibr" target="#b8">[9]</ref>.</p><p>Although both using the model predictions, we would emphasize that the cross-supervision in TCL is different to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> in three aspects: (i) both x</p><p>(1) i and x</p><p>(2) i are involved in back-propagation; (ii) the strong augmentation <ref type="bibr" target="#b2">[3]</ref> used to estimate the true targets can prevent the overfitting of estimated targets; and (iii) TCL employs two entropy regularization terms to avoid the model collapse to one class.</p><p>The final classification loss is given as follows:</p><formula xml:id="formula_17">L cls = L cross + L reg .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning Robust Representations</head><p>To model the data distribution that is robust to noisy labels, we leverage contrastive learning to learn the representations of images. Specifically, contrastive learning performs instance-wise discrimination <ref type="bibr" target="#b37">[38]</ref> using the InfoNCE loss <ref type="bibr" target="#b27">[28]</ref> to enforce the model outputting similar embeddings for the images with semantic preserving perturbations. Formally, the contrastive loss is defined as follows:</p><formula xml:id="formula_18">L ctr = -log exp f (x (1) ) T f (x (2) )/? x?S exp f (x (1) ) T f (x)/? , (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where ? is the temperature and S is the B except x (1) . x (1)  and x (2) are two augmentations of x. Intuitively, InfoNCE loss aims to pull together the positive pair (x (1) , x (2) ) from two different augmentations of the same instance, and push them away from negative examples of other instances. Consequently, it can encourage discriminative representations in a pure unsupervised, or label-free manner.</p><p>Although beneficial in modeling latent representations, contrastive learning cannot introduce compact classes without using the true labels. Since the label y is noisy, we leverage Mixup <ref type="bibr" target="#b45">[46]</ref> to improve within-class compactness, which has been shown its effectiveness against label noise in literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. Specifically, a mixup training pair (x (m) i , t(m) i ) is linearly interpolated between (x i , ti ) and (x j , tj ) under a control coefficient ? ? Beta(?, ?):</p><formula xml:id="formula_20">x (m) i = ?x i + (1 -?)x j , t(m) i = ? ti + (1 -?) tj ,<label>(14)</label></formula><p>where x j is randomly selected within a mini-batch, and ti = (t</p><formula xml:id="formula_21">(1) i + t<label>(2)</label></formula><p>i )/2 is the average of estimated true labels of two data augmentations. Intuitively, we can inject the structural knowledge of classes into the embedding space learned by contrastive learning. This loss can be written as:</p><formula xml:id="formula_22">L align = g(x (m) i ), t(m) i + (p(z|x (m) i ), t(m) i ),<label>(15)</label></formula><p>where the second term can align the representations with estimated labels. In a sense, L align regularizes classification network g and encourages f to learn compact and wellseparated representations. Furthermore, we would point out two differences between TCL and <ref type="bibr" target="#b20">[21]</ref>, although both using mixup to boost the representations. First, <ref type="bibr" target="#b20">[21]</ref> does not explicitly model the data distribution p(z|x (m) i ) like TCL. Second, TCL has leveraged the full training dataset via the corrected label t(m) i instead of a subset of clean examples in <ref type="bibr" target="#b20">[21]</ref>, which leads to stronger robustness of TCL over <ref type="bibr" target="#b20">[21]</ref> on extreme high label noise ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and inference</head><p>The overall training objective is to minimize the sum of all losses:</p><formula xml:id="formula_23">L = L cls + L ctr + L align .<label>(16)</label></formula><p>We find that a simple summation of all losses works well for all datasets and noise levels, which indicates the strong generalization of the proposed method. During inference, the data augmentations are disabled and the class predictions are obtained by argmax k p ? (k|x).</p><p>The training algorithm of the proposed method is shown in Alg. 1. In a sense, the architecture of our method leads to an EM-like algorithm: (1) the E-step updates {(? k , ? k )} K k=1 for TCL, and {w i } N i=1 for each sample in D to form the true targets with the predictions from another data augmentations, and (2) the M-step optimizes the model parameters by Eq. ( <ref type="formula" target="#formula_23">16</ref>) to better fit those estimated targets. Therefore, the convergence of TCL can be theoretically guaranteed, following the standard EM algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct experiments on multiple benchmark datasets with simulated and real-world label noises. We strictly follow the experimental settings in previous literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref> for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on simulated datasets</head><p>Datasets. Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, we validate our method on CIFAR-10/100 <ref type="bibr" target="#b18">[19]</ref>, which contains 50K and 10K images with size 32 ? 32 for training and testing, respectively. We leave 5K images from the training set as the validation set for hyperparameter tuning, then train the model on the full training set for fair comparisons. Two types of label noise are simulated: symmetric and asymmetric label noise. Symmetric noise randomly assigns the labels of the training set to random labels with predefined percentages, a.k.a, noise ratio, which includes 20%, 50%, 80%, and 90% on two datasets in this paper. Asymmetric noise takes into account the class semantic information, and the labels are only changed to similar classes (e.g., truck ? automobile). Here, only experiments on the CIFAR-10 dataset with 40% noise ratio for asymmetric noise are conducted; otherwise, the classes with above 50% label noise cannot be distinguished.</p><p>Training details. Same as previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, we use a PreAct ResNet-18 <ref type="bibr" target="#b13">[14]</ref> as the encoder. We adopt SGD optimizer to train our model with a momentum of 0.9, a weight decay of 0.001, and a batch size of 256 for 200 epochs. The learning rate are linearly warmed up to 0.03 for 20 epochs and decayed with the cosine schedule. The data augmentations of <ref type="bibr" target="#b2">[3]</ref>   <ref type="table">1</ref>. Comparisons with state-of-the-art methods on simulated datasets. The results for previous methods are copied from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref> to avoid the bias of self-implementation, and we strictly follow their experimental settings. Each runs has been repeated 3 times with different randomly-generated noise and we report the mean and std values of last 5 epochs. of classes. The temperature ? of contrastive loss and the ? of mixup are 0.25 and 1. The settings are shared for all experiments, which are significantly different from <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> that adopt specific configurations for different datasets and even for different noise ratios/types. Quantitative results. Table <ref type="table">1</ref> presents the the comparisons with existing state-of-the-art methods. Our method yields competitive performance on low noise ratios, but promising improvements over recent methods on extreme noise ratios and the most challenging CIFAR-100 dataset with 100 classes. In particular, with 90% label noise, there are 7.5% and 5.7% improvements on for CIFAR-10 and CIFAR-100, respectively. We stress that the hyperparameters are consistent for different noise ratios/types. In practical scenarios, the noise ratio for a particular dataset is unknown, so it is hard to tune the hyper-parameters for better performance. Therefore, these results indicate the strong generalization ability of our method regardless of noise ratios/types.</p><p>For fair comparisons, following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, we performed extra experiments on fine-tuning the classification network for 70 epochs with the detected clean samples and mixup augmentation, termed TCL+. Table <ref type="table" target="#tab_2">2</ref> shows that under low label noise (below 50%), TCL+ achieves significant improvements over TCL and outperforms the recent state-of-the-art methods. The benefits from the detected clean subset and longer training, which can fully utilize the useful supervision signals from labeled examples.</p><p>In Appendix A, we also perform the k-NN classification over the learned representations, which indicates that our method has maintained meaningful representations better than the pure unsupervised learning model. In Appendix B, we provide more experimental results and analysis on asymmetric label noise and imbalance data. Qualitative results. Figs. <ref type="figure" target="#fig_1">2(a</ref>) and (b) visualize the learned representations with extremely high noise ratio, demonstrating that our method can produce distinct structures of learned representations with meaningful semantic information. Especially, Fig. <ref type="figure" target="#fig_1">2</ref>(b) presents the samples with noisy labels in the embedding space, in which the label noise can be accurately detected by our proposed method. In addition, by visualizing the histogram of p(y = z|x) for the training set in Fig. <ref type="figure" target="#fig_1">2</ref>(c), we confirm that the proposed method can effectively distinguish the samples noisy and clean labels. We visualize the validation accuracy across training in Fig. <ref type="figure" target="#fig_1">2(d</ref>). As expected, TCL performs stable even with the extreme 90% label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>We conduct ablation studies to validate our motivation and design with the following baselines, and the results are shown in Table <ref type="table" target="#tab_3">3</ref>. (i) Baseline. We start the baseline method by removing the proposed noisy label detection and bootstrap crosssupervision, where the model is directly guided by noisy labels. As expected, the performance significantly degrades for the extremely high noise ratio (i.e., 90%). (ii) Label Noise Detection. We assess the effectiveness of different detection methods including the crossentropy loss <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, k-NN search <ref type="bibr" target="#b28">[29]</ref>, and our out-ofdistribution (OOD) detection. For fair comparisons, the predictions from the images before mixup are employed as the true labels in Eq. <ref type="bibr" target="#b8">(9)</ref>. Obviously, the label noise detection has alleviated the degeneration to some degree (Exp. (i)), where our method consistently outperforms other baselines. Fig. <ref type="figure" target="#fig_2">3</ref> visualizes their AUCs across training. The proposed OOD detection is better at distinguishing clean and wrong labels. Thanks to the representations learned by contrastive learning, k-NN search performs better than cross-entropy loss. However, it is limited due to the use of the original labels to detect noisy ones, while our method constructs a GMM using the model predictions. (iii) Target Estimation. Another key component is the cross-supervision that bootstraps the true targets from the predictions of another data augmentation. We replace it with the temporal ensembling <ref type="bibr" target="#b24">[25]</ref>, where the hyperparameters are set as suggested by <ref type="bibr" target="#b24">[25]</ref>. Furthermore, Exp. (iii) estimates true targets from the images before mixup <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>.   Hyperparameters. We evaluate the most essential hyperparameters to our design, including the temperature ? for contrastive loss and update frequency for TCL on CIFAR-10 with 90% symmetric noise. Here, the update frequency denotes how may epochs that we update the parameters of TCL, {(? k , ? k )} K k=1 and {w i } N i=1 . Fig. <ref type="figure" target="#fig_3">4</ref> shows that our method is robust to different choices of hyperparameters. Even though TCL updates for every 32 epochs, our method has still performed well, which indicates that the computational cost can be significantly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on real-world datasets</head><p>Datasets and training details. We validate our method on two real-word noisy datasets: WebVision <ref type="bibr" target="#b23">[24]</ref> and Clothing1M <ref type="bibr" target="#b39">[40]</ref>. Webvision contains millions of noisilylabeled images collected from the web using the keywords of ImageNet ILSVRC12 <ref type="bibr" target="#b5">[6]</ref>. Following the convension <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, we conducted experiments on the first 50 classes of the Google image subset, termed WebVision (mini) and evaluated on both WebVision and ImageNet validation set. Clothing1M consists of 14 kinds of images collected from online shopping websites. Only the noisy training set is used in our experiments. We used a batch size of 256 on 4 GPUs, and trained a ResNet-50 for 40 epochs (without warm-up) on Clothing1M and a ResNet-18 for 130 epochs (warm-up 10 epochs) on WebVision, respectively. Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>, for Clothing1M, the encoder is initialized with ImageNet pre-trained weights, the initial learning rate is 0.01, and 256 mini-batches are sampled as one epoch. Other hyper-parameters are kept to be the same without further tuning.</p><p>Quantitative results. Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref> present the results on WebVision and Clothing1M datasets. Our method outperforms state-of-the-art methods on both datasets, demonstrating its superior performance in handling real-world noisy datasets. We note that after checking the Clothing1M dataset, there are still lots of mislabeled images in the testing set.</p><p>WebVision ILSVRC12 top1 top5 top1 top5 Forward <ref type="bibr" target="#b29">[30]</ref>  Method Acc (%) Cross-Entropy 69.2 Label Correction <ref type="bibr" target="#b0">[1]</ref> 71.0 Joint-Opt <ref type="bibr" target="#b33">[34]</ref> 72.2 ELR <ref type="bibr" target="#b24">[25]</ref> 72.8 SL <ref type="bibr" target="#b36">[37]</ref> 74.4 DivideMix <ref type="bibr" target="#b19">[20]</ref> 74.4 MentorMix <ref type="bibr" target="#b14">[15]</ref> 74.3 RRL <ref type="bibr" target="#b20">[21]</ref> 74.8 TCL (ours) 74.8 Therefore, the results on Clothing1M may not be such reliable as other datasets to evaluate the true performance of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced TCL, a novel twin contrastive learning model for learning from noisy labels. By connecting the label-free latent variables and label-noisy annotations, TCL can effectively detect the label noise and accurately estimate the true labels. Extensive experiments on both simulated and real-world datasets have demonstrated the superior performance of TCL than existing state-of-the-art methods. In particular, TCL achieves 7.5% performance improvement under extremely 90% noise ratio. In the future, we will improve TCL with semantic information for low noise ratios and explore dynamically updating the GMM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the proposed TCL. The networks g and f with shared encoder and independent two-layer MLP output the class predictions and representations. Then, TCL models the data distribution via a GMM, and detects the examples with wrong labels as out-of-distribution examples. To optimize TCL, these results lead to cross-supervision and robust representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Qualitative results. For the model trained on CIFAR-10 with 90% sym. noise at 200th epoch, we show t-SNE visualizations for the learned representations of (a) testing set where different color denotes different class predicted by g(?) and (b) 10K samples from training set colored by the true labels; the gray '+' denotes the samples with noisy labels. (c) The histogram of p(y = z|x) for full training set colored by the clean and noisy labels. (d) The validation accuracy across training of CIFAR-10 and CIFAR-100 on 90% sym. noise.</figDesc><graphic url="image-10.png" coords="6,281.79,297.68,136.18,129.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Training curve of AUC for noisy label detection trained on CIFAR-10 with 90% sym. noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Ablation results for hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>are applied to two views (ResizedCrop, ColorJitter, and etc). Only crop and horizontal flip are employed for mixup. Both projection and classification heads are a two-layer MLP with the dimension 128 and the numberTable</figDesc><table><row><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell>Noise type/rate</cell><cell>Sym.</cell><cell>Asym. Avg.</cell><cell></cell><cell>Sym.</cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell>20% 50% 80% 90%</cell><cell>40%</cell><cell cols="4">20% 50% 80% 90%</cell></row><row><cell cols="2">Cross-Entropy 82.7 57.9 26.1 16.8</cell><cell>76.0 51.9</cell><cell>61.8</cell><cell>37.3</cell><cell>8.8</cell><cell>3.5 27.8</cell></row><row><cell cols="2">Mixup (17') [46] 92.3 77.6 46.7 43.9</cell><cell>77.7 67.6</cell><cell>66.0</cell><cell cols="2">46.6 17.6</cell><cell>8.1 34.6</cell></row><row><cell cols="2">P-correction (19') [43] 92.0 88.7 76.5 58.2</cell><cell>91.6 81.4</cell><cell>68.1</cell><cell cols="2">56.4 20.7</cell><cell>8.8 38.5</cell></row><row><cell cols="2">M-correction (19') [1] 93.8 91.9 86.6 68.7</cell><cell>87.4 85.7</cell><cell>73.4</cell><cell cols="3">65.4 47.6 20.5 51.7</cell></row><row><cell cols="2">ELR (20') [25] 93.8 92.6 88.0 63.3</cell><cell>85.3 84.6</cell><cell>74.5</cell><cell cols="3">70.2 45.2 20.5 52.6</cell></row><row><cell cols="2">DivideMix (20') [20] 95.0 93.7 92.4 74.2</cell><cell>91.4 89.3</cell><cell>74.8</cell><cell cols="3">72.1 57.6 29.2 58.4</cell></row><row><cell cols="2">MOIT (21') [29] 93.1 90.0 79.0 69.6</cell><cell>92.0 84.7</cell><cell>73.0</cell><cell cols="3">64.6 46.5 36.0 55.0</cell></row><row><cell cols="2">RRL (21') [21] 95.8 94.3 92.4 75.0</cell><cell>91.9 89.8</cell><cell>79.1</cell><cell cols="3">74.8 57.7 29.3 60.2</cell></row><row><cell cols="2">Sel-CL+ (22') [23] 95.5 93.9 89.2 81.9</cell><cell>93.4 90.7</cell><cell>76.5</cell><cell cols="3">72.4 59.6 48.8 64.3</cell></row><row><cell cols="2">TCL (ours) 95.0 93.9 92.5 89.4</cell><cell>92.6 92.7</cell><cell>78.0</cell><cell cols="3">73.3 65.0 54.5 67.7</cell></row><row><cell></cell><cell>?0.1 ?0.1 ?0.2 ?0.2</cell><cell>?0.1</cell><cell cols="4">?0.2 ?0.2 ?0.3 ?0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with SOTAs under low label noise.</figDesc><table><row><cell cols="2">CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>Sym.</cell><cell>Asym.</cell><cell>Sym.</cell></row><row><cell>20% 50%</cell><cell>40%</cell><cell>20% 50%</cell></row><row><cell>DivideMix [20] 95.0 93.7</cell><cell>91.4</cell><cell>74.8 72.1</cell></row><row><cell>MOIT [29] 93.1 90.0</cell><cell>92.0</cell><cell>73.0 64.6</cell></row><row><cell>MOIT+ [29] 94.1 91.8</cell><cell>93.3</cell><cell>75.9 70.6</cell></row><row><cell>RRL [21] 95.8 94.3</cell><cell>91.9</cell><cell>79.1 74.8</cell></row><row><cell>Sel-CL+ [23] 95.5 93.9</cell><cell>93.4</cell><cell>76.5 72.4</cell></row><row><cell>TCL (ours) 95.0 93.9</cell><cell>92.6</cell><cell>78.0 73.3</cell></row><row><cell>TCL+ (ours) 96.0 94.5</cell><cell>93.7</cell><cell>79.3 74.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation results of different components in TCL. Without L reg . We remove L reg and the results indicate that it plays an important role, especially on extremely high label noise. Removing each term in L reg obtains similar results. We argue that L reg works in two aspects: 1) it can avoid the model collapse which outputs single classes, and 2) it can encourage the model to have high confidence for the predictions, which has shown its effectiveness for unlabeled data in semi-supervised learning. (v) Without L align . We remove L align and the performance has decreased, as expected, but is still more promising than other baselines. L align has leveraged mixup augmen-</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">CIFAR-10</cell><cell></cell><cell cols="3">CIFAR-100</cell></row><row><cell></cell><cell></cell><cell>Sym.</cell><cell cols="2">Asym. Avg.</cell><cell cols="2">Sym.</cell><cell>Avg.</cell></row><row><cell cols="2">Noise type/rate</cell><cell cols="2">50% 90% 40%</cell><cell></cell><cell cols="2">50% 90%</cell></row><row><cell>(i)</cell><cell>Baseline</cell><cell>70.0 20.6</cell><cell>77.5</cell><cell>56.1</cell><cell>47.3</cell><cell cols="2">6.8 27.1</cell></row><row><cell cols="2">(ii) Loss [1, 20]</cell><cell>92.5 75.9</cell><cell>73.2</cell><cell>80.6</cell><cell cols="3">71.2 16.0 43.6</cell></row><row><cell></cell><cell>k-NN [29]</cell><cell>92.9 79.7</cell><cell>91.3</cell><cell>88.0</cell><cell cols="3">70.3 39.8 55.1</cell></row><row><cell></cell><cell>OOD (ours)</cell><cell>93.1 82.1</cell><cell>92.0</cell><cell>89.1</cell><cell cols="3">70.7 45.9 58.3</cell></row><row><cell cols="2">(iii) Ensem. [25]</cell><cell>91.3 72.7</cell><cell>89.8</cell><cell>84.6</cell><cell cols="3">68.2 36.9 52.6</cell></row><row><cell></cell><cell cols="2">L cross (ours) 93.9 89.4</cell><cell>92.6</cell><cell>92.0</cell><cell cols="3">73.3 54.5 63.9</cell></row><row><cell cols="2">(iv) w/o L reg</cell><cell>92.0 34.5</cell><cell>90.3</cell><cell>72.3</cell><cell cols="3">68.5 24.3 46.4</cell></row><row><cell>(v)</cell><cell>w/o L align</cell><cell>91.8 84.6</cell><cell>89.7</cell><cell>88.7</cell><cell cols="3">69.4 48.4 58.9</cell></row><row><cell cols="2">(vi) MoCo</cell><cell>94.4 90.7</cell><cell>93.1</cell><cell>92.7</cell><cell cols="3">74.0 57.3 65.6</cell></row><row><cell cols="8">The results suggest that our bootstrap cross-supervision has</cell></row><row><cell cols="6">shown strong robustness on 90% label noise.</cell><cell></cell></row><row><cell cols="8">(iv) tation to regularize both classification and representation</cell></row><row><cell cols="8">learning. Appendix A shows the evaluation of k-NN classifi-</cell></row><row><cell cols="8">cation, demonstrating that L align can also greatly improve</cell></row><row><cell cols="3">the learned representations.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">(vi) Contrastive Framework. We implement TCL into</cell></row><row><cell cols="8">another contrastive framework for representation learning,</cell></row><row><cell cols="8">i.e., MoCo [12]. Based on the MoCo framework, our method</cell></row><row><cell cols="8">has achieved more improvements in various experiments,</cell></row><row><cell cols="8">which benefits from a large number of negative examples</cell></row><row><cell cols="8">in the memory queue and a moving-averaged encoder (we</cell></row><row><cell cols="8">set the queue size and the factor of moving-average to 4,096</cell></row><row><cell cols="3">and 0.99, respectively).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>61.1 82.6 57.3 82.3 D2L [26] 62.6 84.0 57.8 81.3 Iterative-CV [2] 65.2 85.3 61.6 84.9 Decoupling [27] 62.5 84.7 58.2 82.2 MentorNet [16] 63.0 81.4 57.8 79.9 Co-teaching [11] 63.5 85.2 61.4 84.7 ELR [25] 76.2 91.2 68.7 87.8 DivideMix [20] 77.3 91.6 75.2 90.8 RRL [21] 76.3 91.5 73.3 91.2 NGC [22] 79.1 91.8 74.4 91.0 MOIT [29] 77.9 91.9 73.8 91.7 TCL (ours) 79.1 92.3 75.4 92.4 Results on WebVision (mini).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results on Clothing1M.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62101136</rs> and <rs type="grantNumber">62176059</rs>), <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (No. <rs type="grantNumber">2018SHZDZX01</rs>), <rs type="institution">ZJ Lab</rs>, <rs type="funder">Shanghai Municipal of Science and Technology Project</rs> (No. <rs type="grantNumber">20JC1419500</rs>), and <rs type="funder">Shanghai Center for Brain Science and Brain-inspired Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RWzpXgb">
					<idno type="grant-number">62101136</idno>
				</org>
				<org type="funding" xml:id="_w7Baj5m">
					<idno type="grant-number">62176059</idno>
				</org>
				<org type="funding" xml:id="_PpHwU4V">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
				<org type="funding" xml:id="_AZhGyw2">
					<idno type="grant-number">20JC1419500</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Mcguinness</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ben Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Arthur P Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine samples for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Taehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhwan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from noisy data with robust representation learning</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9485" to="9494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mopro: Webly supervised learning with momentum prototypes</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective-supervised contrastive learning with noisy labels</title>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="316" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-objective interpolation training for robustness to label noise</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E O'</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6606" to="6615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from noisy labels by regularized estimation of annotator confusion</title>
		<author>
			<persName><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Daniel C Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11244" to="11253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">IMAE for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">L DMI: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="327" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Contrast to divide: Selfsupervised pre-training for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1657" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
