<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANSFER LEARNING OF LANGUAGE-INDEPENDENT END-TO-END ASR WITH LANGUAGE MODEL FUSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-07">7 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Brno University of Technology</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TRANSFER LEARNING OF LANGUAGE-INDEPENDENT END-TO-END ASR WITH LANGUAGE MODEL FUSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-07">7 May 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1811.02134v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>end-to-end ASR</term>
					<term>multilingual speech recognition</term>
					<term>low-resource language</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work explores better adaptation methods to low-resource languages using an external language model (LM) under the framework of transfer learning. We first build a language-independent ASR system in a unified sequence-to-sequence (S2S) architecture with a shared vocabulary among all languages. During adaptation, we perform LM fusion transfer, where an external LM is integrated into the decoder network of the attention-based S2S model in the whole adaptation stage, to effectively incorporate linguistic context of the target language. We also investigate various seed models for transfer learning. Experimental evaluations using the IARPA BA-BEL data set show that LM fusion transfer improves performances on all target five languages compared with simple transfer learning when the external text data is available. Our final system drastically reduces the performance gap from the hybrid systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Fast system development for low-resourced new languages is one of the challenges in automatic speech recognition (ASR). Recently, end-to-end ASR systems based on the sequence-to-sequence (S2S) architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> are filling up the gap of performance from the conventional HMM-based hybrid systems and showing promising results in many tasks with its extremely simplified training and decoding schemes <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. This is very attractive when building systems in new languages quickly. However, models tend to suffer from the data sparseness problems in the low-resource scenario, especially in S2S models due to its data-driven optimization.</p><p>One possible approach to this problem is to utilize data of other languages. There are various approaches to leverage other languages: (a) to train a model multilingually (multi-task learning with other languages), and then further fine-tune to a particular language <ref type="bibr" target="#b5">[6]</ref>, and (b) to adapt a multilingual model to a new language using transfer learning <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> and additional features obtained from the multilingual model such as multilingual bottleneck features (BNF) <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> and language feature vectors (LFV) <ref type="bibr" target="#b13">[14]</ref> (cross-lingual adaptation). To obtain a multilingual S2S model, a part of parameters can be shared while preparing the output layers per language <ref type="bibr" target="#b5">[6]</ref>, and we can further use a unified architecture with a shared vocabulary among multiple languages <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Since it would take much time to train such systems from scratch for many languages including new languages, we focus on the cross-lingual adaptation approach (b).</p><p>*Part of the work reported here was conducted while the author was visiting Johns Hopkins University.</p><p>While a majority of the conventional transfer learning is concerned with acoustic model, using linguistic context during adaptation has not been investigated yet. The research question in this paper is: Is linguistic context also helpful for adaptation to new languages? The most common approach to integrate the external language model (LM) is referred to as shallow fusion, where LM scores are interpolated with scores from the S2S model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Recently, several methods to leverage an external LM during training of S2S models are proposed: deep fusion <ref type="bibr" target="#b19">[20]</ref> and cold fusion <ref type="bibr" target="#b20">[21]</ref>. In deep fusion, the decoder network in the pre-trained S2S model and an external Recurrent neural network language model (RNNLM) are integrated into a single architecture by the gating mechanism and only the gating part is re-trained. In contrast, cold fusion integrates an external LM during the entire training stage.</p><p>In this paper, we investigate methods to fully utilize text data for adaptation to unseen low-resource languages. We propose LM fusion transfer, where an external LM is integrated into the decoder network of the S2S model only in the adaptation stage <ref type="foot" target="#foot_0">1</ref> , as an extension of cold fusion. Since the decoder network is already welltrained in a language-independent manner, the model can better incorporate linguistic context from the external LM. The extra cost to integrate the external LM during adaptation is trivial in the resource constrained condition. We also investigate various seed multilingual models trained with 600 to 2200-hours speech data and show the effect of the amount and variety of multilingual training data.</p><p>Experimental evaluations on the IARPA BABEL corpus show that the LM fusion transfer improves performance compared to simple transfer learning with shallow fusion when the additional text data is available. The performance of the transferred models is drastically improved by increasing the model capacity and incorporating the external LM, and the resulting models perform comparably with the latest BLSTM-HMM hybrid systems <ref type="bibr" target="#b9">[10]</ref>. To our best knowledge, this is the first results for the S2S model to show the competitive performance to the conventional hybrid systems in the lowresource scenario (∼50 hours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The traditional usage of unpaired text data in the S2S framework is categorized to four approaches: LM integration, pre-training, multitask learning (MTL), and data augmentation. In the LM integration approach, there are three methods: shallow fusion, deep fusion, and cold fusion as described in Section 1. Their differences are in the timing to integrate an external LM and the existence of additional parameters of the gating mechanism. We depict these fusion meth-ods in Fig. <ref type="figure" target="#fig_0">1</ref>. In, <ref type="bibr" target="#b18">[19]</ref>, these fusion methods are compared in middlesize English conversational speech (∼300h) and large-scale Google voice search data. However, none of previous works investigated the effect of them in other languages especially for low-resource languages, which is the focus of this paper. In <ref type="bibr" target="#b20">[21]</ref>, the authors show the effectiveness of cold fusion in a cross-domain scenario. Since the external LM is more likely to be changed frequently than the acoustic model, it is time-consuming to train a new S2S model with the LM integration from scratch every time the external LM is updated. In this work, we conduct LM fusion during adaptation to target languages, which is regarded as a more realistic scenario.</p><p>Another usage of the external LM is to initialize the lower layer in the decoder network with the pre-trained LM <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. However, we transfer almost all parameters in a multilingual S2S model (both encoder and decoder networks), and thus we do not explore this direction. Apart from the external LM, the MTL approach with LM objective are investigated in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. Although the MTL approach does not require any additional parameters, it gets minor gains compared to LM fusion methods <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recently, data augmentation of speech data based on text-tospeech (TTS) synthesis is investigated in the S2S framework <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Since we are interested in the usage of linguistic context during adaptation, we leave this direction to the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">END-TO-END ASR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attention-based sequence-to-sequence</head><p>We build all models with attention-based sequence-to-sequence (S2S) models, which can learn soft alignments between input and output sequences of variable lengths <ref type="bibr" target="#b0">[1]</ref>. They are composed of encoder and decoder networks. The encoder network transforms input features x = (x1, . . . , xT ) to a high-level continuous representation h = (h1, . . . , h T ′ ) (T ′ ≤ T ), interleaved with subsampling layers to reduce the computational complexity <ref type="bibr" target="#b25">[26]</ref>. The decoder network generates a probability distribution PS2S of the corresponding U -length transcription y = (y1, . . . , yU ) conditioned over all previous generated tokens:</p><formula xml:id="formula_0">s S2S u = Decoder(s S2S u−1 , yu−1, cu) PS2S(y|x) = softmax(W o s S2S u + b o )</formula><p>where W o and b o are trainable parameters, s S2S u is a decoder state at the u-th timestep, and cu is a context vector summarizing notable parts from the encoder states h. We adopt the location-based scoring function <ref type="bibr" target="#b1">[2]</ref>. To encourage monotonic alignments, the auxiliary Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b26">[27]</ref> objective is linearly interpolated <ref type="bibr" target="#b27">[28]</ref>.</p><p>During the inference stage, scores from the softmax layer used for the CTC objective are linearly interpolated in log-scale with a tunable parameter λ (0 ≤ λ ≤ 1) to avoid generating incomplete and repeated hypotheses as follows <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_1">ln PASR(y|x) = (1 − λ) ln PS2S(y|x) + λ ln PCTC(y|x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LM fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Shallow fusion</head><p>In the conventional decoding paradigm with an external LM, referred to as shallow fusion, scores from both the S2S model and LM are linearly interpolated to maximize the following criterion: where β is a tunable parameter to define the importance of the external LM. The separate LM, especially trained with a larger external text, has complementary effects to the implicit LM modeled in the decoder network. Therefore, shallow fusion shows performance gains in many ASR tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><formula xml:id="formula_2">y * = arg max y∈Ω * {ln PASR(y|x) + β ln PLM(y)}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Cold fusion (flat-start fusion)</head><p>While shallow fusion uses the external LM only in the inference stage, cold fusion <ref type="bibr" target="#b20">[21]</ref> uses the pre-trained LM during training of the S2S model to provide effective linguistic context. The fine-grained element-wise gating function is equipped to flexibly rely on the LM depending on the uncertainty of prediction:</p><formula xml:id="formula_3">s LM u = W LM d LM u + b LM gu = σ(W g [s S2S u ; s LM u ] + b g ) s CF u = W CF [s S2S u ; gu ⊙ s LM u ] + b CF PS2S(y|x) = softmax(ReLU(W out s CF u + b o ))</formula><p>where W * and b * are trainable parameters, d LM u is a hidden state of RNNLM, s LM u is a feature from the external LM, s CF u is a bottleneck feature before the final softmax layer, gu is a gating function, and ⊙ represents element-wise multiplication. ReLU non-linear function is inserted before the softmax layer as suggested in <ref type="bibr" target="#b20">[21]</ref>. We use the hidden state as a feature from RNNLM instead of logits because we use the universal character vocabulary for multilingual experiments, which results in the large softmax layer and increases the computational time <ref type="bibr" target="#b18">[19]</ref>.</p><p>In the original formulation in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>, scores from the external LM are not used. We found that linear interpolation of log probabilities from the LM with those from the S2S model during the inference as in shallow fusion still has complementary effects to improve performance. Therefore, we adopt it in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Deep fusion (fine-tuning fusion)</head><p>Deep fusion <ref type="bibr" target="#b19">[20]</ref> is another method to integrate an external LM during training. Unlike cold fusion, deep fusion is applied only for finetuning the gating part after parameters of both the pre-trained S2S model and RNNLM are frozen. Although deep fusion is formulated with a scalar gating function in <ref type="bibr" target="#b19">[20]</ref>, we use the same architecture as cold fusion in Section 3.2.2 to make a strict comparison. Then, the difference from the cold fusion is in the timing to integrate the external LM (from scratch or in the middle stage) and which parameters to update after integration (see Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TRANSFER LEARNING OF MULTILINGUAL ASR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adaptation to a target language</head><p>We adapt a seed language-independent end-to-end ASR model to an (unseen) target language. We investigate the following four scenarios: multi10: From non-target 10 languages to an unseen target language high2: From 2 high resource languages (English and Japanese) to an unseen target language multi10+high2: From the mix of non-target 10 languages and 2 high resource languages to an unseen target language multi15: From the mix of non-target 10 languages and target 5 languages to a particular target language</p><p>The top three conditions are regarded as cross-lingual adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LM fusion transfer</head><p>During adaptation, all parameters are copied from the seed languageindependent S2S model, then training is continued toward a target language. We investigate improved adaptation methods by integrating the external LM during and/or after transfer learning from the seed model. Three methods are considered as follows:</p><p>Transfer + SF: Shallow fusion in Section 3.2.1 is conducted in the inference stage after adaptation.</p><p>Cold fusion transfer (CF-transfer): Cold fusion in Section 3.2.2 is conducted during adaptation. We integrate the external RNNLM from the start point of adaptation to a target language. The softmax layer is randomly initialized before adaptation due to the additional gating part.</p><p>Deep fusion transfer (DF-transfer): Deep fusion in Section 3.2.3 is conducted after adaptation. DF-transfer is composed of two stages: (1) adaptation by updating the whole parameters until convergence, and (2) fine-tuning only the gating part after integrating the external RNNLM. The softmax layer is randomly initialized before stage (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setting</head><p>We used data from the IARPA BABEL project <ref type="bibr" target="#b28">[29]</ref> and selected 10 languages as non-target languages for training the seed languageindependent model: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurmanji, Tokpisin and Georgian, and 5 languages for adaptation: Assamese (AS), Swahili (SW), Lao (LA), Tagalog (TA) and Zulu (ZU). Full language pack (FLP) is used for all experiments except for Section 5.2.3, where limited language pack (LLP) which consists of about 10% of FLP is used for adaptation. We sampled 10% of data from the training data for each language as the validation set. In addition, we used Librispeech corpus <ref type="bibr" target="#b29">[30]</ref> and the Corpus of Spontaneous Japanese (CSJ) <ref type="bibr" target="#b30">[31]</ref> as additional high resources.</p><p>We used Kaldi toolkit <ref type="bibr" target="#b31">[32]</ref> for feature extraction. The input features were static 80-channel log-mel filterbank outputs appended with 3-dimensional pitch features computed with a 25ms window and shifted every 10ms. The features were normalized by the mean and the standard deviation on the whole training set. For the vocabulary, we used the universal character set including all characters from all languages <ref type="bibr" target="#b14">[15]</ref>, resulting in the vocabulary size of 5,353 classes including 17 language IDs, sos, eos, unk, and blank labels. For multilingual experiments, we prepended the corresponding language ID so that the decoder network can jointly identify the correct target language while recognizing speech <ref type="bibr" target="#b14">[15]</ref>. Our encoder network is composed of two VGG-like CNN blocks <ref type="bibr" target="#b32">[33]</ref> followed by a max-pooling layer with a stride of 2 × 2, and 5 layers of bidirectional long short-term memory (BLSTM) <ref type="bibr" target="#b33">[34]</ref> with 1024 memory cells, which results in time reduction by a factor of 4. The decoder network consists of two layers of LSTM with 1024 memory cells. For both monolingual and multilingual experiments, we used the same architecture. Training was performed on the minibatch size of 15 utterances using Adadelta <ref type="bibr" target="#b34">[35]</ref> algorithm with an initial epsilon 1e − 8. Epsilon was divided by a factor of 0.01 when the teacher-forcing accuracy does not improve for the validation set at each epoch. Scheduled sampling <ref type="bibr" target="#b35">[36]</ref> with probability 0.4 and dropout for the encoder network with probability 0.2 were performed in all experiments during adaptation. We set the CTC weight during training and decoding to 0.5 and 0.3, respectively. We also set the beam width to 20 and the LM weight to 0.3.</p><p>For RNNLM, we used two layers of LSTM with 650 memory cells. All RNNLMs were trained with transcriptions in the parallel data except for experiments in Table <ref type="table" target="#tab_3">4</ref>. We used stochastic gradient descent (SGD) for RNNLM optimization. All networks are implemented by ESPnet toolkit <ref type="bibr" target="#b36">[37]</ref> with pytorch backend <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Baseline monolingual systems for target 5 languages</head><p>First, we present the results of the baseline monolingual end-to-end systems in Table <ref type="table" target="#tab_0">1</ref>. Our new systems (line 2) significantly outperformed the old baseline reported on <ref type="bibr" target="#b6">[7]</ref>. The gain mostly came from adding VGG blocks before BLSTM encoder and one more decoder LSTM layer though we also tuned other hyper-parameters. Next, changing the unit sizes of each LSTM layer from 320 to 1024 drastically improved the performance. This is surprising because increasing the number of parameters often makes the model overfit to the small amount of training data. Finally, shallow fusion with the monolingual RNNLM further boosted the performance although the RNNLM was trained with the small amount of transcriptions only. We use this setting as default in the rest of experiments.</p><p>We also built BLSTM-HMM hybrid systems for comparison. The BLSTM-HMM architecture includes 3 BLSTM layers each with 512 memory cells and 300 projection units<ref type="foot" target="#foot_1">2</ref> . The BLSTM acoustic model was trained using the latency control technique with 22 past frames and 21 future frames. The acoustic model receives 40dimensional filterbank features as input. N-gram language model is built with the training transcriptions. WERs by our end-to-end sys- tems with shallow fusion are close to those of the hybrid system, just 3.6 and 1.8 % absolute difference for Tagalog and Zulu, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Comparison of seed language-independent models</head><p>We compared the seed language-independent models for adaptation to target languages. All models were transferred, and shallow fusion with the corresponding monolingual RNNLM trained with the parallel data was performed. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. The overall performance was significantly improved by transfer learning. The transferred S2S models achieved comparable WER to BLSTM-HMM for Tagalog and outperformed for Zulu in Table <ref type="table" target="#tab_0">1</ref>. We can see that multi10 model is generally better than high2 model despite the smaller data size, and combination of them (multi10+high2) gives slight improvement. On the other hand, multi15 model that includes the target language does not lead to further improvement even after fine-tuning. We can conclude that the diversity of languages is more important than the total amount of training data, and 10 languages are almost sufficient for learning language-independent feature representation and generalized to other languages well <ref type="bibr" target="#b5">[6]</ref>. Since multi10 shows the competitive results to multi10+high2 only with one third training data, we use multi10 as the seed model and investigate cross-lingual adaptation in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Effect of LM fusion transfer</head><p>The results of our proposed LM fusion transfer are given in Table <ref type="table" target="#tab_2">3</ref>. When training S2S models from scratch, there is no difference among all fusion methods. When transferred from the languageindependent S2S model, significant improvement is observed by integrating the external RNNLM. Shallow fusion was more effective than when training the S2S models from scratch in Table <ref type="table" target="#tab_0">1</ref> because the multilingual training led to generalization and the affinity for the external LM was enhanced. CF-transfer got some improvements compared to transfer learning with shallow fusion for 3 target languages, but the effects of DF-transfer and CF-transfer are not significant. This is because RNNLMs were trained with text in the small parallel data only, therefore linguistic context during adaptation was not so effective. However, CF-transfer in Tagalog outperformed the monolingual hybrid system in Table <ref type="table" target="#tab_0">1</ref>. When compared to the previous work using the same data <ref type="bibr" target="#b6">[7]</ref>, CF-transfer yielded 21.6% gains relatively on average. Furthermore, 6.8% gains were achieved from transfer learning without the external LM.</p><p>To investigate the effect of additional text data, we evaluate the LM fusion transfer with LLP on each target language (∼10 hours). The results are shown in Table <ref type="table" target="#tab_3">4</ref>. We used monolingual RNNLM trained with LLP (parallel data) and FLP (∼50 hours), respectively. The latter setting of a small speech data set (∼10 hours) and a larger text data set (∼50 hours) is regarded as a more realistic scenario in low-resource languages. When training S2S models from scratch,  all models could not converge in our implementation even when reducing the unit sizes. The Babel corpus is mostly composed of conversational telephone speech (CTS), so it is difficult to optimize the S2S model from scratch with just around 10-hour training data. In the transfer learning approach, all three fusion methods got significant gains by using the external LM except for deep fusion in Assamese. For RNNLM trained with LLP, all fusion methods achieved a larger improvement than in Table <ref type="table" target="#tab_2">3</ref>. Interestingly, WER significantly dropped even when each RNNLM was trained with 10-hour data only. But all fusion methods show similar performance. In contrast, CF-transfer significantly outperformed simple transfer learning with shallow fusion on all 5 target languages when the RNNLM was trained with FLP, which is five-times larger than LLP. Therefore, we can conclude that linguistic context is helpful for adaptation when additional text data is available. This shows CF-transfer in Table <ref type="table" target="#tab_2">3</ref> has the potential to surpass transfer learning with shallow fusion if we can access to additional text data <ref type="foot" target="#foot_2">3</ref> . In summary, CF-transfer yielded relative 10.4% and 2.3% gains on average compared to transfer learning without and with shallow fusion, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We explored the usage of linguistic context from the external LM during adaptation of the language-independent S2S model to target low-resource languages. We empirically compared various LM fusion methods and confirmed their effectiveness in resource limited situations. We showed that cold fusion transfer is more effective than simply applying shallow fusion after adaptation when additional text is available, which means linguistic context is also helpful in addition to acoustic adaptation. Our S2S model drastically closed the gap from the BLSTM-HMM hybrid system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: Overview of language model fusion transfer. LM fusion transfer is conducted with monolingual data only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of baseline monolingual systems. None of adaptation methods is conducted.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WER (%)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>AS</cell><cell>SW</cell><cell>LA</cell><cell>TA</cell><cell>ZU</cell></row><row><cell></cell><cell cols="5">(54h) (39h) (58h) (75h) (54h)</cell></row><row><cell>Old baseline [7]</cell><cell>73.9</cell><cell>66.5</cell><cell>64.5</cell><cell>73.6</cell><cell>76.4</cell></row><row><cell>New baseline</cell><cell>64.5</cell><cell>56.6</cell><cell>56.2</cell><cell>56.4</cell><cell>69.5</cell></row><row><cell>+ large units</cell><cell>59.9</cell><cell>50.9</cell><cell>51.7</cell><cell>52.7</cell><cell>65.5</cell></row><row><cell>+ shallow fusion</cell><cell>57.4</cell><cell>46.5</cell><cell>49.8</cell><cell>49.9</cell><cell>62.9</cell></row><row><cell>BLSTM-HMM</cell><cell>49.1</cell><cell>38.3</cell><cell>45.7</cell><cell>46.3</cell><cell>61.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of adaptation from the different seed languageindependent models. Shallow fusion with the corresponding monolingual RNNLM was conducted.</figDesc><table><row><cell>Seed</cell><cell>hours</cell><cell>AS</cell><cell>SW</cell><cell>WER (%) LA</cell><cell>TA</cell><cell>ZU</cell></row><row><cell>multi10</cell><cell cols="6">643 53.4 41.3 46.1 46.4 60.2</cell></row><row><cell>high2</cell><cell cols="6">1,472 57.8 45.0 48.6 49.4 61.9</cell></row><row><cell>multi10+high2</cell><cell cols="6">2,115 53.2 40.7 45.1 45.3 58.5</cell></row><row><cell>multi15</cell><cell cols="6">929 53.4 40.6 45.0 46.1 58.8</cell></row><row><cell>multi15 w/o FT</cell><cell cols="6">929 56.2 44.2 47.1 47.8 60.6</cell></row><row><cell cols="5">(FT: fine-tuning to a target language)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of LM fusion transfer on FLP (∼50h)</figDesc><table><row><cell>Model</cell><cell></cell><cell>AS</cell><cell>SW</cell><cell>WER (%) LA</cell><cell>TA</cell><cell>ZU</cell></row><row><cell cols="2">Transfer [7] SF</cell><cell cols="5">65.3 56.2 57.9 64.3 71.1</cell></row><row><cell></cell><cell>-</cell><cell cols="5">59.9 50.9 51.7 52.7 65.5</cell></row><row><cell>Scratch</cell><cell cols="6">SF DF+SF 57.5 46.4 49.9 49.9 62.6 57.4 46.5 49.8 49.9 62.9</cell></row><row><cell></cell><cell cols="6">CF+SF 57.5 47.3 50.0 50.2 62.9</cell></row><row><cell></cell><cell>-</cell><cell cols="5">56.4 46.4 48.6 50.1 63.5</cell></row><row><cell>Transfer</cell><cell>SF</cell><cell cols="5">53.4 41.3 46.1 46.4 60.2</cell></row><row><cell>(multi10)</cell><cell cols="6">DF+SF 53.5 41.2 46.2 46.2 59.9</cell></row><row><cell></cell><cell cols="6">CF+SF 53.6 41.6 45.9 46.2 59.5</cell></row><row><cell cols="7">(SF: shallow fusion, DF: deep fusion, CF: cold fusion)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of LM fusion transfer on LLP (∼10h)</figDesc><table><row><cell cols="2">Model</cell><cell>LM data</cell><cell>AS (8h)</cell><cell>SW (9h)</cell><cell>WER (%) LA (9h)</cell><cell>TA (9h)</cell><cell>ZU (9h)</cell></row><row><cell>Scratch</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="3">not converge</cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>67.5</cell><cell>59.7</cell><cell>60.3</cell><cell>66.2</cell><cell>75.4</cell></row><row><cell></cell><cell>SF</cell><cell></cell><cell>63.3</cell><cell>52.8</cell><cell>57.2</cell><cell>60.8</cell><cell>71.2</cell></row><row><cell>Transfer (multi10)</cell><cell>DF+SF CF+SF SF</cell><cell>LLP</cell><cell>68.0 63.2 62.7</cell><cell>52.4 52.8 51.7</cell><cell>57.3 58.4 56.4</cell><cell>60.7 60.6 60.0</cell><cell>70.9 71.0 71.0</cell></row><row><cell></cell><cell>DF+SF</cell><cell>FLP</cell><cell>66.8</cell><cell>50.7</cell><cell>56.1</cell><cell>60.0</cell><cell>69.9</cell></row><row><cell></cell><cell>CF+SF</cell><cell></cell><cell>61.7</cell><cell>50.3</cell><cell>56.0</cell><cell>57.9</cell><cell>69.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Although we can perform LM fusion during training of the seed multilingual model, we focus on applying it during adaptation because our goal is to adapt it to a particular language rapidly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Increasing the unit size did not lead to any improvement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Since the provided data only can be used for system training in BABEL rules, we do not explore to crawl text data from the WEB.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katya</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid CTC/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequence-based multi-lingual low resource speech recognition</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4909" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
				<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multilingual training and cross-lingual adaptation on CTC-based acoustic model</title>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10025</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-lingual adaptation of a CTC-based multilingual acoustic model</title>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of multilingual blstm acoustic model on low and high resource languages</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">František</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5789" to="5793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The language-independent bottleneck features</title>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">František</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Egorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
				<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adapting multilingual neural network hierarchy to a new language</title>
		<author>
			<persName><forename type="first">Frantisek</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technologies for Under-Resourced Languages</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual deep neural network based acoustic modeling for rapid language adaptation</title>
		<author>
			<persName><forename type="first">Ngoc Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7639" to="7643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual adaptation of RNN based ASR systems</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5219" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language independent end-to-end architecture for joint language identification and speech recognition</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
				<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4904" to="4908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards language-universal endto-end speech recognition</title>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4914" to="4918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An analysis of incorporating an external language model into a sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5824" to="5828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A comparison of techniques for language model integration in encoder-decoder speech recognition</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10857</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cold fusion: Training seq2seq models together with language models</title>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="387" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using target-side monolingual data for neural machine translation through multi-task learning</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1500" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Back-translation-style data augmentation for end-to-end ASR</title>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
				<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leveraging sequence-to-sequence speech synthesis for enhancing acoustic-to-word speech recognition</title>
		<author>
			<persName><forename type="first">Masato</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sei</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
				<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech recognition and keyword spotting for low-resource languages: BA-BEL project research at CUED</title>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><surname>Shakti</surname></persName>
		</author>
		<author>
			<persName><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technologies for Under-Resourced Languages</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
				<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Corpus of Spontaneous Japanese: Its design and evaluation</title>
		<author>
			<persName><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
				<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Enrique Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">ESPnet: End-to-End Speech Processing Toolkit</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
	<note>Proc. of Interspeech</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
