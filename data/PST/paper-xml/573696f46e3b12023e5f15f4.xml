<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Strong Parts for Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution" key="instit2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution" key="instit3">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution" key="instit2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution" key="instit3">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution" key="instit2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution" key="instit3">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution" key="instit2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution" key="instit3">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning Strong Parts for Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in pedestrian detection are attained by transferring the learned features of Convolutional Neural Network (ConvNet) to pedestrians. This ConvNet is typically pre-trained with massive general object categories (e.g. ImageNet). Although these features are able to handle variations such as poses, viewpoints, and lightings, they may fail when pedestrian images with complex occlusions are present. Occlusion handling is one of the most important problem in pedestrian detection. Unlike previous deep models that directly learned a single detector for pedestrian detection, we propose DeepParts, which consists of extensive part detectors. DeepParts has several appealing properties. First, DeepParts can be trained on weakly labeled data, i.e. only pedestrian bounding boxes without part annotations are provided. Second, DeepParts is able to handle low IoU positive proposals that shift away from ground truth. Third, each part detector in DeepParts is a strong detector that can detect pedestrian by observing only a part of a proposal. Extensive experiments in Caltech dataset demonstrate the effectiveness of DeepParts, which yields a new state-of-the-art miss rate of 11.89%, outperforming the second best method by 10%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection has been studied extensively in recent years <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> and has many applications such as video surveillance and robotics. While pedestrian detection has achieved steady improvements over the last decade, complex occlusion is still one of the obstacles. Referring to a recent survey <ref type="bibr" target="#b7">[8]</ref>, around 70% of the pedestrians captured in street scenes are occluded in at least one video frame. For example, the current best-performing detector SpatialPooling+ <ref type="bibr" target="#b26">[27]</ref> attained 75% reduction of the average miss rate over the VJ detector <ref type="bibr" target="#b33">[34]</ref> on Caltech <ref type="bibr" target="#b7">[8]</ref> test set without occlusion. When heavy occlusions are present, it only attained 21% improvement over VJ <ref type="foot" target="#foot_0">1</ref> .</p><p>Current pedestrian detectors for occlusion handling can be generally grouped into two categories, 1) training specific detectors for different occlusion types <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18]</ref> and 2) modeling part visibility as latent variables <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref>. In the first category, constructing specific detector requires the prior knowledge of the occlusion types. For example, according to the statistics of the occlusion patterns in traffic scenes, <ref type="bibr" target="#b17">[18]</ref> trained a series of biased classifiers for bottom-up and right-left occlusions. In the second category, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> divided pedestrian into several parts and inferred their visibility with latent variables. Although these methods achieved promising results, manually selecting parts may not be the optimal solution and may fail when handling pedestrian detection in other scenarios beyond traffic scenes, such as crowded scenes and market surveillance, where occlusion types change.</p><p>Inspired by <ref type="bibr" target="#b17">[18]</ref>, we introduce the idea of constructing a part pool that covers all the scales of different body parts and automatically choose important parts for occlusion handling. At the training stage, each part detector is learned by fine-tuning ConvNet features, which are pre-trained on Im-ageNet. At the testing stage, we design a shifting handling method within a ConvNet. This method handles the problem that positive proposal windows usually shift away from their corresponding ground truth bounding boxes. Moreover, the part selection is determined by data and the effectiveness of the part pool can be fully explored. Fig. <ref type="figure" target="#fig_1">2</ref> shows 6 body parts which are significant in the Caltech pedestrian dataset. They function complementarily to handle complex occlusions.</p><p>DeepParts has four main contributions. (1) We construct an extensive part pool where different complementary parts can be automatically selected in a data driven manner. The selected parts can be adopted to different scenarios or different datasets. ( <ref type="formula" target="#formula_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>We review related works in three aspects. Part-Based Pedestrian Detectors One stream of partbased approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref> firstly trained part detectors in a fully supervised manner and then combined their outputs to fit a geometric model. For example, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> required part labels and were restricted to a limited number of manually-designed parts. Enzweiler et al. <ref type="bibr" target="#b8">[9]</ref> utilized the depth and motion information to determine the occlusion boundaries. Wu et al. <ref type="bibr" target="#b36">[37]</ref> assumed that the head of a pedestrian is visible and required a complex Bayesian framework to combine different components. In contrast, our method does not need part annotations and can automatically select complementary parts (components of human body) from a large part pool.Another stream of part-based models focused on unsupervised part mining, which does not require part labels. Felzenszwalb et al. <ref type="bibr" target="#b9">[10]</ref> proposed Deformable Part Model (DPM), which learned a mixture of local templates for each body part to handle pose variations. Lin et al. <ref type="bibr" target="#b15">[16]</ref> proposed a promising and effective framework by incorporating DPM into And-Or graph. Recently, Girshick et al. <ref type="bibr" target="#b12">[13]</ref> reformulated DPM as ConvNet. DPM needs to handle complex configurations while our method is much simpler.</p><p>Occlusion Handling Some recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24]</ref> focused on handling specific types of pedestrian occlusion. For example, the Franken-classifiers <ref type="bibr" target="#b17">[18]</ref> learned a small set of classifiers, where each one accounts for a specific type of occlusion. In this work, we extend this idea by constructing an extensive part pool. Unlike <ref type="bibr" target="#b17">[18]</ref> that the parts were pre-defined, our complementary parts are automatically determined by data and may vary in different scenarios or datasets. In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>, occlusions caused by overlaps between two pedestrians were handled. Specifically, Tang et al. <ref type="bibr" target="#b30">[31]</ref> proposed a pedestrian detector tailored to various occlusion levels, while Ouyang et al. <ref type="bibr" target="#b23">[24]</ref> employed a probabilistic framework to model the relationship between the configurations estimated by single-and multi-pedestrian detectors. With the large part pool, our method can cover more occlusion patterns.</p><p>Deep Models Deep learning methods can learn high level features to aid pedestrian detection. For instance, Ouyang et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> introduced a part deformation layer into deep models to infer part visibility. By introducing switchable layers to learn both low-level features and highlevel semantic parts, SDN <ref type="bibr" target="#b16">[17]</ref> achieved further improvement. Because the receptive field of higher layers in Con-vNet is large (sometimes covers most of the input patch), modeling part visibility in a single ConvNet as these methods can not explicitly learn visual patterns for each part and may suffer from part co-adaption. Tian et al. <ref type="bibr" target="#b31">[32]</ref> modeled detection task together with attribute prediction tasks within a single deep model. Finally, Hosang et al. <ref type="bibr" target="#b13">[14]</ref> demonstrated the effectiveness of the R-CNN pipeline <ref type="bibr" target="#b11">[12]</ref> in pedestrian detection and achieved top performance on Caltech <ref type="bibr" target="#b7">[8]</ref> and KITTI <ref type="bibr" target="#b10">[11]</ref>. We follow this framework to train our strong part detectors. Moreover, unlike Part-Based R-CNN <ref type="bibr" target="#b40">[41]</ref>, our DeepParts does not need part annotations in training.</p><p>Another series of methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> focusing on Channel Features and feature selection also achieved stateof-the-art performance for pedestrian detection, but they are not specially designed for occlusion handling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Training Part Detectors</head><p>We take several steps to build our part-based pedestrian detector. Firstly, we construct a part pool, where the parts cover the full body of pedestrian at different positions and scales. We then learn a detector for each of the part. A method is further designed to handle shifting problem of proposal windows. Finally, we infer the full body score over complementary part detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Part Pool</head><p>Occlusions may present at different body parts and have various patterns. For instance, the left-or right-half body may be occluded by a tree, and the lower-half body may be occluded by a car. Thus, we construct an extensive part pool, containing various semantic body parts.</p><p>We consider pedestrian as a rigid object and define a human body grid of 2m × m, where 2m and m indicate the numbers of cells in horizontal and vertical direction, respectively. Each cell is a square and has equal size. Furthermore, we ensure each part to be a rectangle. The scales for parts are defined as</p><formula xml:id="formula_0">S = {(w, h)|W min ≤ w ≤ m, H min ≤ h ≤ 2m, w, h ∈ N + },<label>(1)</label></formula><p>where w and h indicate the width and height of a part respectively, in terms of the number of cells they contain. W min and H min are used to avoid subtle part since we focus on middle-level semantic part. Then, for each (w, h) ∈ S, we slide a h × w window over the human body grid with step size s, to generate parts at different positions. The entire part pool could be expressed as follows</p><formula xml:id="formula_1">P = {(x, y, w, h, i)|x, y ∈ N + , (w, h) ∈ S, i ∈ I},<label>(2)</label></formula><p>where x and y are the coordinates of the top-left cell in the part and i is a unique id. Specifically, the part representing the full body is defined as</p><formula xml:id="formula_2">(1, 1, m, 2m, i f ull ).</formula><p>Large m results in a large part pool, which may cause more computations in the training and testing stages. Also, small values of W min and H min result in subtle parts, such as W min = 0.1 × m. To avoid the above issues, we have m = 3, W min = 2, H min = 2, and s = 1 in our implementation, resulting in a part pool with 45 prototypes. Two examples regarding the parts of head-left-shoulder and leg are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training</head><p>For each part, we train an independent ConvNet classifier. This leads to totally |P | models, where</p><formula xml:id="formula_3">| • | is the counting operator.</formula><p>Training Data The size of training dataset is crucial for ConvNets. In our experiments, we use Caltech dataset <ref type="bibr" target="#b7">[8]</ref>, which is the largest pedestrian benchmark that consists of ∼250k labeled frames and ∼350k annotated bounding boxes. Instead of following the typical Reasonable setting, which uses every 30 th image in the video and has ∼1.7k pedestrians for training, we utilize every frame and employ ∼50k pedestrian bounding boxes as positive training patches. Proposals are obtained by LDCF <ref type="bibr" target="#b20">[21]</ref>. Negative patches are the proposed windows that have IoU &lt; 0.5 with the ground truth bounding boxes.</p><p>Part Specific Patch Generation As shown in Fig. <ref type="figure" target="#fig_4">4</ref>, we take the head-shoulder detector as an example, to illustrate how to generate the training data. (1) Given the definition of a part, we consider the corresponding region within a negative proposal (i.e. patches containing objects/backgrounds other than pedestrians) as the negative sample. (2) Each pedestrian is annotated with two bounding boxes (BBs), which denote the visible part B vis (in green) and full body B f ull , respectively. We divide the full body B f ull into 2m × m cells and compute the visible ratio of each cell. Then we obtain the visible map by thresholding on the visible ratio. If the visible cells of a ground truth can cover the cells of a part, we extract the corresponding region of this part as a positive sample. In our implementation, the threshold for computing the visible map is 0.4. <ref type="bibr" target="#b11">[12]</ref> that fine-tuning a pre-trained ConvNet on ImageNet classification task on object detection and segmentation data can significantly improve the performance. Particularly, the parameters learned at the pre-training phase are directly used as initial values for the fine-tuning stage. Similar strategy can be directly adopted to fine-tune the generic ConvNet image classification models for part recognition. The main disparity between the pre-training and fine-tuning tasks is the type of input data. Image classification task employs the full image or the entire object as input, which contains rich context information while part recognition task can only observe a middle-level patch of a part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training It has been demonstrated in R-CNN</head><p>To understand how to narrow the above disparity, we investigate three popular deep models and three pre-training strategies as below. Three deep models are AlexNet <ref type="bibr" target="#b14">[15]</ref>, Clarifai <ref type="bibr" target="#b38">[39]</ref>, and GoogLeNet <ref type="bibr" target="#b29">[30]</ref>, which are the bestperforming models of the ImageNet <ref type="bibr" target="#b4">[5]</ref> classification challenge in the past several years. AlexNet and Clarifai have ∼60 million parameters and share similar structures, while GoogLeNet uses 12× fewer parameters but is much deeper than the first two models. Our framework is flexible to incorporate other generic deep models.</p><p>Three pre-training strategies include (1) no pre-training, i.e. randomly initializing model parameters with Gaussian distribution (strategy 1), (2) pre-training the deep models by using the ImageNet training data with image-level annotations of 1000 classes, i.e. taking the full images as input, such as <ref type="bibr" target="#b13">[14]</ref> (strategy 2), and (3) pre-training the deep models by using ImageNet training data with object-level annotations of 1000 classes <ref type="bibr" target="#b24">[25]</ref>, i.e. taking the cropped object patches as input (strategy 3).</p><p>Fine-tuning For each part prototype, we use the part specific patches to fine-tune our part detectors. We replace the last d×n classification layer with d×2 randomly initialized part classifier (d and n indicate the feature dimension and the number of pre-training classes, respectively). In our implementation, we uniformly sample 16 positive and 48 negative windows to construct a mini-batch. Experiments show that fine-tuning for 10000 iterations with a learning rate of 0.001 is sufficient to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Handle Shifting in Deep Model</head><p>In pedestrian detection, the localization qualities (i.e. whether the window is tight or not) of the proposals are important for the recognition stage. Pedestrian detectors usually suffer from poor localization quality of the proposals. As reported in <ref type="bibr" target="#b13">[14]</ref>, the best proposal method SpatialPool-ing+ <ref type="bibr" target="#b26">[27]</ref> recalls 93% pedestrians with 0.5 IoU threshold while only recalls 10% with 0.9 IoU threshold. Shifting is one of the major reasons that cause low IoU. As shown in Fig <ref type="figure" target="#fig_5">5</ref>  still a proposal of high quality. However, shifting on both directions leads to 0.68 IoU, such that critical parts are missing, hindering the stages of feature extraction and classification. Except the full body shifting, each body part may also shift and different parts of the same pedestrian may shift towards different directions. In our framework, the positive training samples for each part detector are well aligned while the testing proposals may shift at all directions. Thus, handling shifting for both the full body and each part is necessary.</p><formula xml:id="formula_4">pool5 3 × 3/2 6 × 6 (6 + n) × (6 + n) conv6 6 × 6/1 1 × 1 (1 + n) × (1 + n) conv7 1 × 1/1 1 × 1 (1 + n) × (1 + n) conv8 1 × 1/1 1 × 1 (1 + n) × (1 + n)</formula><p>A straight forward way to handle this problem is that we crop multiple patches around each proposal with jitter, then feed the cropped patches into the deep model and choose the highest or averaged score with penalty as the detection score. However, this method would increase the testing time by k times, where k is the number of cropped patches for each proposal.</p><p>To reduce the testing computation, we firstly reformulate the generic ConvNet models with fully connected layer as fully convolutional neural networks, which does not require fixed input size and can process multiple neighboring patches via only one forward pass. In this case, the input size of the fully convolutional ConvNet can be changed. We take the AlexNet as an example, the original input size of which is 227 × 227. As illustrated in Table <ref type="table" target="#tab_1">1</ref></p><formula xml:id="formula_5">, af- ter reformulating f c6, f c7, f c8 as conv6(1 × 1 × 4096), conv7(1 × 1 × 4096), conv8(1 × 1 × 2)</formula><p>, the fully convolutional AlexNet is able to receive an expanded input size because the convolution and pooling operations are unrelated to the input size. Since the step size of receptive field for the classification layer is 32, the expanded input should be (227 + 32n) × (227 + 32n) in order to keep the forward procedure applicable, where n indicates expanded step size and is a non-negative integer.</p><p>Given a proposed part patch (x min , y min , w, h) and n, the expanded cropping patch is (x ′ min , y ′ min , w ′ , h ′ ), where  </p><formula xml:id="formula_6">s = max 1≤i,j≤n+1 {S i,j − P i,j }<label>(4)</label></formula><p>where P i,j is a penalty term with respect to relative shifting distance from the proposed part box and is defined as </p><formula xml:id="formula_7">P i,j = a × (|i − n + 2 2 | + |j − n + 2 2 |) × 32 227 + b × (|i − n + 2 2 | 2 + |j − n + 2 2 | 2 ) × (<label>32</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Parts Complementarity</head><p>For each part, we directly use the output of its Con-vNet detector as the visible score instead of stacking a linear SVM on the top as the R-CNN framework <ref type="bibr" target="#b11">[12]</ref>. We find that appending a SVM detector for mining hard negatives does not show significant improvement over directly using the ConvNet output, especially for GoogLeNet. This may due to the fact that the training proposals generated by LDCF <ref type="bibr" target="#b20">[21]</ref> are already hard negatives. Thus, we safely remove the SVM training stage to save computation time.</p><p>Then we employ a linear SVM to learn complementarity over the 45 part detector scores. To alleviate the testing computation cost, we simply select 6 parts with highest value of the SVM weight, yielding approximate performance. Experiments show that the performance improvement mainly benefits from the part complementarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>DeepParts is evaluated on the Caltech dataset <ref type="bibr" target="#b7">[8]</ref>, using subsets set00-set05 for training and set06-set10 for testing. We strictly follow the evaluation protocol of <ref type="bibr" target="#b7">[8]</ref>, measuring the log average miss rate over nine points ranging from 10 −2 to 10 0 False-Positive-Per-Image. Three subsets are considered for testing evaluation (Reasonable, Partial occlusion and Heavy occlusion).   • Reasonable subset. Pedestrians are larger than 49 pixels in height and have at least 65 percent visible body parts. Reasonable subset is considered as a more representative evaluation than overall performance on all pedestrians and is the most frequent evaluation setting. Without special illustration, we use Reasonable as our default setting to compare performance.</p><p>• Partial and heavy occlusion subsets where pedestrians are larger than 49 pixels in height and have 1 − 35 and 36 − 80 percent occluded body parts, respectively.</p><p>Because of page limit, we choose three representative parts for illustration. The selected parts are upper, left, and full body respectively, and are shown in Fig. <ref type="figure" target="#fig_8">6</ref>. For each part, we directly use the part detector output (without SVM training) as the whole patch score to evaluate the performance of single part detector. The complete results are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation of data volume</head><p>To investigate how data volume influences the finetuning performance of part detectors, we compare the performance obtained by fine-tuning AlexNet with five different sets of data. Here, AlexNet is pre-trained on ImageNet image-level data (i.e. strat.2). We present results in Table <ref type="table" target="#tab_2">2</ref>. The average miss rate of each part detector shows a decreasing pattern when incorporating more image frames. For example, the upper, left, and full body detector achieve 7.08%, 9.13% and 11.82% improvements, respectively, when the data volume is increased by 30×. The three models are still unsaturated though all training frames have been utilized. Table <ref type="table">3</ref>. Effectiveness of shifting handling via a comparison with no shifting handling and appending SVM on the top. We achieve the best value for SVM by greedily search the value of C and overlap thresholds for positive and negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of models and pre-training</head><p>We evaluate the single detector performance of upper, left, and full body parts with different contemporary deep architectures. All the deep models are pre-trained with three strategies, including (1) random initialization (strat.1), ( <ref type="formula" target="#formula_1">2</ref>) image-level pre-training (strat.2), and (3) object-level pretraining (strat.3). As shown in Fig. <ref type="figure" target="#fig_10">7</ref>, GoogLeNet outperforms AlexNet and Clarifai over all the three body parts with ImageNet pre-training. Fine-tuning an object-level pre-trained GoogLeNet solely on the upper body part can yields 26.02% miss rate, which is already close to the strong LDCF proposals. When the full body is utilized, the miss rate surprisingly reduces to 16.43%, which is the best result that ever reported on Caltech reasonable subset. Besides, it is noticeable that GoogLeNet is inferior to AlexNet and Clarifai with random initialization. We believe it is because of the model structure, where GoogLeNet are much deeper and needs more iterations to converge when training from scratch.</p><p>For all three models, random initialization (strat.1) of network parameters leads to the worst performance. For full body part, object-level pre-training (strat.3) strategy shows around 1.2% superiority over image-level pretraining (strat.2) strategy. However, as for upper part and left part, this gap increases to around 4% for AlexNet and Clarifai, and 2.5% for GoogLeNet. This may reveal the fact that pre-training on object-level data are more capable to model mid-level part variations than on image-level data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of handling shifting</head><p>To understand the effectiveness of shifting handling (SH), we compare the results of shifting handling with that of directly utilizing the single patch score given by the last classification layer (no SH) and that of appending a SVM on top to mine hard negatives (SVM). The results are collected in Table <ref type="table">3</ref>, which shows that Shifting Handling consistently achieves the lowest miss rate over upper, left, and full body parts. Besides, SVM improves 1.47% over network output for the left part but only improves 0.26% for full body part, while the improvement of shifting handling shows a much slower fading pattern as with lower miss rate. Experiments also reveals that handling shifting pro- vides more benefits for smaller parts, i.e. the average improvement for 2 × 2 parts is 4.3% but it drops to 2.1% for 3×3 parts. This is consistent with the fact that smaller parts are more flexible to shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Overall evaluation</head><p>We report the final results on Caltech-Test in two aspects. In the first aspect, we investigate the overall pipeline by adding each component step-by-step, which is summarized in Table <ref type="table" target="#tab_4">4</ref>. The strong LDCF <ref type="bibr" target="#b20">[21]</ref> has 24.80% miss rate. Single full body part detector improves 3.61% by fine-tuning AlexNet, which is pre-trained on image-level data. Pre-training GoogLeNet rather than Alex-Net improves miss rate by 3.67%. Changing the pre-training strategy from image-level annotation to bounding box level annotation also improves 1.09%. Combining all parts detector (i.e. 45 in our implementation. All of them are fine-tuned on GoogLeNet which employed bounding box data for pretraining.) further reduces the miss rate by 3.31%. By adding shifting handling for each part detector, the final average miss rate on the reasonable subset is 11.89%.</p><p>In order to reduce testing time, we picked 6 part detectors from the entire part pool, decided by the top 6 weights of the ensemble SVM. As illustrated in Table <ref type="table" target="#tab_4">4</ref>, the ensemble of the 6 part detectors reaches 12.31%, which shows that the selected models offer the major improvement of the whole part pool. To understand whether the improvement comes from model ensemble itself or learning part complementarity, we construct another two experiments, 1) by combining 6 best independent parts and 2) by combining 6 full body detectors which are fine-tuned on AlexNet, Clarifai, and GoogLeNet with image and bounding box pretraining strategies, respectively. The weights for combination are learned by SVM. As shown in Table <ref type="table" target="#tab_4">4</ref>, simply combining the 6 best-performing part detectors reduces the performance, i.e. the miss rate increases 2.97%. Besides, en- semble of AlexNet, Clarifai, and GoogLeNet only achieves 15.5% miss rate. This reveals that the part complementarity is the major reason for ensemble improvement. The 6 selected parts are given in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>In the second aspect, we compare the overall result of DeepParts with existing best-performing methods , including VJ <ref type="bibr" target="#b32">[33]</ref>, HOG <ref type="bibr" target="#b3">[4]</ref>, MT-DPM <ref type="bibr" target="#b37">[38]</ref>, MT-DPM+Context <ref type="bibr" target="#b37">[38]</ref>, JointDeep <ref type="bibr" target="#b22">[23]</ref>, SDN <ref type="bibr" target="#b16">[17]</ref>, ACF+SDT <ref type="bibr" target="#b27">[28]</ref>, Informed-Haar <ref type="bibr" target="#b41">[42]</ref>, ACF-Caltech+ <ref type="bibr" target="#b20">[21]</ref>, SpatialPooling <ref type="bibr" target="#b26">[27]</ref>, LDCF <ref type="bibr" target="#b20">[21]</ref>, AlexNet+ImageNet <ref type="bibr" target="#b13">[14]</ref>, Katamari <ref type="bibr" target="#b2">[3]</ref>, SpatialPool-ing+ <ref type="bibr" target="#b26">[27]</ref>. Besides, we also compare the DeepParts with all existing deep models on reasonable subset, including Con-vNet <ref type="bibr" target="#b28">[29]</ref>, DBN-Isol <ref type="bibr" target="#b21">[22]</ref>, DBN-Mut <ref type="bibr" target="#b25">[26]</ref> and MultiSDP <ref type="bibr" target="#b39">[40]</ref>.</p><p>Fig. <ref type="figure" target="#fig_11">8</ref>, Fig. <ref type="figure" target="#fig_12">9</ref>, and Fig. <ref type="figure" target="#fig_13">10</ref> report the results on reasonable, partial occlusion, and heavy occlusion subsets, respectively. DeepParts outperforms the second best method (SpatialPooling+ <ref type="bibr" target="#b26">[27]</ref>) by 10 percent on the reasonable subset, which is a large margin. Besides, DeepParts improves the average miss rate on partial and heavy occlusion subsets by 19.32% and 14.23%, showing its potential to handle occlusion at different levels.</p><p>Deep Models Fig. <ref type="figure" target="#fig_14">11</ref> shows that DeepParts achieves the     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">KITTI</head><p>To test the generalization ability and verify whether the 6 complementary parts can be transferred to other street pedestrian detection dataset, we test our DeepParts on KITTI <ref type="bibr" target="#b10">[11]</ref>. We do not use the training data of KITTI and all components are trained on Caltech. DeepParts achieves promising results, i.e., 70.49%, 58.67% and 52.78% AP on easy, moderate, and hard subsets respectively. Fig. <ref type="bibr" target="#b10">[11]</ref> represents the results on moderate subset (pedestrians are no less than 25 pixels in height). The best detector Regionlets <ref type="bibr" target="#b34">[35]</ref> classified both the cyclists and pedestrians. The additional supervision improved its performance of pedestrian detection. It outperforms DeepParts by 2.48%. Without cyclists as supervision, DeepParts is the best-performing detector based on ConvNet, and surpasses R-CNN by 8.54%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed DeepParts to improve the performance of pedestrian detection by handling occlusion with an extensive part pool, showing significant superiority over previous best-performing models. The DeepParts can also be treated as a cascade stack over other pedestrian detectors to further improve performance. Future work lies towards model compression, such as incorporating all part detectors into one ConvNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Pedestrian detection results on Reasonable ∪ HeavyOcclusion subsets, where pedestrians are larger than 49 pixels in height and have at least 20% body part visible. Green, red, and blue represent true positives, false positives, and missing positives, respectively.</figDesc><graphic url="image-1.png" coords="1,326.58,241.67,200.81,150.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Complementary parts on Caltech pedestrian dataset and their normalized weights (i.e. importance).</figDesc><graphic url="image-2.png" coords="2,62.08,78.37,212.51,64.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) To our knowledge, we are the first to extensively explore how single part detector and their ensemble based on ConvNets contribute to pedestrian detection. In experiments, single part detector can achieve state-of-the-art performance while only observing a part of the proposal window, showing the robustness of DeepParts against occlusions. (3) We propose a novel method to handling proposal shifting problem. (4) We show that with complementary part selection, a new state-of-the-art miss rate of 11.89% can be achieved on the Caltech reasonable set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Part prototype examples, (x, y, w, h, i) is defined in Eqn.(2) (a) head-left-shoulder part with 2 grids in height and width; (b) leg part with 2 grids in height and 3 grids in width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Using head-shoulder part prototype as an illustration for training part detectors. (1) data: we extract corresponding regions within negative proposals as negative training samples; we compute the visible map of each ground truth box, and extract the corresponding region as a positive sample only if the part template is fully covered by the visible map. (2) ConvNet: a part detector can be any deep models, such as AlexNet, Clarifai and GoogLeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Best viewed in color. (a) shows how rapidly IoU will decrease with little shifting on horizontal and vertical orientation. (b) shows how to handle shifting problem in AlexNet. A true positive proposal which shifts 14.1%, namely 32/227, on both horizontal and vertical side is scored as 3.52 while the corresponding ground truth is scored as 6.81. With the neighboring search and penalization, our detector adjusts the score value to 5.40. x ′ min = x min − 16n 227 × w, y ′ min = y min − 16n 227 × h, w ′ = (1 + 32n 227 ) × w, h ′ = (1 + 32n 227 ) × h.</figDesc><graphic url="image-61.png" coords="5,50.11,72.00,495.01,220.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )</head><label>3</label><figDesc>Then we resize the patch to (227 + 32n) × (227 + 32n) and feed it into the fully convolutional AlexNet. As a result, (1 + n) × (1 + n) neighboring 227 × 227 patches are evaluated simultaneously while the expanded scale keeps the same as the proposal scale. The final output of conv8 can be viewed as a (1 + n) × (1 + n) score map S and each score corresponds to a 227 × 227 region. The final score of the part patch is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>227 ) 2 ( 5 )</head><label>25</label><figDesc>where a is the single orientation shifting penalty weight (we give the same weight on both horizontal and vertical orientations), and b is a geometrical distance penalty weight.In our implementation, we have n = 2 for all parts and search the values of a, b for each part by a 6-fold cross validation on training set. Fig.5 (b) shows an example of the full body part detector with 9 neighboring patches evaluated, where a = 2 and b = 10. Shifting handling is a kind of context modeling which keeps scale invariant, while simply cropping larger region with padding and resizing to 227 × 227 bring a scale gap between the training and testing stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Part prototypes of upper, left and full body.</figDesc><graphic url="image-62.png" coords="6,85.55,72.00,165.37,87.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fine</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Log-average miss rate (%) on Caltech-Test reasonable subset. The AlexNet, Clarifai and GoogLeNet are represented by blue, red and green, respectively. Strategy 1, 2, 3 indicate random initialization, image-level pre-training and object-level pre-training, respectively.(a) upper body part. (b) left body part. (c) full body part.</figDesc><graphic url="image-63.png" coords="7,62.49,72.00,470.24,167.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Average miss rate on reasonable subset.</figDesc><graphic url="image-64.png" coords="7,314.77,274.68,224.44,133.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Average miss rate on Partial Occlusion subset.</figDesc><graphic url="image-65.png" coords="8,67.83,154.58,200.82,116.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Average miss rate on Heavy Occlusion subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Comparison between deep models.</figDesc><graphic url="image-68.png" coords="8,67.83,467.01,201.08,95.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Results on KITTI with moderate subset.</figDesc><graphic url="image-66.png" coords="8,69.01,298.77,198.45,121.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(a), shifting a ground truth bounding box by 10% on horizontal or vertical direction leads to 0.9 IoU, which is</figDesc><table><row><cell>type</cell><cell>filter/stride</cell><cell>no extension</cell><cell>output map size extended by 32n</cell></row><row><cell></cell><cell></cell><cell>227 × 227</cell><cell>(227 + 32n) × (227 + 32n)</cell></row><row><cell>conv1</cell><cell>11 × 11/4</cell><cell>55 × 55</cell><cell>(55 + 8n) × (55 + 8n)</cell></row><row><cell>pool1</cell><cell>3 × 3/2</cell><cell>27 × 27</cell><cell>(27 + 4n) × (27 + 4n)</cell></row><row><cell>conv2</cell><cell>5 × 5/1</cell><cell>27 × 27</cell><cell>(27 + 4n) × (27 + 4n)</cell></row><row><cell>pool2</cell><cell>3 × 3/2</cell><cell>13 × 13</cell><cell>(13 + 2n) × (13 + 2n)</cell></row><row><cell>conv3</cell><cell>3 × 3/1</cell><cell>13 × 13</cell><cell>(13 + 2n) × (13 + 2n)</cell></row><row><cell>conv4</cell><cell>3 × 3/1</cell><cell>13 × 13</cell><cell>(13 + 2n) × (13 + 2n)</cell></row><row><cell>conv5</cell><cell>3 × 3/1</cell><cell>13 × 13</cell><cell>(13 + 2n) × (13 + 2n)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Fully convolutional structure of AlexNet.</figDesc><table><row><cell>We turn the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>-tuning data upper left full Every 30 th image 41.19 46.96 33.01 Every 10 th image 38.25 46.36 30.45 Every 5 th image 36.63 41.59 23.83 Every 3 rd image 34.60 39.69 23.18 Log-average miss rate (%) on Caltech-Test of upper, left and full body parts with respect to data volume. All results are obtained by fine-tuning from AlexNet which adopted image-level pre-training strategy (i.e. strat.2).</figDesc><table><row><cell>Every image</cell><cell>34.11 37.83 21.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of our pipeline.</figDesc><table><row><cell cols="3">detection pipeline LDCF AlexNet</cell><cell>AlexNet</cell><cell>image</cell><cell>parts</cell><cell>shifting</cell><cell cols="2">6 parts top 6</cell><cell>A,C,G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">to GoogLeNet to box ensemble handling</cell><cell></cell><cell>parts</cell><cell>ensemble</cell></row><row><cell>miss rate (%)</cell><cell>24.80</cell><cell>21.19</cell><cell>17.52</cell><cell>16.43</cell><cell>13.12</cell><cell>11.89</cell><cell>12.31</cell><cell>15.28</cell><cell>15.50</cell></row><row><cell>improvement (%)</cell><cell></cell><cell>+3.61</cell><cell>+3.67</cell><cell>+1.09</cell><cell>+3.31</cell><cell>+1.23</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.vision.caltech.edu/Image_Datasets/ CaltechPedestrians/rocs/UsaTestRocs.pdf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement Ping Luo * is supported by the National Natural Science Foundation of China (61503366). This work is also partially supported by the National Natural Science Foundation of China (91320101, 61472410).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pedestrian detection at 100 frames per second</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, CVRSUAD workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-cue pedestrian classification with partial occlusion handling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eigenstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discriminatively trained and-or graph models for object shape detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<editor>TPAMI</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handling occlusions with franken-classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human detection based on a probabilistic assembly of robust part detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Example-based object detection in images by components</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling mutual visibility relationship in pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Detection and tracking of occluded people</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular 3d scene understanding with explicit occlusion reasoning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust multiresolution pedestrian detection in traffic scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Informed haarlike features improve pedestrian detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
