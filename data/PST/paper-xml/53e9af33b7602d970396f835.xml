<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Handwritten word-spotting using hidden Markov models and universal vocabularies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">José</forename><forename type="middle">A</forename><surname>Rodríguez-Serrano</surname></persName>
							<email>j.a.rodriguez-serrano@lboro.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre de Visió Per Computador (CVC)</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O Campus Bellaterra</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
							<email>florent.perronnin@xrce.xerox.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Chemin de Maupertuis</orgName>
								<address>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Loughborough University</orgName>
								<address>
									<postCode>LE11 3TU</postCode>
									<settlement>Loughborough</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Handwritten word-spotting using hidden Markov models and universal vocabularies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DFC189856ADDDDC29CDE856382114B1</idno>
					<idno type="DOI">10.1016/j.patcog.2009.02.005</idno>
					<note type="submission">Received 10 June 2008 Received in revised form 5 December 2008 Accepted 2 February 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Word-spotting Hidden Markov model Score normalization Universal vocabulary Handwriting recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Handwritten word-spotting is traditionally viewed as an image matching task between one or multiple query word-images and a set of candidate word-images in a database. This is a typical instance of the query-by-example paradigm. In this article, we introduce a statistical framework for the word-spotting problem which employs hidden Markov models (HMMs) to model keywords and a Gaussian mixture model (GMM) for score normalization. We explore the use of two types of HMMs for the word modeling part: continuous HMMs (C-HMMs) and semi-continuous HMMs (SC-HMMs), i.e. HMMs with a shared set of Gaussians. We show on a challenging multi-writer corpus that the proposed statistical framework is always superior to a traditional matching system which uses dynamic time warping (DTW) for wordimage distance computation. A very important finding is that the SC-HMM is superior when labeled training data is scarce-as low as one sample per keyword-thanks to the prior information which can be incorporated in the shared set of Gaussians.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Handwritten word-spotting is the pattern classification task which consists in detecting keywords in handwritten document images <ref type="bibr" target="#b0">[1]</ref>. In this article, we are concerned with the detection of words in handwritten mails for document routing purposes. Nowadays, large institutions and corporations still receive a high volume of communications in paper form, for instance because of their legal significance, as a backlog of old documents to be archived, or as general-purpose correspondence. The detection of keywords can be relevant for automatically routing the document. For example, one might give more priority to documents containing the word "urgent", or one might want to re-route the documents containing "cancellation" (Fig. <ref type="figure" target="#fig_0">1</ref>) to the customer service department. A significant fraction of these documents (mainly post) is handwritten. In the following, we discuss why neither traditional handwriting recognition (HWR) systems nor traditional word-spotting systems are completely suited to our problem. We then describe the proposed solution.</p><p>Traditional methods of handwritten word recognition (HWR) are usually too cumbersome for such a scenario. As was revealed by the first works on word-spotting, it might not be necessary to decode every single character and word in a document in order to perform word search. Additionally, it is also a known fact that HWR systems have not yet achieved competitive performances in free-style cursive handwriting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Another disadvantage of HWR applied to this task is their need to be trained with a huge amount of labeled data. The long, tedious and costly process of data collection and annotation has to be repeated for different languages when they are based on different alphabets. Finally, it was shown that word-spotting systems avoiding HWR can perform at a higher speed.</p><p>Two main types of word-spotting approaches can be identified, depending on how the input is specified: the query-by-string and the query-by-example.</p><p>In query-by-string approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, character models have been trained in advance and at query time the models of the characters forming the string are concatenated into a word model and the probability of each word image is evaluated. Thus, such approaches are very similar to HWR systems. Once trained, these approaches allow searching for any possible keyword. However, they present similar drawbacks to HWR systems.</p><p>In query-by-example approaches <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, the input is an image of the word, and the output is a set of word images that are most similar in appearance to the query image. This can be seen as a contentbased image retrieval (CBIR) task. The search for a desired keyword is subject to having an exemplary image of this keyword available. Because the result is based on a distance measurebetween the query and all candidate word images, no training is involved. However, the performance is limited. Many of the works that belong to this category have shown acceptable performances on datasets which contain data from a single or few writers. However, it remains unclear how these methods would perform in multi-writer conditions. In some cases, the performance can be increased by querying multiple times with different images and combining the results. While it is intuitive that a statistical model could boost the performance by combining the different queries into a single model, to the best knowledge of the authors this option has remained unexplored in the handwritten word-spotting literature.</p><p>The main contribution of the present work is to overcome the drawbacks of both approaches. Instead of querying by string or by example, we query by "word-class": we select one or multiple examples from the desired word and train a probabilistic model for this word. Word candidates are detected by evaluating the posterior probability of the candidate given the model. The statistical tools to achieve this objective are hidden Markov models (HMMs) <ref type="bibr" target="#b11">[12]</ref> for word models and Gaussian mixture models (GMMs) for the score normalization <ref type="bibr" target="#b12">[13]</ref> part. We explore the use of two types of HMMs for the word modeling part: continuous HMMs (C-HMMs) and semi-continuous HMMs (SC-HMMs), i.e. HMMs with a shared set of Gaussians.</p><p>We show experimentally how the proposed approach is consistently superior to a query-by-example system on a challenging multi-writer corpus. A very important finding is that the SC-HMM is superior when labeled training data is scarce-as low as one sample per keyword-thanks to the prior information which can be incorporated in the shared set of Gaussians.</p><p>Compared to a traditional HWR system based on character models which requires large amounts of labeled training samples, our approach requires a limited number of labeled samples for a given word-class. This makes the proposed approach extremely easy to extend to new languages for instance.</p><p>Besides the statistical framework, we also introduce some novelties in terms of feature extraction. The rest of the article is organized as follows. Section 2 discusses the related work. Section 3 presents an overview of the entire word-spotting system. Section 4 describes the features implemented in our system. The main focus of the article is Section 5 describing the statistical modeling in depth. The experimental validation of the system is reported in Section 6 and finally conclusions are drawn in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Originally, word-spotting was formulated for detecting words or phrases in speech messages <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, and then extended to locate words in typed text documents <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b5">[6]</ref> it was first proposed for off-line handwritten documents, as a way to automatically index historical document collections. This enabled the paradigm of search engines for handwritten document images <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>.</p><p>Several works have tried to address word-spotting with strategies similar to HWR systems. Their main idea is to work with character models and then to concatenate them into word models using either HMMs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> or simpler statistical assumptions <ref type="bibr" target="#b22">[23]</ref>. An advantage of these systems is that they allow to search for any keyword (or even a more general entity such as "street name" <ref type="bibr" target="#b23">[24]</ref>) once the character models are trained. Although their aim is not to obtain a complete transcript, the concept is pretty much the same as HWR systems. Hence, the limitations discussed in the Introduction also apply to this kind of methods.</p><p>However, as discussed in the Introduction, the most popular type of approach is the query-by-example. It consists in matching an input image with one or multiple query images to determine a distance (or, equivalently, similarity) that might indicate a correspondence. Two main classes of approaches have been proposed for handwritten word-spotting. The first type of approaches uses holistic techniques. This means that an image is described with a single feature vector and a distance between the vectors is defined. Manmatha et al. <ref type="bibr" target="#b5">[6]</ref> use directly the image pixels as features and apply the Scott and Longuet-Higgins (SLH) distance <ref type="bibr" target="#b24">[25]</ref>, which is invariant to affine transformations. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> employ a set of binary features called GSC (for gradient-structural-concavity) and match them using a correlation-like measure.</p><p>The second type of approaches describes a word image as a set of local features. For instance, Leydier et al. <ref type="bibr" target="#b26">[27]</ref> use the gradient angles as features and a cohesive elastic distance. Rothfeder et al. <ref type="bibr" target="#b27">[28]</ref> use a corner detector and an elastic distance between corner positions. The state-of-the-art technique for computing the distance is dynamic time warping (DTW). A variety of features has been proposed for DTW matching such as word profiles <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, "eigenslits" (PCA projections of the imagepixels) <ref type="bibr" target="#b10">[11]</ref> or contours <ref type="bibr" target="#b9">[10]</ref>. These methods show relatively good accuracy and speed in applications such as historical document retrieval. However, it is also worth mentioning that all the previously referred works, except <ref type="bibr" target="#b25">[26]</ref> report results on test data by a single or few writers. Thus the generalization capabilities of these approaches to more complex detection tasks are not clear. None of the works using the query-by-example approach employs a statistical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System overview</head><p>As mentioned in the Introduction, the problem addressed in this work is the detection of a set of known keywords in an incoming flow of scanned letters. A significant fraction of these letters are handwritten and cannot be processed by OCR systems. Therefore it is generally processed manually by human operators, which results in a tedious, expensive and slow task. However, we should note that, apart from mail routing, the system presented in this work could  be applied to other problems of similar nature, such as indexing historical documents, document categorization and, more generally, metadata extraction from document images.</p><p>In the following sections we will provide the details of the proposed word-spotting system. Although we put the focus on the crucial steps of feature extraction and statistical modeling, an overview of the word-spotting system is provided for completeness in this section. Fig. <ref type="figure">2</ref> graphically summarizes the steps of the current system.</p><p>Segmentation. First, a set of word images is extracted from the document image. The process identifies the document zones and lines using common techniques such as morphological operations and projection profiles analysis <ref type="bibr" target="#b28">[29]</ref>. An example of projection profile is depicted in Fig. <ref type="figure">3</ref>. To separate the words in each of the obtained lines, first the connected components are identified, their convex hull is computed and they are ordered from left to right. A distance is assigned to neighboring components which equals the shortest distance between their convex hulls. This is based on <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> and illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. Distances larger than a threshold are likely to correspond to word gaps. A k-means algorithm with two centroids is used to obtain the threshold in an unsupervised manner.</p><p>While this segmentation method can work reasonably well for clean handwriting, segmentation is usually error-prone in noisy handwriting. For that reason, we consider different thresholds to produce several segmentation hypotheses to ensure that the correct word separation configuration is considered among all the segmented elements. The same idea was used in <ref type="bibr" target="#b31">[32]</ref>. In our case, the gaps in a line are sorted according to their similarity to the threshold, and the first N hyp gaps are used to generate segmentation hypotheses. After preliminary experiments we set the value N hyp = 10.</p><p>Fast rejection. The previous multiple hypothesis segmentation produces about 300 word hypotheses per letter. To reduce the processing cost of the statistical modeling, the list of word images to analyze is pruned using two simple steps. First, images whose width is unlikely to correspond to the width of the keyword are removed, by comparing the image width with the empirical distribution of widths in the training set. Somewhat surprisingly, the image width was foundto be more discriminative than the traditional width/height ratio. For the remaining images, we compute a set of holistic features, by applying the features described in Section 4 to the word image globally. This is a much rougher description of the images but it allows very fast classification using a sparse logistic regression (SLR) classifier <ref type="bibr" target="#b32">[33]</ref>. On a dataset of 630 documents (see Section 6) with more than 180,000 word hypotheses, approximately, 94% of the extracted word hypotheses are pruned while falsely rejecting 10% of the keyword images.</p><p>Normalization. The images surviving the fast rejection step are subject to common normalization processes to reduce their variability. First, the skew is corrected by computing the vertical projection profile of the image in multiple rotations and selecting the rotation that maximizes the standard deviation of the profile. The slant of the image is corrected in a similar way: for a number of shear transforms of the image, we select the one that maximizes the horizontal projection profile (a shear transform is illustrated in Fig. <ref type="figure" target="#fig_3">5(a)</ref>). In the normalization box of Fig. <ref type="figure">2</ref> some examples of skew and slant corrected images are shown. After that, the size of the image is corrected. First, we determine the upperline and baseline as done in <ref type="bibr" target="#b34">[34]</ref>. Then, we rescale the image so that the distance between upperline and baseline (denoted as main body) is a fixed value of = 18 pixels. This produces images of a uniform size without being much influenced by the sizes of ascenders and descenders. Finally, we perform a "silence removal" operation, inspired from speech recognition, which in our case consists in removing the image columns that only contain background pixels (see Fig. <ref type="figure" target="#fig_3">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) for an example).</head><p>We confirmed that all these operations increased the performance of the final system. We do not perform any normalization of width of the words since we will employ HMMs and these can cope with variable-length sequences. The same applies for DTW which is used for comparison in Section 6.4.</p><p>Feature extraction. From each normalized image, a sequence of feature vectors is obtained. A sliding window is shifted along the image from left to right, and at each position a feature vector is extracted. Our system is independent of the particular features computed at this step. Currently, the best performance is obtained with the local gradient histogram (LGH) features, that we proposed in <ref type="bibr" target="#b35">[35]</ref>, but we also consider other features. We will devote Section 4 to the description of the features that are used in this work.</p><p>Statistical modeling. This is the principal focus of this work. The main issues in the statistical modeling part are the choice of the word-models and score normalization models. In this work we have chosen HMMs and GMMs, respectively. We see how these two components are especially linked in the case of the SC-HMM and how the SC-HMM is able to learn from a very small set of samples. The statistical modeling part is examined in detail in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Feature extraction</head><p>To show that our statistical approach is feature-independent, several features were implemented and evaluated. We included features that had been previously employed for modeling handwritten words in a HMM framework <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> as well as our features <ref type="bibr" target="#b35">[35]</ref>. The wellknown feature set in the word-spotting domain <ref type="bibr" target="#b7">[8]</ref> is not considered because it is a subset of the features in <ref type="bibr" target="#b36">[36]</ref> and they showed poorer performance in preliminary experiments with respect to the superset. All the considered features follow a sliding window approach: a fixed-width window shifts column by column from left to right and at each position a feature vector is extracted. The sequence of vectors obtained in this fashion can be suitably modeled with HMMs. We describe the three feature types we evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Column features</head><p>One of the most influencing works in offline handwriting recognition using HMMs is probably the one by Marti and Bunke <ref type="bibr" target="#b36">[36]</ref>.</p><p>In their work and other recent works (cf. <ref type="bibr" target="#b38">[38]</ref>) by these authors, the features are taken columnwise. From the set of foreground pixels in each image column, nine geometrical features are computed, namely: the total number of foreground pixels, the mean, second order moment, minimum and maximum of their positions, the differences between the maximum and minimum values with respect to the previous column, the number of black-white transitions, and the number of foreground pixels between upper-and baseline.</p><p>To obtain features robust to the size variations of ascenders/descenders, the authors of <ref type="bibr" target="#b36">[36]</ref> scale the three main zones of an image (the ones determined by the upperline and baseline) to a predefined height. This would rather belong to the normalization step explained in Section 3. However, we prefer to have a common image normalization step independent of the features. Therefore, we impose a similar strategy at feature extraction time in which we consider that the baseline corresponds to a vertical coordinate of y = 0 and the upperline to y = 1. A further improvement that was found experimentally is to treat each individual feature sequence as a signal and apply a Gaussian convolution to smooth the signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pixel count features</head><p>Contrarily to the column features, in another well-known work by Vinciarelli et al. <ref type="bibr" target="#b37">[37]</ref> the width of the sliding window comprises several columns. At each position, the height of the window is adjusted to the area actually containing pixels, and then it is split into a 4 × 4 cell grid. The pixel counts in each of these cells are concatenated to form a 16-dimensional feature vector. To avoid boundary problems at the very first or very last positions of the sliding window, we assume the area outside the image consists of background pixels.</p><p>Again, two additional improvements were found in practice: (a) to apply a prior Gaussian smoothing to the word images, and (b) to normalize the feature vectors so that the sum of their components is 1, making the features independent of stroke thickness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">LGH features</head><p>The LGH features were proposed in <ref type="bibr" target="#b35">[35]</ref> inspired by the computer vision literature <ref type="bibr" target="#b39">[39]</ref> and show better performance than other common features. Fig. <ref type="figure" target="#fig_4">6</ref> provides an overview of the process. First a Gaussian filter is applied to a word image I(x, y) to obtain the smoothed image L(x, y). From L, the horizontal and vertical gradient components G x and G y are determined as</p><formula xml:id="formula_0">G x (x, y) = L(x + 1, y) -L(x -1, y)<label>( 1 )</label></formula><p>and</p><formula xml:id="formula_1">G y (x, y) = L(x, y + 1) -L(x, y -1).<label>(2)</label></formula><p>Then, the gradient magnitude m and direction are obtained for each pixel with coordinates (x, y) as</p><formula xml:id="formula_2">m(x, y) = G 2 x + G 2 y (3) and (x, y) = ѯ(G y , G x ), (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>where ѯ is a function that returns the direction of the vector (G x , G y ) in the range [-, ] by taking into account tan -1 (G y /G x ) and the signs of G x and G y .</p><p>A sliding window of fixed width traverses the image from left to right to obtain a sequence of overlapping sub-images of word parts. Each sub-image is further reduced to the region actually containing pixels and this region is subdivided into 4 × 4 regular cells. From all the pixels in each cell, a histogram of gradient orientations is constructed. We consider T =8 orientations. Each pixel contributes to the closest bin with an amount m(x, y). Alternatively, the two closest orientations can share the amount m(x, y) as determined by a linear interpolation to reduce the impact of aliasing noise (see Fig. <ref type="figure" target="#fig_5">7</ref>).</p><p>The concatenation of the 4 × 4 histograms of 8 bins gives rise to a 128-dimensional feature vector for each window position. Each feature vector is scaled to have norm 1. This operation can be related to a local contrast normalization and significantly improves performance in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Statistical modeling</head><p>The main contribution of this work is to extend the whole-word query-by-example word-spotting to a statistical modeling framework. In the traditional query-by-example framework, a distance d between a candidate word and a query image is computed and the candidate word is said to be the same term if d is below a predefined threshold. In other cases, several query images are considered. Individual distances d i are computed between the candidate and each query, and a decision rule combines the scores. In this work, instead of using a distance, we train a statistical model using all the queries and the score of the word image is the posterior probability of the model knowing the image. The main advantage with respect to the query-by-example is that we expect superior performance. The main advantage with respect to HWR systems is that we do not need to train subword models, which would require a large labeled word corpus. Instead, we only need keyword samples to train the model. One advantage of this is that the approach could be more easily extended to other languages that do not use the same alphabet, for instance.</p><p>In a Bayesian framework, the score of a word image X on a word W is the posterior p(W|X). Usually, it is preferable to work with logarithms. From Bayes' rule follows that the score (log of posterior) is log p(W|X) = log p(X|W) + log p(W)log p(X).</p><p>(5) We should distinguish between two types of problems. In closed-world problems (as discussed in <ref type="bibr" target="#b37">[37]</ref>), a word W * from the lexicon is selected according to W * = arg max W p(W|X). Since p(X) is constant across all the hypotheses W, the relevant score is log p(X|W)+log p(W). In contrast, in verification problems, as it is our case, an image X is said to correspond to word W n if p(W n |X) &gt; n , n being a decision threshold. In this case, log p(W n ) can be integrated in the decision threshold and the score to be considered is</p><formula xml:id="formula_4">S n (X) = log p(X|W n ) -log p(X).<label>(6)</label></formula><p>The term p(X) has to be explicitly modeled. It can be interpreted as a correction of the nave score p(X|W). This correction is known under the name "score normalization" in the literature. In this work, p(X|W n ) is modeled using a HMM and p(X) is modeled using a GMM, as detailed in Sections 5.1 and 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hidden Markov models</head><p>HMMs are state-of-the-art in handwriting recognition <ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref>. A key advantage in handwriting modeling is that HMMs can cope with variable-length data and non-linear time deformations. The reader is referred to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b47">47]</ref> for technical details on HMMs.</p><p>In practice, a HMM can be employed to represent a whole word or, alternatively, sub-word units such as characters which can be concatenated to form general strings. In our particular problem, the former choice is preferable since it only requires keyword examples as training material. We adopt the common left-to-right topology without skip-state jumps (an example is illustrated in Fig. <ref type="figure" target="#fig_6">8</ref>). Given a training set, the Baum-Welch algorithm <ref type="bibr" target="#b48">[48]</ref> is employed for optimizing the parameters of the model. A trained model can be used to score new input images by means of the forward algorithm (or its approximation using the Viterbi algorithm) <ref type="bibr" target="#b11">[12]</ref>. Of course, word images need to be represented as sequences of feature vectors X = x 1 x 2 ... x T , also known as sequences of frames.</p><p>Inspired by <ref type="bibr" target="#b49">[49]</ref>, we experimented with two different approaches to set the number of states per word HMM. In the first case, the number of states is proportional to the number of characters in the word. In the second case, it is proportional to the average word length (as estimated on a training set). Both approaches lead to very similar detection performance in our case, and we chose the former one as it does not rely on the training material. A number of 10 states per letter is employed in the experiments.</p><p>In the following, we consider two HMM types, namely the C-HMM and the SC-HMM, which differ in the computation of the emission probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Continuous HMM</head><p>In C-HMMs, the likelihood of emitting a frame x t in state j is modeled using a GMM b j (x t ) = N j l=1 w jl N(x t | jl , jl ), <ref type="bibr" target="#b6">(7)</ref> where N j denotes the number of Gaussians in state j, N(x| , ) denotes a normal distribution with mean vector and covariance matrix , and the weights w jl are subject to the constraints 0 w jl and l w jl = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Semi-continuous HMM</head><p>The large amount of Gaussian parameters found in a C-HMM can be reduced if we restrict all the state-dependent Gaussian p.d.f's to come from a global pool of p.d.f.'s <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51]</ref> so that the only statedependent parameters are the mixture weights. This case is known as semi-continuous or tied-mixture HMM. In this case, the observation probabilities in state j are written as</p><formula xml:id="formula_5">b j (x t ) = N l=1 w jl N(x t | l , l ). (<label>8</label></formula><formula xml:id="formula_6">)</formula><p>However, sometimes a large N is needed for obtaining good accuracy, and in practice the number of parameters of the SC-HMM can be larger than in the C-HMM. Nevertheless, in this case the mixture weights tend to be sparse.</p><p>The pool of Gaussians is obtained by training a GMM in an unsupervised way from a large set of frames appearing in many different word samples. Here, the Gaussians represent soft clusters of similar frames, so they can be interpreted as "codewords" of a vocabulary, like in the computer vision literature <ref type="bibr" target="#b52">[52]</ref>. We refer to this GMM as "universal vocabulary" of shapes, since each codeword represents a part of a character, a connector, a whole character, etc. In the speech recognition literature, this is known as universal background model (UBM) <ref type="bibr" target="#b53">[53]</ref>. Since the means and covariance matrices of the SC-HMM are taken from the universal vocabulary, only the transition probabilities and mixture weights have to be updated at training time. A weight w jl is the frequency of the codebook shape l in state j.</p><p>The use of a universal vocabulary brings a crucial benefit to our approach. Usually, C-HMMs shows very poor generalization performance when the training material is scarce, due to overfitting. On the other hand, the SC-HMM is constrained by the a priori information contained in the visual vocabulary. Thus it will obtain reasonable performances in the case where a small amount of training material is available, which is crucial in a word-spotting problem. In fact, experiments in Section 6.4 show that even in the case of a single training sample we outperform a classical DTW-based queryby-example approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Score normalization</head><p>The second component of the statistical model deals with the computation of p(X) in Eq. ( <ref type="formula" target="#formula_4">6</ref>). First, we review existing approaches for score normalization and then we will describe the proposed method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Score normalization approaches</head><p>Score normalization is a well studied topic in speech/speaker recognition applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. More recently, several works in the field of HWR have used score normalization for writer verification <ref type="bibr" target="#b54">[54]</ref> or rejection strategies <ref type="bibr" target="#b55">[55]</ref>.</p><p>In the speech and handwriting recognition literature, two main score normalization approaches can be identified. The first score normalization approach consists in splitting the evidence p(X) into: </p><formula xml:id="formula_7">p(X) = p(W)p(X|W) + p( W)p(X| W),<label>(9)</label></formula><formula xml:id="formula_8">= 1 1 + p( W) p(W)<label>(10)</label></formula><p>p(X| W) p(X|W) <ref type="bibr" target="#b10">(11)</ref> and the following likelihood ratio can be used for scoring:</p><formula xml:id="formula_9">p(X|W) p(X| W) . (<label>12</label></formula><formula xml:id="formula_10">)</formula><p>The most widely used approach to computing p(X| W) is the socalled cohort models <ref type="bibr" target="#b56">[56]</ref>. X is scored against a set of alternative models, the cohorts, and the scores are then combined by taking the average or the max for instance. However, in a small vocabulary word-spotting application there is generally no available set of cohorts and this approach cannot be applied.</p><p>The second approach, which is more suited to the word-spotting problem, consists in modeling directly the distribution p(X) of all the words that might be encountered by the system <ref type="bibr" target="#b14">[15]</ref>. This model is generally referred to as filler model, garbage model, world model or background model. It is trained on a sufficiently large set of representative sample images. The traditional approach is to use for the filler model the same topology as for the word model. This means that if the word model is a HMM with e.g. 100 states, then the filler model will be a HMM with 100 states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Proposed score normalization</head><p>To model p(X), we follow the philosophy of filler models but we propose to use a different topology and use instead a GMM. Indeed, we propose to estimate a universal vocabulary, as explained in Section 5.1 and score a sample X = x 1 x 2 ... x T using the log-likelihood on the vocabulary GMM:</p><formula xml:id="formula_11">log p(X) = T t=1 log p(x t | GMM ), (<label>13</label></formula><formula xml:id="formula_12">)</formula><p>where GMM denotes the parameters of the universal vocabulary. Note how this score normalization is especially convenient for the SC-HMM case since the universal vocabulary is already available from the previous step. In this case, the only extra cost induced by the score normalization is the evaluation of Eq. ( <ref type="formula" target="#formula_11">13</ref>). In the C-HMM case, however, the universal vocabulary has to be explicitly estimated at training time. A summary of the scoring procedure is given in Fig. <ref type="figure" target="#fig_7">9</ref>.</p><p>Despite the fact that a GMM does not take into account the ordering of the frames, contrarily to a HMM, experimental results in Section 6.3 show that the proposed approach is on par in terms of detection accuracy with respect to the filler-model-based normalization. But more importantly, thanks to its simpler structure, it has a significantly lower computational cost at test time. Also, there is only one GMM for score normalization for all keywords. In the case of filler models, if the different keywords have different topologies, different filler models must be used. This increases even more the difference in computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Interpretation of score normalization</head><p>The need for score normalization can be understood using an illustrative example. Let us imagine the image of a word written with a writing style that does not appear in the training set. Even if the image corresponds to keyword W, the log-likelihood p(X|W) might be low. In contrast, when score normalization is considered, the frames will also score low on the universal vocabulary (i.e. p(X) will be low), thus keeping the ratio p(X|W)/p(X) approximately constant. The same effect is found in the opposite case. When a word contains frames that are very frequent in the vocabulary, the log-likelihood p(X|W) will be high, but in this case p(X) is also large and score normalization counter-balances this effect. In general, score normalization compensates for mismatches between training and test conditions and words with frequent patterns (i.e. frequent letters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>The proposed statistical framework for word-spotting is evaluated in a series of experiments. First, the experimental conditions are described. Then, we evaluate the system performance and the effect of score normalization. Finally, we compare our framework with the traditional image matching approach and study the influence of the amount of training material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental setup</head><p>Keyword-spotting experiments are carried out on real mailroom data. We use a dataset containing 630 scanned letters written in French submitted to the customer department of a company. Because we consider the first page of individual letters, it is assumed that each letter is from a different writer. The writing is unconstrained and the letters contain artifacts such as stricken words and spelling mistakes.</p><p>Word hypotheses are segmented from the page images as explained in Section 3 and the sub-images corresponding to specific keywords are manually labelled. The 10 most frequent word classes (e.g. "Monsieur", "Madame", "résilier", "contrat", etc.) are used in the experiments. The number of labeled positive samples for the different keywords ranges from 208 to 750. This does not mean that these words are easy to detect. Actually, these values have to be compared to the 180,000 word hypotheses generated by the segmentation process (as commented in Section 3). For instance, the most frequent word (750 samples) corresponds to only 0.4% of the word hypotheses. Thus the probability that these samples appear by chance among the top retrieved results is very small. We chose the most frequent words to ensure that we have enough positive samples to estimate robustly the retrieval accuracy.</p><p>The letters are split into six folds, numbered from 0 to 5. Samples from fold 0 will be used exclusively for training the universal vocabulary. The other folds will remain for the training and evaluation of keyword models, which is done on a 5-fold cross validation basis.</p><p>The numeric results reported in this section refer to the set of samples not eliminated by the fast rejection step. As mentioned above, about 180,000 word hypotheses are extracted from the 630 documents. In the current setting, the fast rejection step eliminates on average 94% of them for each keyword class (by falsely rejecting 10% of the real positives keywords). This is just an exemplary setting which is application-dependent but we have observed the generality of the approach for other settings in separate experiments. Approximately 25 word images per keyword and per document survive the fast rejection step.</p><p>For evaluation, we plot traditional precision-recall curves. To summarize the performance of a system with a single figure, we measure the average precisions (AP) for the 10 keywords and report the mean (later referred to as mean AP or mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">System performance</head><p>In the first round of experiments, we compare the C-HMM and SC-HMM using the three different feature types described in Section 4. For these experiments we use all the available training data (the influence of the amount of training data is studied in Section 6.4).</p><p>In Table <ref type="table" target="#tab_0">1</ref> we compare the LGH features to the Marti and Bunke and Vinciarelli features for the C-HMM case, in terms of mAP values. This is done for a number of N G = 1, 2, 4, 8, 16 Gaussians per state. For each N G we also have to evaluate the optimal number of Gaussians of the GMM used for score normalization. This is one disadvantage of the C-HMM. The optimal number of Gaussians N of the GMM for each case is given in brackets in Table <ref type="table" target="#tab_0">1</ref>. The precision-recall curve for the word résiliation is shown in Fig. <ref type="figure" target="#fig_9">10</ref>.</p><p>The best performance (mAP = 0.870) is obtained using the LGH features, with a HMM of 16 Gaussians per state and a universal vocabulary of 512 Gaussians. In practical systems we might prefer to use a reduced number of Gaussians, e.g. 8 since the cost of training In each case, the universal vocabulary leading to best performance is used. The number of Gaussians of the best universal vocabulary is given in brackets.     Since such a HMM is described by 1024 weights per state (and 10 states per character), the effective number of parameters is large, but inspection of these values reveals that the weight vectors are very sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Score normalization</head><p>Although the experiments in previous section have considered score normalization, we would like to provide some more experiments to understand their effect. In the current round of experiments we compare the performance of three settings: (1) without score normalization, (2) with a filler model, (3) with the proposed GMMbased score normalization. We provide results for the LGH features but experiments (not shown here) have shown the generality of the approach for the other feature types.</p><p>For the system without score normalization, experiments show that training the HMMs with more than 1 Gaussian per mixture does not give a significant performance improvement. This is surprising when comparing with the results found in the literature, which use a larger number of Gaussians, and also given the large number of training samples. Hence, we keep 1 Gaussian per mixture.</p><p>When using a filler model, the best performance is obtained when both the word HMM and the filler HMM have 16 Gaussians per mixture. Finally, for the proposed system, the best performance is obtained for a HMM with 16 Gaussians per mixture and the normalizing GMM of 512 Gaussians per mixture, as reported in the previous section.</p><p>Table <ref type="table" target="#tab_2">3</ref> provides the mAP values for the three normalization schemes. We can observe a significant improvement in terms of mAP when score normalization is considered. Also, it can be observed that the filler model approach and the proposed GMM approach are on par in terms of detection accuracy. However, the training process of the GMM with 512 Gaussians takes 2.5 less time than training a filler model with 16 Gaussians. And at test time, the scoring on the GMM is 4.6 times faster than on the filler model on average. On top of that, if word-dependent filler models are employed-as in our case-the computational cost must be multiplied by the number of different filler models. Thus it is clear that the proposed GMM-based normalization provides an advantage in terms of computational cost with respect to the traditional filler model approach.</p><p>As for SC-HMM experiments, we found that the absence of score normalization led to very poor performance for our detection problem. Our best explanation is the dimensionality of the LGH features. It is a known fact that in HMMs with continuous emission probabilities, mixture weights play a less and less important role as the dimensionality of the feature vectors increases. For instance, in the case of a model with 512 Gaussians, mixture weights have values on the order of 1/512 = 2 • 10e -3 . This is to be compared with the Gaussian likelihoods which are on the order of 10 -50 for LGH features. As the mixture weights are the only word-dependent parameters of the emission probabilities of an SC-HMM and as their influence is very small compared to the Gaussian likelihoods, the poor performance of the SC-HMM is not surprising. However, when applying the score normalization based on the same pool of Gaussians underlying the SC-HMM, the performance dramatically increases. This is appreciated from the mAP values reported in Table <ref type="table" target="#tab_3">4</ref>.</p><p>Note also that the GMM normalization performs essentially as well as the filler-based normalization. However, the important point is that the GMM is already available from the training phase of the SC-HMM. In contrast, the filler model must be explicitly constructed. Thus the cost saved by using the GMM is very high.</p><p>In the previous section we stated that an advantage of our score normalization approach in the SC-HMM case is that it reduces the mismatch between training and test conditions due to the fact that both p(X|W) and p(X) use the same pool of Gaussians. Let us analyze this statement using an additional experiment. In Table <ref type="table">5</ref> we show the performances for SC-HMM with a SC-HMM of N Gaussians but using a universal vocabulary of N Gaussians for score normalization. One can appreciate that in the cases where N N the performance decreases compared to the cases where the same pool is used (N = N ). In earlier experiments <ref type="bibr" target="#b57">[57]</ref> we observed the same behavior for the two other features, although the performance drop is less exaggerated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison with DTW and evaluation of the influence of training samples</head><p>It is useful to compare the performance of the two proposed systems based on C-HMMs and SC-HMMs to a state-of-the-art queryby-example approach which uses DTW to compute the distance between word-images. We especially study the influence of the number of query samples on the accuracy of the three methods.</p><p>For the DTW baseline, we use the Euclidean distance for the individual vector-to-vector distance and divide the score by the length of the warping path, to conform to <ref type="bibr" target="#b7">[8]</ref>. When querying with multiple images, we compute the distance between the query images and the candidate image and use as score for the candidate the minimum distance.</p><p>The experimental protocol is as follows. We select randomly a set of M samples for querying. We repeat this procedure 10 times to reduce the effects of randomness and report the average. Of course, this reduced set of training images is the same for DTW, SC-HMM and C-HMM. We report experiments with M = 1, 5, 10, 25, 50, 100. In the case of DTW, M = 50, 100 are not considered. Indeed the computational cost of DTW increases linearly with the number of query samples and is thus very high even for a few tens of samples. In contrast, this is not a problem for the C-HMM and the SC-HMM since the evaluation time of a model is fixed, independent of the number of query (i.e. training) samples.</p><p>The results of the described experiments are shown in Fig. <ref type="figure" target="#fig_12">13</ref>. Let us analyze this important result:</p><p>(1) For a high amount of training samples (N &gt; 50), the C-HMM gives the best performance. This is compatible with the results obtained in Section 6.3, where we already observed this fact when training with all the available samples. This result is also not surprising because the C-HMM does not have any constraint and should be able to increase the accuracy with increasing number of training samples.</p><p>(2) More interestingly, when the training material is scarce (N &lt; 50) the SC-HMM is superior to the C-HMM, the latter obtaining poor performance. This result can be explained by the fact that the SC-HMM is constrained to the UBM and therefore incorporates prior information that complements the model in lack of training data. In contrast, because the C-HMM does not include this constraint, it overfits to the training data and does not generalize well.</p><p>(3) SC-HMM is consistently superior to DTW. This fact shows the advantage of combining samples into a statistical model as opposed to computing individual distances. Somewhat surprisingly, this observations hold for a single query sample. Although we would expect a model to be incapable of learning relevant information from a single training sample, again the SC-HMM incorporates prior information and is capable of building a competitive model just from the initial GMM and a single training sample. To the best of the author's knowledge, this advantage of the SC-HMM had not been reported before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We presented a robust handwritten word-spotting system for mail documents using a statistical framework. One property of mail document analysis is the complexity of the data due to the large variety of writing styles and because of the spontaneous writing. The presented approach is significantly more accurate than a traditional query-by-example approach, while being fast and requiring a moderate amount of labeled training data. It incorporates some elements new in the handwriting modeling domain, such as the GMM-based score normalization.</p><p>Apart from the robust performance shown by the system, an important finding of this work is that a SC-HMM does not overfit when trained with few samples. Even with a single training sample, it retrieves correct words more efficiently than the traditional DTW-based approach. This is thanks to the a priori information incorporated in the SC-HMM. To the best knowledge of the authors, this advantage had not been reported previously.</p><p>The aforementioned finding opens a set of new perspectives in problems where it is not possible/desirable to have more than one or a couple of training examples, for instance incremental systems: initially, the system is trained with one or few manually collected examples and then it can incorporate more training samples by means of user feedback or active learning techniques.</p><p>A current limitation of the proposed system is that it uses a generative approach for classification, although, in general, discriminative approaches are superior. This is a general problem of the HMM that we will address in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of the detection task of the word résiliation (translated as "cancellation") in handwritten documents.</figDesc><graphic coords="2,50.15,61.03,236.52,284.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Overview of the proposed handwritten word-spotting system.</figDesc><graphic coords="3,43.60,61.62,498.24,282.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Connected components convex hulls and gap distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Shear transform of angle applied to a rectangle, (b) silence removal operation.</figDesc><graphic coords="4,50.15,61.79,236.34,80.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Feature extraction process.</figDesc><graphic coords="5,112.60,61.56,360.00,199.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Angular bins for T = 8 and angle differences of (x, y) to the two closest bins.</figDesc><graphic coords="5,337.10,318.44,180.00,164.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. An example of left-to-right hidden Markov model.</figDesc><graphic coords="6,50.15,61.17,236.52,141.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. A summary of the training and test procedure.</figDesc><graphic coords="7,112.60,61.26,360.00,220.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>where p( W) = 1p(W) and p(X| W) is an anti-model of word W. The posterior then becomes p(W|X) = p(W)p(W|X) p(W)p(X|W) + p( W)p(X| W)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Precision-recall curve for the different feature types for the keyword résiliation using C-HMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Best ranked samples by the model of the keyword résiliation.</figDesc><graphic coords="8,319.14,402.55,236.43,99.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. In this case, the best performance (mAP = 0.82) is again obtained by the LGH features and is below the performance shown by the C-HMM. It is achieved by a model with 1024 Gaussians. Clearly the number of Gaussian components in the visual vocabulary is the main parameter which governs the performance of the SC-HMM. Since such a HMM is described by 1024 weights per state (and 10 states per character), the effective number of parameters is large, but inspection of these values reveals that the weight vectors are very sparse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Mean average precision (mAP) of the DTW, SC-HMM and HMM in their best settings as a function of the number of training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparison of mAP values of the LGH features with those of the Vinciarelli and Marti and Bunke features, as a function of the number of Gaussians in the HMMs.</figDesc><table><row><cell>NG</cell><cell>Marti and Bunke</cell><cell>Vinciarelli</cell><cell>LGH</cell></row><row><cell>1</cell><cell>0.670 (32)</cell><cell>0.687 (8)</cell><cell>0.807 (8)</cell></row><row><cell>2</cell><cell>0.692 (16)</cell><cell>0.745 (16)</cell><cell>0.836 (32)</cell></row><row><cell>4</cell><cell>0.694 (64)</cell><cell>0.783 (32)</cell><cell>0.857 (128)</cell></row><row><cell>8</cell><cell>0.700 (64)</cell><cell>0.803 (64)</cell><cell>0.867 (256)</cell></row><row><cell>16</cell><cell>0.697 (64)</cell><cell>0.810 (128)</cell><cell>0.870 (512)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>mAP values for the Mart and Bunke,Vinciarelli and  LGH features in the SC-HMM case. For illustration, in Fig.11we plot the 25 samples with highest score in the model of the word résiliation. All samples are actually a correct instance of the keyword. Indeed, the first incorrect sample comes at position 196, despite the variability in the writing. For the other words, a similar behavior is obtained. The first negative sample occurs, in average, at position 156 (min is 37 and max 321).In Table2we report the same results for the SC-HMM. The precision-recall curve for the keyword résiliation are shown in</figDesc><table><row><cell>N</cell><cell>Marti and Bunke</cell><cell>Vinciarelli</cell><cell>LGH</cell></row><row><cell>128</cell><cell>0.727</cell><cell>0.741</cell><cell>0.758</cell></row><row><cell>256</cell><cell>0.748</cell><cell>0.767</cell><cell>0.766</cell></row><row><cell>512</cell><cell>0.755</cell><cell>0.780</cell><cell>0.793</cell></row><row><cell>1024</cell><cell>0.759</cell><cell>0.791</cell><cell>0.815</cell></row><row><cell cols="4">and testing is reduced approximately by half and the difference in</cell></row><row><cell cols="4">mAP is less than 0.01. The obtained results show very robust per-</cell></row><row><cell>formance.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Mean average precision (mAP) values of (i) the system without score normalization, (ii) the filler model based normalization, and (iii) the proposed GMM score normalization, using LGH features and C-HMMs.</figDesc><table><row><cell>Score normalization</cell><cell>mAP</cell></row><row><cell>None</cell><cell>0.771</cell></row><row><cell>GMM</cell><cell>0.870</cell></row><row><cell>Filler</cell><cell>0.868</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Mean average precision (mAP) values of (i) the system without score normalization, (ii) the filler model based normalization, and (iii) the proposed GMM score normalization, using SC-HMMs.N is the number of Gaussians in the SC-HMM pool and N is the number of Gaussians in the universal vocabulary used for score normalization. Of course, in the case N = N (indicated in bold) the same pool is used and this reduces to the results shown in Table2.</figDesc><table><row><cell cols="2">Score normalization</cell><cell></cell><cell></cell><cell>mAP</cell></row><row><cell>None</cell><cell></cell><cell></cell><cell></cell><cell>0.074</cell></row><row><cell>Filler</cell><cell></cell><cell></cell><cell></cell><cell>0.816</cell></row><row><cell>GMM</cell><cell></cell><cell></cell><cell></cell><cell>0.815</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">mAP values for SC-HMM using different pools of Gaussians, for the LGH features.</cell></row><row><cell>N/N</cell><cell>128 G</cell><cell>256 G</cell><cell>512 G</cell><cell>1024 G</cell></row><row><cell>128 G</cell><cell>0.758</cell><cell>0.104</cell><cell>0.078</cell><cell>0.070</cell></row><row><cell>256 G</cell><cell>0.203</cell><cell>0.766</cell><cell>0.178</cell><cell>0.120</cell></row><row><cell>512 G</cell><cell>0.243</cell><cell>0.390</cell><cell>0.793</cell><cell>0.338</cell></row><row><cell>1024 G</cell><cell>0.289</cell><cell>0.407</cell><cell>0.549</cell><cell>0.815</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work of José A. Rodríguez is partially supported by the Spanish projects TIN2006-15694-C02-02 and CONSOLIDER-INGENIO 2010 (CSD2007-00018). The authors would like to thank M. Bressan (XRCE), J. Lladós (CVC) and G. Sánchez (CVC) for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word spotting for historical documents</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognition</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On-line and off-line handwriting recognition: a comprehensive survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="63" to="82" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition of cursive Roman handwriting-past, present and future</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 7th International Conference on Document Analysis and Recognition</title>
		<meeting>eeding of the 7th International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="448" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making Latin manuscripts searchable using gHMMs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vesom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Searching off-line Arabic documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ziftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1455" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word spotting: a new approach to indexing handwriting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 1996 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">631</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A line-oriented approach to word spotting in handwritten documents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Augusteijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word image matching using dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2003 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word image retrieval using binary features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Recognition and Retrieval XI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word matching using single closed contours for indexing handwritten historical documents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognition</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="165" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eigenspace method for text retrieval in historical document images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Terasawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Document Analysis and Recognition</title>
		<meeting>the 8th International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="436" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Confidence measures for speech recognition: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="470" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the use of dynamic time warping for word spotting and connected word recognition, Bell System Tech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="303" to="325" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hidden Markov model based keyword recognition system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speaker dependent keyword spotting for accessing stored speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno>CUED/F-INFENG/TR 193</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Keyword spotting in poorly printed documents using pseudo 2-D hidden Markov models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>Agazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="842" to="848" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Print keyword spotting with dynamically synthesized pseudo 2d HMMs</title>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="999" to="1011" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word spotting in scanned images using hidden Markov models</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bloomberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Content-based retrieval of historical ottoman documents stored as textual images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Saykol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gudukbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno>MM-42</idno>
		<title level="m">A statistical approach to retrieving historical manuscript images</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">CIIR Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A search engine for handwritten documents</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Recognition and Retrieval XIII</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Template-free word spotting in low-quality manuscripts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Advances in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A statistical approach for phrase location and recognition within a text line: an application to street name recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>El-Yacoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gilloux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bertille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="21" to="26" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Binary vector dissimilarity measures for handwriting identification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Recognition and Retrieval X</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="28" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Omnilingual segmentation-free word spotting for ancient manuscripts indexation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Leydier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Emptoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Document Analysis and Recognition</title>
		<meeting>the 8th International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="533" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using corner feature correspondences to rank word images by similarity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rothfeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Document Image Analysis and Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text line segmentation of historical documents: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Likforman-Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zahour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taconet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognition</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="138" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gap metrics for word separation in handwritten lines</title>
		<author>
			<persName><forename type="first">U</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Nagabushnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Document Analysis and Recognition</title>
		<meeting>the Third International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">124</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word segmentation in handwritten Korean text lines based on gap clustering techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Document Analysis and Recognition</title>
		<meeting>the Sixth International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">189</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transcript mapping for historic handwritten document images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Tomai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">413</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse multinomial logistic regression: fast algorithms and generalization bounds</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="957" to="968" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">)</forename><surname>Figueiredo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new normalization technique for cursive handwritten words</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1043" to="1050" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local gradient histogram features for word spotting in unconstrained handwritten documents</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using a statistical language model to improve the performance of an HMM-based cursive handwriting recognition system</title>
		<author>
			<persName><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognition Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="65" to="90" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Offline recognition of unconstrained handwritten texts using HMMs and statistical language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="709" to="720" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Offline grammar-based recognition of handwritten sentences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chappelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="818" to="821" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Off-line handwritten word recognition using a hidden Markov model type stochastic network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="481" to="496" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Off-line cursive handwriting recognition using hidden Markov models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Schukat-Talamazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1399" to="1413" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Handwritten word recognition using segmentationfree hidden Markov modeling and segmentation-based dynamic programming techniques</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="548" to="554" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An off-line cursive handwriting recognition system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="321" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hidden Markov model based word recognition and its application to legal amount reading on French checks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Knerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Baret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="404" to="419" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An HMM-based approach for off-line unconstrained handwritten word modeling and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Yacoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gilloux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="752" to="760" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Handwritten Address Recognition Using Hidden Markov Models, Reading and Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brakensiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="103" to="122" />
			<pubPlace>Berlin; Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
		<title level="m">Markov Models for Pattern Recognition, From Theory to Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Optimizing the number of states, training iterations and Gaussians in an HMM-based handwritten word recognizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ünter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh International Conference on Document Analysis and Recognition</title>
		<meeting>the seventh International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">472</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semi-continuous hidden Markov models for speech signals</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Speech Recognition</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="340" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tied mixture continuous parameter modeling for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2033" to="2045" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted Gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Off-line writer verification: a comparison of a hidden Markov model (HMM) and a Gaussian mixture model (GMM) based system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schlapbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Confidence measures for an address reading system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brakensiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rottland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Document Analysis and Recognition</title>
		<meeting>the 7th International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">294</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The use of cohort normalized scores for speaker verification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing</title>
		<meeting>the International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="599" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Score normalization for HMM-based handwritten word spotting using a universal background model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">UK. His research interests include handwriting recognition, document image analysis, video processing and statistical pattern recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><surname>Rodríguez-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physics in 2003 from the Universitat de Barcelona (UB), Spain, and his Master in Computer Vision in 2006 from the Universitat Autònoma de Barcelona (UAB), Spain. He is about to complete his Ph.D. thesis at the Computer Vision Center, UAB</title>
		<meeting><address><addrLine>Paris, France; Santa Barbara, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Loughborough University</orgName>
		</respStmt>
	</monogr>
	<note>Switzerland. From 2000 to 2001, he was with the Panasonic Speech Technology Laboratory (PSTL). he is a research scientist at the Xerox Research Centre Europe, Meylan, France. His main interests are in the applications of computer vision and pattern recognition to the problems of natural and document image processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
