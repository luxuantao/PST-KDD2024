<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learning for Treatment Effect Estimation from Observational Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
							<email>liuyiyao@buffalo.edu</email>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
							<email>sheng.li@uga.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<email>yaliangli@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Mengdi</forename><surname>Huai</surname></persName>
							<email>mengdihu@buffalo.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
							<email>azhang@buffalo.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Georgia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tencent Medical AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Learning for Treatment Effect Estimation from Observational Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9FD0CB8D8D9A2022F1742A632CBFE2D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to the missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that provides meaningful constraints on the ITE estimation. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating the causal effect of an intervention/treatment at individual-level is an important problem that can benefit many domains including health care <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>, digital marketing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24]</ref>, and machine learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20]</ref>. For example, in the medical area, many pharmaceuticals companies have developed various anti-hypertensive medicines and they all claim to be effective for high blood pressure. However, for a specific patient, which one is more effective? Treatment effect estimation methods are necessary to answer the above question, and it leads to better decision making. Treatment effect could be estimated at either the group-level or individual-level. In this paper, we focus on the individual treatment effect (ITE) estimation.</p><p>Two types of studies are usually conducted for estimating the treatment effect, including the randomized controlled trials (RCTs) and observational study. In RCTs, the treatment assignment is controlled, and thus the distributions of treatment and control groups are known, which is a desired property for treatment effect estimation. However, conducting RCTs is expensive and timeconsuming, sometimes it even faces some ethical issues. Unlike RCTs, observational study directly estimates treatment effects from the observed data, without any control on the treatment assignment. Owing to the easy access of observed data, observational studies, such as the potential outcome framework <ref type="bibr" target="#b26">[27]</ref> and causal graphical models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>, have been widely applied in various domains <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Estimating individual treatment effect from observational data faces two major challenges, missing counterfactuals and treatment selection bias. ITE is defined as the expected difference between the treated outcome and control outcome. However, a unit can only belong to one group, and thus the outcome of the other treatment (i.e., counterfactual) is always missing. Estimating counterfactual outcomes from observed data is a reasonable way to address this issue. However, selection bias makes it more difficult to infer the counterfactuals in practice. For instance, in the uncontrolled cases, people have different preferences to the treatment, and thus there could be considerable distribution discrepancy across different groups. The distribution discrepancy further leads to an inaccurate estimation of counterfactuals.</p><p>To overcome the above challenges, some traditional ITE estimation methods use the treatment assignment as a feature, and train regression models to estimate the counterfactual outcomes <ref type="bibr" target="#b10">[11]</ref>. Several nearest neighbor based methods are also adopted to find the nearby training samples, such as k-NN <ref type="bibr" target="#b7">[8]</ref>, propensity score matching <ref type="bibr" target="#b26">[27]</ref>, and nearest neighbor matching through HSIC criteria <ref type="bibr" target="#b4">[5]</ref>. Besides, some tree and forest based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> view the tree and forests as an adaptive neighborhood metric, and estimate the treatment effect at the leaf node. Recently, representation learning approaches have been proposed for counterfactual inference, which try to minimize the distribution difference between treated and control groups in the embedding space <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>State-of-the-art ITE estimation methods aim to balance the distributions in a global view, however, they ignore the local similarity information. As similar units shall have similar outcomes, it is of great importance to preserve the local similarity information among units during representation learning, which decreases the generalization error in counterfactual estimation. This point has also been confirmed by nearest neighbor based methods. Unfortunately, in recent representation learning based approaches, the local similarity information may not be preserved during distribution balancing. On the other hand, nearest neighbor based methods only consider the local similarity, but cannot balance the distribution globally. Our proposed method combines the advantages of both of them.</p><p>In this paper, we propose a novel local similarity preserved individual treatment effect estimation method (SITE) based on deep representation learning. SITE maps mini-batches of units from the covariate space to a latent space using a representation network. In the latent space, SITE preserves the local similarity information using the Position-Dependent Deep Metric (PDDM), and balances the data distributions with a Middle-point Distance Minimization (MPDM) strategy. PDDM and MPDM can be viewed as a regularization, which helps learn a better representation and decrease the generalization error in estimating the potential outcomes. Implementing PDDM and MPDM only involves triplet pairs and quartic pairs of units respectively from each mini-batch, which makes SITE efficient for large-scale data. The proposed method is validated on both synthetic and realworld datasets, and the experimental results demonstrate its advantages brought by preserving the local similarity information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary</head><p>Individual treatment effect (ITE) estimation aims to examine whether a treatment T affects the outcome Y (i) of a specific unit i. Let x i âˆˆ R d denote the pre-treatment covariates of unit i, where d is the number of covariates. T i denotes the treatment on unit i. In the binary treatment case, unit i will be assigned to the control group if T i = 0, or to the treated group if T i = 1.</p><p>We follow the potential outcome framework proposed by Neyman and Rubin <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>. If the treatment T i has not been applied to unit i (also known as the out-of-sample case <ref type="bibr" target="#b29">[30]</ref>),</p><formula xml:id="formula_0">Y (i) 0 is called the potential outcome of treatment T i = 0 and Y (i) 1</formula><p>the potential outcome of treatment T i = 1. On the other hand, if the unit i has already received a treatment T i (i.e., the within-sample case <ref type="bibr" target="#b29">[30]</ref>), Y Ti is the factual outcome, and Y 1-Ti is the counterfactual outcome. In observational study, only the factual outcomes are available, while the counterfactual outcomes can never been observed.</p><p>The individual treatment effect on unit i is defined as the difference between the potential treated and control outcomes <ref type="foot" target="#foot_1">1</ref> :</p><formula xml:id="formula_1">ITE i = Y (i) 1 -Y (i) 0 .<label>(1)</label></formula><p>The challenge to estimate ITE i lies on how to estimate the missing counterfactual outcome. Existing counterfactual estimation methods usually make the following important assumptions <ref type="bibr" target="#b16">[17]</ref>. Assumption 2.1 (SUTVA). The potential outcomes for any unit do not vary with the treatment assigned to other units, and, for each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes <ref type="bibr" target="#b16">[17]</ref>.</p><p>Assumption 2.2 (Consistency). The potential outcome of treatment t equals to the observed outcome if the actual treatment received is t.</p><p>Assumption 2.3 (Ignorability). Given pretreatment covariates X, the outcome variables Y 0 and Y 1 is independent of treatment assignment, i.e., (Y 0 , Y 1 ) âŠ¥ âŠ¥ T |X.</p><p>Ignorability assumption makes the ITE estimation identifiable. Though it's hard to prove the satisfaction of the assumption, the researchers can make the assumption more plausible if the pretreatment covariates include the variables that affect both the treatment assignment and the outcome as much as possible. This assumption is also called "no unmeasured confounder".</p><p>Assumption 2.4 (Positivity). For any set of covariates x, the probability to receive treatment 0 or 1 is positive, i.e., 0 &lt; P (T = t|X = x) &lt; 1, âˆ€t and x.</p><p>This assumption is also named as population overlapping <ref type="bibr" target="#b8">[9]</ref>. If for some values of X, the treatment assignment is deterministic (i.e., P (T = t|X = x) = 0 or 1), we would lack the observations of one treatment group, such that the counterfactual outcome is unlikely to be estimated. Therefore, positivity assumption guarantees that the ITE can be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation</head><p>Balancing distributions of control group and treated group has been recognized as an effective strategy for counterfactual estimation. Recent works have applied distribution balancing constraints to either the covariate space <ref type="bibr" target="#b15">[16]</ref> or latent space <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Moreover, we assume that similar units would have similar outcomes. This assumption has been well justified in many classical counterfactual estimation methods such as the nearest neighbor matching.</p><p>To satisfy this assumption in the representation learning setting, the local similarity information should be well preserved after mapping units from the covariate space X to the latent space Z.</p><p>One straightforward solution is to add a constraint on similarity matrices constructed in X and Z. However, constructing similarity matrices and enforcing such a "global" constraint is very time and space consuming, especially for a large amount of units in practice. Motivated by the hard sample mining approach in the image classification area <ref type="bibr" target="#b13">[14]</ref>, we design an efficient local similarity preserving strategy based on triplet pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Proposed Method</head><p>We propose a local similarity preserved individual treatment effect estimation (SITE) method based on deep representation learning. The key idea of SITE is to map the original pre-treatment covariate space X into a latent space Z learned by deep neural networks. Particularly, SITE attempts to enforce two special properties on the latent space Z, including the balanced distribution and preserved similarity. The loss function of SITE is as follows:</p><formula xml:id="formula_2">L =L FL + Î²L PDDM + Î³L MPDM + Î»||W || 2 ,<label>(2)</label></formula><p>where L FL is the factual loss between the estimated and observed factual outcomes. L PDDM and L MPDM are the loss functions for PDDM and MPDM, respectively. The last term is L 2 regularization on model parameters W (except the bias term).</p><p>Next, we describe each component of SITE in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Representation Network</head><p>Inspired by <ref type="bibr" target="#b17">[18]</ref>, a standard feed-forward network with d h hidden layers and the rectified linear unit (ReLU) activation function is built to learn latent representations from the pre-treatment covariates.</p><p>For the unit i, we have z i = f (x i ), where f (â€¢) denotes the representation function learned by the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Triplet Pairs Selection</head><p>Given a mini-batch of input units, SITE selects six units according to the propensity scores. Propensity score is the probability that a unit receives the treatment <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22]</ref>. For unit i, the propensity score s i is defined as</p><formula xml:id="formula_3">s i = P t i = 1|X = x i . It's obvious that s i âˆˆ [0, 1]. If s i is close to 1,</formula><p>more treated units should be distributed around the unit i in the covariate space. Analogously, if s i is close to 0, more control units are available near the unit i. Moreover, if s i is close to 0.5, a mixture of both control and treated units can be found around the unit i. Thus, propensity score can kind of reflect the relative location of units in the covariate space, and we choose it as the indicator to select six data points. We use the logistic regression to calculate the propensity score <ref type="bibr" target="#b26">[27]</ref>.</p><p>Selecting three pairs of units in each mini-batch involves three steps, as shown in the left part of Figure <ref type="figure" target="#fig_1">2</ref>.</p><formula xml:id="formula_4">â€¢ Step 1: Choose data pair (x Ã®, x Äµ ) s.t. ( Ã®, Äµ) = argmin iâˆˆT ,jâˆˆC |s i -0.5| + |s j -0.5|,<label>(3)</label></formula><p>where T and C denote the treated group and control group, respectively. x Ã® and x Äµ are the closest units in the intermediate region where both control and treated units are mixed.</p><formula xml:id="formula_5">â€¢ Step 2: Choose (x k, x l) s.t. k = argmax kâˆˆC |s k -s Ã®|, l = argmax l |s l -s k|. (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>x k is the farthest control unit from x Ã®, and is on the margin of control group with plenty of control units.</p><formula xml:id="formula_7">â€¢ Step 3: Choose (x m, x n) s.t. m = argmax mâˆˆT |s m -s Äµ |, n = argmax n |s n -s m|. (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>x k is the farthest control unit from x Ã®, and is on the margin of control group with plenty of control units.</p><p>The pair ( Ã®, Äµ) lies in the intermediate region of control and treated groups. Pairs ( k, l) and ( m, n) are located on the margins that are far away from the intermediate region. The selected triplet pairs can be viewed as hard cases. Intuitively, if the desired property of preserved similarity can be achieved for the hard cases, it will hold for other cases as well. Thus, we focus on preserving such a property for the hard cases (e.g., triplet pairs) in the latent space, and employ PDDM to achieve this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Position-Dependent Deep Metric (PDDM)</head><p>PDDM was originally proposed to address the hard sample mining problem in image classification <ref type="bibr" target="#b13">[14]</ref>.</p><p>We adapt this design to the counterfactual estimation problem.</p><p>In SITE, the PDDM component measures the local similarity of two units based on their relative and absolute positions in the latent space Z. The PDDM learns a metric that makes the local similarity of (z i , z j ) in the latent space close to their similarity in the original space. The similarity Åœ(i, j) is defined as:</p><formula xml:id="formula_9">Åœ(i, j) = W s h + b s ,<label>(6)</label></formula><p>where As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the PDDM structure first calculates the feature mean vector v and the absolute position vector u of the input (z i , z j ), and then feeds v and u to the fully connected layers separately. After normalization, PDDM concatenates the learned vectors u 1 and v 1 , and feeds it to another fully connected layer to get the vector h. The final similarity score Åœ(, ) is calculated by mapping the score h to the R 1 space.</p><formula xml:id="formula_10">h = Ïƒ(W c [ u1 ||u1||2 , v1 ||v1||2 ] T + b c ), u = |z i -z j |, v = |zi+zj | 2 , u 1 = Ïƒ(W u u ||u||2 + b u ), v 1 = Ïƒ(W v v ||v||2 + b v ). W c , W s , W v , W u ,</formula><p>The loss function of PDDM is as follows:</p><formula xml:id="formula_11">L PDDM = 1 5 Ã®, Äµ, k, l, m,n ( Åœ( k, l) -S( k, l)) 2 + ( Åœ( m, n) -S( m, n)) 2 + ( Åœ( k, m) -S( k, m)) 2 +( Åœ( Ã®, m) -S( Ã®, m)) 2 + ( Åœ( Äµ, k) -S( Äµ, l)) 2 , (<label>7</label></formula><formula xml:id="formula_12">) where S(i, j) = 0.75| si+sj 2 -0.5| -| si-sj 2 | + 0.5.</formula><p>Similar to the design of the PDDM structure, the true similarity score S(i, j) is calculated using the mean and the difference of two propensity scores. The loss function L PDDM measures the similarity loss on five pairs in each mini batch: the pairs located in the margin area of the mini batch, i.e., (z k , z l ) and (z m , z n ); the pair that is most dissimilar among the selected points, i.e., (z k , z m ); the pairs located in the margin of the control/treated group, i.e., (z j , z k ) and (z i , z m ). As shown in Figure <ref type="figure" target="#fig_1">2</ref>, minimizing L PDDM on the above five pairs helps to preserve the similarity when mapping the original data into the representation space.</p><p>By using the PDDM structure, the similarity information within and between each of the pairs (z k, z l), (z m, z n), and (z k, z n) will be preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Middle Point Distance Minimization (MPDM)</head><p>To achieve balanced distributions in the latent space, we design the middle point distance minimization (MPDM) component in SITE. MPDM makes the middle point of (z Ã®, z m) close to the middle point of (z Äµ , z k). The units z Ã® and z Äµ are located in a region where the control and treated units are sufficient and mixed. In other words, they are the closest units from treated and control groups separately that lie in the intermediate zone. Meanwhile, z k is the farthest control unit from the margin of the treated group, and z m is the farthest treated unit from the margin of control group. We use the middle points of (z Ã®, z m) and (z Äµ , z k) to approximate the centers of treated and control groups, respectively. By minimizing the distance of two middle points, the units in the margin area are gradually made close to the intermediate region. As a result, the distributions of two groups will be balanced.</p><p>The loss function of MPDM is as follows:</p><formula xml:id="formula_13">L MPDM = Ã®, Äµ, k, m z Ã®+z m 2 - z Äµ +z k 2 2 .<label>(8)</label></formula><p>The MPDM balances the distributions of two groups in the latent space, while the PDDM preserves the local similarity. A 2-D toy example shown in Figure <ref type="figure" target="#fig_4">4</ref> vividly demonstrates the combined effect of MPDM and PDDM. Four units x Ã®, x Äµ , x k and x m are the same as what we choose in Figure <ref type="figure" target="#fig_1">2</ref>. Figure <ref type="figure" target="#fig_4">4</ref> shows that MPDM makes the units that belong to treated group close to the control group, and PDDM restricts the way that the two groups close to each other. PDDM preserves the similarity information between x k and x m.</p><p>x k and x m are the farthest data points in the treated and control groups. When MPDM makes two groups approaching each other, PDDM ensures that the data points x k and x m are still the farthest, which prevents MPDM squeezing all data points into one point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Outcome Prediction Network</head><p>With the components PDDM and MPDM, SITE is able to learn latent representations z i that balance the distributions of treated/control groups and preserve the local similarity of units in the original covariate space. Finally, the outcome prediction network is employed to estimate the outcome Å·(i)</p><p>ti by taking z i as input. Let g(â€¢) denote the function learned by the outcome prediction network. We have</p><formula xml:id="formula_14">Å·(i) ti = g(z i , t i ) = g(f (x i ), t i ).</formula><p>The factual loss function is as follows:</p><formula xml:id="formula_15">L FL = N i=1 (Å· (i) ti -y (i) ti ) 2 = N i=1 (g(f (x i ), t i ) -y (i) ti ) 2 , (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>where y</p><formula xml:id="formula_17">(i)</formula><p>ti is the observed outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Implementation and Joint Optimization</head><p>The representation network and outcome prediction network are standard feed-forward neural networks with Dropout <ref type="bibr" target="#b31">[32]</ref> and ReLU activation function. The overall loss function of SITE in Eq.( <ref type="formula" target="#formula_2">2</ref>) can be jointly optimized. Adam <ref type="bibr" target="#b18">[19]</ref> is adopted to solve the optimization problem. The PDDM and MPDM are calculated on triplet pairs during every batch. Datasets. Due to the missing counterfactual outcomes in reality, it is hard to measure the individual treatment effect estimation on traditional observational datasets. In order to evaluate the proposed method, we conduct the experiment on three datasets with different settings. IHDP and Jobs dataset are adopted in <ref type="bibr" target="#b29">[30]</ref>, one of the state-of-art methods. IHDP dataset aims to estimate the effect of specialist home visits on infant's future cognitive test scores, and Jobs dataset aims to estimate the effect of job training on employee status. Details about the IHDP and Jobs datasets are provided in the supplementary material. The twins dataset comes from the all twins birth in the USA between 1989 -1991 <ref type="bibr" target="#b1">[2]</ref>. We focus on the same sex twin-pairs whose weights are less than 2000g. Each record contains 40 pre-treatment covariates related to the parents, the pregnancy and the birth. The treatment T = 1 is viewed as being the heavier one in the twins, and T = 0 is being the lighter one.</p><p>The outcome is the mortality after one year. After eliminating the records containing missing features, the final dataset contains 5409 records. In this setting, both treated and control outcomes can be observed. In order to create the selection bias, we execute the following procedures to selectively choose one of the twins as the observation and hide the other:</p><formula xml:id="formula_18">T i |x i âˆ¼ Bern(Sigmoid(w T x + n))</formula><p>, where w T âˆ¼ U((-0.1, 0.1) 40Ã—1 ) and n âˆ¼ N (0, 0.1).</p><p>Baselines. We compare the proposed method with the following three groups of baselines. (1) Regression based methods: Least square Regression with the treatment as feature (OLS/LR 1 ), separate linear regressors for each treatment group (OLS/LR 2 ); (2) Nearest neighbor matching based methods: Hilbert-Schmidt Independence Criterion based Nearest Neighbor Matching (HSIC-NNM) <ref type="bibr" target="#b4">[5]</ref>, Propensity score match with logistic regression (PSM) <ref type="bibr" target="#b26">[27]</ref>, k-nearest neighbor (k-NN) <ref type="bibr" target="#b7">[8]</ref>; (3) Tree and forest based method: Causal Forest <ref type="bibr" target="#b32">[33]</ref>. (4) Representation learning based methods: Balancing neural network (BNN) <ref type="bibr" target="#b17">[18]</ref>, counterfactual regression with MMD metric (CFR-MMD) <ref type="bibr" target="#b29">[30]</ref>, counterfactual regression with Wasserstein metric (CFR-WASS) <ref type="bibr" target="#b29">[30]</ref>, and Treatment-Agnostic Representation Network (TARNet) <ref type="bibr" target="#b29">[30]</ref>.</p><p>Performance Measurement.</p><p>On IHDP dataset, the Precision in Estimation of Heterogeneous Effect (E PEHE ) <ref type="bibr" target="#b12">[13]</ref> is adopted as the performance metric, where</p><formula xml:id="formula_19">E PEHE = 1 N N i=1 (E (y (i) 0 ,y (i) 1 )âˆ¼P Y|x i y (i) 0 -y (i) 1 -(Å· (i) 0 - Å·(i)<label>1</label></formula><p>)) 2 ; On jobs dataset, the policy risk R pol <ref type="bibr" target="#b29">[30]</ref> is used as the metric, which is defined as:</p><formula xml:id="formula_20">R pol = 1-E[Y 1 |Ï€(x) = 1]P(Ï€(x) = 1)+E[Y 0 |Ï€(x) = 0]P(Ï€(x) = 0)</formula><p>, where Ï€(x) = 1 if Å·1 -Å·0 &gt; 0 and Ï€(x) = 0, otherwise. The policy risk measures the expected loss if the treatment is taken according to the ITE estimation. For PEHE and policy risk, the smaller value is, the better the performance. On the Twins dataset, the class is imbalanced, so we adopt area over ROC curve(AUC) on outcomes as the performance measure, as suggested in <ref type="bibr" target="#b24">[25]</ref>. The larger AUC is, the better the performance.</p><p>On each dataset, we consider both the within-sample case and out-of-sample case <ref type="bibr" target="#b29">[30]</ref>. In the former case, the observed outcome is available, while in the latter case, only the pre-treatment covariates are available. In the within-sample case, the performance metric is measured on the training dataset, and the out-of-sample case is on the test dataset. Since we never use the ground truth ITE during the training procedure, performance metric is a meaningful metric in both the within-sample and out-of-sample cases.</p><p>Results Analysis<ref type="foot" target="#foot_2">2</ref> . Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> show the performance of 10 realizations of our method and baselines on three datasets. SITE achieves the best performance on the IHDP and Twins datasets, and on the Jobs dataset, SITE achieves similar results to the best baseline. It confirms that preserving the local similarity information during representation learning can help better estimate the counterfactual outcomes and ITE.</p><p>Generally speaking, the representation learning based methods perform better than the linear regression based and nearest neighbor matching based methods. The regression-based methods are not specially designed to deal with counterfactual inference, so the performance is affected by the selection bias. The nearest neighbor based methods incorporate the similarity information  <ref type="bibr" target="#b26">[27]</ref> 0.500 Â± 0.003 0.506 Â± 0.011 k-NN <ref type="bibr" target="#b7">[8]</ref> 0.609 Â± 0.010 0.492 Â± 0.012 BNN <ref type="bibr" target="#b17">[18]</ref> 0.690 Â± 0.008 0.676 Â± 0.008 TARNet <ref type="bibr" target="#b29">[30]</ref> 0.849 Â± 0.002 0.840 Â± 0.006 CFR-MMD <ref type="bibr" target="#b29">[30]</ref> 0.852 Â± 0.001 0.840 Â± 0.006 CFR-WASS <ref type="bibr" target="#b29">[30]</ref>  Compared with CFR-MMD and CFR-WASS, our proposed method SITE not only considers the balancing property (MPDM), but also preserves the local similarity information in the original feature space (PDDM). It is observed that on the IHDP dataset, SITE significantly improves the results in both within-sample case and out-of-sample case. On Jobs and Twins datasets, the performance of SITE are comparable with the best baseline. The results on three datasets demonstrate the effectiveness of preserving local similarity information in the latent space. Moreover, with the specifically designed PDDM and MPDM structures, SITE can efficiently calculate the similarity information and balance the distributions of different treatment groups. The PDDM and MPDM structures only require the selected triplet pairs, which avoids handling the entire dataset. By jointly considering distribution balancing and similarity preserving, the proposed method can effectively and efficiently estimate the individual treatment effect.</p><p>Experiments on PDDM and MPDM. PDDM (for local similarity preserving) and MPDM (for balancing) aim to reduce the generalization error when inferring the potential outcomes. As SITE assumes that similar units shall have similar treatment outcomes, PDDM and MPDM are able to preserve the local similarity information and meanwhile achieve the balanced distributions in the latent space. In order to further comfirm the effect of PDDM and MPDM, we compare SITE with SITE-without-PDDM and SITE-without-MPDM on all the three datasets.   Data Generation. To evaluate the robustness of SITE, we design experiments on a synthetic dataset. Following the settings in <ref type="bibr" target="#b35">[36]</ref>, the synthetic data are generated as follows: we generate 5000 control samples from N (0 10Ã—1 , 0.5 Ã— (Î£ + Î£ T )) and 2500 treated samples from N (Âµ 1 , 0.5 Ã— (Î£ + Î£ T )), where Î£ âˆ¼ U ((-1, 1) 10Ã—10 ). By varying the value of Âµ 1 , data with different levels of selection bias are generated. Kullback-Leibler divergence (KL divergence) is adopted to measure the selection bias. The larger the KL divergence is, the smaller the overlapping of simulated control and treated groups is, and the larger the selection bias is. The outcome is generated as y|x âˆ¼ (w T x + n), where w âˆ¼ U ((-1, 1) 10Ã—2 ), and n âˆ¼ N (0 2Ã—1 , 0.1 Ã— I 2Ã—2 ).</p><p>Result Analysis. We compare the proposed method with the most competitive baselines, TARNet, CFR-MMD and CFR-WASS. The mean and variance of the E P EHE on 10 realizations are reported in Figure <ref type="figure" target="#fig_5">5</ref>. It is observed from the figure that SITE consistently outperforms baseline methods under different levels of divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present an efficient deep representation learning method for estimating individual treatment effect. The proposed method jointly preserves the local similarity information and balances the distributions of control and treated groups. Experimental results on the IHDP, Jobs and Twins datasets show that, in most cases, our method achieves better performance than the stateof-the-art. Extensive evaluation of our method further validates the benefits of preserving local similarity in ITE estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgment</head><p>This work was supported in part by the US National Science Foundation under grants NSF IIS-1747614, IIS-1218393 and IIS-1514204. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Also, we gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Framework of similarity preserved individual treatment effect estimation (SITE).</figDesc><graphic coords="3,165.71,71.69,247.21,146.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Triple pairs selection for PDDM in a mini-batch.The framework of SITE is shown in Figure1, which contains five major components: representation network, triplet pairs selection, position-dependent deep metric (PDDM), middle point distance minimization (MPDM), and the outcome prediction network. To improve the model efficiency, SITE takes input units in a mini-batch fashion, and triplet pairs could be selected from every minibatch. The representation network learns latent embeddings for the input units. With the selected triplet pairs, PDDM and MPDM are able to preserve the local similarity information and meanwhile achieve the balanced distributions in the latent space. Finally, the embeddings of mini-batch are fed forward to a dichotomous outcome prediction network to get the potential outcomes.</figDesc><graphic coords="4,145.92,71.61,272.35,126.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PDDM Structure.</figDesc><graphic coords="5,309.92,314.58,158.85,62.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>b c , b s , b v and b u are the model parameters. Ïƒ(â€¢) is a nonlinear function such as ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The effect of balancing distributions and preserving local similarity by using the proposed SITE method.</figDesc><graphic coords="6,156.03,71.95,297.28,129.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance Comparison on Synthetic Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on IHDP and Jobs Dataset.</figDesc><table><row><cell></cell><cell cols="2">IHDP ( E PEHE )</cell><cell cols="2">Jobs (R pol )</cell></row><row><cell>Method</cell><cell>Within-sample</cell><cell>Out-of-sample</cell><cell>Within-sample</cell><cell>Out-of-sample</cell></row><row><cell>OLS/LR1</cell><cell>10.761 Â± 4.350</cell><cell>7.345 Â± 2.914</cell><cell>0.310 Â± 0.017</cell><cell>0.279 Â± 0.067</cell></row><row><cell>OLS/LR2</cell><cell>10.280 Â± 3.794</cell><cell>5.245 Â± 0.986</cell><cell>0.228 Â± 0.012</cell><cell>0.733 Â± 0.103</cell></row><row><cell>HSIC-NNM [5]</cell><cell>2.439 Â± 0.445</cell><cell>2.401 Â± 0.367</cell><cell>0.291 Â± 0.019</cell><cell>0.311 Â± 0.069</cell></row><row><cell>PSM [27]</cell><cell>7.188 Â± 2.679</cell><cell>7.290 Â± 3.389</cell><cell>0.292 Â± 0.019</cell><cell>0.307 Â± 0.053</cell></row><row><cell>k-NN [8]</cell><cell>4.432 Â± 2.345</cell><cell>4.303 Â± 2.077</cell><cell>0.230 Â± 0.016</cell><cell>0.262 Â± 0.038</cell></row><row><cell>Causal Forest [33]</cell><cell>4.732 Â± 2.974</cell><cell>4.095 Â± 2.528</cell><cell>0.232 Â± 0.018</cell><cell>0.224 Â± 0.034</cell></row><row><cell>BNN [18]</cell><cell>3.827 Â± 2.044</cell><cell>4.874 Â± 2.850</cell><cell>0.232 Â± 0.008</cell><cell>0.240 Â± 0.012</cell></row><row><cell>TARNet [30]</cell><cell>0.729 Â± 0.088</cell><cell>1.342 Â± 0.597</cell><cell>0.228 Â± 0.004</cell><cell>0.234 Â± 0.012</cell></row><row><cell>CFR-MMD [30]</cell><cell>0.663 Â± 0.068</cell><cell>1.202 Â± 0.550</cell><cell>0.213 Â± 0.006</cell><cell>0.231 Â± 0.009</cell></row><row><cell>CFR-WASS [30]</cell><cell>0.649 Â± 0.089</cell><cell>1.152 Â± 0.527</cell><cell>0.225 Â± 0.004</cell><cell>0.225 Â± 0.010</cell></row><row><cell>SITE (Ours)</cell><cell cols="2">0.604 Â± 0.093 0.656 Â± 0.108</cell><cell>0.224 Â± 0.004</cell><cell>0.219 Â± 0.009</cell></row><row><cell>3 Experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3.1 Experiment on Real Dataset</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on twins dataset.</figDesc><table><row><cell></cell><cell cols="2">Twins (AUC)</cell></row><row><cell>Method</cell><cell>Within-sample</cell><cell>Out-of-sample</cell></row><row><cell>OLS/LR 1</cell><cell>0.660 Â± 0.005</cell><cell>0.500 Â± 0.028</cell></row><row><cell>OLS/LR 2</cell><cell>0.660 Â± 0.004</cell><cell>0.500 Â± 0.016</cell></row><row><cell>HSIC-NNM [5]</cell><cell>0.762 Â± 0.011</cell><cell>0.501 Â± 0.017</cell></row><row><cell>PSM</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>WASS have the same dichotomous outcome prediction networks, but they use different integral probability metrics to balance the distributions. The results of BNN, CFR MMD, CFR WASS, and the proposed method SITE indicate that balancing the distributions of different treatment groups indeed helps reduce the negative effect of selection bias.</figDesc><table><row><cell>to overcome the selection bias, but they</cell><cell></cell><cell></cell></row><row><cell>only use the observed outcomes of neigh-</cell><cell></cell><cell></cell></row><row><cell>bors in the other group as their counter-</cell><cell></cell><cell></cell></row><row><cell>factual outcomes, which might be inaccu-</cell><cell></cell><cell></cell></row><row><cell>rate and unreliable.</cell><cell></cell><cell></cell></row><row><cell>Among the representation learning based</cell><cell></cell><cell></cell></row><row><cell>methods, our proposed method outper-</cell><cell></cell><cell></cell></row><row><cell>forms all other baselines. The meth-</cell><cell></cell><cell></cell></row><row><cell>ods considering balancing distributions</cell><cell></cell><cell></cell></row><row><cell>(BNN, CFR MMD, CFR WASS, and the</cell><cell></cell><cell></cell></row><row><cell>proposed method) obtain better perfor-</cell><cell></cell><cell></cell></row><row><cell>mance than the method without balanc-</cell><cell></cell><cell></cell></row><row><cell>ing property (TARNet). BNN balances</cell><cell></cell><cell></cell></row><row><cell>the distributions of two treatment groups</cell><cell></cell><cell></cell></row><row><cell>in the representation space and views the treatment t i as a feature. While TARNet</cell><cell></cell><cell>0.850 Â± 0.002</cell><cell>0.842 Â± 0.005</cell></row><row><cell>doesn't have any regularization in the rep-</cell><cell>SITE (Ours)</cell><cell cols="2">0.862 Â± 0.002 0.853 Â± 0.006</cell></row><row><cell>resentation space, and its outcome predic-</cell><cell></cell><cell></cell></row><row><cell>tion network is dichotomous. CFR-MMD</cell><cell></cell><cell></cell></row><row><cell>and CFR-</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 3 shows the re-sults. It can be observed that SITE outperforms the baselines without PDDM or MPDM structures. Therefore, the two structures, PDDM and MPDM, are necessary to improve the ITE estimation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experiment on PDDM &amp; MPDM: Performance Comparison on Three Datasets.</figDesc><table><row><cell>Dataset</cell><cell>SITE</cell><cell cols="4">SITE-without-PDDM SITE-without-MPDM</cell></row><row><cell>IHDP (E PEHE )</cell><cell>Within-sample 0.604 Â± 0.093 Out-of-sample 0.656 Â± 0.108</cell><cell cols="2">0.635 Â± 0.127 0.685 Â± 0.128</cell><cell></cell><cell>0.859 Â± 0.093 1.416 Â± 0.476</cell></row><row><cell>Jobs (R pol )</cell><cell>Within-sample 0.224 Â± 0.004 Out-of-sample 0.219 Â± 0.009</cell><cell cols="2">0.233 Â± 0.004 0.234 Â± 0.012</cell><cell></cell><cell>0.222 Â± 0.003 0.234 Â± 0.009</cell></row><row><cell>Twins (AUC)</cell><cell>Within-sample 0.862 Â± 0.002</cell><cell cols="2">0.770 Â± 0.033</cell><cell></cell><cell>0.796 Â± 0.040</cell></row><row><cell></cell><cell>Out-of-sample 0.853 Â± 0.006</cell><cell cols="2">0.776 Â± 0.033</cell><cell></cell><cell>0.788 Â± 0.040</cell></row><row><cell cols="2">3.2 Experiment on Synthetic Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CFR-MMD</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CFR-WASS</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>TARNet</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell>SITE</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>12</cell><cell>48</cell><cell>110</cell><cell>195</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KL divergence</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), MontrÃ©al, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Some works<ref type="bibr" target="#b29">[30]</ref> define ITE as the form of CATE:ITEi = E(Y (i) 1 |x) -E(Y (i) 0 |x).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The code of SITE is available at https://github.com/Osier-Yi/SITE.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian inference of individualized treatment effects using multi-task gaussian processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3427" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The costs of low birth weight</title>
		<author>
			<persName><forename type="first">D</forename><surname>Almond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1031" to="1083" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recursive partitioning for heterogeneous causal effects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7353" to="7360" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01271</idno>
		<title level="m">Generalized random forests</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Informative subspace learning for counterfactual inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="1770" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inference on counterfactual distributions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>FernÃ¡ndez-Val</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Melly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2205" to="2268" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian additive regression trees</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="266" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonparametric tests for treatment effect heterogeneity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Crump</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Mitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="405" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sekhon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02582</idno>
		<title level="m">Overlap in observational studies with high-dimensional covariates</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Doubly robust policy evaluation and learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>DudÃ­k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML 2011</title>
		<meeting>the 28th International Conference on Machine Learning, ICML 2011<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-02">June 28 -July 2, 2011. 2011</date>
			<biblScope unit="page" from="1097" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Doubly robust estimation of causal effects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Westreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>StÃ¼rmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davidian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of epidemiology</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal inference in public health</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>HernÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of public health</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="61" to="75" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric modeling for causal inference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="1262" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Causal inference without balance checking: Coarsened exact matching</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Iacus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Covariate balancing propensity score</title>
		<author>
			<persName><forename type="first">K</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ratkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="243" to="263" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Causal inference in statistics, social, and biomedical sciences</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating treatment effect in the wild via differentiated confounder balancing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 13 -17, 2017. 2017</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4069" to="4079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving propensity score weighting using machine learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="346" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matching on balanced nonlinear representations for treatment effects estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="930" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching via dimensionality reduction for estimation of treatment effects in digital marketing campaigns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3768" to="3774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6449" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017. 2017</date>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the application of probability theory to agricultural experiments. essay on principles. section 9</title>
		<author>
			<persName><forename type="first">J</forename><surname>Splawa-Neyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dabrowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Speed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="465" to="472" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Estimation and inference of heterogeneous treatment effects using random forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust tree-based causal inference for complex ad effectiveness analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Permutation-based causal inference algorithms with interventions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5824" to="5833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GANITE: Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3150" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect from educational studies with residual counterfactual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Educational Data Mining</title>
		<meeting>the 10th International Conference on Educational Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
