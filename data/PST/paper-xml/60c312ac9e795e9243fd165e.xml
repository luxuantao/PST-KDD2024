<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MRT: Tracing the Evolution of Scientific Publications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weng</forename><surname>Lam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ment of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University. He got his bachelor degree in Computer Science and Technology from Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technol-ogy in Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">ment of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Hong Kong University of Science and Technol-ogy, and Leuven University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MRT: Tracing the Evolution of Scientific Publications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data Mining</term>
					<term>Information Retrieval</term>
					<term>Document Representation</term>
					<term>Network Embedding</term>
					<term>Document Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fast development of science and technology is accompanied by the booming of cutting edge research. Researchers need to digest more and more recently published publications in order to keep themselves up to date. This becomes tough in particular with the prevalence of preprint publishing such as arXiv, where inspiring works could come out without being peer-reviewed. Is that possible to design an automatic system to help researchers quickly gain a glimpse of a piece of work or gain useful background knowledge for deeply understanding it? To this end, we proposed a practical framework called Master Reading Tree (MRT) to trace the evolution of scientific publications. In this framework, we can build annotated evolution roadmaps for publications and identify important previous works or evolution tracks by generating expressive embeddings and clustering them into various groups. With comprehensive evaluations, our proposed framework demonstrates its superior capability in capturing underlying relations behind publications over several baseline algorithms. Finally, we integrated the proposed MRT framework on AMiner, an online academic platform, where users can generate roadmaps using MRT for free and their interactions are further used to refine the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S CIENCE evolution has been faster than ever before. The rate of growth for journal numbers has increased from 3% to 6% over the last decade, with around 3 million articles published in 2018 <ref type="bibr" target="#b0">[1]</ref>. At top conferences in computer science such as NeurIPS or CVPR, the number of accepted papers has constantly been increasing, which reached over one thousand in 2019, over three times larger than the number ten years before. Even scanning over the latest publications in related fields could cost experienced researchers days and weeks, not to mention reading in detail. The amount of materials to read and comprehend has also been the main obstacle for novice researchers and intelligence analysts to entering new fields quickly, as mentioned in <ref type="bibr" target="#b1">[2]</ref>.</p><p>To help readers find the main topics among such massive information, concept extraction and taxonomy construction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> have been studied for identifying field structures. Algorithm Roadmap <ref type="bibr" target="#b1">[2]</ref> is also proposed to sketch the dynamics of algorithms in specific areas. However, previous works mainly focus on extracting existing terms in publications and making over-simplified summarization on top of them. Analysts may be able to quickly locate their desired topics or related publications, but it is hard for researchers to reason and word2vec <ref type="bibr" target="#b9">[10]</ref>. Two critical methods in BERT, Masked Language Model and Next Sentence Prediction, can be traced back to MaskGAN <ref type="bibr" target="#b10">[11]</ref> and Skip-thought <ref type="bibr" target="#b11">[12]</ref> as well. Some of such kind of information could be relatively easy to be found in the reference (first-order), however, for many important clues, the reader has to dig into the second-order references (references of all cited papers), which accounts for thousands of publications.</p><p>In this work, to assist readers to gain such deep comprehensions, we generate evolution roadmaps to trace the evolution of scientific publications and the development of their undergoing ideas. There are several challenges in discovering the evolution footprints:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restricted Access to Academic Resource</head><p>Although open academic repositories such as arXiv 1 have emerged in recent years, access to many libraries like ACM Digital Library 2 is still restricted. The limitation on publication contents prevents us from analyzing the full source, while the available metadata, including title and abstract, only provides a glimpse of the main body, which hinders our thorough exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficulties for Unsupervised Deep Comprehension</head><p>The deep comprehension of publications needs extensive readings. Both contents and structure information should be considered during analyzing relationships. Latest embedding methods on contents <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref> and graphs <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, either concatenate node features or rely on fine-tuning on downstream tasks, which makes it hard to handle data in unsupervised ways.</p><p>Problems for Importance Identification While previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> attempt to quantify the importance of each citation relation, it is necessary but difficult to identify which aspect of the citing publication the reference made contributions to. As a general-purpose model, BERT attracts readers from various fields such as Question Answering and Machine Translation. Researchers in former field may want to focus on the references relevant to Reading Comprehension such as QANet <ref type="bibr" target="#b16">[17]</ref>, while those in latter field might be more interested in the connection between BERT and other translation models like GoogleNMT <ref type="bibr" target="#b17">[18]</ref>. Distinguishing these prior works in topics can help scholars save time on finding papers related to their concerned areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lack of Ground Truth</head><p>Unlike the comparison of algorithms in <ref type="bibr" target="#b1">[2]</ref>, the relations between references and the citing work are not explicitly disclosed. The intentions of citations have been studied in several works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, where intentions are categorized by coarse functionalities like Background and Method, or sentiments like Acceptance and Rejection. But labeling topic information for citations is much more complex, not to mention annotating the quality of evolution roadmaps. Even for experts, opinions could differ a lot. There is no ground truth telling us how each reference is cited or how ideas evolve, which prevents us from modeling with supervision or evaluating directly.</p><p>In this paper, we propose a novel framework named Master Reading Tree (MRT) to tackle the above challenges 1. https://arxiv.org 2. https://dl.acm.org and generate evolution roadmaps for publications. Our contributions mainly include:</p><p>• We formalize the problem of tracing the evolution of scientific publications, where the evolution roadmap is defined to help solve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We develop the MRT framework to generate evolution roadmap, which contains two core methods: Calculate Embeddings by combining textual and structural information in unsupervised ways and Construct Roadmaps by analyzing paper relations based on pre-computed embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We design multiple intuitive experiments to evaluate our proposed framework and demonstrate its superiority in capturing underlying relationships between publications over several baseline algorithms.</p><p>The rest of the paper is organized as below. Related works will be discussed in section 2. The problem definition will be stated in section 3 and the framework will be introduced in detail in section 4. In section 5, we will explain how experiments are set up and evaluated. The last two sections contain a brief introduction to our deployed online system and a short conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>The target of our work is to find out the evolution of ideas behind scientific publications and some previous works share similar objectives with us. For example, <ref type="bibr" target="#b20">[21]</ref> try to detect topic evolution in scientific literature by leveraging citations networks in adapting topic models. <ref type="bibr" target="#b2">[3]</ref> use term embeddings and hierarchical clustering to construct topic taxonomy and describe the relations between concepts. <ref type="bibr" target="#b1">[2]</ref> proposes to use algorithm roadmap to sketch the dynamics of research areas.</p><p>Compared to topics or concepts, the understanding of publications is more complicated. Before connecting into an academic network, these publications naturally contain lots of textual information. The deep representation of natural language is therefore essential in our task, which has been studied for decades. Traditional methods like TF-IDF, focus on the statistical attributes based on words or phrases and later topic models such as LDA <ref type="bibr" target="#b21">[22]</ref> extract latent features behind documents. While word embeddings like GloVe <ref type="bibr" target="#b22">[23]</ref> produce transferable representations, enabling the use of prior knowledge, pre-trained language models including ELMo <ref type="bibr" target="#b8">[9]</ref> and BERT <ref type="bibr" target="#b4">[5]</ref>, incorporate contexts during encoding word sequences, demonstrating remarkable capability on capturing task-related information inside sentences. However, the lack of extremely lengthy inputs in the pretraining corpus has limited those BERT-based models from expressing long text efficiently, which has been addressed in several very recent works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Even so, they still heavily rely on the fine-tuning process in downstream tasks and meet difficulties for unsupervised settings. Sentence-BERT <ref type="bibr" target="#b25">[26]</ref> presents a modification of BERT <ref type="bibr" target="#b4">[5]</ref> by finetuning under siamese and triplet networks, which derives meaningful sentence embeddings but does not handle the long text problem as well.</p><p>Although texts of publications can provide rich information, to figure out the evolution of their ideas lying behind, the relationships between publications are indispensable. To incorporate network knowledge, various types of graph embedding techniques have been developed. Unsupervised methods, including skip-gram based, such as LINE <ref type="bibr" target="#b26">[27]</ref> and node2vec <ref type="bibr" target="#b12">[13]</ref> and matrix factorization based like NetSMF <ref type="bibr" target="#b27">[28]</ref> and ProNE <ref type="bibr" target="#b28">[29]</ref>, largely focus on the neighborhood characteristics. Neural models such as GCN <ref type="bibr" target="#b13">[14]</ref> and GraphSage <ref type="bibr" target="#b29">[30]</ref>, can capture more complex features, but the training of these models demands the supervision information on downstream tasks.</p><p>With the expressive representations for publications learned, we can categorize publications in vector spaces and summarize each cluster's main idea by labeling them. Some popular clustering methods, including k-means and spectral clustering, have been proven to be special cases in kernel kmeans <ref type="bibr" target="#b30">[31]</ref>, where semi-supervised constraints can be added as well. The technique of automatic labeling for clusters has also been well studied previously. Works such as <ref type="bibr" target="#b31">[32]</ref>, use external knowledge sources like Wikipedia or WordNet to select candidates and introduce neural embeddings to improve ranking. Traditional methods like <ref type="bibr" target="#b32">[33]</ref> borrow the idea from PageRank and <ref type="bibr" target="#b33">[34]</ref> proposes to construct label groups by minimizing the divergence between label distribution and topic distribution.</p><p>To analyze the relevance between publications, we reform this problem into a recommendation scenario, where the opinions from users are introduced with the use of reinforcement learning methods. While value-based <ref type="bibr" target="#b34">[35]</ref> and policy-based <ref type="bibr" target="#b35">[36]</ref> RL algorithms have been successful in games and robotics for years, these methods were reinvented for recommendation problems recently <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Compared to traditional neural recommendation models, RL-based methods can optimize the action according to long-term rewards instead of static CTR (i.e., Click Through Rate).</p><p>Overall, our work can be broken down into several pieces. Some pieces related to popular topics such as document embedding and network embedding have made progresses in the past few years but are still far from been solved. Other related fields like clustering or automatic labeling were studied for a longer time and now have solid choices available but still need adaptations to make them practical in our circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>The problem of tracing the evolution of scientific publications, literally, is to analyze the relation between publications and figure out how later works are influenced by previous ones. Inspired by prior research on topic evolution, we propose to use topics to sketch the relationships. Although it is possible for one reference paper to be cited for multiple reasons, i.e., to inspire the citing paper on several topics, pointing out the ingredient of the citing topics is difficult, as various topics could be overlapping. To this end, we simplified the relation between each reference and the citing paper into one single topic. Then references sharing the same type of relation (same topic) to the citing paper form one evolution track, representing one origination of ideas lying behind. Finally, all the evolution tracks provide a detailed description for the origins of the citing paper, which constitute the evolution roadmap.</p><p>To help readers understand our work, we firstly introduce several definitions inside the evolution roadmap and then formalize the problem of tracing the evolution of scientific publications as below.</p><p>Preliminary Suppose the citing paper we are interested in, is denoted as q (abbreviated to the query publication), and cite(p i , p j ) indicates publication p i cites p j .</p><p>Relation(p j ; p i ) refers to the relation from reference p j to the citing paper p i , which in our configuration, is one single topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>The direct references for q may not be enough for mining evolution, so we enlarge the scope of "reference". Formally, we define first-order references as the set of papers cited by q., i.e., R 1 = {p | cite(q, p)} and second-order references as additional references cited by R 1 , i.e., R ⇤ 2 = {p | cite(p 0 , p), p 0 2 R 1 , p / 2 R 1 }. Higher-order references are included as needed. For simplicity, we do not distinguish second-order references and higher-order references in following sections and we use R 2 to denote the union of them.</p><p>Evolution Track As described above, each evolution track C t refers to a group of references sharing the same topic t, formalized by</p><formula xml:id="formula_0">C t = {p | p 2 R 1 [ R 2 , Relation(p; q) = t}.</formula><p>Additionally, each evolution track is annotated by N l labels, with the i-th label for C t denoted as l ti .</p><p>Importance Score While in each evolution track, multiple references share the same topic, their importance to the citing paper could be very different. Some might be closely related, like prior state of the art model for comparison, but others probably are just mentioned in one or two sentences for introducing backgrounds. Thus, we assign importance scores to each reference and evolution track to identify their impact on the citing paper. In detail, the importance is denoted as w pi for publication p i and w Ct for evolution track C t .</p><p>Evolution Roadmap To describe the evolution of ideas behind publications, a general roadmap could be defined as a directed acyclic graph (DAG), where nodes represent papers and edges express the topics shared between papers. However, as our main interest is the query publication q and a DAG structure can be complex to interpret, we make compromises on the structure of the roadmap and use a tree data structure for the evolution roadmap. Formally, an evolution roadmap for one publication q is defined as a tree (V, E, C, W ), consisting of • V : N p nodes, with q as the root node and other references from R 1 [ R 2 as the rests.</p><p>• E: a set of edges. Each edge (p i , p j ) indicates a potential evolution relation from p i to p j .</p><p>• C: N t evolution tracks, with each one annotated by N l labels.</p><p>• W : N p 1 importance scores for each reference node and N t scores for evolution tracks.</p><p>As mentioned above, each relation Relation(p i , p j ) can be treated as one single topic t and both p i and p j belong to C t (except the root q belongs to none of those evolution tracks).  We recommend related papers for users when some paper is clicked by users.</p><p>Finally, we define the problem of tracing the evolution of scientific publications as the following: Given a publication q, construct a directed acyclic graph where each node is one paper and each edge indicates an evolution relation. In this work, we propose to generate evolution roadmaps to solve this problem, with several compromises made considering the complexity of interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FRAMEWORK</head><p>In this work, we proposed a framework named Master Reading Tree (MRT) to generate the evolution roadmap, which mainly contains two parts:</p><p>(1) Calculating Embeddings To gain a deep comprehension for all publications and analyzing their relations, we generate expressive representations encoding both textual and structural information. A combination of document embedding and graph embedding is proposed in unsupervised styles. (2) Constructing Roadmaps After projecting publications into latent vector spaces, we apply clustering and automatic labeling techniques to build evolution roadmap based on pre-computed representations. As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, the Calculating Embeddings part refers to the second step of the whole procedure and the following three steps form the Constructing Roadmaps part.</p><p>Before looking into the details of publications, we need to decide which publications should be considered. The size of first-order references for one query publication q could vary from several to hundreds, depending on the publishing requirements and q's own contents. Since we only have limited information (title, abstract, and citation links) for each publication due to the restricted access for full source, if the size of considered publications is too small, it will be hard to capture some marginal topics that are not discussed in many papers. On the other hand, if the size is too large, there might be too much irrelevant information that will hurt the speed of the following algorithms.</p><p>As a result, we introduce second-order references as supplementary and select the most relevant N p publications. This selection procedure can be implemented in several different ways, such as using the number of citations, the node degrees in the sampled subgraph, or PageRank scores. We observe that the difference between these choices is minor, as the most valuable works are always kept. It is hard to evaluate their performances directly, but in the experiment section, we will see PageRank reflects the users' interests better so in this work we adopt PageRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Calculating Embeddings</head><p>To find out the underlying evolution tracks and estimate their importance, we need to generate high-quality expressive representations for publications.</p><p>The representations should encode both publication content and citation network structures. However, there are several problems with existing embedding techniques.</p><p>• Recent natural language embedding techniques like BERT <ref type="bibr" target="#b4">[5]</ref> need to be fine-tuned on downstream supervised tasks, which we do not have. Although Sentence-BERT <ref type="bibr" target="#b25">[26]</ref> can produce meaningful sentence embeddings with the use of siamese and triplet architectures, the samples in their training corpus are mostly short sentences, leaving it questionable for encoding long texts. Those recent works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> targeting on long texts also need labeled data for downstream fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Latest network embedding methods such as GCN <ref type="bibr" target="#b13">[14]</ref> rely on labeled data for supervised training. Skip-gram based methods like node2vec <ref type="bibr" target="#b12">[13]</ref> or matrix factorization based methods mainly produce structural embeddings independent of node features. In this work, we need to generate embeddings con-cerning both contents and structure in unsupervised ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The concatenation on embeddings is not easy to use with cosine-similarity or other distance metrics. Let x ↵ i and x i be the two different types of embeddings for element i, the concatenated embedding will be [x ↵ i ; x i ]. The euclidean distance between element i and j will be</p><formula xml:id="formula_1">q ||x ↵ i x ↵ j || 2 + ||x i x j || 2<label>(1)</label></formula><p>and the cosine-similarity will be</p><formula xml:id="formula_2">x ↵ i x ↵ j + x i x j q ||x ↵ i || 2 + ||x i || 2 q ||x ↵ j || 2 + ||x j || 2<label>(2)</label></formula><p>If both embeddings are normalized, the distance or similarity can be seen as a direct addition or average for the results in two vector spaces. If not, there will be an imbalance problem with two types of embeddings, where the vector with longer length will take the major part. While most embedding methods do not ensure the output to be normalized, it is also uncertain whether two embeddings are equally important, since we do not have a fine-tuning process to learn their weights.</p><p>To tackle the problems mentioned above, we first propose to combine TF-IDF and Sentence-BERT <ref type="bibr" target="#b25">[26]</ref> to encode publication contents and then adapt the spectral propagation process proposed in ProNE <ref type="bibr" target="#b28">[29]</ref> to incorporate structure information as shown in Figure <ref type="figure" target="#fig_1">3</ref>. Formally, we first build the TF-IDF and Sentence-BERT embeddings for each publication p i as follows:</p><formula xml:id="formula_3">xt i = TF-IDF(p i ), xs i = S-BERT(p i )<label>(3)</label></formula><p>xt i and xs i are the textual and structural embeddings encoded by TF-IDF and S-BERT respectively. Then these two content embeddings are propagated on the citation graph G, where the nodes are the papers and edges are the citations.</p><formula xml:id="formula_4">xt i = Propagate(x t i , G), xs i = Propagate(x s i , G)<label>(4</label></formula><p>) Finally, they are concatenated and propagated again to construct the final representation.</p><formula xml:id="formula_5">x i = Propagate([x t i ; xs i ], G)<label>(5)</label></formula><p>The TF-IDF algorithm encodes long texts in literal ways and the BERT based encoder is expected to capture more latent information. Despite the difficulties for TF-IDF to find relations between documents sharing the same topic but using different words, this problem is most frequent for short sentences. But in our task, titles and abstracts for most publications provide rich textual information. Furthermore, most publications, centering around the query paper q, are mutually connected through only one or two citation hops, and their descriptions for the same referent usually share lots of expressions. As a result, TF-IDF works pretty well in finding relations between publications.</p><p>The BERT based method has demonstrated its superior capability on capturing latent semantic information behind texts. While extremely long texts could prevent us from applying S-BERT directly, fortunately, around 98% of the text inputs for publications (the concatenation of title and abstract) have less than 512 word tokens, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. So it is relatively safe to discard overflowing parts. However, S-BERT is fine-tuned mainly on some short sentences based corpus and such training data for long texts is hard to access or label. Therefore, S-BERT has limited performance in our task but still provides extra boosting on top of TF-IDF, which will be shown in the following experiments.  For encoding the structure of G, we take spectral propagation proposed in ProNE <ref type="bibr" target="#b28">[29]</ref> to incorporate both neighborhood and global features. For each "Propagate" iteration, the node embedding x i for publication p i is updated by</p><formula xml:id="formula_6">x i D 1 A(I Np L)x i<label>(6)</label></formula><p>where D 1 A is the normalized adjacency matrix for graph G, I is the identity matrix and L is the modulated Laplacian. This method is proposed in ProNE to enhance the node embedding generated by pre-factorization. In graph theory, the random walk normalized Laplacian matrix is defined as 1 , where U and ⇤ are constituted with eigenvectors and eigenvalues. The propagation Lx can be seen as transforming x into spectral space, scaling by eigenvalues and then transforming it back. The modulated L is then defined as Ug(⇤)U 1 where g is the spectral modulator which strengthens the top smallest and largest eigenvalues, corresponding to local and global features respectively.</p><formula xml:id="formula_7">L = I D 1 A. It can be decomposed by L = U ⇤U</formula><p>Therefore, each propagation step can be interpreted as aggregating neighborhood node features by random walk, considering global information at the same time.</p><p>Additionally, in ProNE, SVD is applied to the output embeddings in order to maintain the orthogonality. We found it also helpful while concatenating embeddings with various lengths, as it reduces the dimension of embeddings and keeps the most informative features.</p><p>As for the implementation details, we use the same spectral modulator and Chebyshev expansion as used in ProNE to accelerate the calculation by avoiding explicit eigendecomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Constructing Roadmaps</head><p>With the embeddings generated in the previous step, we can construct evolution tracks by clustering papers and use automatic labeling method to assign labels for them. The importance scores for references and evolution tracks are also calculated in intuitive ways.</p><p>Clustering After projecting papers into latent vector space, we can apply k-means or spectral clustering to group those publications into different evolution tracks. Kernel k-means, which has been proven to be a generalized solution <ref type="bibr" target="#b30">[31]</ref> for both k-means and spectral clustering, is more flexible where external constraints are available as well. The euclidean distance between x i and the centroid m Ct for cluster C t can be represented as</p><formula xml:id="formula_8">||x i m Ct || 2 = K ii 2 P pj 2Ct K ij |C t | + P pj ,p 0 j 2Ct K jj 0 |C t | 2 (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where K is the affinity kernel and |C t | is the size of cluster C t . Under the current context, the affinity kernel is calculated by</p><formula xml:id="formula_10">K ij = x T i x j + ↵A ij + ij<label>(8)</label></formula><p>where x i is the representation for publication p i which we got from the previous step. A is the symmetric adjacency matrix for citation graph G as described above. The first term derives from the original k-means algorithm and the second adjacency term originates from the intention that we want to break as few citation relations as possible from the citation graph G during clustering. The third term is the additional supervision where ij denotes p i and p j have close relationship potentially. This weak supervision will be discussed in the experiment section.</p><p>After publications are categorized into different evolution tracks, they need to be connected into the evolution roadmap. Similar to algorithm roadmap proposed in <ref type="bibr" target="#b1">[2]</ref> where temporal order is used to connect algorithms, we sort papers in each evolution track by their publishing years and citation order. The first-order references are connected into the primary timeline and second-order ones form the secondary timeline. The latest publication p 0 in the secondary timeline is connected to the earliest publication in the primary timeline that is published after p 0 . Finally, the latest publications in the primary timelines for all evolution tracks are joined with q to form the skeleton of the evolution roadmap. The clustering method is depicted in Figure <ref type="figure">5</ref>.</p><p>Labeling For each evolution track generated in the previous step, we create a group of labels to identify it. The process of automatic labeling includes two procedures: candidate label extraction and ranking.</p><p>While extracting label candidates, using external knowledge bases such as Wikipedia may have difficulties in covering the latest academic terminologies. Besides, scientific publication titles and abstracts are usually concise and informative. Therefore, we extract bi-gram and tri-gram phrases directly from the raw text. The threshold of frequency is set to be the half or one third to the most frequent one Embeddings Clustered</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connected into timelines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joined Together</head><p>Fig. <ref type="figure">5</ref>. To generate the skeleton of evolution roadmap, we first cluster papers into evolution tracks based on the pre-computed embeddings and then connect them into timelines by their publishing year and citation order.</p><p>depending on the length of the candidate phrase. Phrases with a frequency lower than the threshold are discarded and the remaining ones are kept as candidates.</p><p>Adapting based on <ref type="bibr" target="#b33">[34]</ref>, we rank our candidate labels following three criteria:</p><p>1) Good evolution track label should have high coverage for the contents of papers inside it. 2) Good evolution track label should be discriminative to labels assigned to other evolution tracks. 3) Good evolution track label should be relevant to the content in the query publication q since those evolution tracks are extended from it.</p><p>The content for each cluster C t can be seen as a distribution of words and the co-occurrences of words with label l can be also interpreted as the contents covered by label l. The KL divergence between them reflects to what extent the label l can represent cluster C t , and can be written as </p><formula xml:id="formula_11">+ KL(C t ||•) Bias(l, •) = E Ct [P MI(word, l|•)] + KL(C t ||•) Bias(l, •)<label>(9)</label></formula><p>where (•) is the global corpus consisting of q and its references. P MI is the pointwise mutual information. The second cluster constant term KL(C t ||•) is identical while choosing labels. The third bias term Bias(l, •) is mainly introduced by using (•) as the context. While C t is selected from (•) in our task, this term can also be ignored, which is same with <ref type="bibr" target="#b33">[34]</ref>. Then the ranking score can be defined as</p><formula xml:id="formula_12">Score(l, C t ) = (1 + µ N t 1 )E Ct [P MI(word, l|•)] µ N t 1 Nt X j=1 E Cj [P MI(word, l|•)] + E q [P MI(word, l|•)]<label>(10)</label></formula><p>where µ and controls the discriminative power and the query publication coverage, respectively.</p><p>While some evolution tracks might contain multiple subtopics, we create a group of N l labels to identify each track. The selection of N l labels in one label group also considers Maximal Marginal Relevance criterion <ref type="bibr" target="#b39">[40]</ref>, which shares a similar formula as described above. The general idea is to make the coverage of label groups as large as possible.</p><p>Importance Score References and evolution tracks are further annotated with importance scores to help readers identify their relative influence or contribution to the development of the query publication.</p><p>Suppose the index for q is i q , the importance score w pi for p i is directly assigned as the kernel weight K, defined in Equation <ref type="formula" target="#formula_10">8</ref>, relative to the query publication q w pi = K iqi <ref type="bibr" target="#b10">(11)</ref> and the score for evolution track C t is defined as the sum of all reference scores inside it.</p><formula xml:id="formula_13">w Ct = X pi2Ct w pi (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>This design is mainly based on the similarity between generated paper embeddings. We will show in the experiment section that this method performs better than simply using the citation numbers or PageRank scores.</p><p>Recommending Although importance scores can be used to identify the most influential papers for the query publication q, users may have different interests and their focus points might also change during navigating the evolution roadmap. To better help readers focus on the most relevant publications, we develop an agent for the roadmap to recommend potentially appealing works. The whole scenario is illustrated in Figure <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reader Roadmap</head><note type="other">GRUCell State Vector Publication Embeddings Publications Recommended Publications Highlighted Roadmap Agent Click Sort Update</note><p>Fig. <ref type="figure">6</ref>. The recommending procedure: when a reader is viewing the evolution roadmap, he might click or hover on some reference paper to see more details of it. This action can be viewed as a potential interest for that paper and our agent will update the state vector, which encodes the user's history. According to the updated state vector, the agent will select relevant papers which the user might want to read next and recommend those papers by highlighting them on the evolution roadmap.</p><p>Our agent's job is to recommend k papers after receiving a user action. For new roadmaps, since we do not have user responses, to tackle the cold start problem, we develop agents following the default strategy: recommend publications with the closest embeddings to the current clicked one.</p><p>After user feedback collected for popular roadmaps, we train agents with the REINFORCE algorithm <ref type="bibr" target="#b37">[38]</ref>, where the reading histories are encoded into state vectors and publications in roadmap G are treated as action candidates. Formally, the Markov Decision Process (MDP) is set up with 1) S: a continuous state space where state vectors encode previous reading history. 2) A: a discrete action space containing all the publications in the roadmap. 3) P: the state transition probability. 4) R: the reward function where r t is the immediate reward for time t. If the next user click falls into the recommended action sets, the immediate reward will be 1, otherwise 0. 5) : the discount factor for future rewards. Set to be 0.9. The goal for the RL algorithm is to find a recommendation policy ⇡ : S ! A to maximize the expected cumulative reward E ⌧ ⇠⇡ [R(⌧ )] where ⌧ is the recommending trajectory.</p><p>For simplicity, S shares the same dimension d with the publication embeddings x. The initial state s 0 is set to be x iq , the embedding of the query publication q. We model P with a single-layer GRU network <ref type="bibr" target="#b40">[41]</ref> as follows:</p><formula xml:id="formula_15">s t+1 = GRU(s t , c t )<label>(13)</label></formula><p>where c t is the user's click at time t, and the policy is calculated with</p><formula xml:id="formula_16">⇡ ✓ (p i |s t ) = Softmax(s T t Wx i )<label>(14)</label></formula><p>where W 2 R d⇥d is a trainable parameter matrix. At time t, publications are sampled from ⇡ ✓ (p i |s t ) during training while publications with top-k highest probabilities are recommended during inference. Following <ref type="bibr" target="#b37">[38]</ref>, the gradient of expected rewards for top-k recommendation is approximated by</p><formula xml:id="formula_17">r ✓ E ⌧ ⇠⇡ ✓ [R(⌧ )] ⇡ X ⌧ ⇠⇡ ✓ |⌧ | X t=0 rt r ✓ log ↵ ✓ (p i |s t )<label>(15)</label></formula><p>where rt = P |⌧ | t 0 =t t 0 t r t denotes the expected future rewards at time t and ↵ ✓ (p i |s t ) = 1 (1 ⇡ ✓ (p i |s t ))  denotes the probability of taking p i in the first  samplings.  ( k) is the total number of samplings to sample k unique publications from ⇡ ✓ (p i |s t ) with replacement.</p><p>In brief summarization, our proposed framework constructs evolution roadmaps based on the publication embeddings. The evolution relations are implicitly encoded in the paper representations and then explicitly expressed by clustering and annotating. While the unsupervised framework might be a bit ad-hoc, we also introduce some user feedback to make the whole algorithm more practical. Besides, despite the lacking of ground truth data to evident our methods, we designed several intuitive experiments to show our choices are reasonable and explainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we will introduce several experiments designed for evaluating different parts of our proposed framework. The problem we want to solve is relatively novel and complex. Without established benchmarks or available existing datasets, we attempt to measure the performance through several indirect criteria. It is hard to say any of these empirical results is convincing solely, but we believe these outcomes together evident that our framework is a practical solution for the raised problem.</p><p>All the experiments in this paper use the configurations described in Table <ref type="table" target="#tab_2">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>The quality of the generated roadmap is closely associated with the completeness of the reference lists. For example, BERT does not only utilize the works from ACL or EMNLP. Hence, instead of using one single conference as our data source, we use open academic platforms including AMiner <ref type="bibr" target="#b41">[42]</ref> (for the online system) and Semantic Scholar <ref type="bibr" target="#b42">[43]</ref> (for the online system and experiments), where metadata of publications from various sources like DBLP are integrated. This is also a notable difference compared with many prior works such as <ref type="bibr" target="#b1">[2]</ref>, where only papers from the same conference or dataset are considered.</p><p>For evaluations, we use publications from KDD and ACL conferences in 2018 and 2019 as query publications and generate roadmaps for them. Their references are retrieved from Semantic Scholar 3 . The statistics of each dataset are shown in Table <ref type="table" target="#tab_3">2</ref>. 1 Papers refer to the publications used as the query publication q. This is also the number of evolution roadmaps we tested. 2 Retrieved References refer to the first-order and secondorder references we retrieved from Semantic Scholar, which are not necessarily inside the same conference with the query publications. 3 Citation Links indicate how many links are considered between publications. This is the number of links we used while using PageRank to select related papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluations and Results</head><p>There are mainly two parts contained in the framework. The first part is to generate publication embeddings and we designed Neighborhood Similarity experiment to test the embedding quality. In the second part, for each of four minor tasks, we also conducted another four experiments to test the effectiveness of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighborhood Similarity</head><p>We first evaluate the document representations generated in the first part. There are two difficulties in measuring the quality of these embeddings.</p><p>3. https://api.semanticscholar.org/ First, our downstream tasks are also unsupervised, making it hard to rely on end-to-end tests. Second, unlike words or sentences, it is even tough for human experts to judge the similarity between publications. Therefore, we proposed to use Neighborhood Similarity to evaluate based on the assumption that similar publications should have more overlaps between their neighborhoods. The neighborhood N (p i ) for publication p i is defined as</p><formula xml:id="formula_18">N (p i ) = {p | cite(p, p i ) _ cite(p i , p)} [ {p i } (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>and the Neighborhood Similarity sim N between publication p i and p j is defined as</p><formula xml:id="formula_20">sim N (p i , p j ) = |N (p i ) \ N (p j )| p |N (p i )| • |N (p j )|<label>(17)</label></formula><p>which is also known as Jaccard index.</p><p>Like other similarity tests <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, we compared the Spearman rank correlation <ref type="bibr" target="#b45">[46]</ref> between embedding cosine similarity and neighborhood similarity over several methods. The baseline methods include using TF-IDF, S-BERT, ProNE, and node2vec individually, and the proposed methods include their combinations. The results are shown in Table <ref type="table" target="#tab_4">3</ref>. 1 For TF-IDF, we select top frequent 2000 features and use n-grams ranging from 1 to 5. 2 For S-BERT, we use the pre-trained model of bert-base-nlistsb-mean-tokens. 3 For ProNE, the embedding dimension is 32 and the order of Chebyshev expansion is 10, according to <ref type="bibr" target="#b28">[29]</ref>. 4 For node2vec, the embedding dimension is 32. Walk length and number of walks are set to be 20 and 60, respectively. The window size is 5.</p><p>As the ground truth is closely related to explicit connections between publications, structure-based algorithms, including ProNE and node2vec, outperform content-based ones by a large margin. Simply concatenating different embeddings (TF-IDF+S-BERT) turns out to be a bad idea that might be caused by the reasons we mentioned above. The propagation on content embeddings yields significant improvement compared to the original ProNE algorithm. As for document content representation, TF-IDF is yet one of the most powerful competitors. The Sentence-BERT encoder is able to provide extra enhancement beyond TF-IDF, probably due to its capability of encoding deeper information, but itself still meets difficulties while dealing with long texts.</p><p>It is worth to be mentioned that this metric is similarly proposed as second-order proximity in LINE <ref type="bibr" target="#b26">[27]</ref> but there is a slight difference. While we were generating embeddings for papers, the network structure we were using is a subgraph of citation network on N p selected papers. The Neighborhood Similarity, on the other hand, is defined by the complete neighborhood on the whole citation network, where many more papers outside N p selected papers are also taken into account. This means in this experiment, we generate embeddings based on a subgraph structure and expect these embeddings to reflect some global attributes.</p><p>Besides, there are two reasons why we do not directly use this similarity to generate evolution roadmaps. First, our generated embeddings only use information from a small subgraph while the neighborhood similarity uses global information which requires more data to fetch. For example, some famous works might have thousands of citations. Since our subgraph is explored through reference links, we do not need to fetch those citations. Second, the citation information is always changing. New citation links may come out as papers published. The subgraph created by reference links will be determined when the query publication is decided so our proposed method for generating publication embeddings is time-invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-mention and MST Trials</head><p>To evaluate the structures of generated roadmaps, we designed two trials to test the quality of clustering and linking.</p><p>The mentions of first-order references inside the query publication provide strong implications for some publication pairs. For example, "They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, which cluster concepts to induce an implicit hierarchy." is a piece of text from <ref type="bibr" target="#b1">[2]</ref>. Co-mentions inside the same boxes, marked as strong co-mentions, such as 14 and 32, indicate their similar functionalities. Co-mentions in the same paragraph, marked as weak co-mentions, such as 14 and 11, also implies potential connections, especially under the context of the citing publication.</p><p>Similar to <ref type="bibr" target="#b1">[2]</ref>, we use pdftotext provided from xpdf 4 to extract the strong co-mention and weak co-mention pairs described above from the full text of query publications. The co-mention hit rate is defined as the percentage of co-mention pairs sharing the same cluster assignment.</p><p>The co-mention hit rate has preferences over allocations with uneven cluster sizes since huge clusters are particularly competitive. To avoid being blinded by this bias, we introduced another trial focusing more on the roadmap structure. As clusters are reorganized into a tree-structure graph, it is helpful to compare the result with an unconstrained solution. To achieve that, we set up a fully connected graph Ĝ with nodes representing all publications. Every two nodes for publication p i , p j are connected by an edge with weight sim N (p i , p j ) defined in equation 17. Kruskal's algorithm <ref type="bibr" target="#b46">[47]</ref> is applied on Ĝ and the total weight for edges from the maximum spanning tree is recorded as BestScore, which is compared with that for the generated roadmap, denoted as RoadmapScore. The relative MST score is defined as RoadmapScore BestScore where higher scores usually mean better structures. The average of all samples inside the dataset is reported in Table <ref type="table">4</ref>.</p><p>Compared to hierarchical or spectral clustering, k-means works best with high-quality embeddings. The kernel k- means (↵ = 1.0, = 1.0), extended from k-means, benefits from incorporating supervision information. For example, the use of strong co-mentions can improve weak co-mention scores and vice versa. We noticed that the additional adjacency matrix introduced in kernel k-means works well for some samples but the overall improvement is limited compared to the original k-means, since the document embeddings have already been propagated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Label Distance and Overlap Rate</head><p>The quality of annotated labels is similarly evaluated using the full source of the query publication as above. If the mentioned location for publication p i from cluster C t is close to the occurrence of the j-th label l tj of C t , then we say label l tj is high-quality. Formally, for each first-order reference p i and each label l tj in C t , we extracted their paragraph positions in the source of q. The closest paragraph distance between the occurrences of p i and l tj is denoted as dis ij . For example, if p i and l tj occur in the same paragraph in the source of q, then dis ij = 1. If l tj does not appear in the source of q, then dis ij = 1. The Inverse Label Distance for roadmap G is defined as</p><formula xml:id="formula_21">ILD(G) = 1 N t Nt 1 X t=0 max j 1 |C t | X pi2Ct 1 dis ij<label>(18)</label></formula><p>Apart from the quality of each cluster label, we also need to consider them collectively. A high overlap rate between clusters (evolution tracks) should be avoided as well. We thus use the overlap rate to measure the labeling algorithm performance further. The overlap rate is defined as the proportion of duplicated labels among all selected labels across clusters, i.e.,</p><formula xml:id="formula_22">Overlap(G) = |{l tj | 8t, j}| N t N l<label>(19)</label></formula><p>We developed a trivial baseline algorithm that directly uses the top frequent bi-gram and tri-gram phrases from publication titles and abstracts as cluster labels. Results are reported in Table <ref type="table" target="#tab_6">5</ref>.</p><p>The results show that our proposed method (µ = 0.8, = 0.1), extended from <ref type="bibr" target="#b33">[34]</ref>, outperforms the baseline algorithm for both inverse label distance and overlap rate. The ablation tests show that the discriminative term, controlled by µ, can efficiently reduce the overlap rate when generating labels across clusters. The additional query publication coverage term, controlled by on the other hand, can improve the coherence between labels and the query publication at the cost of higher overlapping probability.</p><p>Importance Evaluation with User Click The calculation of importance for publications is difficult to judge as there is no ground truth data available. In addition, even for experts, labeling importance for publications is not easy and the decision might be rather subjective as well. For this reason, we choose to use online feedback from users to evaluate the importance score.</p><p>Borrowing the idea from item recommendation and search engine, we use user clicks for ranking publications. Although various users may hold different opinions for which publication is more important, the overall click through rate for publications generally reflects the interests of the majority. The importance scores should be coherent to the common interests so we measure the Spearman correlation between user clicks and importance scores. In addition, we use Normalized Discounted Cumulative Gain (NDCG) to check if the selection for top-K items is reasonable since most users will focus more on the top important publications.</p><p>As for the experiment settings, we collected the 10 most popular MRTs on AMiner with around 10 thousand user clicks in total. Each one received clicks from at least 50 unique users on the internal publications. The citation numbers, node degrees and PageRank scores for publications are treated as baseline methods. The averaged Spearman correlation and NDCG results are reported in Table <ref type="table" target="#tab_7">6</ref>. The out-degrees, in-degrees and PageRank scores are all calculated based on the subgraph of citation network. The subgraph has Np papers as nodes and all their internal citation links.</p><p>From the results, we can see that our proposed importance score provides better ranking compared to baseline algorithms, concerning users' interests. The citation number mainly suffers from two critical problems under our circumstances. First, both citation numbers and PageRank scores favor old well-known pioneer works more due to their high impacts but overlook the contribution of recent emerging works. Second, with the existence of the query publication, the users' interests for reference publications will be biased, which is neglected while using the citation number.</p><p>The PageRank algorithm tackles the above problems in indirect ways. By running on the subgraph centered on the query publication, the PageRank scores will implicitly consider the bias of the query publication. In Table <ref type="table" target="#tab_7">6</ref>, the PageRank scores have much higher coherence to users' interest while the citation number itself has poor performance on that. This observation also explains why we use PageRank scores to select N p papers before calculating embeddings, instead of using citation numbers. In addition, the in-degrees work similar to PageRank and their performances are close. So it might be also practical to use it when selecting papers, as it requires less time to compute compared to PageRank.</p><p>In terms of our proposed importance score, it directly models the similarity between reference publications and the query publication, which receives even better results than PageRank scores. We speculate that directly modeling the environment condition (the query publication) helps capture the interests of users a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommendation Evaluation</head><p>For dynamic recommendation, we evaluated the performance of the REINFORCE algorithm based on the average rewards from user click trajectories. This experiment is conducted based on the user click data collected from the AMiner online platform, and we focused on several top popular roadmaps. Over 10 thousand raw click events are collected and pre-processed into around 1,600 trajectories. In detail, all click data are first grouped by IP addresses, with each sequence sorted by timestamp. Those click sequences are further divided into different sessions where time intervals for adjacent clicks are shorter than 10 minutes. In addition, continuous duplicated clicks in sessions are discarded and finally transformed into user click trajectories.</p><p>As for the experiment, 80% of the data are used as training data and the rest 20% are utilized as validation data. The baseline algorithm is our default strategy mentioned above: recommending using publications with the closest embeddings to the clicked one. The results are shown in table <ref type="table" target="#tab_8">7</ref>. Compared to the static baseline algorithm, the REIN-FORCE algorithm performs better. However, due to the lack of data, most models are far from being fully fit. For the most popular roadmap, the BERT one, we collected over 3,500 user clicks and extracted around 600 user click trajectories, with an average path length at 3.24. We observe that about 20% of users will jump back to previously visited references after one or two hops. The relation between the current click and the last click is also affected by behavior patterns. If the focus of one user moved between publications within the same cluster or the same year previously, the possibility for later occurrence of this pattern would rise by 10% to 20%. Besides, some users seem to prefer moving along citation links instead of timelines. We believe neural models could capture these characteristics, and we leave more comprehensive explorations for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>Influential works like BERT <ref type="bibr" target="#b4">[5]</ref> attract discussions from different fields and have profound impacts on subsequent studies. The generated roadmap for BERT is illustrated in Figure <ref type="figure">7</ref>. Label groups and importance scores for each evolution track are shown in Table <ref type="table" target="#tab_9">8</ref>.</p><p>Two most significant evolution tracks in the roadmap of BERT, share the same topic of Natural Language Model, but focus on slightly different sub-topics, as shown in the label groups. Reading Comprehension and Machine Translation are task-oriented clusters with both model papers and datasets papers contained. Works like OpenAI GPT and GloVe, play particularly important roles in the evolution of BERT. The former one, frequently compared with BERT, is quite similar in functionality and model structure but has different implementations. The latter one, though only mentioned once in BERT, is highly connected to other references of BERT, can be seen as an essential milestone in the evolution process of various ideas.</p><p>Overall, the MRT framework can generate readable evolution roadmaps given the metadata of publications despite a few drawbacks. First, clustering with hard boundaries is not an optimal solution for many publications. For example, works like GloVe and LSTM contribute to multiple topics concurrently, which makes their hard clustering locations unstable and confusing. However, the soft assignment is not convenient for readers to recognize, especially when hundreds of nodes are provided. Second, low-quality phrases exist in cluster labels. Although it is possible to filter out phrases like "Task Including" using Part-Of-Speech (POS) Fig. <ref type="figure">7</ref>. A full static version of evolution roadmap for BERT. Each column represents one evolution track and papers are positioned by time order from top to bottom. Due to the page limit, we only show the most important label in each label group for every evolution track. The circle on the left side of papers is filled by the importance of papers adjacent to it. The more the circle is filled, the higher importance scores the papers on the right side will have. Evolution tracks are also positioned by their importance. The evolution tracks close to the center like Natural Language and Language Model are more important than evolution tracks on the sides. The recommendation function and more details for the evolution roadmap can be viewed on AMiner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page 11 of 13</head><p>Transactions on Knowledge and Data Engineering tagging, these rules fail when high-quality candidates are dropped due to wrong POS tags. Since keywords often exist in publication titles that are not complete sentences usually, the bad precision of POS tagging or other methods frequently causes the low recall problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DEPLOYED SYSTEM</head><p>The MRT framework has been deployed onto AMiner 5 for free use. Over 10 thousand accesses have been made and around 700 evolution roadmaps have been generated by users. Another 10 thousand roadmaps were generated for papers published in recent conferences such as NeurIPS 2019 and ICLR 2020, which also attracted thousands of accesses.</p><p>While the generation for each roadmap only takes tens of seconds without GPU, it is still hard to handle user requests synchronously. The time cost on fetching metadata for papers take the major part as thousands of papers and citation links are needed to fetch. Although it is possible to cache some parts of the metadata, users have diverse interests ranging from computer science to biomedical. The hit rate for the cache is far from enough to provide immediate results. To tackle this problem in our online system, we maintain all user requests in a task queue and develop a backend service to handle these requests sequentially. Those requests favored by more people will have a higher priority in the queue, and users will be notified by email on the completion of roadmap generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we proposed a novel framework called MRT to generate evolution roadmaps for publications helping researchers trace the movements of ideas lying behind. The framework is mainly composed of two parts: calculating embeddings and constructing roadmaps. Embedding methods are blended to encode both semantic and structural information. Semi-supervised clustering and automatic labeling techniques are applied to generate the annotated roadmap. Finally, a reinforce-based algorithm is developed 5. https://www.aminer.cn/mrt for making recommendations helping readers locate their interested materials quickly. The integrated framework is evaluated through several experiments and has attracted thousands of usage already. The flexibility of MRT allows substituting internal modules with more advanced models without changing other parts. We leave better algorithm exploration and several potential problems mentioned above for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The procedure of generating evolution roadmap in the MRT framework. (1): We first retrieve reference data from data sources. (2): Then we calculate paper embeddings according to their content and citation relation. (3): We cluster papers into evolution tracks and construct the skeleton of evolution roadmap. (4): We annotate each evolution tracks with labels and assign importance scores to both clusters and papers. (5)We recommend related papers for users when some paper is clicked by users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The procedures of calculating embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The histogram of text length for publications used in this work after tokenization, where over 200,000 publications from various communities have been considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="11,60.90,286.46,490.19,419.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Main configurations for experiments</cell><cell></cell></row><row><cell cols="2">Symbol Description</cell><cell>Value</cell></row><row><cell>Np</cell><cell>Number of publications for each roadmap</cell><cell>100</cell></row><row><cell>Nt</cell><cell>Number of evolution tracks</cell><cell>6</cell></row><row><cell>N l</cell><cell>Number of labels for each evolution track</cell><cell>5</cell></row><row><cell>k</cell><cell>Number of recommended publications</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Dataset statistics for evaluations.</figDesc><table><row><cell cols="2">Dataset Papers 1</cell><cell>Retrieved References 2</cell><cell>Citation Links 3</cell></row><row><cell>KDD</cell><cell>534</cell><cell>126,499</cell><cell>1,663,063</cell></row><row><cell>ACL</cell><cell>679</cell><cell>88,876</cell><cell>3,202,684</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Neighborhood Similarity Experiment</cell><cell></cell></row><row><cell>Method</cell><cell>KDD</cell><cell>ACL</cell></row><row><cell>TF-IDF 1</cell><cell>0.50</cell><cell>0.49</cell></row><row><cell>S-BERT 2</cell><cell>0.41</cell><cell>0.36</cell></row><row><cell>ProNE 3</cell><cell>0.72</cell><cell>0.75</cell></row><row><cell>node2vec 4</cell><cell>0.65</cell><cell>0.64</cell></row><row><cell>TF-IDF+S-BERT</cell><cell>0.41</cell><cell>0.36</cell></row><row><cell>TF-IDF+ProNE</cell><cell>0.78</cell><cell>0.79</cell></row><row><cell>S-BERT+ProNE</cell><cell>0.75</cell><cell>0.77</cell></row><row><cell>TF-IDF+S-BERT+ProNE</cell><cell>0.81</cell><cell>0.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 Inverse</head><label>5</label><figDesc>Label Distance and Overlap Rate for labeling</figDesc><table><row><cell>Method</cell><cell cols="4">ILD KDD ACL KDD ACL Overlap</cell></row><row><cell>Baseline</cell><cell>0.68</cell><cell>0.69</cell><cell>0.14</cell><cell>0.21</cell></row><row><cell>Proposed Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>µ = 0.8, = 0.1</cell><cell>0.75</cell><cell>0.71</cell><cell>0.13</cell><cell>0.16</cell></row><row><cell>µ = 0.0, = 0.1</cell><cell>0.78</cell><cell>0.73</cell><cell>0.40</cell><cell>0.43</cell></row><row><cell>µ = 0.8, = 0.0</cell><cell>0.73</cell><cell>0.69</cell><cell>0.11</cell><cell>0.14</cell></row><row><cell>µ = 0.8, = 0.5</cell><cell>0.79</cell><cell>0.76</cell><cell>0.24</cell><cell>0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">Importance Evaluation with User Click</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Spearman NDCG@5 NDCG@20</cell></row><row><cell>Citation Number</cell><cell>-0.23</cell><cell>0.19</cell><cell>0.28</cell></row><row><cell>Out-degrees</cell><cell>-0.15</cell><cell>0.21</cell><cell>0.36</cell></row><row><cell>In-degrees</cell><cell>0.36</cell><cell>0.56</cell><cell>0.65</cell></row><row><cell>PageRank</cell><cell>0.38</cell><cell>0.61</cell><cell>0.70</cell></row><row><cell>Importance Score</cell><cell>0.41</cell><cell>0.87</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Average Rewards for Dynamic Recommendation</cell></row><row><cell>Roadmap*</cell><cell cols="2">Models Baseline REINFORCE</cell></row><row><cell>BERT</cell><cell>0.32</cell><cell>0.66</cell></row><row><cell>GAN</cell><cell>0.28</cell><cell>0.40</cell></row><row><cell>ResNet</cell><cell>0.67</cell><cell>0.78</cell></row><row><cell>GraphSage</cell><cell>0.75</cell><cell>0.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Details for evolution tracks in the BERT roadmap</figDesc><table><row><cell></cell><cell>Evolution Track Labels</cell><cell>Importance</cell></row><row><cell>1</cell><cell>Natural Language, Language Model,</cell><cell>45.88</cell></row><row><cell></cell><cell>Language Inference, Wide Range, NLP Task</cell><cell></cell></row><row><cell>2</cell><cell>Language Model, Natural Language,</cell><cell>19.91</cell></row><row><cell></cell><cell>Neural Network, Vector Space, Learning Algorithm</cell><cell></cell></row><row><cell>3</cell><cell>Reading Comprehension,</cell><cell>14.90</cell></row><row><cell></cell><cell>Question Answering, Model Achieves,</cell><cell></cell></row><row><cell></cell><cell>Stanford Question Answering, Natural Language</cell><cell></cell></row><row><cell>4</cell><cell>Machine Translation, Source Sentence,</cell><cell>12.87</cell></row><row><cell></cell><cell>Neural Network, Language Model,</cell><cell></cell></row><row><cell></cell><cell>Neural Machine Translation</cell><cell></cell></row><row><cell>5</cell><cell>Neural Network, Deep Convolutional,</cell><cell>5.84</cell></row><row><cell></cell><cell>Deep Convolutional Neural, Object Recognition,</cell><cell></cell></row><row><cell></cell><cell>Computer Vision</cell><cell></cell></row><row><cell>6</cell><cell>Deep Architecture, Learning Deep,</cell><cell>4.00</cell></row><row><cell></cell><cell>Unsupervised Learning, Learning Algorithm,</cell><cell></cell></row><row><cell></cell><cell>Task Including</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is supported by the National Key R&amp;D Program of China (2018YFB1402600), NSFC for Distinguished Young Scholar (61825602), and NSFC (61836013).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An overview of scientific and scholarly publishing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>The stm report. 5th edition October</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining algorithm roadmap in scientific publications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Taxogen: Constructing topical concept taxonomy by adaptive term embedding and clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constructing and mining heterogeneous information networks from massive text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
	</analytic>
	<monogr>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Maskgan: better text generation via filling in the</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07736</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying meaningful citations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valenzuela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop: Scholarly Big Data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring academic influence: Not all citations are equal</title>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lemire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vellino</surname></persName>
		</author>
		<idno>abs/1501.06587</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural scaffolds for citation intent classification in scientific publications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identifying referential intention with heterogeneous contexts</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting topic evolution in scientific literature: how can citations help</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
				<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for long document classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2004">2004.05150, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>in WWW &apos;15</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Netsmf: Large-scale network embedding as sparse matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1906">1906.11156, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prone: fast and scalable network representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Joint Conf</title>
				<meeting>28th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised graph clustering: a kernel approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cluster labeling by word embeddings and WordNet&apos;s hypernymy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poostchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/U18-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
				<meeting>the Australasian Language Technology Association Workshop<address><addrLine>Dunedin, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">2018. Dec. 2018</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic labeling of multinomial topic models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;07</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1312.5602</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for list-wise recommendations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00209</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Top-k off-policy correction for a reinforce recommender system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in WSDM &apos;19</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Drn: A deep reinforcement learning framework for news recommendation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The use of mmr, diversitybased reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carbinell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein-Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="209" to="210" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Construction of the literature graph in semantic scholar</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skjonsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<idno>abs/1805.02262</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName><forename type="first">L</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Biostatistics: A Foundation for Analysis in the Health Sciences</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>The spearman rank correlation coefficient</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">On the shortest spanning subtree of a graph and the traveling salesman problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
