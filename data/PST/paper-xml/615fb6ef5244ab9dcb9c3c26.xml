<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GNN IS A COUNTER? REVISITING GNN FOR QUESTION ANSWERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-07">7 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
							<email>kuanwang@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Song</surname></persName>
							<email>lsong@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Biomap</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Mbzuai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GNN IS A COUNTER? REVISITING GNN FOR QUESTION ANSWERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-07">7 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.03192v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Essential</term>
					<term>Efficient</term>
					<term>Effective Effective Efficient Interpretable</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies have been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs). However, many problems remain open regarding the reasoning functionality of these GNN-based modules. Can these GNN-based modules really perform a complex reasoning process? Are they under-or overcomplicated for QA? To open the black box of GNN and investigate these problems, we dissect state-of-the-art GNN modules for QA and analyze their reasoning capability. We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning. Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Accessing and reasoning over relevant knowledge is the key to Question Answering (QA). Such knowledge can be implicitly encoded or explicitly stored in structured knowledge graphs (KGs). Large pre-trained language models <ref type="bibr" target="#b9">(Devlin et al., 2018;</ref><ref type="bibr" target="#b39">Radford et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b6">Brown et al., 2020)</ref> are found to be effective in learning broad and rich implicit knowledge <ref type="bibr" target="#b38">(Petroni et al., 2019;</ref><ref type="bibr" target="#b5">Bosselut et al., 2019;</ref><ref type="bibr" target="#b52">Talmor et al., 2020)</ref> and thus demonstrate much success for QA tasks. Nevertheless, pretrained LMs struggle a lot with structured reasoning such as handling negation <ref type="bibr" target="#b43">(Ribeiro et al., 2020;</ref><ref type="bibr">Yasunaga et al., 2021)</ref>. In contrast, the explicit knowledge such as knowledge graphs (KGs) <ref type="bibr" target="#b47">(Speer et al., 2017;</ref><ref type="bibr" target="#b4">Bollacker et al., 2008)</ref> works better for structured reasoning as it explicitly maintains specific information and relations and often produces interpretable results such as reasoning chains <ref type="bibr" target="#b17">(Jhamtani &amp; Clark, 2020;</ref><ref type="bibr" target="#b19">Khot et al., 2020;</ref><ref type="bibr" target="#b8">Clark et al., 2020)</ref>.</p><p>To utilize both implicit and explicit knowledge for QA, many existing works combine large pre-trained LMs with Graph Neural Networks (GNNs; <ref type="bibr" target="#b45">Scarselli et al. (2008)</ref>; <ref type="bibr" target="#b22">Kipf &amp; Welling (2016)</ref>; <ref type="bibr" target="#b54">Veličković et al. (2017)</ref>), which are shown to achieve prominent QA performance. These approaches commonly follow a two-step paradigm to process KGs: 1) schema graph grounding and 2) graph modeling for inference. In Step 1, a schema graph is a retrieved sub-graph of KG related to the QA context and grounded on concepts; such sub-graphs include nodes with concept text, edges with relation types, and their adjacency matrix. In Step 2, graph modeling is carried out via an elaborately designed graph-based neural module. For instance, <ref type="bibr" target="#b26">Lin et al. (2019)</ref> uses GCN-LSTM-HPA which combines graph convolutional networks <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2016)</ref> and LSTM <ref type="bibr" target="#b15">(Hochreiter &amp; Schmidhuber, 1997)</ref> 1 Figure <ref type="figure">1</ref>: We analyze state-of-the-art GNN modules for the task of KG-powered question answering, and find that the counting of edges in the graph plays an essential role in knowledge-aware reasoning. Accordingly, we design an efficient, effective and interpretable graph neural counter module for knowledge-aware QA reasoning.</p><p>with a hierarchical path-based attention mechanism for path-based relational graph representation. <ref type="bibr" target="#b10">Feng et al. (2020)</ref> extends the single-hop message passing of RGCN <ref type="bibr" target="#b46">(Schlichtkrull et al., 2018)</ref> as multi-hop message passing with structured relational attention to obtain the path-level reasoning ability and intractability, while keeping the good scalability of GNN. <ref type="bibr">Yasunaga et al. (2021)</ref> uses a LM to encode QA context as a node in the scheme graph and then utilized graph attention networks <ref type="bibr" target="#b54">(Veličković et al., 2017)</ref> to process the joint graph.</p><p>Given that today's QA systems have become more and more complicated, we would like to revisit those systems and ask several basic questions: Are those GNN-based modules under-or overcomplicated for QA? What is the essential role they play in reasoning over knowledge? To answer these questions, we first analyze current state-of-the-art GNN modules for QA and their reasoning capability. Building upon our analysis, we then design a simple yet effective graph-based neural counter that achieves even better QA performance on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning.</p><p>In the analysis part, we employ Sparse Variational Dropout (SparseVD; <ref type="bibr" target="#b35">Molchanov et al. (2017)</ref>) as a tool to dissect existing graph network architectures. SparseVD is proposed as a neural model pruning method in the literature, and its effect of model compressing serves as an indicator to figure out which part of the model can be pruned out without loss of accuracy. We apply SparseVD to the inner layers of GNN modules, using their sparse ratio to analyze each layer's contribution to the reasoning process. Surprisingly, we find that those GNN modules are over-parameterized: some layers in GNN can be pruned to a very low sparse ratio, and the initial node embeddings are dispensable.</p><p>Based on our observations, we design Graph Soft Counter (GSC), a very simple graph neural model which basically serves as a counter over the knowledge graph. The hidden dimension of GSC layers is only 1, thus each edge/node only has a single number as the hidden embedding for graph-based aggregation. As illustrated in Figure <ref type="figure">1</ref>, GSC is not only very efficient but also interpretable, since the aggregation of 1-dimensional embedding can be viewed as soft counting of edge/node in graphs. Although GSC is designed to be a simplistic model, which has less than 1% trainable parameters compared to existing GNN modules for QA, it outperforms state-of-the-art GNN counterparts on two popular QA benchmark datasets. Our work reveals that the existing complex GNN modules may just perform some simple reasoning such as counting in knowledge-aware reasoning.</p><p>The key contributions of our work are summarized as follows:</p><p>• Analysis of existing GNN modules: We employ SparseVD as a diagnostic tool to analyze the importance of various parts of state-of-the-art knowledge-aware GNN modules. We find that those GNN modules are over-complicated for what they can accomplish in the QA reasoning process.  We adapt SparseVD as a diagnostic tool to dissect GNN-based reasoning modules for QA, getting the sparse ratio of each layer to indicate its importance. We find that some layers and ingredients are completely dispensable, which inspires us to design a simple, efficient and effective GNN module as the replacement of existing complex GNN modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRELIMINARIES</head><p>The knowledge required by QA systems typically comes from two sources: implicit knowledge in pre-trained language models and explicit knowledge in knowledge graphs.</p><p>To use the implicit knowledge, existing works commonly use LMs as the encoder to encode textual input sequence x to contextualized word embeddings, and then pool the embedding of the start token (e.g., <ref type="bibr">[CLS]</ref> for BERT) as the sentence embedding. In addition, a MLP (we use a one layer fully connected layer) is used to map the sentence embedding to the score for the choice.</p><p>To process the explicit knowledge in knowledge graphs, existing works commonly follow a two-step paradigm: schema graph grounding and graph modeling for inference. The schema graph is a retrieved sub-graph of KG grounding on concepts related to the QA context. We define the sub-graph as a multi-relational graph G = (V, E), where V is the set of entity nodes (concepts) in the KG and E ⊆ V × R × V is the set of triplet edges that connect nodes in V with relation types in R. Following prior works <ref type="bibr" target="#b26">(Lin et al., 2019;</ref><ref type="bibr">Yasunaga et al., 2021)</ref>, we link the entities mentioned in the question q and answer choice a ∈ C to the given sub-graph G.</p><p>A wealth of existing works have explored to incorporate pre-trained language models with knowledgeaware graph neural modules. These methods usually have complex architecture design to process complex input knowledge. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, the processed retrieved sub-graph can be summarized as four main components: Node embeddings. Many existing works on GNN-based QA employ external embeddings to initialize the node embeddings in the graph. For example, <ref type="bibr" target="#b26">Lin et al. (2019)</ref> employs TransE <ref type="bibr" target="#b58">(Wang et al., 2014)</ref> with GloVe embeddings <ref type="bibr" target="#b37">(Pennington et al., 2014)</ref> as the initial node embeddings. <ref type="bibr" target="#b10">Feng et al. (2020)</ref> and <ref type="bibr">Yasunaga et al. (2021)</ref> use mean-pooled BERT embeddings of entities across the graph as the initialization of node embeddings.</p><p>Relevance score. In order to measure the quality of a path and prune the sub-graph, <ref type="bibr" target="#b26">Lin et al. (2019)</ref> decomposes the KG into a set of triples, the relevance score of which can be directly measured 1 1) the left plot shows the curves for the embedding layers in QA-GNN, where the node score and the node type reach a low ratio, while the edge encoder preserves a large ratio; 2) the middle plot shows the curves for the layers within the QA-GNN graph attention layer, where the key and query layers converge to a fairly low ratio, while the value layer has a relatively large ratio; 3) the right plot shows the curves of the initial node embedding layers of three representative GNN-based QA methods, where all the ratios are close to 0, indicating that the initial node embeddings are dispensable.</p><p>by the scoring function of the graph embeddings. <ref type="bibr">Yasunaga et al. (2021)</ref> scores the relevance between the retrieved concept and the QA context by the masked LM loss of the stitched sequence. The relevance score determines the priority of the retrieved concepts and also be used as the node score embedding in the input node features.</p><p>Adjacency matrix. The original adjacency matrix of the sub-graph is asymmetric. Typically, the adjacency matrix is converted to be symmetric before feeding into the GNN module: each KG relation is double recorded with opposite directions as symmetric edges in the processed adjacency matrix.</p><p>Edge embeddings. Existing works use various ways to obtain the edge embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DISSECTION</head><p>To investigate the mechanism of these complex systems and how they use the complex information, we introduce a neural model pruning method named Sparse Variational Dropout (SparseVD) <ref type="bibr" target="#b35">(Molchanov et al., 2017)</ref> as a diagnostic tool to automatically dissect the graph network architecture. Note that our dissection tool is pruning method agnostic; other pruning schemes <ref type="bibr" target="#b11">(Han et al., 2015;</ref><ref type="bibr" target="#b13">He et al., 2017;</ref><ref type="bibr" target="#b30">Liu et al., 2017)</ref> may also be applicable. Here we choose SparseVD since it prunes not only the weights with smaller scale but also the weights with higher variance, and it is theoretically supported by stochastic variational inference <ref type="bibr" target="#b16">(Hoffman et al., 2013;</ref><ref type="bibr" target="#b20">Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b42">Rezende et al., 2014;</ref><ref type="bibr" target="#b21">Kingma et al., 2015)</ref>.</p><p>Despite the fact that SparseVD was originally proposed in the field of model compression <ref type="bibr" target="#b12">(Han et al., 2016;</ref><ref type="bibr" target="#b14">He et al., 2018;</ref><ref type="bibr" target="#b27">Lin et al., 2017;</ref><ref type="bibr" target="#b63">Zhou et al., 2018;</ref><ref type="bibr" target="#b55">Wang et al., 2018)</ref>, we use it to investigate which parts of GNN can be pruned out (sparse ratio to zero) without loss of accuracy, which indicates that part of the model is redundant. To be specific, we keep the target model architecture unchanged, and parameterize each part of the weights in the model as a Gaussian distribution. Afterwards, this probabilistic model will be trained with a cross-entropy loss jointly with a KL-divergence regularization term. So the joint loss constrains the weights to our pruning prior. We implement the SparseVD with the default threshold as in <ref type="bibr" target="#b35">Molchanov et al. (2017)</ref>. Eventually, we get the pruned model with different sparsified ratios for the layers. As shown in Table <ref type="table">1</ref>, we investigate three representative GNN-based QA methods, and the sparsified models could achieve the accuracy of their original counterparts, which indicates that our dissection regularization does not hurt the models, so that we can dissect into each layer to see what helps the model to do reasoning.</p><p>w/o SparseVD w/ SparseVD Methods IHdev-Acc. (%) IHtest-Acc. (%) IHdev-Acc. (%) IHtest-Acc. (%) KagNet <ref type="bibr" target="#b26">(Lin et al., 2019)</ref> 73.47 (±0.22) 69.01 (±0.76) 75.18 (±1.05) 70.48 (±0.77) MHGRN <ref type="bibr" target="#b10">(Feng et al., 2020)</ref> 74.45 (±0.10) 71.11 (±0.81) 77.15 (±0.32) 72.66 (±0.61) QAGNN <ref type="bibr">(Yasunaga et al., 2021)</ref> 76  <ref type="bibr">(left)</ref>. The GSC layers are parameter-free and only keep the basic graph operations: 1) update each edge embedding with incoming node (in-node) embeddings; 2) update each node embedding by aggregating the edge embeddings. Since we reduce the hidden dimension to only 1, GSC can be viewed as a soft counter over the graph for generating the final score of the output node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">OBSERVATIONS AND HYPOTHESIS</head><p>In Figure <ref type="figure" target="#fig_1">3</ref>, we plot the sparse ratio of some representative layers during the SparseVD training of GNN reasoning modules. Due to the limited space, refer to Appendix A.1 for the full plots of curves. According to thees plots, we summarize our key observations as follows: 1) the left plot shows that the edge encoder that encodes edge triplets (edge/node type information) preserves a relatively higher sparse ratio, while the node score embedding layer can be fully pruned; 2) the middle plot shows the layers inside GNN, and all the sparse ratios are low while the value layer has a relatively higher sparse ratio than key/query layers; 3) the right plot shows the concept embedding layers of three representative GNN methods, which process the initial node embeddings, can be completely discarded. Inspired by these findings, we come up with the following design guidelines for a much simpler yet effective graph neural module for knowledge-aware reasoning: Node embeddings. The initial node embedding and the extra score embedding are shown to be dispensable in these scenarios, thus we can directly remove the embedding layers. Edge embeddings. The edge encoder layers are hard to prune which indicates that edge/node type information are essential to reasoning, so we further leverage it to get a better representation.</p><p>Algorithm 1 PyTorch-style code of GSC # qa_context: question answer pair context # adj: edge index with shape 2 x N_edge # edge_type: edge type with shape 1 x N_edge # node_type: node type with shape 1 x N_node edge_emb = edge_encoder(adj, edge_type, node_type) node_emb = torch.zeros(N_node) for i_layer in range(num_gsc_layers):</p><p># propagate from node to edge edge_emb += node_emb[adj <ref type="bibr">[1]</ref>] # aggregate from edge to node node_emb = scatter(inputs, adj[0]) graph_score = node_emb[0] context_score = fc(roberta(qa_context)) qa_score = context_score + graph_score Message passing layers. The linear layers inside GNN layers (query, key, value, etc) can be pruned to a very low sparse ratio, suggesting that GNN may be overparameterized in these systems, and we can use fewer parameters for these layers. Graph pooler. The final attention-based graph pooler aggregates the node representation over the graph to get a graph representation. We observe that the key/query layers inside the pooler can be pruned out; as a result, the graph pooler can be reduced to a linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH SOFT COUNTER</head><p>Based on our findings in Section 2, we design the Graph Soft Counter (GSC), a simplistic graph neural module to process the graph information. As demonstrated in Algorithm 1, there are only two basic components in GSC to compute the graph score: Edge encoder and Graph Soft Counter layers. We can get the QA choice score by simply summing up the graph score and context score.</p><p>KagNet MHGRN QAGNN GSC (Ours)</p><formula xml:id="formula_0">Adj-matrix Edge-type Node-type × Node-embedding × Relevance-score × × × #Learnable Param 700k 547k 2845k 3k Model size 819M 819M 821M<label>3k</label></formula><p>Table 2: Our GSC do not use node embedding and node score versus the previous works, which makes our model size is extremely small (without counting LM). And our model do not have learnable parameter except the edge encoder, so our method has much less chance to be over-parameterized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Time Space</head><p>G is a dense graph</p><formula xml:id="formula_1">L-hop KagNet O |R| L |V| L+1 L O |R| L |V| L+1 L • D L-hop MHGRN O |R| 2 |V| 2 L O (|R||V|L • D) L-layer QAGNN O |V| 2 L O (|R||V|L • D) L-layer GSC O (|V|L) O (|R||V|L)</formula><p>G is a sparse graph with maximum node degree ∆ |V| Graph Soft Counter layers are parameter-free and only keep the basic graph operations: Propagation and aggregation. To overcome overparameterization, one straightforward way is to reduce the hidden size, and the extreme case is reducing it to only 1. As shown in Figure <ref type="figure">4</ref>, the GSC layer simply propagates and aggregates the numbers on the edges and node following the two-step scheme: 1) update the edge value with in-node value, and this is done by simply indexing and adding; 2) update the node value by aggregating the edge values, and this is done by scattering edge values to out-node.</p><formula xml:id="formula_2">L-hop KagNet O |R| L |V|L∆ L O |R| L |V|L∆ L • D L-hop MHGRN O |R| 2 |V|L∆ O (|R||V|L • D) L-layer QAGNN O (|V|L∆) O (|R||V|L • D) L-layer GSC O (|V|L) O (|R||V|L)</formula><p>Since we reduce the hidden size to only 1, there are basically numbers propagating and aggregating on the graph. We can interpret these numbers as soft counts representing the importance of the edges and nodes. The aggregation of GSC can be regarded as accumulating the soft counts, so call it Graph Soft Counter. In addition, we can also formulate GSC as an extremely efficient variant of current mainstream GNN such as GAT and GCN. As shown in Table <ref type="table" target="#tab_3">3</ref>, the computation complexity of GSC is much smaller than the baselines regard to both time and space. Table <ref type="table">2</ref> showing the trainable parameters in GSC is remarkably less than previous methods since our message-passing layers are parameter-free. The model size of GSC is also extremely small since we do not use any initial node embedding.</p><p>To draw a more conclusive conclusion, we handcraft a counting-based feature and use a simple two-layer MLP to map it to graph score. The feature is computed by counting the occurrence of each possible edge triplet. Surprisingly, this simple model could achieve similar performance and can even outperform baselines, which further proves that Counter is a basic and crucial part in QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>4.1 SETTINGS Datasets and KG. We conduct extensive experiments on CommonsenseQA <ref type="bibr" target="#b51">(Talmor et al., 2018)</ref> and OpenBookQA <ref type="bibr" target="#b33">(Mihaylov et al., 2018)</ref>, two popular QA benchmark datasets that heavily rely on knowledge-aware reasoning capability. CommonsenseQA is a 5-way multiple choice QA task that requires reasoning with commonsense knowledge, which contains 12,102 questions. The test set of CommonsenseQA is not publicly available, and the model predictions can only be evaluated once every two weeks via the official leaderboard. Hence, following the data split of <ref type="bibr" target="#b26">Lin et al. (2019)</ref>, we experiment and report the accuracy on the in-house dev (IHdev) and test (IHtest) splits. We also report the accuracy of our final system on the official test set. OpenBookQA is a 4-way multiple choice QA task that requires reasoning with elementary science knowledge, which contains 5,957 questions. Following <ref type="bibr" target="#b7">Clark et al. (2019)</ref>, methods with AristoRoBERTa use textual evidence as an external input of the QA context. In addition, we use ConceptNet <ref type="bibr" target="#b47">(Speer et al., 2017)</ref>, a general commonsense knowledge graph, as our structured knowledge source G for both of the above tasks. Given each QA context (question and answer choice), we retrieve the sub-graph G sub from G following <ref type="bibr" target="#b10">Feng et al. (2020)</ref>. We only use the concepts that occur in the QA context, as detailed in Table <ref type="table" target="#tab_11">8</ref>.  <ref type="bibr" target="#b57">(Wang et al., 2019)</ref> 72.61( ±0.39) 68.59 (±0.96) + KagNet <ref type="bibr" target="#b26">(Lin et al., 2019)</ref> 73.47 (±0.22) 69.01 (±0.76) + RN <ref type="bibr" target="#b44">(Santoro et al., 2017)</ref> 74.57 (±0.91) 69.08 (±0.21) + MHGRN (Feng et al., 2020)  74.45 (±0.10) 71.11 (±0.81) + QAGNN <ref type="bibr">(Yasunaga et al., 2021)</ref> 76   <ref type="bibr" target="#b31">(Lv et al., 2020)</ref> 75.3 RoBERTa + MHGRN <ref type="bibr" target="#b10">(Feng et al., 2020)</ref> 75.4 ALBERT + PG <ref type="bibr" target="#b56">(Wang et al., 2020)</ref> 75.6 RoBERTa + QA-GNN <ref type="bibr">(Yasunaga et al., 2021)</ref> 76.1 ALBERT <ref type="bibr">(Lan et al., 2019) (ensemble)</ref> 76.5 UnifiedQA (11B) * <ref type="bibr" target="#b18">(Khashabi et al., 2020)</ref> 79.1</p><p>RoBERTa + GSC (Ours) 76.2</p><p>Table <ref type="table">5</ref>: Test accuracy on CommonsenseQA's official leaderboard. The previous top system, Uni-fiedQA (11B params) is 30x larger than our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines.</head><p>For GSC and all the other methods, RoBERTa-large <ref type="bibr" target="#b29">(Liu et al., 2019)</ref> is used for both CommonsenseQA and OpenBookQA, and AristoRoBERTa <ref type="bibr" target="#b7">(Clark et al., 2019)</ref> is used for OpenBookQA as an additional setting for fair comparison. We experiment to compare GSC with existing GNN-based QA methods, including RN <ref type="bibr" target="#b44">(Santoro et al., 2017)</ref>, GconAttn <ref type="bibr" target="#b57">(Wang et al., 2019)</ref>, RGCN <ref type="bibr" target="#b46">(Schlichtkrull et al., 2018)</ref>, KagNet <ref type="bibr" target="#b26">(Lin et al., 2019)</ref>, MHGRN <ref type="bibr" target="#b10">(Feng et al., 2020)</ref> and QA-GNN <ref type="bibr">(Yasunaga et al., 2021)</ref>, which only differ in the design of GNN reasoning modules. We report the performance of baselines referring to <ref type="bibr">Yasunaga et al. (2021)</ref> and all the test results are evaluated on the best model on the dev split.   <ref type="bibr">(2019)</ref> as an additional input to the QA context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>CommonsenseQA. As shown in Table <ref type="table" target="#tab_6">4</ref>, GSC outperforms the previous best model with 2.57% mean accuracy on In-house dev split and 1.07% mean accuracy on the inhouse test split. We observe that the performance variance of GSC is smaller than the baselines, indicating that our GSC are both effective and stable. On the official leaderboard of CommonsenseQA in Table <ref type="table">5</ref>, GSC also outperforms all the GNN-based QA systems. Note that the previous top system UnifiedQA (11B params) uses T5 <ref type="bibr" target="#b41">(Raffel et al., 2020)</ref> as the pre-trained LM model, which is 30x larger than our model and uses much more pre-training data.</p><p>OpenBookQA. From Table <ref type="table" target="#tab_8">6</ref> our GSC outperforms the previous best model with 2.53% mean test accuracy with normal setting and 3.9% test accuracy with the AristoRoBERTa setting. More remarkably, as shown in Table <ref type="table">7</ref>, our GSC ranks top one on the official leaderboard of OpenBookQA, which even surpasses the performance of UnifiedQA (11B), which is 30x larger than our model.  RoBERTa as text encoder. The order of the percentage of overlap of left four exactly matches the order of the performance of each models: GSC &gt; QA-GNN &gt; MHGRN &gt; RoBERTa. This indicates that the reasoning ability learned by our GSC basically covers that of other GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Test</head><p>Careful Selection <ref type="bibr" target="#b2">(Banerjee et al., 2019)</ref> 72.0 AristoRoBERTa 77.8 KF + SIR <ref type="bibr" target="#b1">(Banerjee &amp; Baral, 2020)</ref> 80.0 AristoRoBERTa + PG <ref type="bibr" target="#b56">(Wang et al., 2020)</ref> 80.2 AristoRoBERTa + MHGRN <ref type="bibr" target="#b10">(Feng et al., 2020)</ref> 80.6 ALBERT + KB 81.0 AristoRoBERTa + QA-GNN 82.8 T5 * <ref type="bibr" target="#b41">(Raffel et al., 2020)</ref> 83.2 UnifiedQA(11B) * <ref type="bibr" target="#b18">(Khashabi et al., 2020)</ref> 87.2</p><p>AristoRoBERTa + GSC (Ours) 87.4</p><p>Table <ref type="table">7</ref>: Test accuracy on OpenBookQA leaderboard. All listed methods use the provided science facts as an additional input to the language context. The previous top 2 systems, UnifiedQA (11B params) and T5 (3B params) are 30x and 8x larger than our model.</p><p>As mentioned above, we observe that the initial node embeddings are dispensable. Then we start to explore how the maximum number of retrieved nodes (also related to edges) affects the model. We experiment various numbers of nodes for our GSC, and summarize the results in Table <ref type="table" target="#tab_11">8</ref>. We find that larger maximum number of node does not benefit the model, and the model achieves the best performance when we use all and only the entity nodes directly occur in question and answer, whose number of nodes is generally less than 32. This indicates that 1-hop retrieval is adequate for our methods, and this could be done super efficiently than multi-hop retrieval.</p><p>To further analyze the reasoning capacity of GSC, we draw the Venn diagram for the predictions of different methods and ground truth (GT) in Figure <ref type="figure" target="#fig_2">5</ref>. We find that even for the different runs of the same GSC model, the correct overlap is only 69%, showing that the datasets are relatively noisy and there exists decent variance in the prediction results. We also observe that GSC has a larger overlap for GNN-based systems (e.g., QA-GNN, MHGRN), while at the same time having less overlap for non-GNN methods. The ALBERT model has the least overlap since all the other methods use RoBERTa as the text encoder.   <ref type="bibr">(upper)</ref> and the maximum number of retrieved nodes (bottom), showing that 1) the hard counter achieves performance on par with GNN-based methods; 2) GSC works well even when only using entities occurred in QA context, which typically contains less than 32 nodes.</p><p>We also observe that the order of the percentage of overlap of left four exactly matches the order of the performance of each model: GSC &gt; QA-GNN &gt; MHGRN &gt; RoBERTa. This indicates that GSC has quite similar behaviors versus other GNNbased systems, and the reasoning capability of GSC is on par with existing GNN counterparts. This further reveals that counting plays an essential role in knowledge-aware reasoning for QA.</p><p>To verify our observations, we handcraft a hard edge counting feature feeding into a simple two-layer MLP. As shown in Table 8, this hard counting model with 2-hop edge feature could achieve a comparable performance of GSC, which even outperforms other GNN baselines. These impressive results not only show that our GSC is effective, but also prove that counting is an essential functionality in the process of the current knowledge-aware QA systems. As shown in Figure <ref type="figure">6</ref>, for the retrieved sub-graph of each answer choice, we can directly observe the behavior of the GSC by printing out the output edge/node values (soft count) of each layer. In this way, we can trace back to see why the model scores the answers like that. We list the runtime soft counts of GSC's edge encoder in Appendix A.3. A higher soft count means that the edge/node is more important, and can contribute more to the final graph score. This demonstrates the advantage of GSC as an interpretable reasoning module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>KG-powered QA. For KG-powered QA, traditional methods use semantic parsers to map the question to logical form <ref type="bibr" target="#b3">(Berant et al., 2013;</ref><ref type="bibr" target="#b61">Yih et al., 2015)</ref> or executable program on KG <ref type="bibr" target="#b25">(Liang et al., 2016)</ref>, which typically require domain-specific rules and fine-grained annotations, making them difficult to handle the noise in questions and KG. To address these challenges, various deep neural models have been applied to KG-powered QA, such as memory networks <ref type="bibr" target="#b23">(Kumar et al., 2015;</ref><ref type="bibr" target="#b49">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b34">Miller et al., 2016)</ref>, neural programmer for knowledge table QA <ref type="bibr" target="#b36">(Neelakantan et al., 2016)</ref>, neural knowledge-aware reader <ref type="bibr" target="#b59">(Xiong et al., 2019;</ref><ref type="bibr" target="#b50">Sun et al., 2019)</ref>, etc.</p><p>GNN for KG-powered QA. To further improve the neural reasoning capability, recent studies have explored applying GNNs to KG-powered QA, where GNNs naturally fit the graph-structured knowledge and show prominent results. KagNet <ref type="bibr" target="#b26">(Lin et al., 2019)</ref> proposes GCN-LSTM-HPA for path-based relational graph representation. MHGRN <ref type="bibr" target="#b10">(Feng et al., 2020)</ref> extend Relation Networks <ref type="bibr" target="#b44">(Santoro et al., 2017)</ref> to multi-hop relation scope and unifies both path-based models and <ref type="bibr">RGCN Schlichtkrull et al. (2018)</ref> to enhance the interpretability and the scalability. QA-GNN <ref type="bibr">(Yasunaga et al., 2021)</ref> proposes a LM+GAT framework to joint reasoning over language and KG. We do not use conventional GNNs and our GSC is extremely simple and efficient even without parameters inside the GSC layers. KG embeddings are generally used in these systems. <ref type="bibr" target="#b62">Zhang et al. (2018)</ref> employs propagation of KG embeddings to perform multi-hop reasoning. KagNet pre-trains KG Embedding using TransE initialized with GloVe embeddings. Recently, MHGRN and QA-GNN are proposed, which leverage pre-trained LM to generate embeddings for KG. In addition, QA-GNN also scores KG node relevance using LM as extra node embedding. We follow the data pre-processing procedure of QA-GNN and MHGRN, but our model does not use any KG embedding or node score embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We investigate state-of-the-art GNN-based QA systems, and discover that they are over-parameterized. Our diagnostic analysis using SparseVD shows that the initial node embeddings and some GNN layers are completely dispensable. Inspired by our observations, we design a much simpler yet effective counter-based model named Graph Soft Counter (GSC). Surprisingly, with less than 1% trainable parameters, our model outperforms state-of-the-art GNN counterparts on two popular QA benchmark datasets. Our work reveals that counting plays a crucial role in the knowledgeaware reasoning process. It remains as a challenging open problem to build more comprehensive human-level reasoning modules for QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 SPARSE RATIO CURVE</head><p>We attach the details of the training curve of SparseVD dissection for QA-GNN as following. We find the linear query and key layers inside GNN layers and graph pooler generally can be compressed to a relatively low sparse ratio and this is more obvious at the fist three GNN layers. This indicated the these layers may be over-parameterized and the attention mechanism <ref type="bibr" target="#b53">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b54">Veličković et al., 2017)</ref> may degenerate to linear function in this case if we remove these layers.</p><p>Figure <ref type="figure">7</ref>: The sparse ratio curve when do SparseVD training for the QA-GNN systems.</p><p>As expected, the final fully connected layer (FC) preserved the highest sparse ratio, since it serves for directly generating the overall score of a choice and it is very efficient (the output dimension is 1). Except that, the edge encoder with a two-layer MLP has very high sparse ratio, which can be explained as the edge encoding is essential for GNN reasoning in these tasks. This is also why we only keep the edge encoder with relatively more learnable parameters and simplify the other components in GNN to design our Graph Soft Counter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 OPENBOOKQA RESULTS</head><p>We find a small error on the Table <ref type="table" target="#tab_6">4</ref> of QA-GNN paper <ref type="bibr">(Yasunaga et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SOFT COUNT OF EDGES</head><p>We list the top-30 edge triplets with highest soft counts that encoded by the edge encoder of our GSC as following. These combinations of edge types and node types have relatively higher counts, which means they can contribute more to the final graph score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 SPARSEVD</head><p>To take a deep scope of the graph neural networks based knowledge-aware systems, we introduce a neural model pruning method Sparse Variational Dropout (SparseVD, <ref type="bibr" target="#b35">(Molchanov et al., 2017)</ref>) into this scenario as a dissection tool to automatically dissect the graph network architecture. We implemented the SparseVD optimization based layer refer to the PyTorch code ‡ released by the author. To keep the dissection in strict accordance with the theoretical derivation of <ref type="bibr" target="#b35">Molchanov et al. (2017)</ref>, we apply regulation to all the linear layers in the GNN modules. We attach the SparseVD problem formulation as well as the optimization target as following.</p><p>Consider a dataset D which is constructed from N pairs of objects (x n , y n ) N n=1 . Our goal is to tune the parameters w of a model p(y | x, w) that predicts y given x and w. In Bayesian Learning we usually have some prior knowledge about weights w, which is expressed in terms of a prior distribution p(w). After data D arrives, this prior distribution is transformed into a posterior distribution p(w | D) = p(D | w)p(w)/p(D). This process is called Bayesian Inference. Computing posterior distribution using the Bayes rule usually involves computation of intractable multidimensional integrals, so we need to use approximation techniques.</p><p>One of such techniques is Variational Inference. In this approach the posterior distribution p(w | D) is approximated by a parametric distribution q φ (w). The quality of this approximation is measured in terms of the Kullback-Leibler divergence D KL (q φ (w) p(w | D)). The optimal value of variational parameters φ can be found by maximization of the variational lower bound: L(φ) = L D (φ) − D KL (q φ (w) p(w)) → max (2)</p><p>It consists of two parts, the expected log-likelihood L D (φ) and the KL-divergence D KL (q φ (w) p(w)), which acts as a regularization term.</p><p>to a Binary Dropout rate p ij → 1 (recall α = p 1−p ). Intuitively it means that the corresponding weight is almost always dropped from the model. Therefore its value does not influence the model during the training phase and is put to zero during the testing phase.</p><p>We can also look at this situation from another angle. Infinitely large α ij corresponds to infinitely large multiplicative noise in w ij . It means that the value of this weight will be completely random and its magnitude will be unbounded. It will corrupt the model prediction and decrease the expected log likelihood. Therefore it is beneficial to put the corresponding weight θ ij to zero in such a way that α ij θ 2 ij goes to zero as well. It means that q(w ij | θ ij , α ij ) is effectively a delta function, centered at zero δ(w ij ).</p><formula xml:id="formula_3">θ ij → 0, α ij θ 2 ij → 0 ⇓ q(w ij | θ ij , α ij ) → N (w ij | 0, 0) = δ(w ij ) (4)</formula><p>In the case of linear regression this fact can be shown analytically. We denote a data matrix as X N ×D and α, θ ∈ R D . If α is fixed, the optimal value of θ can also be obtained in a closed form. θ = (X X + diag(X X)diag(α)) −1 X y</p><p>(5)</p><p>Assume that (X X) ii = 0, so that i-th feature is not a constant zero. Then from (5) it follows that θ i = Θ(α −1 i ) when α i → +∞, so both θ i and α i θ 2 i tend to 0. In our dissection scenario, we optimize the weight θ ij to zero no longer for compressing the model but for figuring out which part of the model can be pruned out (sparse ratio to zero) without loss of accuracy, which indicates that part of model is not important and its output information is unused in optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The retrieved sub-graph of KG is formulated as entity nodes representing concepts connected by edges representing relations and the central context node connected to all question entity nodes and answer entity nodes. The pre-processed graph data generally has the following ingredients: node embedding initialized with pre-trained KG embeddings, relevance score computed by LM, adjacency matrix representing the topological graph structure, edge embeddings to encode the node types, and edge information of relation types. We adapt SparseVD as a diagnostic tool to dissect GNN-based reasoning modules for QA, getting the sparse ratio of each layer to indicate its importance. We find that some layers and ingredients are completely dispensable, which inspires us to design a simple, efficient and effective GNN module as the replacement of existing complex GNN modules.</figDesc><graphic url="image-9.png" coords="3,262.71,222.68,84.15,54.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The sparse ratio curves obtained from the SparseVD training of GNN reasoning modules: 1) the left plot shows the curves for the embedding layers in QA-GNN, where the node score and the node type reach a low ratio, while the edge encoder preserves a large ratio; 2) the middle plot shows the curves for the layers within the QA-GNN graph attention layer, where the key and query layers converge to a fairly low ratio, while the value layer has a relatively large ratio; 3) the right plot shows the curves of the initial node embedding layers of three representative GNN-based QA methods, where all the ratios are close to 0, indicating that the initial node embeddings are dispensable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Venn diagrams for the prediction overlap of different models and ground truth (GT) of the IHtest split of CommonsenseQA. The ALBERT has the least overlap since the all left four use RoBERTa as text encoder. The order of the percentage of overlap of left four exactly matches the order of the performance of each models: GSC &gt; QA-GNN &gt; MHGRN &gt; RoBERTa. This indicates that the reasoning ability learned by our GSC basically covers that of other GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>q φ (w) [log p(y n | x n , w)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Importance of edge counting: We demonstrate that the counting of edges in the graph plays a crucial role in knowledge-aware reasoning, since our experiments show that even</figDesc><table><row><cell>Retrieved Sub-graph</cell><cell cols="2">Context Node</cell><cell cols="2">Multi-relational Edge</cell></row><row><cell cols="2">Initialized with LM/TransE</cell><cell></cell><cell cols="2">Encode as One-hot Triples</cell></row><row><cell cols="2">Question Entity Node</cell><cell cols="3">Answer Entity Node</cell></row><row><cell>Node Embedding</cell><cell>Relevance Score</cell><cell cols="2">Adjacency Matrix</cell><cell>Edge Embedding</cell></row><row><cell>GNN</cell><cell cols="2">Which ingredients are crucial?</cell><cell>GNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(Pruned)</cell></row><row><cell></cell><cell cols="2">SparseVD Dissection</cell><cell></cell></row><row><cell></cell><cell>Sparse Ratio</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">KL-loss</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell></row></table><note>a simple hard counting model can achieve QA performance comparable to state-of-the-art GNN-based methods.• Design of GSC module: We propose Graph Soft Counter (GSC), a simple yet effective neural module as the replacement for existing complex GNN modules. With less than 1% trainable parameters compared to existing GNN modules for QA, our method even outperforms those complex GNN modules on two benchmark QA datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 4: Graph Soft Counter (right) extremely simplifies the architecture of conventional GNNs</figDesc><table><row><cell cols="3">Graph Attention Layer</cell><cell></cell><cell cols="2">Graph Soft Counter Layer</cell><cell></cell><cell>1.2</cell><cell cols="2">output node</cell><cell>0.7</cell></row><row><cell></cell><cell>MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.4</cell><cell></cell></row><row><cell>sparsely</cell><cell>+ ×</cell><cell>aggregate</cell><cell></cell><cell>update node value</cell><cell>+</cell><cell>0.5</cell><cell></cell><cell cols="2">0.0 aggregate</cell></row><row><cell cols="2">Q SoftMax K ×</cell><cell>V</cell><cell>Adjacency Matrix</cell><cell></cell><cell></cell><cell>1.2</cell><cell>0.6</cell><cell>0.5 0.7</cell><cell>0.5 0.6</cell><cell>0.7</cell></row><row><cell></cell><cell>+</cell><cell>+</cell><cell>Edge Embedding</cell><cell>dimension = 1 update edge value</cell><cell>+</cell><cell>0.5 0.7</cell><cell>0.6 0.1</cell><cell>0.2 0.1 0.9</cell><cell>0.3 0.3 0.3</cell><cell>0.2 0.4</cell></row><row><cell cols="3">Node Embedding</cell><cell></cell><cell cols="2">Node Value</cell><cell cols="5">add in-node value to edge</cell></row><row><cell></cell><cell></cell><cell cols="2">SoftMax</cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>.54 (±0.21) 73.41 (±0.92) 77.64 (±0.50) 73.57 (±0.48) Table 1: To preserve the reasoning ability for analysis, our SparseVD tool prunes the GNN-based models without loss of accuracy on CommonsenseQA in-house split.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The edge encoder is a two-layer MLP with dimension [47 × 32 × 1] followed by a Sigmoid function to encode the edge triplets to a float number in the range of (0, 1). The triplets is represented as a concatenated one-hot vector [u s , e st , u t ], where u s , u t indicates 4 node types and e st indicates 38 relation types (17 regular relations plus question/answer entity relations and their reversions).</figDesc><table /><note>GSC is extremely efficient compared to the computation complexity of L-hop reasoning models with hidden dimension D on a dense / sparse graph G = (V, E) with the relation set R.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on CommonsenseQA in-house split (controlled experiments). As the official test is hidden, here we report the in-house dev (IHdev) and test (IHtest) accuracy, following the data split of<ref type="bibr" target="#b26">Lin et al. (2019)</ref>.</figDesc><table><row><cell>Implementation and training details.</cell><cell></cell></row><row><cell>We use a two-layer MLP with hidden di-</cell><cell></cell></row><row><cell>mension 47 × 32 × 1 followed by a Sig-</cell><cell></cell></row><row><cell>moid function as our edge encoder, and the</cell><cell></cell></row><row><cell>number of GSC layers is 2. Since GSC</cell><cell></cell></row><row><cell>is extremely efficient, we set the dropout</cell><cell></cell></row><row><cell>(Srivastava et al., 2014) rate to 0. We use</cell><cell></cell></row><row><cell>RAdam (Liu et al., 2020) as the optimizer</cell><cell></cell></row><row><cell>and set the batch size to 128. The learning</cell><cell></cell></row><row><cell>rate is 1e-5 for RoBERTa and 1e-2 for GSC.</cell><cell></cell></row><row><cell>The maximum number of epoch is set to</cell><cell></cell></row><row><cell>30 for CommonsenseQA and 75 for Open-</cell><cell></cell></row><row><cell>BookQA. On a single Quadro RTX6000</cell><cell></cell></row><row><cell>GPU, each GSC training only takes about</cell><cell></cell></row><row><cell>2 hours to converge, while other methods</cell><cell></cell></row><row><cell>often take 10+ hours.</cell><cell></cell></row><row><cell>Methods</cell><cell>Test</cell></row><row><cell>RoBERTa (Liu et al., 2019)</cell><cell>72.1</cell></row><row><cell cols="2">RoBERTa + FreeLB (Zhu et al., 2019) (ensemble) 73.1</cell></row><row><cell>RoBERTa + HyKAS (Ma et al., 2019)</cell><cell>73.2</cell></row><row><cell>RoBERTa + KE (ensemble)</cell><cell>73.3</cell></row><row><cell>RoBERTa + KEDGN (ensemble)</cell><cell>74.4</cell></row><row><cell>XLNet + GraphReason</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy on OpenBookQA. Methods with AristoRoBERTa use the textual evidence by Clark et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the hard counter with MLP</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Personal ficus could live in the front yard.Figure6: Our GSC is highly interpretable. For the retrieved sub-graph of each answer choice, we can directly observe the behaviour of the model by print out the output edge/node values of each layer, so that we can trace back to see how the model score the answer.</figDesc><table><row><cell cols="8">Where could a personal ficus live?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">A. front yard</cell><cell></cell><cell cols="5">B. cabin in the woods</cell><cell></cell><cell cols="2">C. california</cell><cell cols="5">D. conservatory</cell><cell cols="5">E. tropical forest</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">Personal ficus could live in the tropical forest.</cell></row><row><cell></cell><cell></cell><cell>QE</cell><cell></cell><cell></cell><cell>AE</cell><cell>AE</cell><cell></cell><cell>QE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>QE</cell><cell></cell><cell></cell><cell></cell><cell>AE</cell><cell>AE</cell><cell></cell><cell>AE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ficus</cell><cell cols="3">AtLocation AtLocation</cell><cell cols="2">yard</cell><cell>T y p eO f</cell><cell cols="4">personality CapableOf</cell><cell cols="7">tropical forest AtLocation ficus AtLocation</cell><cell cols="2">Ty pe O f</cell><cell cols="3">tropical Related</cell></row><row><cell cols="6">Sub-graph(Choice A)</cell><cell></cell><cell></cell><cell cols="4">front yard</cell><cell cols="5">Sub-graph(Choice E)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">forest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.6</cell><cell cols="2">2.6 &gt; 2.1</cell><cell></cell><cell></cell><cell></cell><cell>2.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.3</cell><cell></cell></row><row><cell>0.0</cell><cell>0.2 0.4</cell><cell>0.5</cell><cell>0.3</cell><cell>0.3</cell><cell>0.5 0.4</cell><cell>0.0</cell><cell>0.2 0.4</cell><cell>0.7</cell><cell>0.8</cell><cell>0.5</cell><cell>0.7 0.9</cell><cell>0.0</cell><cell>0.2 0.4</cell><cell>0.2</cell><cell>0.5</cell><cell>0.8</cell><cell>0.6 0.4</cell><cell>0.0</cell><cell>0.2 0.4</cell><cell>0.2</cell><cell>0.3</cell><cell>0.3</cell><cell>0.1 0.3</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell>0.3</cell><cell cols="2">0.5</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.5</cell><cell cols="2">0.7</cell><cell>A &gt; E</cell><cell>0.2</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.1</cell></row><row><cell cols="3">Layer-1 output</cell><cell></cell><cell>0.2</cell><cell></cell><cell cols="3">Layer-2 output</cell><cell cols="2">0.2</cell><cell></cell><cell cols="3">Layer-2 output</cell><cell></cell><cell>0.7</cell><cell></cell><cell cols="3">Layer-1 output</cell><cell></cell><cell>0.5</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>. According to the principle that the test results should be evaluated on the best model on the dev split, instead of directly picking the best model on test split. So we compute the result of QA-GNN on OpenBookQA dataset refer to the official worksheets † released by the author. We find this result is different from the result reported on the paper: We report [seed0: 70.20, seed1: 64.80, seed2: 68.40, mean: 67.80, std: 2.75] as the test results which are evaluated the best model on the dev split, while the original paper report [seed0: 70.20, seed1: 69.60, seed2: 71.80, mean: 70.53, std: 1.14] as the test results which are directly picking the best model on test split. We mention this minor difference of the reported numbers here in case someone may be confused by it. And QA-GNN is a excellent method to solve QA tasks with GNN and LM.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">* See Appendix A.2 for the detail of this result.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">† https://worksheets.codalab.org/worksheets/0xf215deb05edf44a2ac353c711f52a25f ‡ https://colab.research.google.com/github/bayesgroup/deepbayes-2019/ blob/master/seminars/day6/SparseVD-solution.ipynb</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We formulate the model prunings (parsification) problem pruning problem as a variational inference problem and use the Kullback-Leibler Divergence approximation following <ref type="bibr" target="#b0">(Achterhold et al., 2018;</ref><ref type="bibr" target="#b35">Molchanov et al., 2017)</ref> to constrain the model parameters to converge to the pruning prior: Original −D KL was obtained by averaging over 10 7 samples of with less than 2 × 10 −3 variance of the estimation.</p><p>Where σ(•) denotes the sigmoid function. We can see that −D KL term increases with the growth of α. It means that this regularization term favors large values of α. The case of α ij → ∞ corresponds</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational network quantization</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Achterhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Mathias</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anke</forename><surname>Schmeink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ry-TW-WAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Knowledge fusion and semantic knowledge ranking for open domain question answering</title>
		<author>
			<persName><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03101</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Careful selection of knowledge to solve open book question answering</title>
		<author>
			<persName><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6120" to="6129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
				<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Çelikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">From&apos;f&apos;to&apos;a&apos;on the ny regents science exams: An overview of the aristo project</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalvi</forename><surname>Bhavana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><surname>Tandon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01958</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05867</idno>
		<title level="m">Transformers as soft reasoners over language</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00646</idno>
		<title level="m">Scalable multi-hop relational reasoning for knowledge-aware question answering</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02626</idno>
		<title level="m">Learning both weights and connections for efficient neural networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Matthew D Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabhwaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<title level="m">Unifiedqa: Crossing format boundaries with a single qa system. EMNLP -findings</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Qasc: A dataset for question answering via sentence composition</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8082" to="8090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Runtime Neural Pruning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)</title>
				<meeting>the Eighth International Conference on Learning Representations (ICLR 2020)</meeting>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Jingfei du, mandar joshi, danqi chen, omer levy, mike lewis, luke zettlemoyer, and veselin stoyanov</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph-based reasoning over heterogeneous external knowledge for commonsense question answering</title>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8449" to="8456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards generalizable neuro-symbolic systems for commonsense question answering</title>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Oltramari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6003</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-6003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</title>
				<meeting>the First Workshop on Commonsense Inference in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2498" to="2507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04118</idno>
		<title level="m">Beyond accuracy: Behavioral testing of nlp models with checklist</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01427</idno>
		<title level="m">A simple neural network module for relational reasoning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PullNet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1242</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1242" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="2380" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00937</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06609</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Haq: hardware-aware automated quantization</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08886</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Connecting the dots: A knowledgeable path generator for commonsense question answering</title>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00691</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improving natural language inference using external knowledge in the science questions domain</title>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bassem</forename><surname>Makni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Mattei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7208" to="7215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improving question answering over incomplete KBs with knowledge-aware reader</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1417</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1417" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4258" to="4264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Qa-gnn: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Explicit loss-error-aware quantization for low-bit deep neural networks</title>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9426" to="9435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for natural language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
