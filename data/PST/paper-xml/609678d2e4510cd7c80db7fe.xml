<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning for End-to-End ASR to Deal with Low-Resource Problem in Persian Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maryam</forename><forename type="middle">Asadolahzade</forename><surname>Kermanshahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmad</forename><surname>Akbari</surname></persName>
							<email>akbari@iust.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Nasersharif</surname></persName>
							<email>bnasersharif@kntu.ac.ir</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Engineering Department K. N</orgName>
								<orgName type="institution">Toosi University of Technology Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning for End-to-End ASR to Deal with Low-Resource Problem in Persian Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/CSICC52343.2021.9420540</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic speech recognition (ASR)</term>
					<term>end-to-end ASR</term>
					<term>phoneme (phone) recognition</term>
					<term>transfer learning</term>
					<term>lowresource language</term>
					<term>Persian (Farsi) language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end models are state of the art for Automatic Speech Recognition (ASR) systems. Despite all their advantages, they suffer a significant problem: huge amounts of training data are required to achieve excellent performance. This problem is a serious challenge for low-resource languages such as Persian. Therefore, we need some methods and techniques to overcome this issue. One simple, yet effective method towards addressing this issue is transfer learning. We aim to explore the effect of transfer learning on a speech recognition system for the Persian language. To this end, we first train the network on 960 hours of English LibriSpeech corpus. Then, we transfer the trained network and fine-tune it on only about 3.5 hours of training data from the Persian FarsDat corpus. Transfer learning exhibits better performance while needing shorter training time than the model trained from scratch. Experimental results on FarsDat corpus indicate that transfer learning with a few hours of Persian training data can achieve 31.48% relative Phoneme Error Rate (PER) reduction compared to the model trained from scratch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Automatic Speech Recognition (ASR) task is to convert a spoken signal into the text. So far, most research works of ASR have focused on high-resource languages such as English. While, more attention should be paid to languages other than English, especially low-resource languages. On the other hand, in this field, end-to-end approaches are the current state of the art <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. The crucial point is that such approaches need large amounts of data for training <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. This could be a fundamental issue for low-resource languages, for which there is no sufficient data for training. Some algorithms exist for improving ASR to encounter the low resource scenarios, including representation learning using self-supervised methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, semi-supervised methods to take advantages of unlabeled speech data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref>, data augmentation techniques <ref type="bibr" target="#b12">[13]</ref>, pre-training methods using Autoencoders <ref type="bibr" target="#b13">[14]</ref> and transfer learning from a highresource language to a low-resource one <ref type="bibr" target="#b5">[6]</ref>.</p><p>Transfer learning is one of the most straightforward techniques to deal with the low-resource problem, which has been applied successfully in some previous studies <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>In transfer learning for ASR, we first train a neural network using a high-resource language. Then, we fine-tune this network on the target language, which is usually low-resource. Since there has not been adequate research in Persian ASR, to date, we thus decide to study ASR on the Persian language which can be considered low-resource. However, there exist some, yet limited research works on Persian ASR using DNN-based methods. For instance, in <ref type="bibr" target="#b15">[16]</ref>, the authors proposed (deep neural network-hidden semi-Markov model) DNN-HSMM approach and showed its superiority against (Gaussian mixture model-hidden Markov model) GMM-HMM and (deep neural network-hidden Markov model) DNN-HMM. The authors in <ref type="bibr" target="#b16">[17]</ref> also attempted to use an endto-end model for the Persian phoneme recognition system. However, to advance Persian ASR, there is still a need for further studies to investigate different factors in improving the Persian ASR. This motivated us to provide a result for an endto-end phoneme recognition system for the Persian language in order to alleviate future studies on this area for those researchers working on Persian ASR.</p><p>To the best of our knowledge, transfer learning for ASR has not been applied to the Persian language yet. Therefore, we explore the impact of transfer learning on Persian ASR. Furthermore, we aim to discover, to what extent, we can increase the current Persian end-to-end ASR using the transfer learning technique.</p><p>We present the evaluations of transfer learning method using the Persian FarsDat corpus <ref type="bibr" target="#b17">[18]</ref>. We use a fully convolutional neural network <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref> as our ASR model, and train it by Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b19">[20]</ref>. In the first step, we train the network on 960 hours of the English LibriSpeech corpus <ref type="bibr" target="#b20">[21]</ref>. Afterward, we fine-tune it on about 3.5 hours of training data from the Persian FarsDat corpus. As a result, transfer learning leads to investing in time and hardware resources. More importantly, it achieves better accuracy while needing a shorter training time.</p><p>The remainder of the paper is structured as follows.</p><p>Section II reviews related work on ASR systems using transfer learning methods. Section III describes our approach, architecture, its input, and output. Section IV explains the experimental setup, including corpora and performance measure. Section V discusses the experimental results.</p><p>Finally, Section VI concludes the results and presents future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review the previous work on ASR using transfer learning. The first attempt for transfer leaning in the DNN-based ASR system was conducted in <ref type="bibr" target="#b14">[15]</ref>. They used four European languages to build a multilingual DNN. They used one DNN such that all the hidden layers of the DNN are shared among the languages except the last Softmax layer. Each language has its Softmax layer. After training with the four languages, they evaluated transfer learning for two new target languages which had not been used during training. They evaluated the effectiveness of transfer learning for two different languages in terms of phonetics, including American English and Mandarin Chinese. American English is phonetically close to the European languages used for training the initial network. At the same time, Mandarin Chinese is very different from the European languages. For this purpose, a Softmax layer for each language specifically was added to the network.</p><p>The results showed that transferring hidden layers sharing across languages can improve ASR accuracy for two new languages. They also concluded when the amount of training data for the target language is small, i.e. three hours, the effect of transfer learning is very significant. In this situation, it would be better merely train Softmax layer instead of retraining more layers.</p><p>The authors in <ref type="bibr" target="#b5">[6]</ref> used a fully convolutional end-to-end ASR model. They trained the model on the English data then transfer it for the German data. Transferring from English to German performs better than the ASR model, which was trained from scratch by only German data.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, a multilingual ASR model with 10 BABEL languages from the BABEL speech corpus was built. Then, the model was used for four other BABEL languages using the transfer learning approach. Their results showed that the transfer learning from the multilingual ASR model is more beneficial than monolingual ASR models.</p><p>A more recent work <ref type="bibr" target="#b22">[23]</ref> proposed a language-adversarial transfer learning method. This method helped that the shared layers across multiple languages contain less unnecessary language-dependent information. By using adversarial learning, the shared layers can learn language invariant features. They demonstrated promising results on IARPA Babel dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We aim to perform an ASR system for the Persian language. To do this, we first trained an ASR system for the English language. Then, we transferred the trained network and fine-tuned it on Persian data. Fine-tuning involves updating the weights of the initial network using Persian data. In transfer learning, we can freeze some bottom layers so that they will be fixed during training. The overall diagram of our method is shown in Fig. <ref type="figure">1</ref>.</p><p>In the following, we explain the input, output, and architecture of the ASR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input of the neural network:</head><p>The input of the neural network is the spectrogram extracted from the speech signal. We first segment the signal into frames. We used 32 ms frame windows, which spanned every 8 ms. Then we applied the Short-time Fourier transform (STFT), and then we Mel-scaled them. We normalized features per input sequence to have zero mean and unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output of the neural network:</head><p>The output of the network is the corresponding phoneme sequence of the input speech signal.</p><p>Fig. <ref type="figure">1</ref>. The illustration of transfer learning for the ASR model with English as the source language and Persian as the target language. The network architecture for ASR is Fully convolutional and is the same for training both source and target languages. The size of kernel width, stride, and the number of channels for each layer are specified. Here, "×7" denotes the layer is repeated seven times. In this figure, the first layer of the right network is frozen. So, no weight updates are allowed. The remaining ten top layers can be fine-tuned using the Persian corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of the Neural Network:</head><p>We used 11 1Dconvolutional layers on top of each other based on the architecture introduced in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b18">[19]</ref>. We used zero-padding for each layer since we aimed to preserve the dimension of the input. The activation function for the first ten layers is Rectified Linear Unit (ReLU). For the last layer, Softmax activation was considered to get the probability distribution on phonemes. The last layer has 32 channel outputs, each of which corresponds to one of the Persian phonemes.</p><p>We trained the network using the CTC loss function <ref type="bibr" target="#b19">[20]</ref>. If we denote the input of acoustic features and desired output sequence by , , … , and , , … , , respectively. CTC computes the conditional probability, | , by calculating the sum over all possible alignments between input signal and transcription output. After training, the network predicts the probability of each phoneme for every input frame.</p><p>In our experiments, we did not use any beam search algorithm or language model at inference time. We just used greedy decoding for generating the phoneme sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section, we first introduce the corpora used for the experiments. After that, we present the evaluation method of a phoneme recognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corpora</head><p>We used two corpora which are explained in the following:</p><p>English corpus: For training ASR model for the English language, we used the LibriSpeech corpus <ref type="bibr" target="#b20">[21]</ref>. This corpus consists of 1000 hours of read-speech, sampled at 16 kHz, from the domain of audiobooks. This corpus is freely available for download. Training set that includes 960 hours' speech data was used for training the English ASR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persian corpus:</head><p>For training ASR model for the Persian language, we used FarsDat corpus <ref type="bibr" target="#b17">[18]</ref>, which contains the utterances of 304 female and male speakers from 10 dialect regions in Iran. Each speaker uttered 20 sentences in two sessions. This corpus is about 5 hours in size. TABLE <ref type="table" target="#tab_0">I</ref>. an overview of the partitioning we used for different sets of the FarsDat corpus.</p><p>It is worth mentioning that our experiments are speakerindependent because the speakers in the training set are different from those in the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metric</head><p>The standard measure to evaluate a phoneme recognition system is phoneme error rate (PER). In this measure, the recognized and reference phoneme sequences should be compared by matching using a dynamic programming algorithm, edit distance. Considering the number of substitution errors, deletion errors, and insertion errors shown as S, D, and I, respectively, the PER is defined as:</p><formula xml:id="formula_0">(1)</formula><p>where N is the total number of phonemes in the reference label <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSIONS</head><p>In this section, we perform two experiments to evaluate the performance of transfer learning. First, we investigate the effect of the number of frozen layers in the transferred network. Second, we evaluate the effectiveness of transfer learning by comparing its result against ASR trained without exploiting the transferred network.</p><p>As mentioned in section III, our network consists of 11 layers. To fine-tune the network on Persian corpus, we freeze K number of the lower layers; therefore, the weights of 11-K top layers can be updated during fine-tuning.</p><p>To determine a suitable number for K, we display training curves for different values of K on the validation set in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>For this figure, we also include the result of the ASR model trained from scratch using the Persian corpus (without any transferring from the English ASR model). In other words, we train the network on only Persian corpus.</p><p>With the help of this figure, we aim to analyze the effect of both number of frozen layers and speed of convergence. We also have a comparison between behavior of transfer learning and learning from scratch. Note that in both cases "learning from scratch" and "transfer learning without any frozen layer (K=0)" all layers are allowed to be trained. However, in the first situation, we start learning from random weights. In contrast, in the second situation, we start learning from the weights initialized by the model trained by English corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of number of frozen layers:</head><p>We find that PER for K=9 or 10 frozen layers is not satisfactory. However, the difference in performance between 10 and 9-frozen layers is considerable. It suggests that by freezing a fewer number of layers, we can attain better results. As a consequence, the optimum value for K is 0 or 1. Perhaps this is because the lower layers for Persian ASR model still require some weight updates using Persian language to adapt well to Persian speech data. Our results are consistent with the finding of previous work in <ref type="bibr" target="#b5">[6]</ref>. The speed of convergence: As Fig. <ref type="figure" target="#fig_0">2</ref> makes clear, transfer learning results in faster convergence. For transfer learning case, it takes fewer than ten steps of training to reach reasonable and fairly fixed performance. However, in the case of learning from scratch, it takes much longer to converge.</p><p>The reason is that transfer learning can be viewed as a kind of weight initialization or pre-training. However, in training from scratch, the model starts from some random weights. Instead, with the help of transfer learning, we can start training the network from suitable values for weights, which are obtained previously from the model trained by a large corpus, LibriSpeech.</p><p>Comparing transfer learning with learning from scratch: According to Fig. <ref type="figure" target="#fig_0">2</ref>, performance decreases as the number of frozen layers increases. However, even ASR with 9-frozen layers (i.e. only two top layers are trainable) still outperforms the ASR model trained from scratch. Moreover, we compare the final results of transfer learning and learning from scratch on the test set in Fig. <ref type="figure" target="#fig_1">3</ref>. The best number of steps required for training each model is chosen using the validation set. As expected, transfer learning outperforms model trained from scratch. For comparison, an output example of both methods is provided in Fig. <ref type="figure" target="#fig_2">4</ref>. The reference transcription and recognized transcriptions for some specific steps along with their PERs are also presented. The results in Fig. <ref type="figure" target="#fig_2">4</ref> suggest the advantage of transfer learning. The results indicate that learning from scratch requires a longer time to achieve a good transcription. By contrast, in transfer learning case, the model is capable of generating better transcription at early steps, even at the first step. However, learning from scratch, even at step 100, is still far from the result of transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this work, to combat low-resource issue for the Persian language, we evaluated the effect of transfer learning for Persian ASR on the FarsDat corpus. We demonstrated that transfer learning can reduce the required time for training; meanwhile, it significantly outperforms the model trained from scratch. The reason for this improvement is that transfer learning acts as a kind of weight initialization. Besides, we provided a comparative framework for end-to-end ASR model for future studies on Persian ASR. For future work, to extend our work, we will use the language model and beam search decoding to achieve additional improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The impact of different values for K (number of frozen layers) on transfer learning on the validation set. The result of learning from scratch (without using any transfer learning from English) is included in this figure.</figDesc><graphic url="image-3.png" coords="3,306.48,339.36,234.00,214.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparing the results of transfer learning and learning from scratch on the test set.</figDesc><graphic url="image-4.png" coords="4,52.56,141.60,228.96,145.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. We show recognized transcription, along with its PER for a speech signal, which is chosen from validation set, over steps. The transcriptions are shown for two models: the model by transfer learning with zero frozen layers (in green) and the model trained from scratch (in blue). The reference transcription is "sil h I C f e k k a r d i C e r A d a r s m I K A n i sil". Here, "sil" shows the silence phoneme. Its equivalent sentence in the Persian language is " ‫ﻫ‬ ‫ﻴﭻ‬ ‫ﮐﺮﺩ‬ ‫ﻓﮑﺮ‬ ‫ی‬ ‫ﻣ‬ ‫ﺩﺭﺱ‬ ‫ﭼﺮﺍ‬ ‫ﯽ‬ ‫ﺧﻮﺍﻧﯽ‬ ".</figDesc><graphic url="image-5.png" coords="4,46.20,381.12,241.08,196.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>STATISTIC OF EACH SET OF THE FARSDAT CORPUS CONSIDERED IN OUR EXPERIMENTS FOR PERSIAN ASR.</figDesc><table><row><cell>Set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>ID number of speakers #Speakers Duration (in hour:min:sec)</head><label></label><figDesc></figDesc><table><row><cell>Train set</cell><cell>1 -220</cell><cell>220</cell><cell>3:25:29</cell></row><row><cell>Validation set</cell><cell>221 -250</cell><cell>30</cell><cell>0:32:04</cell></row><row><cell>Test set</cell><cell>251 -304</cell><cell>54</cell><cell>1:02:41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 02:58:32 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequenceto-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4774" to="4778" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<title level="m">wav2vec: Unsupervised Pre-training for Speech Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7084" to="7088" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transfer Learning for Speech Recognition on a Budget</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kurenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johannsmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd ACL Workshop on Representation Learning for NLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Virtual</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6989" to="6993" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative Pseudo-Labeling for Speech Recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<biblScope unit="page" from="1006" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved Noisy Student Training for Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2817" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence-to-sequence ASR using unpaired speech and text</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="3790" to="3794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised end-to-end speech recognition using textto-speech and autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6166" to="6170" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep contextualized acoustic representations for semi-supervised speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6429" to="6433" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7304" to="7308" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Phoneme Sequence Recognition using Phoneme Duration Information in DNN-HSMM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kermanshahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Homayounpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of AI and Data Mining</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning model to recognize Farsi speech from raw input</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alisamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ahadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seyedin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FARSDAT-The speech database of Farsi spoken language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bijankhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sheikhzadegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Roohani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Australian Conference on Speech Science and Technology</title>
				<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="826" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<title level="m">Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</title>
				<imprint>
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
				<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language-adversarial transfer learning for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="630" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The HTK book</title>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
