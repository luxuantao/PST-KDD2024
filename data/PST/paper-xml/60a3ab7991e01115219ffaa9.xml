<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Graph Neural Networks with Simple Architecture Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-17">17 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sunil</forename><surname>Kumar Maurya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology Tokyo</orgName>
								<address>
									<settlement>Xin Liu</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AIRC</orgName>
								<orgName type="institution" key="instit2">AIST Tokyo</orgName>
								<address>
									<country>Japan Tsuyoshi Murata</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Graph Neural Networks with Simple Architecture Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-17">17 May 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2105.07634v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Node Classification</term>
					<term>Model Design</term>
					<term>Feature Selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks have emerged as a useful tool to learn on the data by applying additional constraints based on the graph structure. These graphs are often created with assumed intrinsic relations between the entities. In recent years, there have been tremendous improvements in the architecture design, pushing the performance up in various prediction tasks. In general, these neural architectures combine layer depth and node feature aggregation steps. This makes it challenging to analyze the importance of features at various hops and the expressiveness of the neural network layers. As different graph datasets show varying levels of homophily and heterophily in features and class label distribution, it becomes essential to understand which features are important for the prediction tasks without any prior information. In this work, we decouple the node feature aggregation step and depth of graph neural network and introduce several key design strategies for graph neural networks. More specifically, we propose to use softmax as a regularizer and "Soft-Selector" of features aggregated from neighbors at different hop distances; and "Hop-Normalization" over GNN layers. Combining these techniques, we present a simple and shallow model, Feature Selection Graph Neural Network (FSGNN), and show empirically that the proposed model outperforms other state of the art GNN models and achieves up to 64% improvements in accuracy on node classification tasks. Moreover, analyzing the learned soft-selection parameters of the model provides a simple way to study the importance of features in the prediction tasks. Finally, we demonstrate with experiments that the model is scalable for large graphs with millions of nodes and billions of edges.</p><p>Source code at https://github.com/sunilkmaurya/FSGNN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) have opened a unique path to learning on data by leveraging the intrinsic relations between entities that can be structured as a graph. By imposing these structural constraints, additional information can be learned and used for many types of prediction tasks. With rapid development of the field and easy accessibility of computation and data, GNNs have been used to solve a variety of problems like node classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>, link prediction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, graph classification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, prediction of molecular properties <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, natural language processing <ref type="bibr" target="#b18">[19]</ref>, node ranking <ref type="bibr" target="#b19">[20]</ref> and so on.</p><p>In this work, we focus on the node classification task using graph neural networks. Since the success of early GNN models like GCN <ref type="bibr" target="#b14">[15]</ref>, researchers have successively proposed numerous variants <ref type="bibr" target="#b29">[30]</ref> to address various shortcomings in model training and to improve the prediction capabilities. Some of the techniques used in these variants include neighbor sampling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, attention mechanism to assign different weights to neighbors <ref type="bibr" target="#b26">[27]</ref>, use of Personalized PageRank matrix instead of adjacency matrix <ref type="bibr" target="#b15">[16]</ref> and simplified model design <ref type="bibr" target="#b28">[29]</ref>. Also, there has been a growing interest in making the models deeper by stacking more layers and using the residual connections to improve the expressiveness of the model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref>. However, most of these models by design are more suitable for homophily datasets where nodes linked to each other are more likely to belong in the same class. They may not perform well with heterophily datasets which are more likely to have nodes with different labels connected together. Zhu et al. <ref type="bibr" target="#b34">[35]</ref> highlight this problem and propose node's ego-embedding and neighbor-embedding separation to improve performance on heterophily datasets.</p><p>In general, GNN models combine feature aggregation and transformation using a learnable weight matrix in the same layer, often referred to as graph convolutional layer. These layers are stacked together with the non-linear transformation (e.g., ReLU) and regularization(e.g., Dropout) as a learning framework on the graph data. Stacking the layers also has the effect of introducing powers of adjacency matrix (or laplacian matrix), which helps to generate a new set of features for a node by aggregating neighbor's features at multiple hops, thus encoding the neighborhood information. The number of these unique features depends on the propagation steps or the depth of the model. The final node embeddings are the output of just stacked layers or, for some models, also has skip connection or residual connection combined at final layer.</p><p>However, such a combination muddles the distinction between the importance of features and expressiveness of MLP. It becomes challenging to analyze which features are essential and how much expressiveness MLP requires over a specific task. To overcome this challenge, we provide a framework to treat the feature propagation and learning separately. With this freedom, we propose a simple GNN model with three unique design considerations: Soft-selection of features using softmax function, Hop-Normalization, and unique mapping of features. With experimental results, we show that our simple 2-layer GNN outperforms other state-of-art GNN models (both shallow and deep) and achieves up to 64% higher node classification accuracy. In addition, analyzing the model parameters gives us an insight into identifying which features are most responsible for classification accuracy. One interesting observation we find is regarding Chameleon and Squirrel datasets. These are dense graph datasets and are generally regarded as being low-quality heterophily datasets. However, in our experiments with our proposed model, we find them to be showing strong heterophily properties with improved classification results.</p><p>Furthermore, we demonstrate that due to the simple design of our model, it can scale up for very large graph datasets. We run experiments on ogbn-papers100M dataset, which is the largest publicly available node classification dataset, and achieve higher accuracy than the state of the art models.</p><p>The rest of the paper is organized as follows: Section 2 outlines formulation of graph neural networks and details node classification task. In Section 3, we discuss design strategies for GNNs and propose the GNN model FSGNN. In Section 4, we briefly introduce relevant GNN literature. Section 5 contains the experimental details and comparison with other GNN models. In Section 6, we empirically analyze our proposed design strategies and their effect on the model's performance. Section 7 summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Let ğº = (ğ‘‰ , ğ¸) be an undirected graph with ğ‘› nodes and ğ‘š edges. For numerical calculations, graph is represented as adjacency matrix denoted by ğ´ âˆˆ {0, 1} ğ‘›Ã—ğ‘› with each element ğ´ ğ‘– ğ‘— = 1 if there exists an edge between node ğ‘£ ğ‘– and ğ‘£ ğ‘— , otherwise ğ´ ğ‘– ğ‘— = 0. If self-loops are added to the graph then, resultant adajcency matrix is denoted as Ãƒ = ğ´ + ğ¼ . Diagonal degree matrix of ğ´ and Ãƒ are denoted as ğ· and D. Each node is associated with a d-dimensional feature vector and the feature matrix for all nodes is represented as ğ‘‹ âˆˆ R ğ‘›Ã—ğ‘‘ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>Graph Neural Networks (GNNs) leverage feature propagation mechanism <ref type="bibr" target="#b9">[10]</ref> to aggregate neighborhood information of a node and use non-linear transformation with trainable weight matrix to get the final embeddings for the nodes. Conventionally, a simple GNN layer is defined as</p><formula xml:id="formula_0">ğ» (ğ‘–+1) = ğœ ( Ãƒğ‘ ğ‘¦ğ‘š ğ» (ğ‘–) ğ‘Š (ğ‘–) )<label>(1)</label></formula><p>where Ãƒğ‘ ğ‘¦ğ‘š = Dâˆ’ 1 2 Ãƒ Dâˆ’ 1 2 is a symmetric normalized adjacency matrix with added self-loops. ğ» ğ‘– represents features from the previous layer, ğ‘Š ğ‘– denotes the learnable weight matrix, and ğœ is a non-linear activation function, which is usually ReLU in most implementation of GNNs. However, this formulation is suitable for homophily datasets as features are cumulatively aggregated i.e. node's own features are added together with neighbor's features.</p><p>For heterophily datasets, we require a propagation scheme to separate features of neighbors from node's own features. So we use the following formulation for the GNN layer,</p><formula xml:id="formula_1">ğ» (ğ‘–+1) = ğœ (ğ´ ğ‘ ğ‘¦ğ‘š ğ» (ğ‘–) ğ‘Š (ğ‘–) )<label>(2)</label></formula><p>where ğ´ ğ‘ ğ‘¦ğ‘š = ğ· âˆ’ 1 2 ğ´ğ· âˆ’ 1 2 is symmetric normalized adjacency matrix without added self-loops. To combine features from multiple hops, concatenation operator can be used before the final layer.</p><p>Following the conventional GNN formulation using Ãƒ, a simple 2-layered GNN can be represented as <ref type="bibr" target="#b14">[15]</ref>, ğ‘ = Ãƒğ‘ ğ‘¦ğ‘š ğœ ( Ãƒğ‘ ğ‘¦ğ‘š ğ‘‹ğ‘Š (0) )ğ‘Š (1)  (3) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Node Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Homophily vs Heterophily</head><p>Node classification problem relies on the graph structure and features of the nodes to identify the labels of the node. Under homophily, nodes are assumed to have neighbors with similar features and labels. Thus, the cumulative aggregation of node's self-features with that of neighbors reinforce the signal corresponding to the label and help to improve accuracy of the predictions. While in the case of heterophily, nodes are assumed to have dissimilar features and labels. In this case, the cumulative aggregation will reduce the signal and add more noise causing neural network to learn poorly and causing drop in performance. Thus it is essential to have node's self-features separate from the neighbor's features. In real-world datasets, homophily and heterophily levels may vary, hence it is optimal to have both aggregation schemes (Eq. 1 &amp; 2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED ARCHITECTURE</head><p>For the design of a GNN with good generalization capability and performance, there are many aspects of the data that needs to be considered. The feature propagation and aggregation scheme is governed by if the class label distribution has strong homophily or heterophily or some combination of both. The number of hops (and depth of the model for many GNN models) for feature aggregation are dependent on graph structure and size as well as label distribution among neighbors of the nodes. Also, the type and amount of regularization during training needs to be decided, for example, using dropout on input features or on graph edges. Keeping these aspects under consideration, we propose three design strategies that help to create a versatile and simple GNN model.  As discussed in Sec. 2.1, these features can be aggregated cumulatively (homophily-based) or non-cumulatively (heterophily-based). Moreover, the features can also be combined based on some arbitrary criteria. We assume a function,</p><formula xml:id="formula_2">CONCAT ğ‘–=1 ğ¾ Î± ğ‘– = 1 . . . . . . Logits âˆ¥â€¢âˆ¥ ğŸ âˆ¥â€¢âˆ¥ ğŸ âˆ¥â€¢âˆ¥ ğŸ ğ´ ğ‘ ğ‘¦ğ‘š ğ‘‹ áˆš ğ´ ğ‘ ğ‘¦ğ‘š ğ‘‹ ğ‘‹ ğ‘¾ ğŸ‘ (ğŸ) ğ‘¾ ğŸ (ğŸ) ğ‘¾ ğŸ (ğŸ) ğ‘¾ (ğŸ) áˆš ğ´ ğ‘ ğ‘¦ğ‘š ğ¾ ğ‘‹ ğ’ ğˆ ğœ¶ ğŸ ğœ¶ ğŸ ğœ¶ ğŸ‘ Softmax Normalized ğ‘¯ (ğŸ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Strategies for GNNs</head><formula xml:id="formula_3">ğ‘”(ğ‘‹, ğ´, ğ¾) â†¦ â†’ {ğ‘‹ 1 , ğ‘‹ 2 , . . . , ğ‘‹ ğ‘ }</formula><p>The function takes ğ‘‹ as node features matrix, ğ´ as an adjacency matrix, ğ¾ as the power of the adjacency matrix or number of hops to propagate features and outputs a set of aggregated features. These features then can be combined using sum or concatenation operation to get final representation of the node. However, in the node classification task, for a given label distribution, only a subset of these features are useful to predict the label of the node. For example, features of node's neighbors that lie at a greater distance in the graph may not be sufficiently informative or useful for node's label prediction.</p><p>Conventionally, GNN models have feature propagation and transformation combined into a single layer, and the layers are stacked together. This step makes it difficult to distinguish the importance of the features and the role of MLP. To overcome this limitation, we propose to separate the feature generation step and representation learning over features separately. This provides us with three main benefits.</p><p>(i) Features generated for nodes are not constrained by the design of the GNN model. We get the freedom to choose the feature set as required by the problem and the neural network design, which is sufficiently expressive.</p><p>(ii) We can precompute and fix the node features set and experiment with the neural network architectures for the best performance. Precomputing features also helps to scale the training of the model for large graphs with batchwise training. (iii) In conventional GNN setting, stacking many layers also causes oversmoothing of node features <ref type="bibr" target="#b4">[5]</ref> and adversely affects the performance of the model. Recently proposed models use skip connection or residual connection to overcome this issue. However, they fail to demonstrate which features are useful. We provide an alternate scheme where the model can learn weights that identify which features are useful for the prediction task.</p><p>For the model design, instead of a single input channel, we propose to have all these features as input in parallel. Please refer Fig. <ref type="figure" target="#fig_0">1</ref> for the illustration. Each feature is mapped to a separate linear layer. Hence the linear transformations are uniquely learned for all input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Feature Selection.</head><p>As features are aggregated over many hops, some features are useful and correlate with the label distribution, while others are not very useful for learning and act more like the noise for the model. As we propose to input the feature set in parallel channels, we can design the model to learn which features are more relevant for lower loss value and giving higher weights to those features while simultaneously reducing the weights on other features. We propose to weight these features with a single scalar value that is multiplied to each input feature matrix and impose a constraint on these values by softmax function. Let ğ›¼ ğ‘– be the scalar value for the ğ‘– ğ‘¡â„ feature matrix, then ğ›¼ ğ‘– scales the magnitude of the features as ğ›¼ ğ‘– ğ‘‹ ğ‘– ğ‘Š (0) ğ‘– . Softmax function is used in deep learning as a nonlinear normalizer, and its output is often practically interpreted as probabilities. Before training, the scalar values corresponding to each feature matrix are initialized with equal values and softmax is applied on these values. The resultant normalized values ğ›¼ ğ‘– are then multiplied with the input features, and the concatenation operator is applied. Considering ğ¿ number of input feature matrices ğ‘‹ ğ‘™ , ğ‘™ âˆˆ {1 .. ğ¿} , the formulation can be described as,</p><formula xml:id="formula_4">ğ» (1) = ğ¿ ğ‘™=1 ğ›¼ ğ‘™ ğ‘‹ ğ‘™ ğ‘Š (0) ğ‘™ (4)</formula><p>where</p><formula xml:id="formula_5">ğ¿ âˆ‘ï¸ ğ‘™=1 ğ›¼ ğ‘™ = 1</formula><p>While training, the scalar values of relevant features corresponding to the labels increase towards 1 while others decrease towards 0. The features that are not useful and represent more noise than signal have their magnitudes reduced with corresponding decreasing in their scalar values. Since we are not using a binary selection of features, we term this selection procedure as "soft-selection" of features.</p><p>This formulation can be understood in two ways. As GNNs have represented with a polynomial filter,</p><formula xml:id="formula_6">ğ‘” ğœƒ (ğ‘ƒ) = ğ¾âˆ’1 âˆ‘ï¸ ğ‘˜=0 ğœƒ ğ‘˜ ğ‘ƒ ğ‘˜<label>(5)</label></formula><p>where ğœƒ âˆˆ R ğ¾ is a vector of polynomial coefficients and P can be adjacency matrix <ref type="bibr" target="#b14">[15]</ref>[7], laplacian matrix <ref type="bibr" target="#b20">[21]</ref> or PageRank based matrix <ref type="bibr" target="#b1">[2]</ref>. As the polynomial coefficients are scalar parameters then our scheme can be considered as applying regularization on these parameters using the softmax function. The other way to look is to simply consider it as a weighting scheme. As the input features can be arbitrarily chosen, and instead of a scalar weighting scheme, a more sophisticated scheme can be used.</p><p>For practical implementation, since all weights are initialized as equal, they can be set equal to 1. After normalizing with softmax function, the individual scalar values becomes equal to 1/ğ¿. During training, these values change, denoting the importance of the features. In some cases, initial ğ›¼ ğ‘™ = 1/ğ¿ value may be too small and may adversely affect training. In that case, a constant ğ›¾ may be multiplied after softmax normalization to increase the initial magnitude as ğ›¾ğ›¼ ğ‘™ ğ‘‹ ğ‘™ ğ‘Š (0) ğ‘™ . Since ğ›¾ remains constant during the training, it does not affect the softmax regularization of the scalar parameters.</p><p>As the scalar values affect the magnitude of the features, they also affect the gradients propagated back to the linear layer, which transforms the input features. Hence it is important to have a unique weight matrix for each input feature matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Hop-Normalization.</head><p>The third strategy we propose is Hop-Normalization. It is a common practice in the deep learning field to use different types of normalization schemes, for example, batch normalization <ref type="bibr" target="#b13">[14]</ref>, layer normalization, weight normalization, and so on. However, in graph neural network frameworks, normalization of activations after hidden layers are not commonly used. It may be in part due to the common practice of normalizing node/edge features and symmetric/non-symmetric normalization of the adjacency matrix.</p><p>We propose to normalize all aggregated features from different hops after linear transformation, hence the term "Hop-Normalization". We propose row-wise L2-normalize the hidden layer activations as,</p><formula xml:id="formula_7">â„ ğ‘– ğ‘— = â„ ğ‘– ğ‘— âˆ¥ â„ ğ‘– âˆ¥ 2<label>(6)</label></formula><p>where â„ ğ‘– represents the ğ‘– ğ‘¡â„ row vector of activations and â„ ğ‘– ğ‘— represents individual values. L2-normalization scales the node embedding vectors to lie on the "unit sphere". In the later section, we empirically show significant improvements in the performance of the model with the use of this scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Selection Graph Neural Network</head><p>Combining the design strategies proposed earlier, we propose a simple and shallow (2-layered) graph GNN model called Feature Selection Graph Neural Network (FSGNN). Figure <ref type="figure" target="#fig_0">1</ref> shows the diagrammatic representation of our model. Input features are precomputed using ğ´ ğ‘ ğ‘¦ğ‘š and Ãƒğ‘ ğ‘¦ğ‘š and transformed using a linear layer unique to each feature matrix. Hop-normalization is applied on the output activations of the first layer and weighted with scalar weights regularized by the softmax function. Output features are then concatenated and non-linearly transformed using ReLU and mapped to the second linear layer. Cross-entropy loss is calculated with output logits of second layer. ğ›¼ vector of dimension 2K+1; Output : Logits</p><formula xml:id="formula_8">1 ğ›¼ ğ‘– â† 1.0, ğ‘– = 1...2ğ¾ + 1 2 ğ›¼ â† ğ‘†ğ‘‚ğ¹ğ‘‡ ğ‘€ğ´ğ‘‹ (ğ›¼) 3 ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¡ â† [ğ‘‹ ] 4 ğ‘‹ ğ´ â† ğ‘‹ 5 ğ‘‹ Ãƒ â† ğ‘‹ 6 for ğ‘˜ = 1...ğ¾ do 7 ğ‘‹ ğ´ â† ğ´ ğ‘ ğ‘¦ğ‘š ğ‘‹ ğ´ 8 ğ‘‹ Ãƒ â† Ãƒğ‘ ğ‘¦ğ‘š ğ‘‹ Ãƒ 9 ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¡ .ğ´ğ‘ƒğ‘ƒğ¸ğ‘ ğ· ( ğ‘‹ ğ´ ) 10 ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¡ .ğ´ğ‘ƒğ‘ƒğ¸ğ‘ ğ· ( ğ‘‹ Ãƒ ) 11 end 12 ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘ğ‘ğ‘¡ = ğ¿ğ¼ğ‘†ğ‘‡ () 13 for ğ‘— = 1...2ğ¾ + 1 do 14 ğ‘‹ ğ‘“ â† ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¡ [ ğ‘—] 15 ğ‘‚ğ‘¢ğ‘¡ â† ğ»ğ‘‚ğ‘ƒğ‘ğ‘‚ğ‘…ğ‘€ ( ğ‘‹ ğ‘“ ğ‘Š (0) ğ‘— ) 16</formula><p>ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘ğ‘ğ‘¡ .ğ´ğ‘ƒğ‘ƒğ¸ğ‘ ğ· ( ğ›¼ ğ‘— âŠ™ ğ‘‚ğ‘¢ğ‘¡ ) 17 end 18 ğ» (1) â† ğ¶ğ‘‚ğ‘ğ¶ğ´ğ‘‡ ( ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘ğ‘ğ‘¡ ) 19 ğ‘ â† ğ‘…ğ‘’ğ¿ğ‘ˆ ( ğ» (1) )ğ‘Š (2)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>GNNs have emerged as an indispensable tool to learn graph-centric data. Many prediction tasks like node classification, link prediction, graph classification, etc. <ref type="bibr" target="#b7">[8]</ref>[15] introduced a simple end-toend training framework using approximations of spectral graph convolutions. Since then, there has been a focus in the research community to improve the performance of GNNs, and a variety of techniques have been introduced. Earlier GNN frameworks utilized a fixed propagation scheme along all edges, which is sometimes not scalable for larger graphs. GraphSAGE <ref type="bibr" target="#b11">[12]</ref> and FastGCN <ref type="bibr" target="#b5">[6]</ref> introduce neighbor sampling approaches in graph neural networks. GAT <ref type="bibr" target="#b26">[27]</ref> introduces the use of the attention mechanism to provide weights to features that are aggregated from the neighbors. APPNP <ref type="bibr" target="#b15">[16]</ref>, JK <ref type="bibr" target="#b30">[31]</ref> and Geom-GCN <ref type="bibr" target="#b21">[22]</ref> aim to improve the feature propagation scheme within layers of the model. More recently, researchers are proposing to make GNN models deeper. However, deeper models suffer from oversmoothing, where after stacking many GNN layers, features of the node become indistinguishable from each other, and there is a drop in the performance of the model. DropEdge <ref type="bibr" target="#b22">[23]</ref> proposes to drop a certain number of edges to reduce the speed of convergence of oversmoothing and relieves the information loss. GCNII <ref type="bibr" target="#b6">[7]</ref> use residual connections and identity mapping in GNN layers to enable deeper networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the empirical performance of our proposed model on real-world datasets on the node classification task and compare with other graph neural network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For fully-supervised node classification tasks, we perform experiments on nine datasets commonly used in graph neural networks literature. Details of the datasets are presented in Table <ref type="table" target="#tab_1">1</ref>. Homophily ratio <ref type="bibr" target="#b34">[35]</ref> denotes the fraction of edges which connects two nodes of the same label. A higher value (closer to 1) indicates strong homophily, while a lower value (closer to 0) indicates strong heterophily in the dataset. Cora, Citeseer, and Pubmed <ref type="bibr" target="#b24">[25]</ref> are citation networks based datasets and in general, are considered as homophily datasets. Graphs in Wisconsin, Cornell, Texas <ref type="bibr" target="#b21">[22]</ref> represent links between webpages, Actor <ref type="bibr" target="#b25">[26]</ref> represent actor cooccurrence in Wikipedia pages, Chameleon and Squirrel <ref type="bibr" target="#b23">[24]</ref> represent the web pages in Wikipedia discussing corresponding topics. These datasets are considered as heterophily datasets. To provide a fair comparison, we use publicly available data splits taken from <ref type="bibr" target="#b21">[22]</ref> <ref type="foot" target="#foot_1">1</ref> . These splits have been frequently used by researchers for experiments in their publications. Results of comparison methods presented in this paper are also based on this split.</p><p>In the analysis section, to demonstrate the scalability of the model for large graphs, we use ogbn-papers100M dataset<ref type="foot" target="#foot_2">2</ref> , which is the largest publicly available node classification dataset. Many nodes in this dataset do not have labels assigned, hence homophily ratio is not calculated. We use standard split provided <ref type="bibr" target="#b12">[13]</ref> to train and evaluate the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Preprocessing</head><p>We follow the same preprocessing steps used by <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b6">[7]</ref>. Other models also follow the same set of procedures. Initial node features are row-normalized. To account for both homophily and heterophily, we use the adjacency matrix and adjacency matrix with added-self loops for feature transformation. Both matrices are symmetrically normalized. For efficient computation, adjacency matrices are stored and used as sparse matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Settings and Baselines</head><p>For a fully-supervised node classification task, each dataset is split evenly for each class into 60%, 20%, and 20% for training, validation, and testing. We report the performance as mean classification accuracy over 10 random splits.</p><p>We fix the embedding size to 64 and set the initial learnable scalar parameter with respect to each hop to 1 and ğ›¾ is set to 1. Thus, the initial scalar value ğ›¼ ğ‘– is set to 1/ğ¿. Hyper-parameter settings of the model for best performance are found by performing a grid-search over a range of hyper-parameters.</p><p>We compare our model to 8 different baselines and use the published results as the best performance of these models. GCNII <ref type="bibr" target="#b6">[7]</ref> and H2GCN <ref type="bibr" target="#b34">[35]</ref> have proposed multiple variants of their model. We have chosen the variant with the best performance on most datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the comparison of the mean classification accuracy of our model with other popular GNN models. On heterophily datasets, our model shows significant improvements especially 64% on Squirrel and 23% on Chameleon dataset. Similarly, on Wisconsin, Texas, and Cornell, improvements are 2%, 3%, and 7%, respectively. H2GCN has closer performance to our model than other GNN models as its architecture design accounts for the heterophily present in class labels and distinguishes node's self-features from neighbor's features. However, with our proposed model, we are able to achieve higher accuracy. The performance of other GNN models is quite a bit lower as their design is more suitable for homophily datasets.</p><p>On homophily datasets, we observe most of the models have comparable performance with GCNII and GEOM-GCN in the lead. Our model is still comparable to state of the art and coming as second-best among various comparison measures. In this section, we consider the effect of various proposed design strategies on the performance of the model. In general, graph neural networks are sensitive to the hyperparameters used in training and require some amount of tuning to get the best performance. Since each dataset may have different set of best hyperparameters, it can be difficult to judge design decisions based just on best performance of the model with single hyperparameter setting. To provide a comprehensive evaluation, we compare the average accuracy of the model over 1080 combinations of the hyperparameters. The hyperparameters we tune are learning rate and weight decay of layers and dropout value applied as regularization between layers. Table <ref type="table" target="#tab_3">3</ref> shows the average of classification accuracy values under various settings. For most datasets, our proposed design schemes lead to better average accuracy. Cora and Citeseer show better average performance without softmax regularization, however, the peak performance is marginally less with regularization. Even though Wisconsin shows higher average accuracy without normalization, however, the best performance on the dataset was achieved with the normalization layer. We found that Actor was the only dataset where performance reduced with the addition of the normalization layer. Without the normalization layer, our model achieves 37.63% accuracy. However, to maintain consistency, we do not include it in the main results. These variations also highlight the fact that a single set of design choices may not apply to all datasets/tasks and some level of exploration is required.</p><p>It is interesting to note that performance on almost all datasets is sensitive to the choice of the hyperparameters for training the model as there is a wide gap between best and average performance.   that self-looped features are given more importance. Among heterophily datasets, Wisconsin, Cornell, Texas, and Actor have the most weights on node's ego features. In these datasets, graph structure plays a limited role in the performance accuracy of the model. For Chameleon and Squirrel datasets, we observed that the node's own features and first-hop features(without self-loop) were more useful for classification than any other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hop-Normalization</head><p>In our experimental results, we find Chameleon and Squirrel datasets have significant improvements. To understand the results better, we create 2-dimensional plot of the trained embeddings of both datasets using t-SNE <ref type="bibr" target="#b16">[17]</ref>. Figure <ref type="figure" target="#fig_6">4</ref> shows the comparison of embeddings with and without hop-normalization. Without hop-normalization, embeddings of the nodes are not separated clearly, thus resulting in lower classification performance. We observe similar performance on other GNN models. While with hop-normalization, the node embeddings are well separated into clusters corresponding to their label leading to a higher observed performance with our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Scalability</head><p>Many GNN models by design are not scalable for large graph datasets with millions of nodes. We compare the accuracy of our model with SGC <ref type="bibr" target="#b28">[29]</ref>, Node2Vec <ref type="bibr" target="#b10">[11]</ref> and SIGN <ref type="bibr" target="#b8">[9]</ref>. Similar to our method, input features can be precomputed in SGC and SIGN, thus making them scalable for larger datasets. Once features are computed, the model can be trained with small input batches of node features on the GPU. Many other GNN models cannot be trained for larger graphs as the feature generation, and model training are combined.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows the mean node classification accuracy along with published results of other methods taken from <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b12">[13]</ref>. Our model outperforms all other methods, with SIGN having a closer performance to ours. However, SIGN uses the adjacency matrix of both directed and undirected versions of the graph for feature transformations, while our model only utilizes the adjacency matrix of the undirected graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Effect of increase in hops</head><p>In this section, we evaluate the change in model's performance with increase in the hops for aggregation. We choose one homophily dataset (Cora) and one heterophily dataset (Chameleon). Experiments are run with hop values set to 3,8,16, and 32. Figure <ref type="figure" target="#fig_6">4</ref> shows the performance of the model for each hop setting. We observe that there is little variation in the performance of the model. This result  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We discuss three GNN design strategies: separation of feature aggregation and representation learning; soft-selection of features, and hop-normalization. Using these simple and effective strategies, we propose a novel GNN model, called FSGNN. Using extensive experiments, we show that FSGNN outperforms the current state of the art GNN models on the node classification task. Analysis of the learned parameters provides us the crucial information of feature importance. Furthermore, we show that our model can be scaled for graphs with millions of nodes and billions of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION DETAILS</head><p>For reproducibility of experimental results, we provide the details of our experiment setup and hyperparameters of the model. We use PyTorch 1.6.0 as deep learning framework on Python 3.8. Model training is done on Nvidia V100 GPU with 16 GB graphics memory and CUDA version 10.2.89.</p><p>For node classfication results (2), we do grid search for learning rate and weight decay of the layers and dropout between the layers. Hyperparameters are set for first layer ğ‘“ ğ‘1, second layer ğ‘“ ğ‘2 and scalar weight parameter ğ‘ ğ‘ğ‘. ReLU is used as non-linear activation and Adam is used as the optimizer. Table <ref type="table" target="#tab_6">5</ref> shows details of hyperparameter search space. Table <ref type="table" target="#tab_8">6 and 7</ref> show the best hyperparameters for the model in 3-hop and 8-hop configuration respectively.</p><p>For experiments on ogbn-papers100M dataset, we did not do grid search. Based on the data from earlier experiments we manually tuned the hyperparameters to get the accuracy result. Batch size of 10000 was used for training data. Table <ref type="table" target="#tab_9">8</ref> shows the relevant hyperparameters for the model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Figure shows model diagram of FSGNN. Input features are generated based on powers of ğ´ and Ãƒ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 . 1</head><label>11</label><figDesc>Decouple feature generation and representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudo Code FSGNN (Forward propagation) Input :ğ´ ğ‘ ğ‘¦ğ‘š ; Ãƒğ‘ ğ‘¦ğ‘š ; No. of hops ğ¾; weight matrices ğ‘Š (ğ‘˜) ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>One exception is Pubmed, where the model's performance is relatively unperturbed under various hyperparameter combinations.XAX (A + I) 2 X A 2 X (A + I) 2 X A 3 X (A + I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Heatmap of average of learned soft-selection scalar for all datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Figure shows t-SNE plots of trained embeddings (3-hop) of Squirrel and Chameleon datasets without (left) and with hop-normalization (right). Points represent nodes and colors represent their respective labels. Mean classification accuracy without and with hop-normalization are 39.92% and 73.48% for Squirrel; 61.38% and 78.14% for Chameleon datasets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Figure shows the effect on classification accuracy of FSGNN with increase in the number of hops of feature aggregation on Cora (homophily) and Chameleon (heterophily) dataset. x-axis is in logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the node classification datasets</figDesc><table><row><cell>Datasets</cell><cell>Hom. Ratio</cell><cell>Nodes</cell><cell cols="3">Edges Features Classes</cell></row><row><cell>Cora</cell><cell>0.81</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>0.74</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell>Pubmed</cell><cell>0.80</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row><row><cell>Chameleon</cell><cell>0.23</cell><cell>2,277</cell><cell>36,101</cell><cell>2,325</cell><cell>4</cell></row><row><cell>Wisconsin</cell><cell>0.21</cell><cell>251</cell><cell>499</cell><cell>1,703</cell><cell>5</cell></row><row><cell>Texas</cell><cell>0.11</cell><cell>183</cell><cell>309</cell><cell>1,703</cell><cell>5</cell></row><row><cell>Cornell</cell><cell>0.30</cell><cell>183</cell><cell>295</cell><cell>1,703</cell><cell>5</cell></row><row><cell>Squirrel</cell><cell>0.22</cell><cell>5,201</cell><cell>198,353</cell><cell>2,089</cell><cell>5</cell></row><row><cell>Actor</cell><cell>0.22</cell><cell>7,600</cell><cell>26,659</cell><cell>932</cell><cell>5</cell></row><row><cell cols="2">ogbn-papers100M</cell><cell cols="2">111,059,956 1,615,685,872</cell><cell>128</cell><cell>172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean classification accuracy on fully-supervised node classification task. Results for GCN, GAT, GraphSAGE, Cheby+JK, MixHop and H2GCN-1 are taken from<ref type="bibr" target="#b34">[35]</ref>. For GEOM-GCN and GCNII results are taken from the respective article. Best performance for each dataset is marked as bold and second best performance is underlined for comparison.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell cols="2">Chameleon Wisconsin</cell><cell>Texas</cell><cell>Cornell</cell><cell>Squirrel</cell><cell>Actor</cell></row><row><cell>GCN</cell><cell cols="3">87.28Â±1.26 76.68Â±1.64 87.38Â±0.66</cell><cell>59.82Â±2.58</cell><cell>59.80Â±6.99</cell><cell>59.46Â±5.25</cell><cell>57.03Â±4.67</cell><cell>36.89Â±1.34</cell><cell>30.26Â±0.79</cell></row><row><cell>GAT</cell><cell cols="3">82.68Â±1.80 75.46Â±1.72 84.68Â±0.44</cell><cell>54.69Â±1.95</cell><cell>55.29Â±8.71</cell><cell>58.38Â±4.45</cell><cell>58.92Â±3.32</cell><cell>30.62Â±2.11</cell><cell>26.28Â±1.73</cell></row><row><cell cols="4">GraphSAGE 86.90Â±1.04 76.04Â±1.30 88.45Â±0.50</cell><cell>58.73Â±1.68</cell><cell>81.18Â±5.56</cell><cell>82.43Â±6.14</cell><cell>75.95Â±5.01</cell><cell>41.61Â±0.74</cell><cell>34.23Â±0.99</cell></row><row><cell>Cheby+JK</cell><cell cols="3">85.49Â±1.27 74.98Â±1.18 89.07Â±0.30</cell><cell>63.79Â±2.27</cell><cell>82.55Â±4.57</cell><cell>78.38Â±6.37</cell><cell>74.59Â±7.87</cell><cell>45.03Â±1.73</cell><cell>35.14Â±1.37</cell></row><row><cell>MixHop</cell><cell cols="3">87.61Â±0.85 76.26Â±1.33 85.31Â±0.61</cell><cell>60.50Â±2.53</cell><cell>75.88Â±4.90</cell><cell>77.84Â±7.73</cell><cell>73.51Â±6.34</cell><cell>43.80Â±1.48</cell><cell>32.22Â±2.34</cell></row><row><cell>GEOM-GCN</cell><cell>85.27</cell><cell>77.99</cell><cell>90.05</cell><cell>60.90</cell><cell>64.12</cell><cell>67.57</cell><cell>60.81</cell><cell>38.14</cell><cell>31.63</cell></row><row><cell>GCNII</cell><cell cols="3">88.01Â±1.33 77.13Â±1.38 90.30Â±0.37</cell><cell>62.48Â±2.74</cell><cell>81.57Â±4.98</cell><cell>77.84Â±5.64</cell><cell>76.49Â±4.37</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>H2GCN-1</cell><cell cols="3">86.92Â±1.37 77.07Â±1.64 89.40Â±0.34</cell><cell>57.11Â±1.58</cell><cell>86.67Â±4.69</cell><cell>84.86Â±6.77</cell><cell>82.16Â±4.80</cell><cell cols="2">36.42Â±1.89 35.86Â±1.03</cell></row><row><cell cols="4">Ours(3-hop) 87.73Â±1.36 77.19Â±1.35 89.73Â±0.39</cell><cell>78.14Â±1.25</cell><cell cols="3">88.43Â±3.22 87.30Â±5.55 87.03Â±5.77</cell><cell>73.48Â±2.13</cell><cell>35.67Â±0.69</cell></row><row><cell cols="4">Ours(8-hop) 87.93Â±1.00 77.40Â±1.93 89.75Â±0.39</cell><cell>78.27Â±1.28</cell><cell cols="5">87.84Â±3.37 87.30Â±5.28 87.84Â±6.19 74.10Â±1.89 35.75Â±0.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study over 1080 different hyperparameter settings. 48Â±1.44 89.24Â±0.27 72.48Â±4.16 81.48Â±5.62 78.80Â±5.88 78.09Â±2.22 63.57Â±6.83 33.54Â±1.21</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell cols="2">Chameleon Wisconsin</cell><cell>Texas</cell><cell>Cornell</cell><cell>Squirrel</cell><cell>Actor</cell></row><row><cell cols="4">Proposed 74.Without soft-selection 83.68Â±2.22 87.07Â±0.26 76.45Â±0.27 89.09Â±0.39</cell><cell>72.27Â±1.34</cell><cell>78.03Â±6.55</cell><cell>76.28Â±6.72</cell><cell>74.32Â±6.54</cell><cell>61.73Â±4.15</cell><cell>34.15Â±0.64</cell></row><row><cell>Common weight (ğ‘Š (0) )</cell><cell>83.19Â±1.41</cell><cell>72.15Â±1.02</cell><cell>88.96Â±0.28</cell><cell>68.24Â±6.03</cell><cell cols="2">70.56Â±10.94 68.45Â±7.65</cell><cell>68.18Â±9.13</cell><cell>56.63Â±8.54</cell><cell>32.73Â±1.48</cell></row><row><cell cols="4">Without Hop-normalization 77.12Â±3.49 71.40Â±10.01 87.72Â±0.77</cell><cell>53.06Â±6.18</cell><cell cols="2">82.60Â±2.68 76.33Â±3.87</cell><cell>76.18Â±3.43</cell><cell cols="2">32.60Â±6.38 36.66Â±0.55</cell></row><row><cell>6 DISCUSSION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6.1 Ablation Studies</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Mean classification accuracy on ogbn-100M dataset. SGC result is taken from<ref type="bibr" target="#b12">[13]</ref> and Node2Vec and SIGN results are taken from<ref type="bibr" target="#b8">[9]</ref>. Best performance is marked bold and second best performance is underlined.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>SGC</cell><cell>63.29Â±0.19</cell></row><row><cell cols="2">Node2Vec 58.07Â±0.28</cell></row><row><cell>SIGN</cell><cell>65.11Â±0.14</cell></row><row><cell>FSGNN</cell><cell>67.17Â±0.14</cell></row><row><cell cols="2">is intuitive as aggregated features from higher hops are not very</cell></row><row><cell cols="2">useful, and the model can learn to place low weights on them.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter search space</figDesc><table><row><cell>Hyperparameter</cell><cell>Values</cell></row><row><cell>ğ‘Š ğ· ğ‘ ğ‘ğ‘</cell><cell>0.0, 0.0001, 0.001, 0.01, 0.1</cell></row><row><cell>ğ¿ğ‘… ğ‘ ğ‘ğ‘</cell><cell>0.04, 0.02, 0.01, 0.005</cell></row><row><cell>ğ‘Š ğ· ğ‘“ ğ‘1</cell><cell>0.0, 0.0001, 0.001</cell></row><row><cell>ğ‘Š ğ· ğ‘“ ğ‘2</cell><cell>0.0, 0.0001, 0.001</cell></row><row><cell>ğ¿ğ‘… ğ‘“ ğ‘</cell><cell>0.01, 0.005</cell></row><row><cell>ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡</cell><cell>0.5, 0.6, 0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters of the 3-hop model Datasets ğ‘Š ğ· ğ‘ ğ‘ğ‘ ğ¿ğ‘… ğ‘ ğ‘ğ‘ ğ‘Š ğ· ğ‘“ ğ‘1 ğ‘Š ğ· ğ‘“ ğ‘2 ğ¿ğ‘… ğ‘“ ğ‘ ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡</figDesc><table><row><cell>Cora</cell><cell>0.1</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.6</cell></row><row><cell>Citeseer</cell><cell cols="2">0.0001 0.005</cell><cell>0.001</cell><cell>0.0</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Pubmed</cell><cell>0.01</cell><cell cols="2">0.005 0.0001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.7</cell></row><row><cell>Chameleon</cell><cell>0.1</cell><cell>0.005</cell><cell>0.0</cell><cell>0.0</cell><cell>0.005</cell><cell>0.5</cell></row><row><cell>Wisconsin</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Texas</cell><cell>0.001</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0</cell><cell>0.01</cell><cell>0.7</cell></row><row><cell>Cornell</cell><cell>0.0</cell><cell>0.01</cell><cell>0.001</cell><cell>0.001</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Squirrel</cell><cell>0.1</cell><cell>0.04</cell><cell>0.0</cell><cell>0.001</cell><cell>0.01</cell><cell>0.7</cell></row><row><cell>Actor</cell><cell>0.0</cell><cell>0.04</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters of the 8-hop model Datasets ğ‘Š ğ· ğ‘ ğ‘ğ‘ ğ¿ğ‘… ğ‘ ğ‘ğ‘ ğ‘Š ğ· ğ‘“ ğ‘1 ğ‘Š ğ· ğ‘“ ğ‘2 ğ¿ğ‘… ğ‘“ ğ‘ ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡</figDesc><table><row><cell>Cora</cell><cell>0.1</cell><cell>0.02</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.6</cell></row><row><cell>Citeseer</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Pubmed</cell><cell>0.01</cell><cell>0.02</cell><cell>0.0001</cell><cell>0.0</cell><cell>0.005</cell><cell>0.7</cell></row><row><cell>Chameleon</cell><cell>0.1</cell><cell>0.01</cell><cell>0.0</cell><cell>0.0</cell><cell>0.005</cell><cell>0.5</cell></row><row><cell>Wisconsin</cell><cell>0.001</cell><cell>0.02</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Texas</cell><cell>0.01</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0</cell><cell>0.01</cell><cell>0.7</cell></row><row><cell>Cornell</cell><cell>0.0</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Squirrel</cell><cell>0.1</cell><cell>0.02</cell><cell>0.0</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>Actor</cell><cell>0.0001</cell><cell>0.04</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.01</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters for the ogbn-paper100M dataset Dataset ğ‘Š ğ·ğ‘ ğ‘ğ‘ ğ¿ğ‘…ğ‘ ğ‘ğ‘ ğ‘Š ğ· ğ‘“ ğ‘1 ğ‘Š ğ· ğ‘“ ğ‘2</figDesc><table><row><cell>ğ¿ğ‘… ğ‘“ ğ‘1</cell><cell>ğ¿ğ‘… ğ‘“ ğ‘2 ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Conference'17, July 2017, Washington, DC, USA Maurya et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https://github.com/graphdml-uiuc-jlu/geom-gcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://ogb.stanford.edu/docs/nodeprop/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work was supported by JSPS Grant-in-Aid for Scientific Research (Grant Number 21K12042, 17H01785), JST CREST (Grant Number JPMJCR1687), and the New Energy and Industrial Technology Development Organization (Grant Number JPNP20006)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive diffusions for scalable learning over graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berberidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Signal Processing</title>
				<imprint>
			<date type="published" when="2019-03">Mar. 2019</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1307" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<idno>ArXiv, abs/1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03211</idno>
		<idno>arXiv: 1909. 03211</idno>
		<imprint>
			<date type="published" when="2019-11-18">Nov. 18, 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FastGCN: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">Feb. 15. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks. ICML</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09375[cs,stat]</idno>
		<idno>arXiv: 1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
				<imprint>
			<date type="published" when="2016-06-30">June 30, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">SIGN: scalable inception graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<idno>arXiv: 2004. 11198</idno>
		<imprint>
			<date type="published" when="2020-11-03">Nov. 3, 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Node2vec: scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Open graph benchmark: datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<idno>arXiv: 2005.00687</idno>
		<imprint>
			<date type="published" when="2021-01-23">Jan. 23, 2021</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-06-01">June 1, 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Predict then propagate: combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Visited on 01/29/2021</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">93</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">GraphNVP: an invertible flow model for generating molecular graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Madhawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11600</idno>
		<idno>arXiv: 1905.11600</idno>
		<imprint>
			<date type="published" when="2019-05-28">May 28, 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">2017. Sept. 2017</date>
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast approximations of betweenness centrality using graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Maurya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stacked graph filter</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10988</idno>
		<idno>arXiv: 2011.10988</idno>
		<imprint>
			<date type="published" when="2020-11-22">Nov. 22, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Geom-GCN: geometric graph convolutional networks. ICLR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DropEdge: towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13021</idno>
		<idno>arXiv: 1909. 13021</idno>
		<imprint>
			<date type="published" when="2020-03-10">Mar. 10, 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;09</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009-06-28">June 28, 2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<idno>ArXiv, abs/ 1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: when experts are not enough</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020-01-23">Jan. 23, 2020</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<idno>arXiv: 1901. 00596</idno>
		<imprint>
			<date type="published" when="2019-12-03">Dec. 3, 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-03">July 3. 2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for webscale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
				<meeting>the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>USA; MontrÃ©al, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
	<note>event-place</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
