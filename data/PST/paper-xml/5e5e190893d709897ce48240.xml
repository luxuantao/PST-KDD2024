<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiwen</forename><surname>Wang</surname></persName>
							<email>wanghaiwen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruijie</forename><surname>Wang</surname></persName>
							<email>xwang8@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuhao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuting</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinbing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author name ambiguity causes inadequacy and inconvenience in academic information retrieval, which raises the necessity of author name disambiguation (AND). Existing AND methods can be divided into two categories: the models focusing on content information to distinguish whether two papers are written by the same author, the models focusing on relation information to represent information as edges on the network and to quantify the similarity among papers. However, the former requires adequate labeled samples and informative negative samples, and are also ineffective in measuring the high-order connections among papers, while the latter needs complicated feature engineering or supervision to construct the network. We propose a novel generative adversarial framework to grow the two categories of models together: (i) the discriminative module distinguishes whether two papers are from the same author, and (ii) the generative module selects possibly homogeneous papers directly from the heterogeneous information network, which eliminates the complicated feature engineering. In such a way, the discriminative module guides the generative module to select homogeneous papers, and the generative module generates high-quality negative samples to train the discriminative module to make it aware of high-order connections among papers. Furthermore, a self-training strategy for the discriminative module and a random walk based generating algorithm are designed to make the training stable and efficient. Extensive experiments on two real-world AND benchmarks demonstrate that our model provides significant performance improvement over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>A person name is used to identify a certain individual. However, different people may have the same or the similar name in the real world, which is referred to as the name ambiguity. For example, Michael J can remind people of the US basketball player, the King of Pop or the machine learning professor from UC Berkeley. Name ambiguity causes inadequacy and inconvenience in information retrieval. With the rapid development of the scholar community, academic information in digital libraries becomes increasingly tremendous. However, names appearing in the digital papers or the webpages also suffer from the ambiguity issues, which means that the author name cannot be used to reliably identify all scholarly authors. The inadequacy of author name ambiguity becomes evident in many practical scenarios, e.g., scholar searching, influence evaluating and mentor recommendation, which raises the necessity of author name disambiguation <ref type="bibr" target="#b14">(Smalheiser and Torvik 2009)</ref>.</p><p>Author name disambiguation is to split the papers under the same name into several homogeneous groups, which has attracted substantial attention from information retrieval and data mining communities. Most existing methods solve this problem in a two-stage framework: (i) quantify the similarity among papers; (ii) cluster papers into homogeneous groups. Hierarchical clustering algorithm works well for the second part, while the first part remains largely unsolved. To quantify the similarity among papers, content information and relation information are used. The former includes title, abstract, introduction and keywords etc. Methods focusing on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr">Yoshida et al. 2010</ref>) usually leverage supervised learning algorithms to learn the pairwise similarity functions. However, they solve the problem in a local way, which means that they cannot measure the high-order connections among papers. Methods focusing on relation information <ref type="bibr" target="#b9">(Kanani, McCallum, and Pal 2007;</ref><ref type="bibr" target="#b0">Bekkerman and McCallum 2005)</ref> usually solve the problem on the bibliographic network, where the relation information is represented as edges on the network. They account that papers connected in the network are likely to be written by the same author. Thus constructing the network becomes the critical part of these methods, e.g., paper network <ref type="bibr" target="#b21">(Zhang et al. 2018)</ref>, paper-author network <ref type="bibr" target="#b20">(Zhang and Al Hasan 2017)</ref>. However, either complicated feature engineering or the supervision <ref type="bibr" target="#b21">(Zhang et al. 2018</ref>) is required.</p><p>The two categories of methods are like the two sides of the same coin. The first introduces supervision but cannot process high-order connections, while the second models the high-order connections but requires the supervision. An intuitive idea is to combine them together to build a unified model which can eliminate the requirement of labeled samples and complicated feature engineering to some extent. Inspired by generative adversarial networks <ref type="bibr" target="#b4">(Goodfellow et al. 2014)</ref>, we may combine the two categories in an adversarial way. In this paper, we propose a unified framework with discriminative module and generative module. The discrimi-native module directly distinguishes whether two papers are written by the same author based on feature vectors. This module is learned in a self-training way, and it requires negative samples generated by the generative module. The generative module works on the heterogeneous information network and selects papers viewed as the homogeneous pairs.</p><p>In this framework, the discriminative module can guide the exploration of the generative module to select homogeneous papers on the raw network. And the generative module can generate high-quality samples with high-order connections for the discriminative module, which can make it aware of the topology of the networks. We verify the performance of the proposed model on two benchmark datasets. The results demonstrate the significant superiority of our proposed method over the state-of-the-art author name disambiguation solutions.</p><p>In sum, the contributions of this paper are three-fold.</p><p>• We comprehensively take the content information and relation information into consideration by constructing the heterogeneous information network which eliminates the requirement for complicated feature engineering.</p><p>• We design a unified framework combining a discriminative module and a generative module based on the heterogeneous information network for author name disambiguation task. Experimental results on two real-world datasets verify the advantages of our method over stateof-the-arts.</p><p>• To support AND research, we construct a sufficiently large benchmark dataset consisting of 17,816 authors and 130,655 papers. Compared with the existing benchmark datasets, it is the largest AND dataset with rich content information and relation information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Author Name Disambiguation. To measure the similarity among papers, the existing methods can be divided into two categories according to the information they focus on. The first are based on the content information <ref type="bibr" target="#b6">(Han et al. 2004;</ref><ref type="bibr" target="#b8">Huang, Ertekin, and Giles 2006;</ref><ref type="bibr" target="#b12">Louppe et al. 2016;</ref><ref type="bibr">Yoshida et al. 2010)</ref>, which usually solve the problem in a discriminative way. These methods calculate the content similarity with the help of TF-IDF, exact-matching, and etc. Then they train supervised models by the labeled samples.  <ref type="bibr">(Tang et al. 2015)</ref> tries to preserve both of first-order and second-order network structures. Some literature explores NRL on heterogeneous networks <ref type="bibr">(Tang, Qu, and Mei 2015;</ref><ref type="bibr" target="#b3">Dong, Chawla, and Swami 2017)</ref>. However, existing algorithms are designed to preserve the topology information of the network in an unsupervised way. We implement it by the reward from the discriminative model in an adversarial framework. Generative Adversarial Networks. Recently, generative adversarial nets (GAN) <ref type="bibr" target="#b4">(Goodfellow et al. 2014</ref>) has attracted a great deal of attention. Original purpose of GAN is to generate data from the underlying true distribution, e.g., image <ref type="bibr" target="#b2">(Denton et al. 2015)</ref>, sequence <ref type="bibr" target="#b19">(Yu et al. 2017)</ref>, dialogue <ref type="bibr" target="#b11">(Li et al. 2017)</ref>. Some following literature modifies the framework for the purpose of the adversarial training. IRGAN <ref type="bibr" target="#b18">(Wang et al. 2017)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries Problem Formulation</head><p>Given an author name reference a, let P a = {p a 1 , . . . , p a N } be a set of N papers written by the authors with name a. Each paper p a i ∈ P a has the content feature set I a i including</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content2Vec</head><p>Figure 1: An overview of the proposed framework.</p><p>title, abstract, publish date, and etc. And it has the relation feature set R a i , which contains the relation of paper p a i to the entities in the academic domain including co-author, institute, field of study and venue. Given this, we define the problem of author name disambiguation as follows. Definition 1 Author Name Disambiguation. The task is to find a function Φ to partition P a into a set of disjoint clusters based on the content and relation feature sets I a , R a , i.e.,</p><formula xml:id="formula_0">Φ(P a |I a , R a ) → C a , where C a = {C a 1 , C a 2 , . . . , C a k }</formula><p>, where C a i means the homogeneous paper subset written by the ith author named a. We omit the superscript a in the following description if there is no ambiguity. Definition 2 Paper homogeneity. For the convenience of discussion, we define two papers are homogeneous if and only if they are written by the same author. Furthermore, let y ij denote the homogeneity of papers p i and p j , where y ij = 1 if p i and p j are homogeneous, and y ij = 0 otherwise. We denote the generated negative samples as S generated consisting of (p i , p j , y ij = 0), and the pseudo-positive samples for self-training as S pseudo consisting of (p i , p j , y ij = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneous Information Network</head><p>We solve this task with the help of academic heterogeneous information network (HIN), thus content information and relation information can be efficiently processed. We define the HIN as follows: Definition 3 Heterogeneous Information Network. The HIN under name reference a is defined as G = (V, R, I) where V is the vertex set including paper, co-author, field of study, institute and venue respectively, and R = T =V \P P ×T is the relation set representing the relations among papers and other classes of vertecs, and I is the content information of each p ∈ P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework</head><p>The proposed framework is shown in Figure <ref type="figure">1</ref>. In order to represent the information of the heterogeneous information</p><formula xml:id="formula_1">* )+-( 1) 1) ) ) ) &amp;)%+ '*,+ *#' *'$ ' )*)* )+-( )(+#,#. ))*+ ) ) ) ) )</formula><p>Policy gradient Then to integrate the content information and the relation information, and to select homogeneous papers in an adversarial way, we employ a generative adversarial module. The generative module aims to explore possible homogeneous papers from the heterogeneous information network, while the discriminative module tries to distinguish the generated negative papers and pseudo-positive papers. In such a way, the reward from the discriminative module guides the exploration for the generative module to select homogeneous papers. Moreover, the high-quality papers generated with high-order connections can make the discriminative module aware of the topology of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Learning Module</head><p>Content representation Papers written by different authors have various topics and literary styles. We extract those content features by integrating Doc2vec module (Le and Mikolov 2014) into our framework. This module learns a low dimension vector u i ∈ R k to represent the information from the content feature set I i of paper p i . The module updates the parameters by maximizing the log probability of the content sequence, i.e.,</p><formula xml:id="formula_2">θ u = arg max θ log Pr(w −b : w b |p i ; θ), (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where w is a word in I i of p i , b is the window size of the word sequence. After optimizing this objective, we can obtain the content representation u i of each paper p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation representation</head><p>The topology of HIN we described integrates the relation features of papers. Papers having relation features in common are connected in the HIN. Consequently, We can represent the relation features by preserving the connectivity information of the HIN. We use node2vec <ref type="bibr" target="#b5">(Grover and Leskovec 2016)</ref> to represent these features by v i ∈ R k , where papers are close in the feature space if they have similar relation information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Adversarial Module</head><p>The core part of our model is shown in Figure <ref type="figure" target="#fig_0">2</ref>, integrating content information and relation information of the papers in an adversarial way. A self-training strategy is added to our discriminative model, which uses top-relevant papers as positive samples iteratively. And to make the generative module aware of relation information, following <ref type="bibr" target="#b18">(Wang et al. 2018a)</ref>, we design a random walk based generating strategy. Given a paper p k , we design two modules as follows:</p><p>) ) ) )</p><p>) ) )</p><p>)</p><p>)</p><p>)</p><p>)</p><p>) ) )</p><p>) ) ) Generative module. G(p|p k ; θ G ), which learns to select the possible homogeneous papers under the guidance of reward. It will iteratively approximate the true underlying homogeneity distribution Pr true (p|p k ).</p><formula xml:id="formula_4">) ) ) ) ) ) ) '*, '!,#. ))* '*, )(+#,#. ))* +*. '!,#. ))* +*. )(+#,#. ))* *)* !'*, ))*+ ' )+-( )(+#,#. ))*+ #+,#'!-#+" )(+#,#. +&amp;)%+ ' '!,#. +&amp;)%+ *'$ ' )*)* )+-( )(+#,#. ))*+</formula><p>Two modules are combined by playing a minimax game: the generative module will try to choose the papers possibly written by the same author as the given paper p k , and therefore can fool the discriminative module; the discriminative module will distinguish between the selected papers and the ground truth papers. Formally, generative module G and discriminative module D are playing the following two-player minimax game with value function V (G, D):</p><formula xml:id="formula_5">min θ G max θ D V (G, D) = p k ∈P (E p∼Pr true (•|p k ) [log D(p, p k ; θ D )] + E p∼G(•|p k ;θ G ) [log(1 − D(p, p k ; θ D )]).</formula><p>(2)</p><p>The trainable parameters are the representation of all papers. They are learned by alternately minimizing and maximizing the value function in Eq. ( <ref type="formula">2</ref>) until the training procedure converges.</p><p>Implementation of Discriminative Module Given a paper pair (p, p k ), we employ a two-layer neural network as our discriminative module to integrate u and v together:</p><formula xml:id="formula_6">d pi = δ( W T 1 δ( W T 0 [ u i , v i ] + b 0 ) + b 1 ),<label>(3)</label></formula><formula xml:id="formula_7">D(p, p k ) = sigmoid( d T p d p k ),<label>(4)</label></formula><p>where δ(•) is non-linear activation function, d pi is the representation vector of paper p i for D, and θ D is the union of all d. According to Eq. ( <ref type="formula" target="#formula_7">4</ref>), the content information and relation information can be integrated simultaneously in d.</p><p>To eliminate the requirement for labeling process, we apply the idea of self-training <ref type="bibr">(Riloff and Jones 1999;</ref><ref type="bibr">Levin et al. 2012)</ref> to select positive samples. Before each D iteration, we select the top possibility papers based on the present results. The selected papers are viewed as a pseudopositive sample set in the next training process until another selection is performed. The training process of the discriminative module is shown in Figure <ref type="figure" target="#fig_1">3</ref>. We update d by ascending the gradient concerning the pseudo-positive samples and the generated negative samples: </p><formula xml:id="formula_8">) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) *#!#'% ",</formula><formula xml:id="formula_9">∇ θ D = ∇ θ D (log(D(p, p k )), if (p, p k , 1) ∈ S pseudo ; ∇ θ D (log(1 − D(p, p k )), if (p, p k , 0) ∈ S generated .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of Generative Module</head><p>The generator aims to select the papers which are possibly homogeneous from the constructed HIN. Once the discriminator cannot distinguish whether the papers are selected by the generator, the generator is guided to find the rules to select the homogeneous papers. To update θ G , we follow <ref type="bibr" target="#b14">(Schulman et al. 2015)</ref> to compute the gradient of V (G, D) by policy gradient:</p><formula xml:id="formula_10">∇ θ G V (G, D) = pi∈P E p∼G(•|p k ) [∇ θ G log G(p|p k ) log(1 − D(p, p k ))].<label>(6)</label></formula><p>During each G iteration, the generator selects the most similar papers from the HIN. The reward log(1 − D(p, p k )) from the discriminator pushes the generator to update θ G , thus the similarity among papers will finally indicate the homogeneity among papers.</p><p>As for the quantification of similarity, a straightforward way is to define it as a softmax function over all other papers:</p><formula xml:id="formula_11">G(p|p k ) = exp( g T p g p k ) p∈P,p =p k exp( g T p g p k ) , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where g p , g p k are the k-dimension representation vectors of papers p and p k respectively for generator. And the parameters θ G are the union of all g vectors. However, two limitations still exist:</p><p>1. It entirely depends on the reward from the discriminator, ignoring the content information and relation information. We expect a way that generator can comprehend the information and make a wiser selection.</p><p>2. It is time-consuming, because the similarity between each pair of papers need to be calculated. A more efficient generating strategy is required for the large-scale application.</p><p>Here, we describe an information-aware generating strategy in detail, which is shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>At first, let N (i) p be the set of the papers that have i-order relation with p. For i = 1, we define the homogeneity probability of p ∈ N (1) p given p k as follows:</p><formula xml:id="formula_13">Pr(p|p k ) = tm∈T share exp( g p g T tm • g p k g T tm ) pj ∈N (1) p k t∈T exp( g pj g T t • g p k g T t )</formula><p>, (8)</p><p>where g pi is the representation of papers for G, and g tm is the representation of t ∈ V \P . It indicates that the papers that are connected by more entities are more possibly written by the same author.</p><p>We then define G(p|p k ) as follows:</p><formula xml:id="formula_14">G(p|p k ) = Pr(p|p k ), if p ∈ N (1) p k ; m Pr(p|pm)G(pm|p k ), if p ∈ N (i) p k , i = 1.<label>(9)</label></formula><p>Eq. ( <ref type="formula" target="#formula_14">9</ref>) models the possibility of homogeneity among papers which have high-order connections. In practice, two papers written by the same author can be connected by a complicated path instead of two edges, e.g., p − A − p i − I − p k , p and p k do not have straight connection, but they both have close relation with p i , indicating that all of three papers are written by the same author.</p><p>Since Eq. ( <ref type="formula" target="#formula_14">9</ref>) is computationally inefficient, we implement it with the help of paper network. Based on the heterogeneous network, we construct the paper network G p at first. The weights of edges are decided by Eq. ( <ref type="formula">8</ref>). Then we construct a tree T p k : (i) Add the given paper p k into T p k ; (ii) Add the edge (p i , p j ) with highest weight into T p k , where (p i , p j ) / ∈ T p k ; (iii) Repeat step 2 until all papers in G p are added into T p . Then there is a path P p k →pi from p k to p i on the spanning tree T p . The G(p|p k ) is simplified as follows:</p><formula xml:id="formula_15">G(p|p k ) = Π (pm,pm+1)∈Pp k →p i Pr(p m |p m+1 ). (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>A straightforward interpretation of this process based on spanning tree is that given a paper p k , we first select the papers that are very similar to it as the homogeneous group. Then papers that are similar to the papers in the group are also possibly written by the same author. The spanning tree based strategy preserves the high-order connections among papers, which integrates the relation information.</p><p>Next, we discuss the selecting strategy for generator. We perform a random walk on T p starting at paper p k with respect to Eq. ( <ref type="formula">8</ref>). During this process, once the generator decides to visit a paper that has been visited, the random walk is halted and the papers in the path will be selected by generator. These papers are selected by generator as the homogeneous papers with p k , then they will be fed into discriminator as negative papers.</p><p>The algorithm maintains time efficiency and informationawareness:</p><p>• Given a paper, the generator only considers papers from the connected component N p k as candidates, which means there is no need to calculate the pairwise possibility with all the other papers. • While selecting papers, it takes the information from the heterogeneous network into consideration. First, to calculate the pairwise possibility, Eq. ( <ref type="formula">8</ref>) integrates the re-lation information from the heterogeneous network. Besides, the random walk based generating algorithm comprehensively takes the high-order connection among papers into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering</head><p>Based on the final representation d, g of papers, we perform hierarchy agglomerative clustering (HAC) to partition N papers into disjoint homogeneous sets.</p><p>The process for the author name disambiguation is summarized in Algorithm 1. Update θ G according to Eq. ( <ref type="formula" target="#formula_10">6</ref>), ( <ref type="formula">8</ref>), (10); </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Datasets</head><p>To evaluate the proposed method, we collect two real-world author name disambiguation datasets for experiments: • AMiner-AND<ref type="foot" target="#foot_0">1</ref> . The dataset is released by <ref type="bibr" target="#b21">(Zhang et al. 2018)</ref>, which contains 500 author names for training and 100 author names for testing. We construct the heterogeneous network including papers, co-authors, author affiliations (which are referred as institutes in our model), keywords (which are referred as fields of study in our model) and venues. However, there is no abstract in this dataset, so we can only use the titles as our content information in the experiment on this dataset. To illustrate our model's ability to combine content information and relation information and to support the researches which study the author name disambiguation task using content information, we construct a new dataset collected from AceKG <ref type="bibr" target="#b18">(Wang et al. 2018b</ref>). The benchmark dataset consists of 130,655 papers from 17,816 distinguished authors. Each sample has the relation information and content information required by the proposed model. The labeling process is carried out comprehensively based on the e-mail address of authors, the co-author information and the institute information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our model against three state-of-the-art name disambiguation methods. We perform the hierarchical agglomerative clustering algorithm based on the results from these models and compare them by the pairwise Precision, Recall, and F1-score.</p><p>Zhang and Al Hasan (2017): This model constructs three networks under each name reference. The vertices are authors and papers. The weights of edges represent the connections among them. A designed network embedding is learned with an aim to preserve the connectivity of the constructed networks. <ref type="bibr" target="#b12">Louppe et al. (2016)</ref>: This model trains a function to measure the similarity between each pair of papers using the carefully designed pairwise features, including author names, titles, institute names etc.</p><p>AMiner <ref type="bibr" target="#b21">(Zhang et al. 2018</ref>): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on the first stage. In the local stage, it constructs a paper network, where the weight of edge reflects the similarity among papers. Then it uses graph convolutional network to preserve the connectivity of the paper network and learn the representation of papers.</p><p>To further evaluate the performance of each module, we also compare our performance at different stages.</p><p>Con. This is the result based on the content representation result produced by Doc2vec module. This module represents the abstract and title information by a vector.</p><p>Rel. This is the result based on the relation representation results, which maps the nodes in heterogeneous information network into low-dimension representation space.</p><p>Dis. The result is from discriminator which aims to distinguish whether two papers are homogeneous based on information representation and relation representation.</p><p>Gen. The result is from the generator which aims to approximate the underlying homogeneity distribution and to extract the high-order connections on HIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Results</head><p>We examine our model with several state-of-the-art models on AMiner-AND and AceKG-AND. In the experiment on AMiner-AND, we use 100 names for testing and compare the result with the results of other models reported in <ref type="bibr" target="#b21">(Zhang et al. 2018</ref>). In the experiment on AceKG-AND, we sample 85 names for testing. Since Louppe et al. and AMiner are supervised algorithms, the results from 5-folds crossvalidation are reported. Hierarchy agglomerative clustering is performed on the results produced, where the number of clusters is given in advance.</p><p>Table <ref type="table" target="#tab_4">2</ref> shows the overall performances of different models on two datasets. All the reported metrics are the macro-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>To evaluate the performance of each module, we also present our performance at different stages in Table <ref type="table" target="#tab_5">3</ref>. It can be seen that the generative module achieves the most significant result. It can mine some high-order connections among papers and thus covers more homogeneous papers as candidate set.</p><p>Content representation module achieves a good result on AceKG-AND, while the result on AMiner-AND is low. Because this dataset only provides title as content information. The experiment has illustrated that content information like abstract is valuable for this task.</p><p>The discriminative module achieves the highest Prec on two datasets. Because it mainly measures the pairwise similarity, the papers written by the same author can be discovered precisely. The problem is that it solves the problem from a local perspective, which leads to a low Rec result.</p><p>Experiments show that relation representation results achieve F1-scores 56.34% and 53.65% on two datasets respectively. For those homogeneous papers which are connected tightly by the relations, they are close in the relation representation space, which works for the clustering stage. However, for those which are content related but have few relations, this module can not group them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Analysis</head><p>To dig into how each module works, we visualize the results of each stage in a 2-D way, which is presented in Figure <ref type="figure" target="#fig_5">5</ref>. We analyze the layout of blue points in feature space. After a global measurement by content representation module, the papers in the same C a are preliminarily clustered together in Figure <ref type="figure" target="#fig_5">5a</ref>. Figure <ref type="figure" target="#fig_5">5b</ref> shows the results of relation representation module. It can be seen that homogeneous papers are grouped much better. The clustering results of discriminator and generator are much better, for they consider both of the content information and relation information. The blue points are grouped into one cluster successfully. And clusters in Figure <ref type="figure" target="#fig_5">5d</ref> have clearer boundary than clusters in Figure <ref type="figure" target="#fig_5">5c</ref>, which corresponds to the fact that the generator achieves a better result than discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel adversarial representation learning model for heterogeneous information network in the academic domain. We employ this model to deal with author name disambiguation task, which integrates the advantages from both generative methods and discriminative methods. To eliminate the requirement for labeled samples and to measure high-order connections among papers well, a self-training strategy for discriminator and a random walk based exploration for the generator are designed. Experimental results on AceKG-AND and AMiner-AND datasets verify the advantages of our method over state-of-the-art name disambiguation methods. Besides, we plan to employ the proposed adversarial representation learning model on paper recommendation and mentor recommendation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of generative adversarial module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Self-training strategy of the discriminative module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Generating strategy of the generative module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on d and select top-relevant papers into S pseudo for p k ∈ P a ; 12 for D-steps do 13 Sample positive papers from S pseudo and negative papers from S generated ; 14 Update θ D according to Eq. (3algorithm based on representation result θ D and θ G ; 18 return Φ(P a |I a , R a ) → C a ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: t-SNE Visualization of embedding spaces on a name reference Yang Liu in AceKG-AND. Each color in (a), (b), (c), (d) denotes a homogeneous cluster according to ground truth. Con Emb. represents the content representation result by Doc2vec. Rel Emb. represents the relation representation result by Node2vec. Dis Emb. and Gen Emb. are the results based on discriminator and generator results respectively. The dashed black ellipses circle the points of the same author.</figDesc><graphic url="image-1.png" coords="7,53.84,53.82,121.23,95.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1: The proposed framework. Data: Paper set P a and information set I a , R a . Result: The partition result, Φ(P a |I a , R a ) → C a . 1 Construct the heterogeneous informaton network G a ; 2 Utilize content2vec module to learn content representation u; 3 Utilize node2vec module to learn content representation v; 4 Initialize G(p|p k ; θ G ) and D(p, p k ; θ D ) based on u, v; 5 while model not converge do 6 Construct G p k and T p k according to Eq. (8); Perform random walk on T p k and generate papers into S generated for each p k ∈ P a ;</figDesc><table><row><cell>7</cell><cell>for G-steps do</cell></row><row><cell>8</cell><cell></cell></row><row><cell>9</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The detailed results on AceKG-AND.</figDesc><table><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell cols="3">AMiner (Zhang et al. 2018)</cell><cell cols="3">Louppe et al. (2016)</cell><cell cols="3">Zhang and Al Hasan (2017)</cell></row><row><cell>Name</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell>A. Kumar</cell><cell cols="6">74.56 50.30 60.07 63.59 66.61 65.07</cell><cell cols="2">73.70 43.83</cell><cell cols="4">54.97 46.01 25.05 32.44</cell></row><row><cell>Bo Jiang</cell><cell cols="6">90.11 51.79 65.77 69.28 54.77 61.18</cell><cell cols="2">62.28 56.81</cell><cell cols="4">59.42 97.47 92.94 95.15</cell></row><row><cell cols="7">Chi Zhang 81.36 78.39 79.85 53.88 49.46 51.58</cell><cell cols="2">61.71 48.66</cell><cell cols="4">54.42 78.63 73.81 76.14</cell></row><row><cell>Dong Xu</cell><cell cols="6">95.64 93.38 94.50 78.46 73.61 75.96</cell><cell cols="6">39.72 100.00 56.86 96.12 62.64 75.85</cell></row><row><cell cols="7">Fan Zhang 80.37 80.07 80.22 50.76 66.67 57.64</cell><cell cols="2">72.80 84.60</cell><cell cols="4">78.26 92.95 80.87 86.49</cell></row><row><cell>Hui Li</cell><cell cols="6">65.83 43.90 52.68 56.53 32.98 41.65</cell><cell cols="2">59.60 36.32</cell><cell cols="4">45.14 58.26 24.45 34.45</cell></row><row><cell>Jie Liu</cell><cell cols="6">66.32 80.27 72.63 47.94 28.93 36.08</cell><cell cols="2">47.80 38.45</cell><cell cols="4">42.61 84.39 49.32 62.26</cell></row><row><cell>Jie Yang</cell><cell cols="6">91.39 88.72 90.03 71.60 70.68 71.14</cell><cell cols="2">46.90 55.61</cell><cell cols="4">50.89 90.18 72.34 80.28</cell></row><row><cell>Lin Ma</cell><cell cols="6">92.82 85.33 88.92 60.55 63.46 61.97</cell><cell cols="2">66.35 65.51</cell><cell cols="4">65.93 87.73 68.42 76.88</cell></row><row><cell>Lin Zhang</cell><cell cols="6">76.13 73.62 74.85 70.20 55.80 62.18</cell><cell cols="2">53.05 38.69</cell><cell cols="4">44.74 91.18 59.38 71.93</cell></row><row><cell cols="7">Qian Wang 96.55 84.70 90.24 73.80 73.02 73.40</cell><cell cols="2">70.19 63.96</cell><cell cols="4">66.93 85.04 74.71 79.54</cell></row><row><cell>Tao Chen</cell><cell cols="6">89.50 82.23 85.71 63.86 40.28 49.40</cell><cell cols="2">53.40 44.31</cell><cell cols="4">48.43 90.91 41.80 57.27</cell></row><row><cell>Wei Gao</cell><cell cols="6">92.62 94.99 93.79 78.34 73.78 75.99</cell><cell cols="2">70.18 40.68</cell><cell cols="4">51.51 85.19 63.50 72.76</cell></row><row><cell>Wei Lu</cell><cell cols="6">71.60 55.90 62.79 53.88 45.01 49.04</cell><cell cols="2">52.45 34.11</cell><cell cols="4">41.34 62.44 31.19 41.60</cell></row><row><cell>Yong Xu</cell><cell cols="6">91.64 89.01 90.31 49.28 55.59 52.24</cell><cell cols="2">56.72 54.80</cell><cell cols="4">55.74 68.40 54.55 60.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results of author name disambiguation. 78.26 70.73 73.71 * * indicates that the F 1 score of our model is the significant result over other models, with p-value less than 10 −6 .</figDesc><table><row><cell></cell><cell cols="2">AMiner-AND</cell><cell></cell><cell></cell><cell>AceKG-AND</cell><cell></cell></row><row><cell>Model</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell cols="7">Zhang and Al 70.63 59.53 62.81 72.35 54.24 60.71</cell></row><row><cell cols="7">Louppe et al. 57.09 77.22 63.10 56.69 57.82 55.88</cell></row><row><cell>AMiner Ours</cell><cell cols="6">77.96 63.03 67.79 58.57 55.41 56.21 82.23 67.23 72.92</cell></row></table><note>*  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of components in the framework.</figDesc><table><row><cell></cell><cell cols="2">AMiner-AND</cell><cell></cell><cell cols="2">AceKG-AND</cell></row><row><cell cols="2">Model Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell>Con</cell><cell cols="6">15.74 9.31 11.30 69.57 47.68 55.40</cell></row><row><cell>Rel</cell><cell cols="6">74.32 51.38 56.34 69.74 45.84 53.65</cell></row><row><cell>Dis</cell><cell cols="6">84.58 59.83 68.00 84.80 55.09 65.41</cell></row><row><cell>Gen</cell><cell cols="6">82.23 67.23 72.92 78.26 70.73 73.71</cell></row><row><cell cols="7">averaged scores of each metric of all test names. Our</cell></row><row><cell cols="7">model outperforms all the other baselines by at least 5.13%</cell></row><row><cell cols="7">and 13.00% in F1 score on the two datasets respectively.</cell></row><row><cell cols="7">On AMiner-AND, our model outperforms the baselines in</cell></row><row><cell cols="7">terms of F1-score (+10.11% over Zhang et al., +9.82%</cell></row><row><cell cols="7">over Louppe et al. and +5.13% over AMiner relatively). On</cell></row><row><cell cols="7">AceKG-AND, the superiority is the same. As shown in Ta-</cell></row><row><cell cols="7">ble 1, almost all the metrics of 15 random name references</cell></row><row><cell cols="7">are improved by our model, which demonstrates the signifi-</cell></row><row><cell cols="5">cant superiority of our proposed method.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.aminer.cn/na-data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by National Key R&amp;D Program of China 2018YFB1004700, NSF China under Grant 61822206, Grant 61960206002, Grant 61532012, Grant 61602303, Grant 61829201, Grant 61702327, Grant 61632017.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Disambiguating web appearances of people in a social network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bekkerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;05</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kbgan: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metap-ath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two supervised learning approaches for name disambiguation in author citations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL&apos;04</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entity disambiguation in anonymized graphs using graph kernels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hermansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;13</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1037" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient name disambiguation for large-scale databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD&apos;06</title>
				<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="536" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving author coreference by resource-bounded information gathering from the web</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;07</title>
				<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Citation-based bootstrapping for large-scale author disambiguation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno>II-1188-II-1196</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2014. 2012</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1030" to="1047" />
		</imprint>
	</monogr>
	<note>JMLR.org. Levin,</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;17</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Engineering and Semantic Web</title>
				<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2016. 2013a. 2013b</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note type="report_type">CoRR abs/1301.3781. Mikolov,</note>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning dictionaries for information extraction by multi-level bootstrapping</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI &apos;99/IAAI &apos;99</title>
				<imprint>
			<publisher>American Association for Artificial Intelligence</publisher>
			<date type="published" when="1999">2014. 1999</date>
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
	<note>KDD&apos;14</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Smalheiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Torvik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2015. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="43" />
		</imprint>
	</monogr>
	<note>Author name disambiguation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for name disambiguation in digital library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C M</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="975" to="987" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Irgan: A minimax game for unifying generative and discriminative information retrieval models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Acm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2017. 2018a. 2018b. 2010</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
	<note>SIGIR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2852" to="00092858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Name disambiguation in aminer: Clustering, maintenance, and human in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining, KDD &apos;18</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
