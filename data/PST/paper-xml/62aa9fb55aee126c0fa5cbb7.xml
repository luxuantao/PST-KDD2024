<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Large-scale Subsurface Simulations with a Hybrid Graph Network Simulator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-15">15 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tailin</forename><surname>Wu</surname></persName>
							<email>tailin@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
							<email>rexying@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rok</forename><surname>Sosi?</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saudi</forename><surname>Aramco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Hamam</surname></persName>
							<email>hassan.hamam@aramco.com</email>
						</author>
						<author>
							<persName><forename type="first">Marko</forename><surname>Maucec</surname></persName>
							<email>marko.maucec@aramco.com</email>
						</author>
						<author>
							<persName><forename type="first">Qinchen</forename><surname>Wang</surname></persName>
							<email>qinchenw@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
							<email>yinanzy@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ridwan</forename><surname>Jalali</surname></persName>
							<email>ridwan.jalali@aramco.com</email>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><forename type="middle">2022</forename><surname>Leskovec</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Qinchen Wang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Yinan Zhang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University Kaidi Cao</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Stanford University Ridwan Jalali</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Large-scale Subsurface Simulations with a Hybrid Graph Network Simulator</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-15">15 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539045</idno>
					<idno type="arXiv">arXiv:2206.07680v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>subsurface simulations</term>
					<term>hybrid graph neural network</term>
					<term>multi-scale</term>
					<term>large-scale</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subsurface simulations use computational models to predict the flow of fluids (e.g., oil, water, gas) through porous media. These simulations are pivotal in industrial applications such as petroleum production, where fast and accurate models are needed for highstake decision making, for example, for well placement optimization and field development planning. Classical finite difference numerical simulators require massive computational resources to model large-scale real-world reservoirs. Alternatively, streamline simulators and data-driven surrogate models are computationally more efficient by relying on approximate physics models, however they are insufficient to model complex reservoir dynamics at scale.</p><p>Here we introduce Hybrid Graph Network Simulator (HGNS), which is a data-driven surrogate model for learning reservoir simulations of 3D subsurface fluid flows. To model complex reservoir dynamics at both local and global scale, HGNS consists of a subsurface graph neural network (SGNN) to model the evolution of fluid flows, and a 3D-U-Net to model the evolution of pressure. HGNS is able to scale to grids with millions of cells per time step, two orders of magnitude higher than previous surrogate models, and can accurately predict the fluid flow for tens of time steps (years into the future). Using an industry-standard subsurface flow dataset (SPE-10) with 1.1 million cells, we demonstrate that HGNS is able to reduce the inference time up to 18 times compared to standard subsurface simulators, and that it outperforms other learning-based models by reducing long-term prediction errors by up to 21%. Project website can be found at http://snap.stanford.edu/hgns/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Subsurface simulation is a discipline, where computational models are used to predict the flow of fluids (e.g., oil, water, gas) through porous media. It is pivotal for effective management of hydrocarbon and groundwater resources. In the petroleum industry, reservoir simulation is essential for efficient oil and gas field development planning, where decisions on well placement and well management require fast and reliable subsurface simulation models that are dynamically calibrated, i.e., history matched. Such reservoir models reproduce historical operational events and facilitate production forecasting and optimization under reservoir uncertainty <ref type="bibr" target="#b8">[9]</ref>.</p><p>The goal of simulations is to take as input static properties of the rock, initial states of quantities such as oil and water, and external control variables such as injection of water, and then predict the evolution of pressure and saturation of fluids over time (Fig. <ref type="figure">1</ref>). Two key challenges need to be addressed for practical, large-scale applications. First, subsurface flow is a complicated multi-scale problem. On the smaller spatial scale, it needs to model multiphase fluids flow through the complex subsurface structures: between neighboring cells (a cell is a discretization of space containing porous media through which fluid can flow), with various well configurations (wells can inject or produce fluids externally), and in presence of flow barriers (e.g., fault planes that can divert or prevent fluid paths). On a larger spatial scale, the flow of fluids is Figure <ref type="figure">1</ref>: Subsurface simulation model <ref type="bibr" target="#b1">[2]</ref>. Water and oil exist in the porous rock which is discretized into cells, representing a computational grid. To produce oil under waterflooding scheme, water is pumped into the reservoir using injectors (blue pins), which creates a pressure gradient and preferential fluid flow path from the injectors to the oil producers (red pins). Colors indicate oil saturation level.</p><p>driven by convection forces and pressure gradient, whereas the dynamics of reservoir pressure is governed by global pore fluid distribution and reaches the equilibrium much faster. Therefore, we need to model both small and large spatial scales. Second, the models often needs to scale to extremely large grids. For example, a standard industry problem typically consists of millions or tens of millions of cells. Field development applications, such as well configuration and completion for optimal dynamic performance and improved sweep, could benefit significantly from subsurface simulators that can perform fast inference and scale to large grids.</p><p>Standard subsurface simulators employ domain-specific implicit partial differential equation (PDE) solvers. For large grids with tens of millions of cells, they still need to solve an equation involving the full grid, which may be computationally exhaustive and requires substantial inference time. Recently, data-driven surrogate models provide a promising complementary approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. They learn directly from data and may alleviate years of engineering development. Moreover, prior works in other domains show that the models can learn forward evolution <ref type="bibr" target="#b18">[19]</ref> and can have larger spatial and temporal intervals <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. However, these models are insufficient to address the two challenges of subsurface simulation. They are not able to model the multi-scale dynamics, because they utilize Gaussian process <ref type="bibr" target="#b11">[12]</ref>, polynomial chaos <ref type="bibr" target="#b3">[4]</ref>, feed-forward neural network <ref type="bibr" target="#b7">[8]</ref>, convolutional neural networks <ref type="bibr" target="#b25">[26]</ref>, or recurrent R-U-Net <ref type="bibr" target="#b19">[20]</ref>, which cannot simultaneously model lower-level interactions between neighboring cells as well as global dynamics such as pressure. Additionally, they do not scale to large-scale simulations as they have only been applied to 2D grids with up to 10k vertices (while a practical simulation requires 3D grids), and up to 20k nodes for machine-learning-based surrogate models developed in other domains, e.g. GNS <ref type="bibr" target="#b18">[19]</ref>, which is two to three orders of magnitude smaller than needed for standard industry applications.</p><p>Present work. Here we introduce a Hybrid Graph Network Simulator (HGNS) for subsurface simulation, which addresses the above two challenges. HGNS consists of a Subsurface Graph Neural Network (SGNN) designed to model the fluid flow through the complicated subsurface structure, and a 3D-U-Net <ref type="bibr" target="#b6">[7]</ref> to model the more global dynamics of pressure. SGNN and 3D-U-Net cooperatively learn to evolve the subsurface dynamics. For large grids, which make training especially hard (exceeding GPU memory, training may take weeks), we developed a sector-based training. It allows training on grids of up to 1.1 million of cells, two orders of magnitude larger than previous machine-learning-based surrogate models. The ability to deal with models of this size makes HGNS the first fully machine-learning-based subsurface model applied to realistic 3D scenarios.</p><p>We use an industry-standard subsurface flow dataset to evaluate our model's generalization capabilities in a challenging setting where the initial conditions, static properties and well locations are different than that of training, and compare its performance with strong baselines. We show that HGNS produces more accurate long-term evolution, and outperforms other learning-based models by reducing long-term prediction errors by up to 21%. Moreover, our model running on a single GPU is between 2 to 18 times faster than a standard subsurface solver using 8 CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There has been extensive research for developing surrogate models for subsurface flow. One line of research focuses on physics-based methods, for example coarse-grid modeling, reduced-physics modeling, or proper orthogonal decomposition (POD)-based reducedorder modeling (ROM) <ref type="bibr">[6, 13-15, 22, 24, 25]</ref>. These methods typically simplify aspects of the problem to make it tractable. For example, POD-based ROM can be thought of as using a linear encoder to map the full trajectory into a low-dimensional space. In addition, Fraces et al. <ref type="bibr" target="#b9">[10]</ref> use physics-informed neural networks (PINN) and apply transfer learning and generative methods to solve an inference problem for 2-phase immiscible transport. While Cai et al. <ref type="bibr" target="#b4">[5]</ref> and references therein provide a review of PINN for solving inverse problems in fluid mechanics, Fuks and Tchelepi <ref type="bibr" target="#b10">[11]</ref> demonstrate that physics-informed ML approaches fail to approximate the fluid-flow dynamics governed by nonlinear PDEs in the presence of sharp variations of saturation and propose the solution by adding a small amount of diffusion to the conservation equation.</p><p>Data-driven surrogate models. Data-driven surrogate modeling relies on the data to learn evolution models. Hamdi et al. <ref type="bibr" target="#b11">[12]</ref> utilize Gaussian process to model a 20-parameter unconventional-gas reservoir system. Bazargan et al. <ref type="bibr" target="#b3">[4]</ref> employ polynomial chaos to model a 40-parameter 2D fluvial channelized reservoir undergoing waterflooding. Apart from the above, feed-forward neural networks <ref type="bibr" target="#b7">[8]</ref>, convolutional neural networks (CNN) <ref type="bibr" target="#b25">[26]</ref>, recurrent R-U-Net <ref type="bibr" target="#b19">[20]</ref> are also used to learn subsurface surrogate models. However, the models above are insufficient when it comes to modeling how the fluid flows through the complicated subsurface structures. These physics-based methods are not able to generalize to trajectories that are not sufficiently close to that of training <ref type="bibr" target="#b19">[20]</ref>,  and are insufficient to model the nonlinear multiscale behavior of the subsurface flow. For example, it is hard for convolution-based models (e.g. CNN, U-Net) to model the fluid flow between normal cells, well perforations, and faults where fluid cannot flow through.</p><p>Our HGNS utilizes Graph Neural Networks (GNN) to model the fluid flow, whose relation-based prior can naturally capture the complicated interaction among the subsurface structures. For example, faults can be modeled by cutting the edges on a fault plane for the GNN, so by design the cells on the two sides cannot interact why CNN may need much data to learn that behavior. Moreover, prior works in data-driven subsurface modeling are limited to 2D subsurface simulations, with up to 10k cells. In comparison, our HGNS is the first learned surrogate model applied to 3D subsurface, and can operate on data two orders of magnitude larger than prior applications. A concurrent work <ref type="bibr" target="#b16">[17]</ref> applies interaction network <ref type="bibr" target="#b2">[3]</ref> to model the subsurface fluid dynamics with up to 1 million cells, but requires the solver to provide the ground-truth for pressure.</p><p>Generic learned surrogate models. Outside of subsurface simulation, there has been active work to develop generic learned surrogate models. Our work builds upon prior work of Graph Network Simulators (GNS) <ref type="bibr" target="#b18">[19]</ref>, a state-of-the-art GNN-based model for particle-based simulation. Compared to GNS, our Hybrid Graph Network Simulator (GHNS) is a hybrid architecture that only uses GNN to model the fluid part. Furthermore, we use a different method to construct node and edge features, compute the message, and incorporate the domain knowledge of subsurface simulation. Furthermore, our sector-based training allows HGNS to learn on datasets with at least two orders of magnitude more nodes than GNS, and our multi-step objective allows better long-term prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>We consider the problem of subsurface simulation of oil-water flow, which models how the pressure and saturation of fluids evolve over time, given initial states, static properties of the rock, and external control variables such as injection of water. Here we present a simplified Partial Differential Equation (PDE) for the system:</p><formula xml:id="formula_0">?(?? ? ? ? ) ?? = ? ? ( ? ? ? ? ? ? ? (? ? )k??) + ? ?<label>(1)</label></formula><p>Here ? = ?, ? denotes different components/phases, with ? for water and ? for oil. ? ? ? [0, 1] is the saturation for phase ? (saturation is the ratio between the present volume of component ? and the pore volume ? the rock can hold at a location), and ? is the pressure. ? ? is phase density, ? ? is the phase viscosity, and ? is the rock porosity. ? ? ? (? ? ) is the relative permeability that is a function of the saturation ? ? , usually obtained via laboratory measurements. k is the absolute permeability tensor. ? ? is the source/sink term, which corresponds to the injecting or producing of component ? at the well location. In this equation, the saturation ? ? and pressure ? are dynamic variables that vary with time and space. ?, ? ? , ? ? , k are static variables that are constant in time but typically vary in space.</p><p>The injection of water is externally controlled, and the production of water/oil ? ? at producer wells is also a dynamic variable. We can intuitively understand this equation as follows: the change of water/oil saturation ? ? is due to two terms: the divergence of the flux</p><formula xml:id="formula_1">? ? = - ? ?</formula><p>? ? ? ? ? (? ? )k?? and a source/sink ? ? term. The flux ? ? is driven by the pressure gradient ??, where fluids flow from places with higher pressure to those with lower pressure. Note that the coefficient ? ? ? (? ? ) depends on the dynamic variable ? ? , making the dynamics nonlinear.</p><p>Eq. ( <ref type="formula" target="#formula_0">1</ref>) is a simplified model. The problem we consider here is much more complicated and high-dimensional. Current (nonmachine learning) approaches use implicit methods to evolve the system, which involves solving a system of equations containing up to tens of millions of equations (each cell of the discretized grid contributes one equation). Solving such a system can be slow even after linearization. Moreover, in reality there are complex well configurations (vertical wells, slanted wells, horizontal wells), faults planes in rock that reduce, divert or prevent fluid flow. This in addition adds significant complexity to modeling and evolving the system in an accurate way. In contrast, machine learning represents an attractive approach to alleviate these issues and develop faster, more scalable and accurate simulators of such complex phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>In this section, we introduce our Hybrid Graph Network Simulator (HGNS) architecture (Fig. <ref type="figure" target="#fig_1">2</ref>) and its learning method for subsurface simulation, addressing the two challenges raised in the introduction.</p><p>We formalize the problem of learning subsurface simulation as follows. A learnable simulator ? ? takes as input the dynamic variables ? ? ? R ? ?? ? at time ?, static variables ? ? R ? ?? ? , computed variables ?(? ? ) ? R ? ?? ? , and control variables ? ? ? R ? ?? ? , all defined on the cells of a grid (? is the number of cells), and predicts the dynamic variable X?+1 at the next time step:</p><formula xml:id="formula_2">X?+1 = ? ? (? ? , ?, ?(? ? ), ? ? )</formula><p>(2) Here ? ? , ? ? , ? ? , ? ? are the number of features for each variable. The dynamic variable ? ? = (? ? ? , ? ? ? , ? ? ) consists of water saturation ? ? ? , oil saturation ? ? ? and pressure ? ? . The static variable ? consists of time-constant variables on each cell such as cell porosity, absolute permeability in ?, ?, ? directions, cell depth, and pore volume. The computed variable ?(? ? ) consists of features that we opt to compute to facilitate the learning, for example the relative permeability ? ? ? = ? ? ? (? ? ). The control variable ? ? may include the injection of water at each time step. During inference, we can apply Eq. ( <ref type="formula">2</ref>) in an autoregressive way to predict the long-term future, using the previous prediction as input, i.e. X?+?+1 = ? ? ( X?+? , ?, ?( X?+? ), ? ? +? ), ? = 0, 1, ...?</p><p>where we only provide X? = ? ? as initial state and also provide ? ? +? , ? = 1, 2, ...? as external control at each time step. The goal of learning is to learn the parameter ? of ? ? such that the prediction X?+? is as near the ground-truth ? ? +? as possible, even if ? is large (a long-term prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HGNS Architecture</head><p>To address the multi-scale behavior of the subsurface dynamics, we introduce a Subsurface Graph Neural Network (SGNN) to model the dynamics of fluids (water, oil) on a finer scale, and a 3D-U-Net <ref type="bibr" target="#b6">[7]</ref> to model the more global dynamics of pressure. They combine to form the HGNS architecture. Concretely, our HGNS ? ? can be written as:</p><formula xml:id="formula_4">??+1 = ? ? (? ? , ?, ?(? ? ), ? ? ) + ? ? P?+1 = ? ? (? ? , ?, ?(? ? ), ? ? ) + ? ?<label>(4)</label></formula><p>Here ?? = (? ? ? , ? ? ? ) is the saturation for water and oil. ? ? is the SGNN model and ? ? is a 3D-U-Net <ref type="bibr" target="#b6">[7]</ref>. They both predict the change of the dynamic variables rather than the value itself. Further details about these two components and the computed features ?(? ? ) are provided next. In order to effectively model subsurface simulation through message passing, we also highlight the domain-specific designs of SGNN, such as the use of transmissibility for message computation, and augmentation of computed features as a prior.</p><p>Subsurface Graph Neural Network (SGNN) for Fluid Saturation Prediction. Our SGNN uses the encoder-processor-decoder architecture, similar to <ref type="bibr" target="#b18">[19]</ref>.</p><p>Encoder. The encoder embeds the input (? ? , ?, ?(? ? ), ? ? ) into a latent graph G (0) = (V (0) , E (0) ), where</p><formula xml:id="formula_5">V (0) = {? (0)</formula><p>? } is the collection of input node features and E = {? (0) ? ? } is the collection of edge features. Specifically, we regard each cell as a node, which is connected to its 6 neighbors (since we are modeling 3D grids) with bidirectional edges. Each node has complete features (? ? , ?, ?(? ? ), ? ? ), and the edge features ? (0) ? ? consist of transmissibility (capturing how easy it is for the fluid to flow between neighboring cells) and a one-hot indicator of the direction of the edge (since we consider gravity, the edge pointing up, down and horizontally should be treated differently). After obtaining G (0) , the encoder has a Multilayer Perceptron (MLP) that encodes the node features V into some latent embedding. We will detail the specific features encoded in Appendix A.1, as well as Appendix A.2.</p><p>Processor. The processor is a stack of ? graph neural network layers, each one performing one round of message passing that mimics the flow of fluid between neighboring cells. Specifically, given the latent graph G (?) = (V (?) , E (?) ) outputted from the previous layer, it first computes the message on the edges based on neighboring nodes:</p><formula xml:id="formula_6">? (?) ? ? = ??? ? ([? (?) ? , ? (?) ? -? (?) ? , ? (0) ? ? ])<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">? (?)</formula><p>? ? is the message from node ? to node ? on the ? -1 th layer, "[]" is concatenation on the feature dimension. The above Eq. ( <ref type="formula" target="#formula_6">5</ref>) utilizes the input edge feature ? (0) ? ? , the source node ?'s feature, and the difference between source node ? and target node ?'s features, to compute the message using an MLP ? . After computing the message, each node will aggregate the messages sent to it by summation, obtains ? (?) ? ? , and then performs node update:</p><formula xml:id="formula_8">? (?+1) ? = ??? ? ([? (?) ? , ? (?) ? ? ])<label>(6)</label></formula><p>Then we perform group normalization <ref type="bibr" target="#b22">[23]</ref> with 2 groups before the next layer. The value of our specific hyperparameter is given in Appendix A.2.</p><p>Decoder. We use an MLP that maps the output of the processor back to the predicted dynamic variables at the next time step.</p><p>Overall, our SGNN models the complex subsurface flow by encoding the properties and dynamics of each cell and cell-cell relation into node and edge features, then uses an expressive edge-level MLP ? to compute the interaction between neighboring cells, and a node-level MLP ? to update the state of the cells. In this way, it can model subsurface flow through complex subsurface structures.</p><p>3D-U-Net for Pressure Prediction. The subsurface pressure dynamics has faster equilibrium time than fluid, and therefore its evolution is more global within the time scale (months) of fluid. Therefore, we utilize 3D-U-Net <ref type="bibr" target="#b6">[7]</ref> to model the dynamics of pressure, whose hierarchical structure can capture this more global dynamics. We modify 3D-U-Net's order of operation such that for each convolution layer, it first performs 3D-convolution, then applies activation (ReLU) followed by group normalization. We make this modification since we observe that this improves performance compared to the default order of Conv ? BN ? ReLu.</p><p>Computed Features. As shown in Eq. ( <ref type="formula" target="#formula_0">1</ref>), the coefficient of the flux ? ? = -</p><formula xml:id="formula_9">? ?</formula><p>? ? ? ? ? (? ? )k?? depends on the relative permeability ? ? ? (? ? ), ? = ?, ?, which is a function of the dynamic variable ? ? and the function is provided via a table of laboratory measurements. Thus, to facilitate neural network's modeling, we add [? ? ? , ? ?? ] = [? ? ? (? ? ), ? ?? (? ? )] as computed features, using linear interpolation to compute the value between the measured points. Furthermore, we add spatial gradient of the dynamic variable ?? as another part of computed features, to allow the neural network better to utilize a cell's neighboring feature to predict its future (e.g. as shown in Eq. 1 the flux ? ? is proportional to the spatial gradient of pressure ??). Overall, we have that the computed feature is given by</p><formula xml:id="formula_10">?(? ? ) = [? ? ? (? ? ? ), ? ?? (? ? ? ), ?? ? ]<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>The goal of the training is for the model to have small long-term prediction error for grids consisting of millions or tens of millions of cells. To address the challenge of the large grid size, we introduce sector-based training. For more accurate long-term prediction, we develop the technique of multi-step rollout during training.</p><p>Sector-based Training. Suppose that the grid has 10 million cells, training such large size will certainly exceed the memory limit of a single GPU, which typically has up to 32GB of memory.</p><p>To address this problem, we note that both the SGNN and the 3D-U-Net are "local" models, meaning that to predict the cell's state at ? + 1, we only need a cell's neighbors up to a certain distance away, instead of the full grid. Based on this observation, we partition the full grid into "sectors", where each sector is a cube of cells (e.g. 40 ? 40 ? 40), with strides (e.g. 20) in each direction to obtain all sectors. Then during training, the model only needs to make predictions on the sectors instead of the full grid. In this way, we can train with a full grid of arbitrary size, because we can always partition the full grid into sectors with constant size to fit into GPU's memory.</p><p>To properly perform sector-based training, a few things need to be taken into account, and below is our design to address them:</p><p>? Boundary of the sectors: The cells near the boundaries may suffer from artificial boundary effect, where they lack neighboring cells in certain directions to properly perform prediction. The total loss will be biased if we include the loss on those cells. To address this, we use a sector-specific mask which masks out the cells up to a certain distance from the sector's boundary (the cells on the real boundary of the full grid are not masked out). The cells masked out will not contribute to the loss during training. ? Sector stride: Because we neglect the loss on the sector's boundary, if the stride is the same as the sector's size, some cells will never contribute to the loss. To address this, we make the stride smaller than the sector's size, making sure that all cells must contribute to the loss computation in at least one sector. ? Mixing sectors: The governing equation for the dynamics is the same regardless of where the sector is, what time step the evolution is at, or which trajectory the grid corresponds to. Therefore, it is beneficial to mix sectors randomly across all places, time steps and trajectories into minibatches during training. This way, we can reduce the correlation between examples and help the network learn the dynamics that can be applied in a more general way. where ? ( X?+1 , ? ? +1 ) = ( X?+1 -? ? +1 ) 2 . However, we find that even with a very small 1-step training loss, the multi-step rollout (Eq.</p><p>3) can have a large error due to error accumulation. To improve long-term prediction, we develop multi-step rollout during training.</p><p>It performs multiple steps of rollout, and the loss is given by:</p><formula xml:id="formula_11">? = E ? ? ?=1 ? ? ? ? ? ? ( X?+? , ?, ?( X?+? ), ? ? +? ), ? ? +?+1 X?+? = ? ? ( X?+?-1 , ?, ?( X?+?-1 ), ? ? +?-1 ), ? = 1, 2, ...?<label>(9)</label></formula><p>Here X? := ? ? is the initial state for the rollout. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the multi-step loss. It uses the model ? ? to perform rollout autoregressively, using the predicted X?+? as the next state's input, and the loss is a weighted sum of loss on the autoregressive predictions X?+1 , ..., X?+?+1 for all rollout steps, weighted by weights</p><formula xml:id="formula_12">? ? .</formula><p>During training, the backpropagation can pass through the full rollout steps, so that the model is directly trained to minimize long-term prediction error. In practice, there is a tradeoff between computation and accuracy. The larger the total rollout steps ? in training, the better potential accuracy it can achieve, but the more compute and memory it requires (scales linearly with ?). We find that using ? = 4 strikes a good balance, which uses reasonable compute, achieves far better accuracy than 1-step loss, and there is minimal gain to further increase ?. In our experiments, we use ? = 4 and (? 1 , ? 2 , ? 3 , ? 4 ) = (1, 0.1, 0.1, 0.1). The weights for the steps &gt; 1 are smaller, since at the beginning of training when the model is inaccurate, the multi-step loss is much worse. Having a larger weight on those steps would make the model not able to learn anything. Having 1-step loss dominates as we are using helps the model to find a good minimum of the loss landscape first, and can further improve by the multi-step part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To  trajectories differ in their static properties, initial states and well locations. See Fig. <ref type="figure" target="#fig_4">4</ref> for an example configuration of the input grid. We randomly choose 16 trajectories for training, and the other 4 trajectories for testing. Since different trajectories have drastically different static properties, initial states and well locations, the performance at the test set evaluates how the models are able to generalize to novel subsurface structures. In the testing, we provide the models the initial state at ? = 3 (allowing for 3 steps of initial stabilization), and autoregressively roll out the models for 10 and 20 steps, and compute the Mean Absolute Error (MAE) between the model's prediction and the ground-truth. In the experiment, we use volume of water ? ? = ? ? ? ? and oil ? ? = ? ? ? ? instead of saturation ? ? , ? ? as dynamic variable (here ? is the pore volume that is a static property denoting the total volume of fluid a cell can hold, ? ? + ? ? = ? ), to allow the models to better utilize mass conservation.</p><p>Baselines. We compare our HGNS model, trained with 4-step rollout, with two other strong baselines, 3D-U-Net <ref type="bibr" target="#b6">[7]</ref> and standard CNN, each trained with 4-step rollout, to test how the architectures influence the performance. In addition, we also compare with a "predict no change" baseline, in which the model simply copies the previous time step as its prediction. In this way, it gives a baseline scale of error. Table <ref type="table" target="#tab_1">1</ref> shows the result. The ground-truth pressure is typically on the level of 6000-10000 psi, and the water and oil volume is typically on the level of 10-20 barrels. Typically, a 100 psi error on the pressure and 1 barrel error on the water/oil on a cell in a long-term prediction is deemed as acceptable. Note that the compared models already use our encoding of computed features, sector-based training, and multi-step rollout during training for some models; three techniques that enable training of such large datasets and deliver better performance.</p><p>Accuracy of 10-and 20-month predictions. From Table <ref type="table" target="#tab_1">1</ref>, we see that our HGNS outperforms other baselines by a wide margin, achieving 5.5% and 6.7% reduction of pressure error at 10-step and 20-step rollout, 21% and 8.4% reduction of water error at 10-step  Figure <ref type="figure">5</ref>: Comparison of models during rollout on the fraction of cells whose error of (a) pressure (b) water volume (c) oil volume is above a given threshold. The lower the fraction, the better the prediction. HGNS outperforms 3D-U-Net and CNN in both scenarios, achieving a reduction of fraction of 71% for pressure, 29.7% for water volume, and 28.7% for oil volume, compared with the best performing model. and 20-step, and 21% and 8.3% reduction of oil error<ref type="foot" target="#foot_0">1</ref> at 10-step and 20-step than the best-performing model. We also see that the error of HGNS is significantly less than the "predict no change" baseline, showing that it has learned non-trivial dynamics of the system. Another important angle to evaluate the prediction is to compute the fraction of cells whose prediction error is less than 100 psi for pressure and 1 barrel for water/oil volume. Fig. <ref type="figure">5</ref> shows the above fractions vs. rollout steps for HGNS and the two other compared models. We see that HGNS outperforms the other models consistently across all rollout steps, in fraction for pressure, water and oil. Moreover, even after 20 steps (months) rollout of HGNS, the fraction of cells whose pressure error is greater than 100 psi remains below 4.1%, 71% lower than the best performing model (3D-U-Net) whose maximum fraction is 14.2%. Similarly, HGNS's fraction of cells whose water and oil volume error is below than 1 barrel remains below 14.8% and 18.9%, respectively, 29.7% and 28.7% lower than the best performing model. This shows that the prediction of HGNS is above standard for a majority of cells, and achieve significant improvement over strong baselines.</p><p>We visualize our HGNS's 20-step water and oil prediction and compare it with ground-truth, in one typical test dataset, as shown  Ablations. To evaluate how the hybrid design and multi-step training contribute to the improved performance, we compare HGNS with its ablations: one without 3D-U-Net (using SGNN to predict both the pressure and fluid), one without SGNN (using 3D-U-Net to predict pressure and fluid, same as in Table <ref type="table" target="#tab_1">1</ref>), and one with only 1-step loss. The results are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>From Table <ref type="table" target="#tab_2">2</ref>, we see that using only 3D-U-Net and SGNN to predict the full pressure and fluid results in worse performance than the hybrid design, confirming that our hybrid design better captures the global dynamics of pressure and more fine-grained dynamics of fluid flow. We also see that without 4-step loss, HGNS renders a much worse result. Even though HGNS 1-step has a better 10step pressure error than HGNS, its longer-term pressure prediction (20-step) is worse, demonstrating that our multi-step training helps improving long-term prediction. Runtime comparison. One advantage of our HGNS, compared with standard subsurface PDE solvers, is that it can perform explicit forward prediction to obtain the state at the next time step, while standard subsurface PDE solvers need implicit method to predict the next time step due to numerical stability issues. The implicit method requires solving an equation for the full grid, and the larger the grid, the slower it is to solve such systems of equations. On the dataset above with 1.1 million cells per trajectory, our HGNS took <ref type="bibr" target="#b19">20</ref>.7s to roll out 20-steps with an NVIDIA Quadro RTX 8000 GPU, compared to approximately 46s-370s (varying depending on the number of wells) required by the standard solver using 4 compute nodes, each with 2 CPUs Intel(R) Xeon(R) E5-2680 v3 2.50GHz, a 2 to 18-fold reduction in execution time. We expect that HGNS gains will be even more prominent with larger grid sizes (e.g. over 10 million). Industry deployment. We are finishing up deploying and integrating our HGNS into an industry pipeline, for it to be used for speeding up the subsurface simulations and field development planning. We have addressed many additional engineering challenges during deployment, e.g. difference in the research and production environment, scaling up from the current 1.1 million cell model to significantly larger grid sizes, etc. The HGNS in deployment shares the same interface as the standard solver in the pipeline. During field development planning, HGNS will be used for fast rollout and finding candidate solutions to accelerate high-level well placement and operational decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we have introduced a Hybrid Graph Network Simulator (HGNS) for learning subsurface simulations. It employs a hybrid Subsurface Graph Neural Network (SGNN) to model the fluid flow through the complicated subsurface structures, and a 3D-U-Net to model the more global dynamics of pressure, addressing the challenge of multi-scale dynamics. We introduce sector-based training, to allow learning large-grid size possible and is able to perform training and inference on grid size of at least millions, two orders of magnitude higher than previous learning-based subsurface surrogate models. Experiments show that our HGNS outperforms strong baselines by a large margin, and our hybrid design and multi-step objective both contribute to the improved performance. Compared to standard subsurface solvers, we achieve a 50% speedup with 1.1 million cells. Our model is deployed in industry pipeline for speeding up simulations for well placement and production forecasting.</p><p>Future work includes extending our HGNS to more complicated Dual Porosity Dual Permeability (DPDP) subsurface models, where fractures act as conduits for fast fluid flow. Based on our learned simulator, accelerated history matching can also be performed, where static properties can be inferred and updated by solving the inverse problem, conditioned to observed dynamic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDICES A.1 Features Encoded</head><p>Table <ref type="table">3</ref> shows all features we used for the experiments, consisting of dynamic, static, computed and control features.</p><p>Table <ref type="table">3</ref>: Encoded dynamic, static, computed and control features for our HGNS model and compared models. Here the node type one-hot encoding denotes whether a cell is a normal cell, injector, producer. The boundary encoding is a 3-vector encoding if a cell is near the boundary of the full grid, and has value ramping from 0 to 1 if it is from 5 to 1 cell distance from the boundary.  Table <ref type="table">5</ref> shows the detailed structure of the HGNS pressure model's encoder. The initial input ? is forward propagated through each of the encoders in a sequential order from Encoder(0) ? Encoder(1) ? Encoder(2). Table <ref type="table">6</ref> shows the structure of the HGNS pressure model from the first decoder layer to the final convolution output layer. The output from Encoder(2) will be forward propagated in the order of Decoder(0) ? Decoder(1) ? Final convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic features ?</head><p>Table <ref type="table">7</ref> shows the general structure of the HGNS fluid model structure, as well as the detailed structure for the node level MLP (MLP ? ) and edge level MLP (MLP ? ). The structure of MLP ? and MLP ? in the two GNLayers are the same.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our HGNS architecture and 1-step loss computation. HGNS consists of a Subsurface Graph Neural Network (SGNN) to model the fluid dynamics, and a 3D-U-Net to model the pressure dynamics. The input grid on the left is treated as a graph by modeling each cell as a node, and connecting the adjacent cells via edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Multi-step rollout during training. The surrogate model is applied autoregressively to predict the future states, using its prediction of dynamic variables as the next prediction's input. The loss consists of error on each time step, and during training, the gradient can pass through the full rollout. This multi-step loss enables better long-term prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The sector-based training also enables multi-GPU training. If training on a single GPU, a typical training with 40 epochs will take approximately 1-2 weeks, since each epoch typically needs to go through 1 to 10 billion cells due to multiple trajectories and up to hundreds of time steps per trajectory. Sector-based training allows us to assign sectors into multiple GPUs, where the GPUs accumulate gradients to a central GPU. With ? GPUs, the time typically reduces by ?/2 to ? fold with some offset. This reduces our training time to around 2 days. In summary, sector-based training makes training such large grid possible, allowing us to train with grids that have at least 2 orders of magnitude more cells than prior learning-based subsurface models. It also makes training with multi-GPU possible and reduces training time significantly. Multi-step Rollout During Training. A standard learning objective for learned simulation is to minimize the following Mean Squared Error (MSE) on the 1-step prediction: ? = E ? ? (? ? (? ? , ?, ?(? ), ? ), ? ? +1 )(8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example configuration of the dataset we use (SPE-10 model). The color shows the absolute permeability in the x direction (? ? ), with blue, purple and red representing values from low to high. There is one injector (INJ) in the middle and 4 producers (PRD) at the four corners.</figDesc><graphic url="image-3.png" coords="6,353.99,83.69,168.17,160.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fraction of cells whose absolute error of water volume is greater than 1 barrel. Fraction of cells whose absolute error of oil volume is greater than 1 barrel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) HGNS rollout of water volume (barrel) for 20 steps (months) (b) Ground-truth of water volume (barrel) for 20 steps (c) HGNS rollout of oil volume (barrel) for 20 steps (d) Ground-truth of oil volume (barrel) for 20 steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison between (a) HGNS 20-step (20-month) rollout of water volume vs. (b) ground-truth water dynamics, and (c) HGNS 20-step rollout of oil volume vs. (d) ground-truth oil dynamics, on one of the trajectories, at a cross section of depth 20. Notice that HGNS reliably captures water flow from 4 injectors (red dots) to the producer (blue dot at the middle), and the oil flow and drainage due to the producer.</figDesc><graphic url="image-7.png" coords="8,129.46,436.56,353.06,100.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean Absolute Error (MAE) of our HGNS and other baseline models on the test set for pressure, water and oil prediction, after 10-step (10 months) and 20-step rollout. HGNS outperforms other strong baselines by an pressure error reduction of 5.5% and 6.7% for 10-step and 20-step, water error reduction of 21% and 8.4% for 10-step and 20-step, and oil error reduction of 21% and 8.3% for 10-step and 20-step, compared with the best performing scenario in other models.</figDesc><table><row><cell></cell><cell cols="3">10-step prediction MAE</cell><cell cols="3">20-step prediction MAE</cell></row><row><cell>Model</cell><cell>Pressure (psi)</cell><cell>Water (barrel)</cell><cell>Oil (barrel)</cell><cell>Pressure (psi)</cell><cell>Water (barrel)</cell><cell>Oil (barrel)</cell></row><row><cell>Predict no change</cell><cell>210.8</cell><cell>0.941</cell><cell>0.941</cell><cell>296.1</cell><cell>1.541</cell><cell>1.541</cell></row><row><cell>CNN</cell><cell>77.9</cell><cell>0.628</cell><cell>0.608</cell><cell>104.2</cell><cell>1.157</cell><cell>1.090</cell></row><row><cell>3D-U-Net</cell><cell>94.6</cell><cell>0.361</cell><cell>0.361</cell><cell>142.6</cell><cell>0.725</cell><cell>0.724</cell></row><row><cell>HGNS (ours)</cell><cell>73.6</cell><cell>0.286</cell><cell>0.286</cell><cell>97.2</cell><cell>0.664</cell><cell>0.664</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean Absolute Error (MAE) of our HGNS and its ablations on the test set for pressure, water and oil prediction, after 10-step (10 months) and 20-step rollout. Hybrid design and multi-step training of HGNS improve performance by an average error reduction of 15.7% and 22.4%, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">10-step prediction MAE</cell><cell cols="2">20-step prediction MAE</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>Pressure (psi)</cell><cell>Water (barrel)</cell><cell>Oil (barrel)</cell><cell>Pressure (psi)</cell><cell>Water (barrel)</cell><cell>Oil (barrel)</cell></row><row><cell></cell><cell></cell><cell>HGNS (ours)</cell><cell>73.6</cell><cell>0.286</cell><cell>0.286</cell><cell>97.2</cell><cell>0.664</cell><cell>0.664</cell></row><row><cell></cell><cell></cell><cell>HGNS without 3D-U-Net (only SGNN)</cell><cell>74.8</cell><cell>0.307</cell><cell>0.307</cell><cell>110.5</cell><cell>0.829</cell><cell>0.829</cell></row><row><cell></cell><cell></cell><cell>HGNS without SGNN (only 3D-U-Net)</cell><cell>94.6</cell><cell>0.361</cell><cell>0.361</cell><cell>142.6</cell><cell>0.725</cell><cell>0.724</cell></row><row><cell></cell><cell></cell><cell>HGNS with 1-step</cell><cell>47.3</cell><cell>0.500</cell><cell>0.500</cell><cell>122.8</cell><cell>1.144</cell><cell>1.144</cell></row><row><cell></cell><cell>0.8 1.0</cell><cell>HGNS 3D-U-Net CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fraction</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>rollout step (month) 0 2 4 6 8 10 12 14 16 18 20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(a) Fraction of cells whose absolute error of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">pressure is greater than 100 psi.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? Static features ? Computed features ?(? ? ) Control features ? ? Pressure P Depth of cell Water relative permeability ? ? ? (? ? ) Water injection rate ? ? ?,?? ? Water volume ? ? Porosity ? Oil relative permeability ? ?? (? ? ) Pressure at the well injector location Oil volume ? ? Pore volume ? Spatial gradient of dynamic features ?? ? Connate water volume ? ?? Permeability in ? direction ? ? Permeability in ? direction ? ? Permeability in ? direction ? ? Transmissibility in ? direction ? ? Transmissibility in ? direction ? ? Transmissibility in ? direction ? ? Node type one-hot encoding Boundary encoding A.2 Hyperparameters for HGNS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>shows the hyperparameter values used for HGNS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters used for HGNS</figDesc><table><row><cell>Parameter name</cell><cell>Value</cell></row><row><cell>Number of GNLayers for the processor</cell><cell>2</cell></row><row><cell>Latent size for the processor</cell><cell>16</cell></row><row><cell>Activation</cell><cell>elu</cell></row><row><cell>Type of nomalization</cell><cell>Group normalization</cell></row><row><cell>Number of layers for each MLP</cell><cell>2</cell></row><row><cell>Convoluion type</cell><cell>GNLayer</cell></row><row><cell>Number of neurons for each layer of MLP in the processor</cell><cell>128</cell></row><row><cell>Number of neurons for encoder MLP</cell><cell>128</cell></row><row><cell>Number of neurons for decoder MLP</cell><cell>128</cell></row><row><cell>Number of layers for encoder MLP</cell><cell>2</cell></row><row><cell>Number of layers for decoder MLP</cell><cell>2</cell></row><row><cell>Number of layers for the pooling and unpooling models</cell><cell>1</cell></row><row><cell>Number of groups for GroupNorm</cell><cell>2</cell></row><row><cell>Residual connection to use in GNN model</cell><cell>None</cell></row><row><cell>Fluid decoder model</cell><cell>MLP</cell></row><row><cell cols="2">Number of feature maps for first conv layer of U-Net encoder 32</cell></row><row><cell>Number of levels in the U-Net encoder/decoder path</cell><cell>3</cell></row><row><cell>Number of groups in U-Net group norm</cell><cell>2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this dataset, since we only have two phases of fluids (water and oil), their volume sum to the pore volume (a static variable): ? ? + ? ? = ? . All models seem to learn this conservation law, resulting in similar errors between water and oil.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details for training</head><p>Table <ref type="table">8</ref> shows the values of all hyperparameters used for training. The noise added refers to the random walk noise added during training. As explained by <ref type="bibr" target="#b18">[19]</ref>, adding random walk noise brings the training distribution closer to the distribution during prediction rollout. The weight of cell refers to a weight we assign to different cells depending on their distance to a well location. In cases where we want more accurate prediction results near well locations, we can assign a higher weight to the cell near the well, and a gradually decreasing weight for cells further away following some density function (e.g. Gaussian function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Details for dataset and pre-processing</head><p>The datasets we used for training consist of 16 different trajectories, and the test set consists of 4 different trajectories. Each trajectory is the evolution of a 85 ? 220 ? 60 grid, in the depth (vertical), length and width direction, respectively, and spans over 61 times steps. Such grid size amounts to a total of 1122000 cells.</p><p>We create an edge in the graph between edge valid cell and its valid neighbor. If either the cell or its neighbor is an invalid cell, no edge will be created between this cell pair.</p><p>There can be a varying number of wells (producer or injector) in each trajectory, all between 5 and 10 in our dataset. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Sintef</surname></persName>
		</author>
		<ptr target="https://www.sintef.no/projectweb/geoscale/results/msmfem/spe10/" />
		<title level="m">the 10th SPE Comparative Solution Project, Model 2</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed visualization of complex black oil reservoir models</title>
		<author>
			<persName><forename type="first">Frederico</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waldemar</forename><surname>Celes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Eurographics conference on Parallel Graphics and Visualization</title>
		<meeting>the 9th Eurographics conference on Parallel Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00222</idno>
		<title level="m">Interaction networks for learning about objects, relations and physics</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surrogate accelerated sampling of reservoir models with complex structures using sparse polynomial chaos expansion</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Bazargan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">H</forename><surname>Elsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Water Resources</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="385" to="399" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks (PINNs) for fluid mechanics: A review</title>
		<author>
			<persName><forename type="first">Shengze</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minglang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mechanica Sinica</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Development and application of reduced-order modeling procedures for subsurface flow simulation</title>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">J</forename><surname>Marco A Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallav</forename><surname>Durlofsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal for numerical methods in engineering</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1322" to="1350" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">?zg?n</forename><surname>?i?ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Application of artificial neural networks in a history matching process</title>
		<author>
			<persName><forename type="first">Lu?s</forename><surname>Augusto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagasaki</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?lio</forename><surname>Maschio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">Jos?</forename><surname>Schiozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Petroleum Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="30" to="45" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New frontiers in large scale reservoir simulation</title>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Ali H Dogru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usuf</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><surname>Middya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tareq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Al-Shaalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahn</forename><surname>Byer</surname></persName>
		</author>
		<author>
			<persName><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nabil</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Al-Zamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kesavalu</forename><surname>Pita</surname></persName>
		</author>
		<author>
			<persName><surname>Hemanthkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPE Reservoir Simulation Symposium</title>
		<imprint>
			<publisher>OnePetro</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Cedric</forename><forename type="middle">G</forename><surname>Fraces</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamdi</forename><surname>Tchelepi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05172</idno>
		<title level="m">Physics Informed Deep Learning for Transport in Porous Media. Buckley Leverett Problem</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Limitations of physics informed machine learning for nonlinear two-phase transport in porous media</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Fuks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName><surname>Tchelepi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning for Modeling and Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">Hamidreza</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Couckuyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">Costa</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Dhaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gaussian Processes for history-matching: application to an unconventional gas reservoir</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="267" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reduced-order modeling for compositional simulation by use of trajectory piecewise linearization</title>
		<author>
			<persName><forename type="first">Jincong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">J</forename><surname>Durlofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPE Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="858" to="872" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reduced-order flow modeling and geological parameterization for ensemble-based data assimilation</title>
		<author>
			<persName><forename type="first">Jincong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallav</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">J</forename><surname>Durlofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; geosciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="54" to="69" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reduced-order modeling of CO2 storage operations</title>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">J</forename><surname>Durlofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Greenhouse Gas Control</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine learning-accelerated computational fluid dynamics</title>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Kochkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayya</forename><surname>Alieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GeoDIN-Geoscience-Based Deep Interaction Networks for Predicting Flow Dynamics in Reservoir Simulation Models</title>
		<author>
			<persName><forename type="first">Marko</forename><surname>Maucec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ridwan</forename><surname>Jalali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPE Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flexible neural representation for physics prediction</title>
		<author>
			<persName><forename type="first">Damian</forename><surname>Mrowca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A deep-learning-based surrogate model for data assimilation in dynamic subsurface flow problems</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><forename type="middle">J</forename><surname>Durlofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">413</biblScope>
			<biblScope unit="page">109456</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Kiwon</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Holl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00016</idno>
		<title level="m">Solver-in-theloop: Learning from differentiable physics to interact with iterative PDE-solvers</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducedorder optimal control of water flooding using proper orthogonal decomposition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Jorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Van Doren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Dirk</forename><surname>Markovinovi?</surname></persName>
		</author>
		<author>
			<persName><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geosciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="158" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-intrusive subdomain POD-TPWL for reservoir history matching</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olwijn</forename><surname>Leeuwenburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Xiang Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Heemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geosciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="537" to="565" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast multiscale reservoir simulations with POD-DEIM model reduction</title>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Ghasemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Gildin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalchin</forename><surname>Efendiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Calo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPE Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2141" to="2154" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian deep convolutional encoderdecoder networks for surrogate modeling and uncertainty quantification</title>
		<author>
			<persName><forename type="first">Yinhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
