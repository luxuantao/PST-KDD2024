<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiplex Heterogeneous Graph Convolutional Network</title>
				<funder ref="#_ZDhKGZm #_EqtdbUP #_567se76 #_qPeqZ6p">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_uKCVKQt #_styXfDQ">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-12">12 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pengyang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chaofan</forename><surname>Fu</surname></persName>
							<email>fuchaofan@stu.ouc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yanwei</forename><surname>Yu</surname></persName>
							<email>yuyanwei@ouc.edu.cn</email>
							<affiliation key="aff8">
								<orgName type="laboratory">Speedup of MHGCN over HAN. ** Speedup of MHGCN over GTN</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<email>chaohuang75@gmail.com</email>
							<affiliation key="aff8">
								<orgName type="laboratory">Speedup of MHGCN over HAN. ** Speedup of MHGCN over GTN</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongying</forename><surname>Zhao</surname></persName>
							<email>zyzhao@sdust.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Junyu</forename><surname>Dong</surname></persName>
							<email>dongjunyu@ouc.edu.cn</email>
							<affiliation key="aff8">
								<orgName type="laboratory">Speedup of MHGCN over HAN. ** Speedup of MHGCN over GTN</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ocean University of China</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ocean University of China</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Ocean University of China</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Shandong University of Science and Technology Qingdao</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Ocean University of China</orgName>
								<address>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiplex Heterogeneous Graph Convolutional Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-12">12 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539482</idno>
					<idno type="arXiv">arXiv:2208.06129v1[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Network Embedding</term>
					<term>Graph Representation Learning</term>
					<term>Multiplex Heterogeneous Networks</term>
					<term>Graph Convolutional Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heterogeneous graph convolutional networks have gained great popularity in tackling various network analytical tasks on heterogeneous network data, ranging from link prediction to node classification. However, most existing works ignore the relation heterogeneity with multiplex network between multi-typed nodes and different importance of relations in meta-paths for node embedding, which can hardly capture the heterogeneous structure signals across different relations. To tackle this challenge, this work proposes a Multiplex Heterogeneous Graph Convolutional Network (MHGCN) for heterogeneous network embedding. Our MHGCN can automatically learn the useful heterogeneous metapath interactions of different lengths in multiplex heterogeneous networks through multi-layer convolution aggregation. Additionally, we effectively integrate both multi-relation structural signals and attribute semantics into the learned node embeddings with both unsupervised and semi-supervised learning paradigms. Extensive experiments on five real-world datasets with various network analytical tasks demonstrate the significant superiority of MHGCN against state-of-the-art embedding baselines in terms of all evaluation metrics. The source code of our method is available at: https://github.com/NSSSJSS/MHGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Mathematics of computing ? Graph algorithms; ? Computing methodologies ? Learning latent representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Network representation learning has emerged as a new learning paradigm to embed complex network into a low-dimensional vector space while preserving the proximities of nodes in both network topological structures and intrinsic properties. Effective network representation advances various network analytical tasks, ranging from link prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>, node classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>, to recommendation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. In recent years, Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b15">[16]</ref>, a class of neural networks designed to learn graph representation for complex networks with rich feature information, have been applied to many online services, such as E-commerce <ref type="bibr" target="#b16">[17]</ref>, social media platforms <ref type="bibr" target="#b34">[35]</ref> and advertising <ref type="bibr" target="#b7">[8]</ref>.</p><p>While many efforts have been made to study the representation learning over homogeneous graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>, the exploration of preserving network heterogeneous properties in graph representation paradigms has attracted much attention in recent studies, e.g., metapath2vec <ref type="bibr" target="#b3">[4]</ref> and HERec <ref type="bibr" target="#b26">[27]</ref>. Inspired by the strength of Graph Neural Networks (GNNs) in aggregating contextual signals from neighboring nodes, various graph neural models have been introduced to tackle the challenge of heterogeneous graph learning, such as HAN <ref type="bibr" target="#b29">[30]</ref>, MAGNN <ref type="bibr" target="#b4">[5]</ref> and HetGNN <ref type="bibr" target="#b39">[40]</ref>.</p><p>Albeit the effectiveness of existing heterogeneous network embedding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>, these works are generally designed for heterogeneous networks with a single view. In real-world scenarios, however, many networks are much more complex, comprising not only multi-typed nodes and diverse edges even between the same pair-wise nodes but also a rich set of attributes <ref type="bibr" target="#b0">[1]</ref>. For example, in E-commerce networks, there are two types of nodes (i.e., users and items), and multiple relations (e.g., click, purchase, add-to-cart, or add-to-preference) between the same pairs of users and items <ref type="bibr" target="#b35">[36]</ref>. The connections between multiple types of nodes in such networks are often heterogeneous with relation diversity, which yields networks with multiple different views. It is worth noting that the multiplicity of the network is fundamentally different from the heterogeneity of the network. Two types of nodes, users and items, in a E-commerce network reflect the heterogeneity of the network. At the same time, users may have several types of interactions (e.g., click, purchse, review) with items <ref type="bibr" target="#b32">[33]</ref>, which reflects the multiplex relationships of the network. Because different user-item interactions exhibit different views of user and item, and thus should be treated differently. We term this kind of networks with both multiplex network structures with multi-typed nodes and node attribute information as attributed multiplex heterogeneous networks (AMHENs).</p><p>Performing representation learning on the AMHENs is of great importance to network mining tasks, yet it is very challenging due to such complicated network structures and node attributes. While some recent studies propose to solve the representation learning problem on multiplex heterogeneous network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, several key limitations exist in those methods. i) The success of current representation learning models largely relies on the accurate design of meta-paths. How to design an automated learning framework to explore the complex meta-path-based dependencies over the multiplex heterogeneous graphs, remains a significant challenge. ii) Unlike the homogeneous node aggregation scheme, with the heterogeneous node types and multiplex node relationships, each meta-path can be regarded as relational information channel. An effective meta-path dependency encoder is a necessity to inject both the relation heterogeneity and multiplexity into the representations. iii) In real-world graph representation scenarios, efficiency is an important factor to handle the graph data with large number of heterogeneous nodes and multiplex edges. However, most current methods are limited to serve the large-scale network data, due to their high time complexity and memory consumption.</p><p>To address the aforementioned challenges, we propose a new Multiplex Heterogeneous Graph Convolutional Network, named MHGCN, for AMHEN embedding. Specifically, we first decouple the multiplex network into multiple homogeneous and bipartite sub-networks, and then re-aggregate the sub-networks with the exploration of their importance (i.e., weights) in node representation learning. To automatically capture meta-path information across multi-relations, we tactfully design a multilayer graph convolution module, which can effectively learn the useful heterogeneous meta-path interactions of different lengths in AMHENs through multilayer convolution aggregation in both unsupervised and semisupervised learning paradigms. To improve the model efficiency, we endow our MHGCN with a simplified graph convolution for feature aggregation, in order to significantly reduce the model computational cost. Our evaluations are conducted on several real-world graph datasets to evaluate the model performance in both link prediction and node classification tasks. Experimental results show that our MHGCN framework can obtain the substantial performance improvement compared with state-of-the-art graph representation techniques. With the designed graph convolution module, our MHGCN achieves better model efficiency when competing with state-of-the-art GNN baselines for AMHENs by up to two orders of magnitudes (see efficiency analysis in the supplemental material). . We summarize the contributions of this paper as follows:</p><p>? We propose an effective multiplex heterogeneous graph neural network, MHGCN, which can automatically capture the useful relation-aware topological structural signals between nodes for heterogeneous network embedding.</p><p>? MHGCN integrates both network structures and node attribute features in node representations, and gains the capability to efficiently learn network representation with a simplified convolution-based message passing mechanism.</p><p>? We conduct extensive experiments on five real-world datasets to verify the superiority of our proposed model in both link prediction and node classification when competing with state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph Neural Networks. The goal of a GNN is to learn a lowdimensional vector representation for each node, which can be used for many downstream network mining tasks. Kipf et al. <ref type="bibr" target="#b15">[16]</ref> proposes to perform convolutional operations over graph neighboring node for information aggregation. GraphSAGE <ref type="bibr" target="#b6">[7]</ref> is an inductive GNN framework, which uses the general aggregating functions for efficient generation of node embeddings. To differentiate the influence of neighboring nodes, GAT <ref type="bibr" target="#b28">[29]</ref> has been proposed as an attentive message passing mechanism to learn the explicit weights of neighbor node embeddings. R-GCN <ref type="bibr" target="#b25">[26]</ref> considers the influence of different edge types on nodes, and uses weight sharing and coefficient constraints to apply to multi-graphs with large numbers of relations. To simplify the design of graph convolutional network, LightGCN <ref type="bibr" target="#b8">[9]</ref> omits the embedding projection with non-linearity during the message passing. Additionally, AM-GCN <ref type="bibr" target="#b31">[32]</ref> is proposed to adaptively learn deep correlation information between topological structures and node features. However, all algorithms mentioned above are developed for the homogeneous networks, and thus cannot effectively preserve the heterogeneous and multiplex graph characteristics for the network representation task.</p><p>Heterogeneous Graph Representation. Modeling the heterogeneous context of graphs has already received some attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref>. For example, some studies leverage random walks to construct meta-paths over the heterogeneous graph for node embeddings, including metapath2vec <ref type="bibr" target="#b3">[4]</ref> and HERec <ref type="bibr" target="#b26">[27]</ref>. As graph neural networks (GNNs) have become a popular choice for encoding graph structures, many heterogeneous graph neural network models are designed to enhance the GNN architecture with the capability of capturing the node and edge heterogeneous contextual signals. For example, HetGNN <ref type="bibr" target="#b39">[40]</ref> jointly encodes the graph topology and context heterogeneity for representation learning. HeGAN <ref type="bibr" target="#b9">[10]</ref> incorporates generative adversarial networks (GAN) for heterogeneous network embedding. NARS <ref type="bibr" target="#b37">[38]</ref> first generates relation subgraphs, learns node embeddings by 1D convolution on the subgraphs and then aggregates the learned embeddings. Fu et al. <ref type="bibr" target="#b4">[5]</ref> performs both the intra-and inter-metapath aggregation so as to distill the metapath-based relational context for learning node representations. However, most of those approaches rely on selecting the useful metapaths to guide the process of heterogeneous representation, which may need the external domain knowledge for constructing relevant metapaths.</p><p>In addition, there exist some recent studies attempting to relax the requirement of metapath construction for heterogeneous graph</p><formula xml:id="formula_0">Multilayer Graph Convolution Module ? ? (?) = ? $ ? (?$?) $ ? (?) ? ? (?) = ? $ ? (?) $ ? (?) ? (?) = ? $ ? $ ? (?) U I U U U U U U U I U I I I I I I I U I I U I U U I ? ? U I U ? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ? (?) ? ? (?) ? (?) ?? ? ?? ? ?? ? ?? ? ?? ? ?? ? ? ? Unsupervised Semi-supervised click buy cart collect User &amp; Item Attributes U 3 U 1 U 2 I 3 I 1 I 2 f 1 f 2 f 4 f 5 f 3 f 6 f 7 f 9 f 10 f 8 ? = ' $%&amp; |?| ? $ A )</formula><p>Multiplex Relation Aggregation 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 4 0 0 0 0 4 5 0 0 0 0 0 0 5 5 4 0 0 0 0 4 5 0 0 0 0 0 0 5 0 0 0 representations. In particular, HGT <ref type="bibr" target="#b10">[11]</ref> proposes to incorporate the self-attention into the graph-based message passing mechanism for modeling the dynamic dependencies among heterogeneous nodes. HPN <ref type="bibr" target="#b13">[14]</ref> eliminates semantic confusion by mapping nodes in metapath to semantic space, and then aggregates the embeddings of nodes under different metapaths to obtain the final representation. However, most of the above heterogeneous graph embedding models ignore the multiplex relational context of real-life graph data, in which multi-typed relationships exist among nodes. Multiplex Heterogeneous Network Embedding. Real-world graphs are often inherently multiplex, which involves various relations and interactions between two connected nodes. To tackle this challenge, many multiplex network embedding techniques are proposed to project diverse node edges into latent representations. For example, MNE <ref type="bibr" target="#b40">[41]</ref> introduces a global transformation matrix for each layer of the network to align the embeddings with different dimensions for each relation type. GATNE <ref type="bibr" target="#b0">[1]</ref> splits the node representation by learning base embedding, edge embedding as well as attribute embedding. The self-attention is utilized to fuse neighborhood information for generating edge representation. Motivated by the mutual information maximization scheme, DMGI <ref type="bibr" target="#b22">[23]</ref> is proposed as an unsupervised learning approach which aims to minimize the difference among relation-aware node representations. HGSL <ref type="bibr" target="#b42">[43]</ref> first obtains the node representation based on metapaths, and then uses GNN to jointly train the heterogeneous graph, node representation and node attributes to obtain the final embedding. However, the generality of the above methods is limited by their manual construction of meta-paths.</p><formula xml:id="formula_1">? 1 ? 2 ? 3 ? 4 ? A ! A " A # A $</formula><p>Recently, FAME <ref type="bibr" target="#b19">[20]</ref> develops a spectral graph transformation component to aggregate information from sub-networks by preserving relation-aware node dependencies. However, this model is built on the random projection and sacrifices the adaptive parameter learning in exchange for fast embedding projection. Furthermore, to learn the node embeddings of multiplex bipartite graph, Dual-HGCN <ref type="bibr" target="#b36">[37]</ref> firstly generates two sets of homogeneous hypergraphs and then perform the information propagation with the spectral hypergraph convolutions. In HDI <ref type="bibr" target="#b14">[15]</ref>, Jing et al. explores the highorder mutual information to construct the supervision signals for enhancing the node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>We define graph G = {V, E} with the set of nodes V and edges E. Each edge in E represents the connections among nodes. Definition 1 (Attributed Multiplex Heterogeneous Network, or AMHEN). Given the defined graph G, we further associate all nodes in V with the attribute feature vectors X ? R ??? . Here, the size of node set V and attribute vector is represented by ? and ?, respectively. With the consideration of node and edge heterogeneity, we define the node type and edge type mapping function as ? : V ? O and ? : E ? R. Here, the set of node types and edge types is set with the size of O and R, respectively. Each node ? ? V and edge ? ? E are associated with a specific type in O and R, respectively. Additionally, with the consideration of edge multiplexity (i.e., |O| + |R| &gt; 2), the same pair of nodes can be connected through multi-typed edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 (Meta-path).</head><p>A meta-path P is defined as a path in the form of ? 1</p><formula xml:id="formula_2">? 1 --? ? 2 ? 2 --? ? ? ? ? ? -1 ---? ? ? which describes a composite relation ? = ? 1 ? ? 2 ? ? ? ? ?-1</formula><p>between node types ? 1 and ? ? , where ? denotes the composition operator on relations.</p><formula xml:id="formula_3">For example, ? 1 ????? ----? ? 2 ??? ---? ? 2 is a meta-path sample of meta-path ? ??? ????? ----? ???? ??? ---? ? ??? .</formula><p>Based on the above definitions, we formally present the representation learning task over the multiplex heterogeneous graph as follows:</p><p>Problem (Attributed Multiplex Heterogeneous Graph Representation). The objective of our representation learning task over the attributed multiplex heterogeneous graph G = {V, E, X} is to learn low-dimensional latent embedding (with the hidden dimensionality of ? ? ? |V |) for each node ? ? V, with the preservation of node and edge heterogeneity and multiplexity.</p><p>We summarize the key notations of our technical solution in Table <ref type="table" target="#tab_5">4</ref> presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>This section describe our framework MHGCN with the overall architecture shown in Figure <ref type="figure" target="#fig_0">1</ref>. Particularly, our MHGCN contains two key learning modules: (i) multiplex relation aggregation and (ii) multilayer graph convolution module. Multiplex relation aggregation aims to aggregate the multi-relations among heterogeneous nodes in multiplex heterogeneous networks by differentiating each relation with importance. Multilayer graph convolution module can automatically capture the heterogeneous meta-paths of different lengths across multi-relations by aggregating neighboring nodes' characteristics to learn the low-dimensional representation of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multiplex Relation Aggregation</head><p>As defined in Sec. 3, there exit different types of nodes and multiple types of edges between these nodes in AMHENs, and each type of edge has a different role and impact on node representation. Therefore, following <ref type="bibr" target="#b19">[20]</ref>, we first generate multiple sub-graphs by differentiating the types of edge connections between nodes in the multiplex and heterogeneous graph. Afterwards, we aggregate the relation-aware graph contextual information with different importance weights.</p><p>We denote our generated sub-graph as {G ? |? = 1, 2, . . . , |R|} with the corresponding adjacent matrix {A ? |? = 1, 2, . . . , |R|}. Considering the scenario of multiplex user-item relations in online retailer (e.g., click, purchase, review), the decomposed sub-graph corresponds to individual type of relationship between user and item. For instance, for the graph representation learning in E-commerce platforms, different relationships (different edge types) between user and item nodes exhibit various dependency semantics. For example, the diverse behaviors of users (e.g., click, add-to-favorite, purchase) reflect different preferences of users over items. Hence, multiplex user-item interactions with various relation semantics will have different impacts on the learning process of user representations. To capture such multi-typed node dependencies, our proposed MHGCN learns the relation-aware weights ? ? to aggregate edge-type-specific sub-graph adjacent matrix as:</p><formula xml:id="formula_4">A = | R | ? =1 ? ? A ? .</formula><p>Notice that the set of weights {? ? |? = 1, 2, . . . , |R|} should not be a set of hyperparameter, but should be dynamically changed according to different tasks, so we set them as trainable parameters to be learned in model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multilayer Graph Convolution Module</head><p>Different from homogeneous networks, heterogeneous networks contain different types of nodes and edges. The specified types of edges and nodes form a meta-path, which has an obvious effect on the representation learning of heterogeneous networks. Previous works require manually defined meta-paths and learn node representations on the sampled heterogeneous meta-paths. However, setting and sampling meta-paths artificially is a complex task. In a large-scale network, the number of meta-paths is very large. It takes a long time to sample such a large number of meta-paths. At the same time, aggregating meta-paths into meta-path graph also requires a lot of memory overhead. Additionally, the type of meta-paths has an important impact on node representation, which almost determines the performance of network embedding in various downstream tasks. The number of types of heterogeneous meta-paths is also very large, involving different lengths and different relation interactions. Therefore, it is difficult to select the appropriate meta-path types for heterogeneous network embedding methods based on meta-path aggregation. Our MHGCN effectively solves the above problems. We now present our multilayer graph convolution module that automatically captures the the short and long meta-paths across multi-relations in AMHENs.</p><p>It is worth noting that our model uses a multi-layer fusion GCN. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our graph convolution module consists of multiple graph convolutional layers. Its purpose is to capture metapath information of different lengths. Next, we take a two-layer GCN as an example to illustrate how our model capture meta-path information. For a single layer GCN:</p><formula xml:id="formula_5">H (1) = A ? X ? W (1) ,<label>(1)</label></formula><p>where H (1) ? R ??? is the output of first layer (i.e., hidden representation of network), X ? R ??? is the node attribute matrix, and W (1) ? R ??? is the learnable weight matrix. Notice that our convolution adopts the idea of simplifying GCN <ref type="bibr" target="#b33">[34]</ref>, that is, no non-linear activation function is used.</p><p>For the two-layer GCN, the message passing process can be represented as below:</p><formula xml:id="formula_6">H (2) = A ? H (1) ? W (2) = A ? (A ? X ? W (1) ) ? W (2) = A 2 ? X ? W (1) ? W (2) ,<label>(2)</label></formula><p>where W (2) ? R ??? is the learnable weight matrix for second layer.</p><p>We present an illustrated example with a graph generated from E-commerce data in Figure <ref type="figure" target="#fig_9">2</ref> based on two types of node relations, namely users' buy and click behaviors on items. As shown in Figure <ref type="figure" target="#fig_9">2</ref>, aggregated matrix A can be regarded as a meta-path graph matrix generated by the 1-length meta-paths with importance (i.e., all linked node pairs across all edge types with weights). For example, A (1,3) = 1.5 contains two 1-length meta-path samples with weights, i.e., ? 1 1 * ??? -----? ? 1 : 1 and ? 1 0.5 * ????? -------? ? 1 : 0.5. Therefore, the single-layer GCN can effectively learn the node representation that contains 1-length meta-path information. Similarly, the second power of A automatically captures the 2-length meta-path information with importance weights for all node pairs, including original sub-network high-order structures. For example, A 2</p><p>(1,1) = 2.5 implies five 2-length meta-path samples across multi-relations with importance, i.e., ? -------? ? 1 : 0.25. The sum of the importance of these five meta-path samples is 2.5.</p><formula xml:id="formula_7">1 1 * ??? -----? ? 1 1 * ??? -----? ? 1 : 1, ? 1 1 * ??? -----? ? 1 0.5 * ????? -------? ? 1 : 0.5, ? 1 0.5 * ????? -------? ? 1 0.5 * ????? -------? ? 1 : 0.25, ? 1 0.5 * ????? -------? ? 1 1 * ??? -----? ? 1 : 0.</formula><p>At the same time, considering that the influence of meta-paths with different lengths on embedding should also be different, the learnable weight matrices W (?) in our multilayer graph convolution module can just play this role. Eventually, we fuse the outputs of</p><formula xml:id="formula_8">I 1 I 2 U 1 U 2 U 1 U 2 I 1 I 2 ? ? I 1 I 2 U 1 U 2 U 1 U 2 I 1 I 2 ? ? ? = ? ! + 0.5? " 0 0</formula><p>1.5 0.5 0 0 0.5 1 1.5 0.5 0 0 0.5 1 0 0  </p><formula xml:id="formula_9">U 1 U 1 1 0.5 ? " = (? ! + 0.5? " ) "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-commerce Network</head><formula xml:id="formula_10">U 1 U 2 user item U 1 0.5 U2 I 2 1 U 1 U 1 U 1 1 0.5 U 1 U 1 0.25 U 1 U 1 0.5 U 1 U 1 0.25 I 2 I 1 U 1 0.5 U 1 0.25 U 2 0.5 U 2 U 2 U 2 1 U 2 0.25</formula><formula xml:id="formula_11">H = 1 2</formula><p>(H (1) + H (2) ).</p><p>The final embedding H ? R ??? contains all 1-length and 2-length meta-path information.</p><p>To capture the more length heterogeneous meta-paths, we can extend it to ?-layer:</p><formula xml:id="formula_13">H (?) = A ? H (?-1) ? W (?) = A ? (A ? H (?-2) ? W (?-1) ) ? W (?) = A ? ? ? (A ? ?X ? W (1) ) ? ? ? W (?) ? = A ? ? X ? W (1) ? ? ? W (?) ? (4)</formula><p>Therefore, our multilayer graph convolution module fuses outputs of all layers to capture all meta-path information of different length across multi-relations:</p><formula xml:id="formula_14">H = 1 ? ? ?? ?=1 H (?) = 1 ? ? ?? ?=1 A ? H (?-1) ? W (?) ,<label>(5)</label></formula><p>where H (0) is the node attribute matrix X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Learning</head><p>This part presents the defined objective function to train our model to learn the final node representation. Depending on the requirements of different downstream tasks and the availability of node labels, we can train MHGCN in two major learning paradigms, i.e., unsupervised learning and semi-supervised learning.</p><p>For unsupervised learning, we can optimize the model parameters by minimizing the following binary cross-entropy loss function through negative sampling:</p><formula xml:id="formula_15">L = - ?? (?,?) ?? log ? (&lt; H T ? , H ? &gt;) - ?? (? ? ,? ? ) ?? - log ? (-&lt; H T ? ? , H ? ? &gt;),<label>(6)</label></formula><p>where H ? is the representation of node ?, T denotes matrix transposition, ? (?) is the sigmoid function, &lt;, &gt; can be any vector similarity measure function (e.g., inner product), ? is the set of positive node pairs, ? -is the set of negative node pairs sampled from all unobserved node pairs. That is, we use the loss function to increase the similarities between the node representations in the positive samples and decrease the similarities between the node representations in the negative samples simultaneously.</p><p>For semi-supervised learning, we can optimize the model parameters by minimizing the cross entropy via backpropagation and gradient descent. The cross entropy loss over all labeled nodes between the ground-truth and the prediction is formulated as:</p><formula xml:id="formula_16">L = - ?? ? ?V ??? Y ? ln(C ? H ? ),<label>(7)</label></formula><p>where  </p><formula xml:id="formula_17">V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 5.1 Datasets</head><p>In our evaluation, five publicly available real-world datasets are used in experimental evaluation, i.e., Alibaba 1 , Amazon 2 , AMiner 3 , IMDB 4 , and DBLP 5 . Detailed dataset description can be found in the supplement. Due to the scalability limitation of applying some baselines in the whole Alibaba networ data, we evaluate all models on a sampled dataset from Alibaba. The statistics of these five datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare our MHGCN against the following eighteen graph learning baselines, which are divided into three categories. Homogeneous network embedding methods:</p><p>? node2vec [6] -node2vec is a representative method for graph representation by leveraging the random walk to generate node sequences over graphs.</p><p>? RandNE <ref type="bibr" target="#b41">[42]</ref> -RandNE performs projection process in an iterative way to capture the high-order graph structures with the matrix factorization objective. ? FastRP [2] -This method generates similarity matrix for modeling the transitive relations among nodes. Then, FastRP leverages sparse random projection to reduce dimension. ? SGC <ref type="bibr" target="#b33">[34]</ref> -SGC proposes to simplify the graph convolutional networks by removing the non-linear projection during the information propagation between graph layers. ? AM-GCN <ref type="bibr" target="#b31">[32]</ref> -AM-GNN is a state-of-the-art graph convolutional network, which is an adaptive multi-channel graph convolutional networks for semi-supervised classification.</p><p>Heterogeneous network embedding methods:</p><p>? R-GCN <ref type="bibr" target="#b25">[26]</ref> -R-GCN further considers the influence of different edge types on nodes, and uses weight sharing and coefficient constraints to apply to heterogeneous networks. ? HAN <ref type="bibr" target="#b29">[30]</ref> -HAN applies graph attention network on multiplex network considering the inter-and intra-network interactions, which exploit manually selected meta-paths to learn node embedding.  The network types handled by the competitor methods are summarized in Table <ref type="table" target="#tab_6">5</ref> in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Setting</head><p>For baseline implementations, we either leverage OpenHGNN <ref type="foot" target="#foot_0">6</ref> or use the released source code for evaluation. In our experiments, we keep ?=200 for all compared methods. Others hyperparameter settings are considered according to their original papers. For our MHGCN, we set the number of convolution layers ? to 2. For fair comparison, we uniformly set the number of training rounds to 500 for link prediction and the number of training rounds to 200 for node classification. More detailed experimental settings can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Link Prediction</head><p>We first evaluate the model performance by comparing our MHGCN with fifteen baselines on link prediction task in an unsupervised learning manner. The results are shown in Table <ref type="table" target="#tab_3">2</ref>, where the best We can see that MHGCN significantly outperforms all baselines in terms of all evaluation metrics on five datasets. Specifically, MHGCN achieves average gains of 5.68% F1 score in comparison to the best performed GNN baselines across all datasets (i.e., FAME, MAGNN and HPN). Our MHGCN realizes a high accuracy of more than 96% on three datasets (Alibaba, Amazon, and IMDB), especially more than 99% prediction performance on Alibaba network. This is because MHGCN automatically captures effective multirelational topological structures through multiplex relation aggregation and multilayer graph convolution on the generated meta-paths across multiplex relations. Especially, compared with GATNE and MAGNN, our model has achieved better results, showing the ability of our model in automatically capturing meta-paths compared with manually setting meta-paths. FAME that use spectral graph transformation achieving the second best performance on most datasets also verifies the ability of multiplex relation aggregation to automatically capture useful heterogeneous meta-paths. However, MHGCN obtains better performance than FAME on all networks as MHGCN learns meaning node representations for AMHENs using multilayer graph convolution in a learning manner. Additionally, MHGCN also shows significant performance advantages on general heterogeneous networks (e.g., IMDB and DBLP). This may be because our MHGCN uses a weighted approach to differentiate the effects of different types of relations on node representation, which cannot be achieved by traditional meta-path sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Node Classification</head><p>We next evaluate the effectiveness of our model on the node classification task compared with state-of-the-art methods. The results are shown in Table <ref type="table" target="#tab_4">3</ref>, where the best is shown in bold. The first  As we see, MHGCN also achieves state-of-the-art performance on all tested networks. Specifically, our MHGCN achieves average 11.22% and 14.49% improvement over state-of-the-art GNN model HGSL across all datasets in terms of Macro-F1 and Micro-F1, respectively. Considering that the performance gain in node classification task reported in some recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref> is usually around 2-4%, this performance improvement achieved by our MHGCN is significant. Furthermore, we also observe that MHGCN performs much better than competitor methods on general heterogeneous network with multi-typed nodes (e.g., IMDB and AMiner), achieving 23.23% and 22.19% improvement in Macro-F1 and Micro-F1 on IMDB network. The possible reason is that our MHGCN effectively learns node representations for classification by exploring all meta-path interactions across multiple relations with different importance (i.e., weights), which is ignored by the heterogeneous network embedding approaches based on manually setting meta-path sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>To validate the effectiveness of each component of our model, we further conduct experiments on different MHGCN variations. Here  MHGCN-R does not consider the importance of different relations, that is, we set the weights ? ? to 1; MHGCN-L uses only a twolayer GCN to obtain the embedding, so it can only capture the 2-length meta-paths. We report the results of ablation study on four datasets for node classification in Figure <ref type="figure" target="#fig_6">3</ref>, where the performance on Alibaba refers to the right-ordinate axis.</p><p>It can be seen from the results that the two key components both contribute to performance improvement of our MHGCN. The comparison between MHGCN-R and MHGCN highlights the effectiveness of the importance of different relations. We can observe that MHGCN-R performs worse than MHGCN on all datasets in terms of both Macro-F1 and Micro-F1 metrics, reducing 9.68% performance in Macro-F1 score on Alibaba, which demonstrates the crucial role of our designed multiplex relation aggregation module in capturing the importance of different relations for node representation learning. The comparison between MHGCN-L and MHGCN reflects the importance of our multilayer graph convolution module. Compared with MHGCN-L, MHGCN improves 2.97%, 18.98%, 4.09% and 1.51% over MHGCN-L in terms of Macro-F1 on AMiner, Alibaba, IMDB, and DBLP, respectively. This indicates that our proposed multilayer graph convolution module effectively captures useful meta-paths of different lengths across multiplex relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Parameter Sensitivity</head><p>We conduct hyperparameter study in our new MHGCN in terms of key parameters, i.e.the number of graph layers ?, the hidden dimension size of embeddings ?, and the number of training rounds. We report Macro-F1 score on node classification task with different parameter settings on four datasets in Figure <ref type="figure" target="#fig_7">4</ref>. Notice that the performance on Alibaba refers to the ordinate on the right.</p><p>From the results shown in Figure <ref type="figure" target="#fig_7">4</ref>(a), we can observe that the best performance can be achieved with two layers. This observation indicates that considering the meta-path interactions with two-hops is sufficient to capture the node dependencies in the graph. Performing the message passing across more graph layers may involve noisy information for node representation. With the growth of GCN layers, the representation of nodes would be flattened after multiple convolutions, resulting in performance degradation. Additionally, we can notice that the increasing of embedding dimension first brings benefits for the performance improvement, and then leads to the performance degradation. The best prediction accuracy can be achieved with the setting of ? = 128. This is because the features of all nodes are compressed into a small embedding space when dimension ? is small, thus it is difficult to retain the characteristics proximities of all node pairs. Conversely, a larger dimension would also flatten the distance between all node embeddings. Figure <ref type="figure" target="#fig_7">4(c)</ref> illustrates the performance of our MHGCN with respect to the number of training rounds in learning model weights. We can find that our MHGCN can converge quickly and efficiently achieve stable performance within 80 rounds on all tested datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose an embedding model MHGCN for attributed multiplex heterogeneous networks. Our model mainly includes two key components: multiplex relation aggregation and multilayer graph convolution module. Through multiplex relation aggregation, MHGCN can distinguish the importance of the relations between different nodes in multiplex heterogeneous networks. Through multilayer graph convolution module, MHGCN can automatically capture the short and long meta-path interactions across multi-relations, and learn meaning node embeddings with model parameter learning during training phase. Experiments results on five real-world heterogeneous networks show the superiority of the proposed MHGCN in different graph representation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SUPPLEMENTAL MATERIAL A.1 Notations</head><p>We summarize the key notations used in the paper as well as their definitions in Table <ref type="table" target="#tab_5">4</ref>. the hidden representation for the ?-th layer ? the hidden dimensionality of embeddings ?, ? the number of nodes and attributes ? ? the learnable weight for edge type ? W (?)  the learnable weight matrix for the ?-th layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Algorithm Pseudo-Code</head><p>Algorithm 1 shows the pseudo-code of our proposed MHGCN framework guided by the above objective functions (i.e., Eq. ( <ref type="formula" target="#formula_15">6</ref>) or Eq. ( <ref type="formula" target="#formula_16">7</ref>)).  </p><formula xml:id="formula_18">: Calculate A = | R | ? =1 ? ? A ? 3: for ? = 1 to ? do 4: Calculate H (?) ? A ? H (?-1) ? W (?) 5: end for 6: H = 1</formula><p>? (H (1) + ? ? ? + H (?) ) 7: Calculate L using Eq. ( <ref type="formula" target="#formula_15">6</ref>) or Eq. ( <ref type="formula" target="#formula_16">7</ref>); 8: Back propagation and update parameters in MHGCN 9: Return H A.3 Detailed Dataset Description i) For Alibaba dataset, four types of user-item interactions are regarded as the node-wise multiplex relationships. The item categories are considered as the ground truth labels for node classification. ii) For Amazon dataset, the multiplex edges are represented as the co-viewing and co-purchasing relations between different products. The node attributes include the external features of products, e.g., category, sales-rank, brand and price information. iii) For the AMiner dataset, three types of nodes (i.e., author, paper and conference) are included in the heterogeneous graph. The node labels are the paper domains. iv) For the IMDB dataset, movie, director and actor construct the heterogeneous nodes. We consider the genres of movies as the node labels. The bag-of-words representations are considered as the node attributed feature vectors. v) For the DBLP dataset, four types of nodes are involved in the heterogeneous graph, namely, author, paper, venue, and term. We regard the research field of authors as the node class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Detailed Experimental Settings</head><p>For link prediction task, we treat the connected nodes in network as positive node pairs, and consider all unlinked nodes as negative node pairs. For each edge type, we divide the positive node pairs into training set, verification set and test set according to the proportion of 85%, 5% and 10%. At the same time, we randomly select the same number of negative node pairs to add into training set, validation set and test set. Notice that we predict each type of edge using all types of edges in datasets, and finally take the average of all edges as the final result. In particular, the training, validation and test sets are generated with the ratio of 80%, 10% and 10%, respectively. In experiments, we train a logistic regression classifier for node classification. Notice that we repeat each experiment 10 times to report average results.</p><p>For fair comparison, we uniformly set the number of training rounds to 500 for link prediction and the number of training rounds to 200 for node classification. For node2vec method, the parameters ? and ? for random walk control are set as 2 and 0.5, respectively. For GATNE approach, the parameters ? ? and ? ? are set as 1 for each edge type. For the MNE, we set the dimension of additional vectors to 10, set the length of walk as 10, set the number of walks as 20. For compared neural network-based models, the learning rate is searched from the range of {0.0005, 0.0001, 0.005, 0.001, 0.05, 0.01, }. We configure the multi-head attention with 8 head-specific representation spaces, and apply the dropout ratio of 0.6. For GTN method, the number of graph Transformer layers is set as 3. For DMGI baseline, the parameters ?, ?, ? are chosen from the value range of {0.0001, 0.001, 0.01, 0.1}. The weight ? for self-connection is set as 3.</p><p>For FAME, we use optuna 7 to tune the parameters over ? 1 , . . . , ? ? and ? 1 , . . . , ? | R | as described in the original paper. For AM-GCN, we tune loss aggregation parameters ?, ? in {0.0001, 0.001, 0.01, 0.1}. For MAGNN, we set the number of independent attention mechanisms ? = 4. For HPN, we set iterations in semantic propagation ? = 3 and value of restart probability ? = 0.1. For HGSL, we set the number of GNN layers to 2 and the hidden layer output dimension to 64. For R-GCN, we set the batch size to 126, the number of GNN layers to 2, and the hidden layer dimension to 64. For NARS, we set the number of hops to 2, and the number of feed-forward layers to 2. For DualHGNN, we use the asymmetric operator and set ? as 0.5.</p><p>For our MHGCN, we set the number of convolution layers ? to 2, learning rate to 0.05, dropout to 0.5, and weight-decay to 0.0005. The configurations of system platform for efficiency evaluation are as followed. CPU: Intel Xeon E5-2660 (2.2GHz), Memory: 80GB, 2 GPU units: GeForce RTX 2080 (8G).</p><p>The source code of our model implementation is available at https://github.com/NSSSJSS/MHGCN. </p><formula xml:id="formula_19">? ? ? ? ? ? RandNE ? ? ? ? ? ? FastRP ? ? ? ? ? ? SGC ? ? ? ? ?/? ? AM-GCN ? ? ? ? ? ? R-GCN ? ? ? ? ?/? ? HAN ? ? ? ? ? ? NARS ? ? ? ? ? ? MAGNN ? ? ? ? ?/? ? HPN ? ? ? ? ?/? ? PMNE ? ? ? ? ? ? MNE ? ? ? ? ? ? GATNE ? ? ? ? ? ? GTN ? ? ? ? ? ? DMGI ? ? ? ? ? ? FAME ? ? ? ? ? ? HGSL ? ? ? ? ? ? DualHGNN ? ? ? ? ? ? MHGCN ? ? ? ? ?/? ? A.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines</head><p>The publicly source codes of baselines can be available at the following URLs:</p><p>? node2vec -https://github.com/aditya-grover/node2vec The network types handled by the baseline methods are summarized in Table <ref type="table" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Additional Experimental Results</head><p>A.6.1 Model Efficiency Analysis. We also compare the efficiency of our MHGCN with other GNN baselines for semi-supervised node classification. We report the experimental results on four datasets in Table <ref type="table" target="#tab_8">6</ref>.</p><p>As can be seen from Table <ref type="table" target="#tab_8">6</ref>, our MHGCN achieves the fourthbest performance after three heterogeneous network embedding methods (i.e., R-GCN, NARS and HPN). However, from the above experimental results (Tables <ref type="table" target="#tab_3">2</ref> and<ref type="table" target="#tab_4">3</ref>), MHGCN is significantly better than these three methods in both link prediction and node classification. MHGCN is significantly faster than the best performed GNN baseline in node classification task (i.e., HGSL) on all datasets under the same number of training rounds. More specifically, our MHGCN achieves up to 135? speedup over state-of-the-art embedding method HAN. MHGCN is faster than state-of-the-art AMHEN embedding method GTN by 21.25 times on multiplex Alibaba network. MHGCN is even 2.33 times and 16.58 times faster than stateof-the-art heterogeneous GNN model MAGNN on Alibaba and AMiner, respectively. The main reason is because our MHGCN adopts the idea of simplifying graph convolutional networks, that is, omitting non-linear activation function. Therefore, the training efficiency of MHGCN can be significantly improved. In fact, according to the above experimental results in Figure <ref type="figure" target="#fig_7">4</ref>(c), our model can converge quickly within 80 rounds for node classification on four tested datasets, that is, our model does not need to be trained for 200 rounds set in our experimental evaluation and thus can achieve faster efficiency. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of the proposed MHGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 , and ? 1 0</head><label>51</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>U 1 I 2 I 2 I 2 I 2 I 2 I 2 I 1 I 1 I 1 I 1 I 1 I 1 I 1 I 1 I 1 Figure 2 :</head><label>12222221111111112</label><figDesc>Figure 2: Illustration of meta-paths with importance for a toy example single-layer GCN and two-layer GCN:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>??? is the set of node indices that have labels, Y ? is the label of the ?-th node, C is the node classifier parameter, and H ? is the representation of the ?-th node. With the guide of a small fraction of labeled nodes, we can optimize the proposed model and then learn the embeddings of nodes for semi-supervised classification.Notice that {W (?) |? = 1, 2, . . . , ? } and {? ? |? = 1, 2, . . . , |R|} in our model can be learned during training phase. The pseudo-code of our proposed MHGCN is shown in Algorithm 1 in the supplement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results of ablation study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hyperparameter impact study of the proposed method w.r.t. #layers, dimension ?, and #rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1</head><label>1</label><figDesc>The Learning Procedure of our MHGCN Model Input: The generated AMHEN G and node feature matrix X. Output: The node embeddings H of graph G. 1: We generate the adjacency matrices {A ? |? = 1, 2, . . . , |R|} by decoupling the attributed multiplex heterogeneous network into homogeneous and bipartite graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistical information of evaluation network datasets (node type: n-type, edge type: e-type, features: feat., and Multiplex network: Mult.) Dataset #nodes #edges #n-type #e-type #feat. Mult.</figDesc><table><row><cell cols="2">Alibaba 21,318 41,676</cell><cell>2</cell><cell>4</cell><cell>19</cell><cell>?</cell></row><row><cell cols="2">Amazon 10,166 148,865</cell><cell>1</cell><cell>2</cell><cell>1,156</cell><cell>?</cell></row><row><cell cols="2">AMiner 58,068 118,939</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>?</cell></row><row><cell>IMDB</cell><cell>12,772 18,644</cell><cell>3</cell><cell>2</cell><cell>1,256</cell><cell>?</cell></row><row><cell>DBLP</cell><cell>26,128 119,783</cell><cell>4</cell><cell>3</cell><cell>4,635</cell><cell>?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Model performance comparison for the task of link prediction on different datasets. Out Of Time (36 hours). OOM: Out Of Memory; DMGI runs out of memory on the entire AMiner data. R-AUC: ROC-AUC.is shown in bold. The first seven baselines are homogeneous or heterogeneous network embedding methods, and the last eight are multiplex network embedding methods.</figDesc><table><row><cell>Method</cell><cell cols="14">AMiner R-AUC PR-AUC F1 R-AUC PR-AUC F1 R-AUC PR-AUC F1 R-AUC PR-AUC F1 R-AUC PR-AUC F1 Alibaba IMDB Amazon DBLP</cell></row><row><cell cols="2">node2vec 0.594</cell><cell cols="3">0.663 0.602 0.614</cell><cell cols="2">0.580 0.593 0.479</cell><cell cols="3">0.568 0.474 0.946</cell><cell cols="3">0.944 0.880 0.449</cell><cell cols="2">0.452 0.478</cell></row><row><cell>RandNE</cell><cell>0.607</cell><cell cols="3">0.630 0.608 0.877</cell><cell cols="2">0.888 0.826 0.901</cell><cell cols="3">0.933 0.839 0.950</cell><cell cols="3">0.941 0.903 0.492</cell><cell cols="2">0.491 0.493</cell></row><row><cell>FastRP</cell><cell>0.620</cell><cell cols="3">0.634 0.600 0.927</cell><cell cols="2">0.900 0.926 0.869</cell><cell cols="3">0.893 0.811 0.954</cell><cell cols="3">0.945 0.893 0.515</cell><cell cols="2">0.528 0.506</cell></row><row><cell>SGC</cell><cell>0.589</cell><cell cols="3">0.585 0.567 0.686</cell><cell cols="2">0.708 0.623 0.826</cell><cell cols="3">0.889 0.769 0.791</cell><cell cols="3">0.802 0.760 0.601</cell><cell cols="2">0.606 0.587</cell></row><row><cell>R-GCN</cell><cell>0.599</cell><cell cols="3">0.601 0.610 0.674</cell><cell cols="2">0.710 0.629 0.826</cell><cell cols="3">0.878 0.790 0.811</cell><cell cols="3">0.820 0.783 0.589</cell><cell cols="2">0.592 0.566</cell></row><row><cell>MAGNN</cell><cell>0.663</cell><cell cols="3">0.681 0.666 0.961</cell><cell cols="2">0.963 0.948 0.912</cell><cell cols="3">0.923 0.887 0.958</cell><cell cols="3">0.949 0.915 0.690</cell><cell cols="2">0.699 0.684</cell></row><row><cell>HPN</cell><cell>0.658</cell><cell cols="3">0.664 0.660 0.958</cell><cell cols="2">0.961 0.950 0.900</cell><cell cols="3">0.903 0.892 0.949</cell><cell cols="3">0.949 0.904 0.692</cell><cell cols="2">0.710 0.687</cell></row><row><cell>PMNE-n</cell><cell>0.651</cell><cell cols="3">0.669 0.677 0.966</cell><cell cols="2">0.973 0.891 0.674</cell><cell cols="3">0.683 0.646 0.956</cell><cell cols="3">0.945 0.893 0.672</cell><cell cols="2">0.679 0.663</cell></row><row><cell>PMNE-r</cell><cell>0.615</cell><cell cols="3">0.653 0.662 0.859</cell><cell cols="2">0.915 0.824 0.646</cell><cell cols="3">0.646 0.613 0.884</cell><cell cols="3">0.890 0.796 0.637</cell><cell cols="2">0.640 0.629</cell></row><row><cell>PMNE-c</cell><cell>0.613</cell><cell cols="3">0.635 0.657 0.597</cell><cell cols="2">0.591 0.664 0.651</cell><cell cols="3">0.634 0.630 0.934</cell><cell cols="3">0.934 0.868 0.622</cell><cell cols="2">0.625 0.609</cell></row><row><cell>MNE</cell><cell>0.660</cell><cell cols="3">0.672 0.681 0.944</cell><cell cols="2">0.946 0.901 0.688</cell><cell cols="3">0.701 0.681 0.941</cell><cell cols="3">0.943 0.912 0.657</cell><cell cols="2">0.660 0.635</cell></row><row><cell>GATNE</cell><cell>OOT</cell><cell cols="3">OOT OOT 0.981</cell><cell cols="2">0.986 0.952 0.872</cell><cell cols="3">0.878 0.791 0.963</cell><cell cols="3">0.948 0.914 OOT</cell><cell cols="2">OOT OOT</cell></row><row><cell>DMGI</cell><cell cols="4">OOM OOM OOM 0.857</cell><cell cols="2">0.781 0.784 0.926</cell><cell cols="3">0.935 0.873 0.905</cell><cell cols="3">0.878 0.847 0.610</cell><cell cols="2">0.615 0.601</cell></row><row><cell>FAME</cell><cell>0.687</cell><cell cols="3">0.747 0.726 0.993</cell><cell cols="2">0.996 0.979 0.944</cell><cell cols="3">0.959 0.897 0.959</cell><cell cols="3">0.950 0.900 0.642</cell><cell cols="2">0.650 0.633</cell></row><row><cell>DualHGNN</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>0.974</cell><cell>0.977 0.966</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell cols="15">MHGCN 0.711 0.753 0.730 0.997 0.997 0.992 0.967 0.966 0.959 0.972 0.974 0.961 0.718 0.722 0.703</cell></row><row><cell>OOT:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Node classification performance comparison of different methods on four datasetsOOT: Out Of Time (36 hours), OOM: Out Of Memory. The standard deviations are reported in the parentheses.</figDesc><table><row><cell>Method</cell><cell cols="2">AMiner Macro-F1 Micro-F1</cell><cell>Alibaba Macro-F1 Micro-F1</cell><cell>Macro-F1</cell><cell>IMDB</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>DBLP</cell><cell>Micro-F1</cell></row><row><cell cols="10">node2vec 0.522 (0.0032) 0.532 (0.0051) 0.238 (0.0125) 0.347 (0.0093) 0.363 (0.0237) 0.382 (0.0703) 0.352 (0.0103) 0.351 (0.0112)</cell></row><row><cell cols="10">RandNE 0.641 (0.0074) 0.672 (0.0064) 0.319 (0.0170) 0.358 (0.0093) 0.373 (0.0143) 0.392 (0.0185) 0.351 (0.0153) 0.372 (0.0150)</cell></row><row><cell>FastRP</cell><cell cols="9">0.650 (0.0086) 0.690 (0.0074) 0.301 (0.0180) 0.392 (0.0119) 0.363 (0.0236) 0.381 (0.0140) 0.343 (0.0201) 0.375 (0.0199)</cell></row><row><cell>MNE</cell><cell cols="9">0.643 (0.0069) 0.686 (0.0045) 0.289 (0.0155) 0.390 (0.0021) 0.374 (0.0153) 0.382 (0.0680) 0.366 (0.0117) 0.384 (0.0109)</cell></row><row><cell>GATNE</cell><cell>OOT</cell><cell>OOT</cell><cell cols="4">0.291 (0.0086) 0.390 (0.0014) 0.369 (0.0132) 0.333 (0.0005)</cell><cell>OOT</cell><cell></cell><cell>OOT</cell></row><row><cell>DMGI</cell><cell cols="9">0.473 (0.0155) 0.626 (0.0093) 0.220 (0.0214) 0.392 (0.0026) 0.548 (0.0190) 0.544 (0.0189) 0.781 (0.0303) 0.787 (0.0235)</cell></row><row><cell>FAME</cell><cell cols="9">0.722 (0.0114) 0.727 (0.0091) 0.323 (0.0154) 0.393 (0.0060) 0.593 (0.0135) 0.594 (0.0143) 0.842 (0.0183) 0.868 (0.0127)</cell></row><row><cell>DualHGNN</cell><cell>/</cell><cell>/</cell><cell>0.347 (0.0114) 0.402 (0.0127)</cell><cell>/</cell><cell></cell><cell>/</cell><cell>/</cell><cell></cell><cell>/</cell></row><row><cell>SGC</cell><cell cols="9">0.516 (0.0047) 0.587 (0.0157) 0.286 (0.0231) 0.361 (0.0175) 0.489 (0.0106) 0.563 (0.0133) 0.622 (0.0009) 0.623 (0.0009)</cell></row><row><cell cols="10">AM-GCN 0.702 (0.0175) 0.713 (0.0223) 0.307 (0.0232) 0.399 (0.0156) 0.610 (0.0021) 0.640 (0.0013) 0.867 (0.0105) 0.878 (0.0112)</cell></row><row><cell>R-GCN</cell><cell cols="9">0.690 (0.0078) 0.692 (0.0106) 0.265 (0.0326) 0.381 (0.0125) 0.544 (0.0172) 0.572 (0.0145) 0.862 (0.0053) 0.870 (0.0070)</cell></row><row><cell>HAN</cell><cell cols="9">0.690 (0.0149) 0.726 (0.0086) 0.275 (0.0327) 0.392 (0.0081) 0.552 (0.0112) 0.568 (0.0078) 0.806 (0.0078) 0.813 (0.0100)</cell></row><row><cell>NARS</cell><cell cols="9">0.722 (0.0103) 0.721 (0.0097) 0.297 (0.0201) 0.392 (0.0195) 0.565 (0.0037) 0.574 (0.0048) 0.794 (0.0255) 0.804 (0.0320)</cell></row><row><cell cols="10">MAGNN 0.755 (0.0105) 0.757 (0.0133) 0.348 (0.0488) 0.398 (0.0405) 0.614 (0.0073) 0.615 (0.0089) 0.881 (0.0284) 0.895 (0.0396)</cell></row><row><cell>HPN</cell><cell cols="9">0.710 (0.0612) 0.732 (0.0490) 0.263 (0.0346) 0.392 (0.0405) 0.578 (0.0023) 0.584 (0.0021) 0.822 (0.0201) 0.830 (0.0201)</cell></row><row><cell>GTN</cell><cell>OOM</cell><cell>OOM</cell><cell cols="7">0.255 (0.0420) 0.392 (0.0071) 0.615 (0.0108) 0.616 (0.0093) 0.852 (0.0137) 0.868 (0.0125)</cell></row><row><cell>HGSL</cell><cell cols="9">0.754 (0.0100) 0.758 (0.0103) 0.338 (0.0121) 0.398 (0.0238) 0.620 (0.0048) 0.638 (0.0030) 0.893 (0.0284) 0.902 (0.0396)</cell></row><row><cell cols="10">MHGCN 0.868 (0.0160) 0.875 (0.0200) 0.351 (0.0204) 0.458 (0.0160) 0.764 (0.0145) 0.782 (0.0138) 0.945 (0.0221) 0.952 (0.0203)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Summary of key notations</figDesc><table><row><cell>Notation</cell><cell>Definition</cell></row><row><cell>G</cell><cell>The target graph</cell></row><row><cell>V, E</cell><cell>the set of nodes and edges in G</cell></row><row><cell>O, R</cell><cell>the set of node and edge types in G</cell></row><row><cell>X</cell><cell>the matrix of node attributes in G</cell></row><row><cell>G ?</cell><cell>the sub-network w.r.t. edge type ?</cell></row><row><cell>A ?</cell><cell>the adjacency matrix of G ?</cell></row><row><cell>A</cell><cell>the aggregated adjacency matrix</cell></row><row><cell>H</cell><cell>the node embeddings</cell></row><row><cell>H (?)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The types of graphs handled by different methods (Heter.: Heterogenous node and edge types, Multi.: Multiplex edges, Attr.: Node attributed information, Unsup.: Unsupervised learning, Auto.: Automatic meta-path).</figDesc><table><row><cell>Method</cell><cell>Heter. Node Edge</cell><cell>Multi. Attr. Unsup. Auto.</cell></row><row><cell>node2vec</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>? RandNE -https://github.com/ZW-ZHANG/RandNE ? FastRP -https://github.com/GTmac/FastRP ? SGC -https://github.com/Tiiiger/SGC ? AM-GCN -https://github.com/zhumeiqiBUPT/AM-GCN ? R-GCN -https://github.com/BUPT-GAMMA/OpenHGNN ? HAN -https://github.com/Jhy1993/HAN ? NARS -https://github.com/BUPT-GAMMA/OpenHGNN ? MAGNN -https://github.com/cynricfu/MAGNN ? HPN -https://github.com/BUPT-GAMMA/OpenHGNN ? PMNE -The source code of PMNE used in this work is released by the authors of MNE at https://github.com/HKUST-KnowComp/MNE ? MNE -https://github.com/HKUST-KnowComp/MNE ? GATNE -https://github.com/THUDM/GATNE ? GTN -https://github.com/seongjunyun/Graph_Transformer_ Networks ? DMGI -https://github.com/pcy1302/DMGI ? FAME -https://github.com/ZhijunLiu95/FAME ? HGSL -https://github.com/Andy-Border/HGSL ? DualHGNN -https://github.com/xuehansheng/DualHGCN For homogeneous network embedding methods and heterogeneous network embedding methods to deal with multiplex networks, we feed separate graphs with a single-layer view into them to obtain different node embeddings, then perform mean pooling to generate final node embedding. Since DualHGNN is designed only for multiplex bipartite networks, it can only work on Alibaba network.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Runtime comparison of GNN methods (Second)</figDesc><table><row><cell>Method</cell><cell>AMiner</cell><cell>Alibaba</cell><cell>IMDB</cell><cell>DBLP</cell></row><row><cell>AM-GCN</cell><cell>8703.71</cell><cell>2519.82</cell><cell>24280.12</cell><cell>2786.73</cell></row><row><cell>R-GCN</cell><cell>153.04</cell><cell>301.25</cell><cell>155.40</cell><cell>192.85</cell></row><row><cell>HAN</cell><cell>87105.55</cell><cell>4226.95</cell><cell>70510</cell><cell>22315.36</cell></row><row><cell>NARS</cell><cell>172.21</cell><cell>211.54</cell><cell>75.81</cell><cell>108.54</cell></row><row><cell>MAGNN</cell><cell>10361.20</cell><cell>2320.62</cell><cell>731.03</cell><cell>2125.33</cell></row><row><cell>HPN</cell><cell>172.82</cell><cell>249.47</cell><cell>176.64</cell><cell>109.49</cell></row><row><cell>GTN</cell><cell>OOM</cell><cell>21166.83</cell><cell>4287.20</cell><cell>18233.64</cell></row><row><cell>HGSL</cell><cell>1684.03</cell><cell>2120.93</cell><cell>1758.21</cell><cell>2037.10</cell></row><row><cell>DualHGN</cell><cell>/</cell><cell>11295.92</cell><cell>/</cell><cell>/</cell></row><row><cell>MHGCN</cell><cell>645.20</cell><cell>996.52</cell><cell>677.23</cell><cell>970.29</cell></row><row><cell cols="2">Speedup * 135.05?</cell><cell>4.37?</cell><cell>104.15?</cell><cell>23.01?</cell></row><row><cell>Speedup **</cell><cell>/</cell><cell>21.25?</cell><cell>6.33?</cell><cell>18.80?</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0"><p>https://github.com/BUPT-GAMMA/OpenHGNN</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> under grant Nos. <rs type="grantNumber">62176243</rs>, <rs type="grantNumber">62072288</rs>, <rs type="grantNumber">61773331</rs> and <rs type="grantNumber">41927805</rs>, and the <rs type="funder">National Key Research and Development Program of China</rs> under grant Nos. <rs type="grantNumber">2018AAA0100602</rs> and <rs type="grantNumber">2019YFC1509100</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZDhKGZm">
					<idno type="grant-number">62176243</idno>
				</org>
				<org type="funding" xml:id="_EqtdbUP">
					<idno type="grant-number">62072288</idno>
				</org>
				<org type="funding" xml:id="_567se76">
					<idno type="grant-number">61773331</idno>
				</org>
				<org type="funding" xml:id="_qPeqZ6p">
					<idno type="grant-number">41927805</idno>
				</org>
				<org type="funding" xml:id="_uKCVKQt">
					<idno type="grant-number">2018AAA0100602</idno>
				</org>
				<org type="funding" xml:id="_styXfDQ">
					<idno type="grant-number">2019YFC1509100</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation Learning for Attributed Multiplex Heterogeneous Network</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD. 1358-1368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and Accurate Network Embeddings via Very Sparse Random Projection</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Syed Fahad Sultan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PME: projected metric embedding on heterogeneous networks for link prediction</title>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1177" to="1186" />
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Xue Li</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Click-Through Rate Prediction with Multi-Modal Hypergraphs</title>
		<author>
			<persName><forename type="first">Li</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="690" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial Learning on Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WWW. 2704-2710</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recent advances in heterogeneous relation learning for recommendation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03455</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xiaoping Lai, and Yanfang Ye. 2021. Knowledge-aware coupled graph neural network for social recommendation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huance</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Propagation Network</title>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hdmi: High-order deep multiplex infomax</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2414" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<title level="m">Hierarchical bipartite graph neural networks: Towards large-scale e-commerce applications. In ICDE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1677" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Principled multilayer network embedding. In ICDMW</title>
		<author>
			<persName><forename type="first">Weiyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sailung</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingli</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Motif-preserving dynamic attributed network embedding</title>
		<author>
			<persName><forename type="first">Zhijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Dong</surname></persName>
		</author>
		<idno>WWW. 1629-1638</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast Attributed Multiplex Heterogeneous Network Embedding</title>
		<author>
			<persName><forename type="first">Zhijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baode</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="995" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social Recommendation with Self-Supervised Metagraph Informax Network</title>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huance</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1160" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation structureaware heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4456" to="4463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Attributed Multiplex Network Embedding</title>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5371" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Netsmf: Large-scale network embedding as sparse matrix factorization</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>WWW. 1509-1520</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous information network embedding for recommendation</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno>WWW. 1067-1077</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic heterogeneous information network embedding with meta-path based proximity</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Am-gcn: Adaptive multi-channel graph convolutional networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<idno>KDD. 1243-1253</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Contrastive meta learning with behavior multiplicity for recommendation</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno>KDD. 1120-1128</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with markov random field reasoning for social spammer detection</title>
		<author>
			<persName><forename type="first">Yongji</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1054" to="1061" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiplex Behavioral Relation Learning for Recommendation via Memory Augmented Transformer Network</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multiplex bipartite network embedding using dual hypergraph convolutional networks</title>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lin</surname></persName>
		</author>
		<idno>WWW. 1649-1660</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09679</idno>
		<title level="m">Scalable graph neural networks for heterogeneous graphs</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable multiplex network embedding</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3082" to="3088" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Billion-Scale Network Embedding with Iterative Random Projection</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Structure Learning for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
