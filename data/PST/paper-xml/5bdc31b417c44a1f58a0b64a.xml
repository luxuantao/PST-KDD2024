<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Applications of Deep Reinforcement Learning in Communications and Networking: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cong</forename><surname>Nguyen</surname></persName>
							<email>clnguyen@ntu.edu.sg</email>
						</author>
						<author>
							<persName><roleName>Dinh</roleName><forename type="first">Thai</forename><surname>Luong</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">D T</forename><surname>Hoang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Luong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
							<email>dniyato@ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="middle">P</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y.-C</forename><surname>Liang</surname></persName>
							<email>liangyc@ieee.org</email>
						</author>
						<author>
							<persName><forename type="middle">D I</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Iot</forename><surname>9%</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Intelligent Systems Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Center for Intelligent Networking and Communications (CINC)</orgName>
								<orgName type="institution">with University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Applications of Deep Reinforcement Learning in Communications and Networking: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B078D6AFA3D51C0E47281E3CCB240DAB</idno>
					<idno type="DOI">10.1109/COMST.2019.2916583</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/COMST.2019.2916583, IEEE Communications Surveys &amp; Tutorials This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/COMST.2019.2916583, IEEE Communications Surveys &amp; Tutorials This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/COMST.2019.2916583, IEEE Communications Surveys &amp; Tutorials This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/COMST.2019.2916583, IEEE Communications Surveys &amp; Tutorials 8 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/COMST.2019.2916583, IEEE Communications Surveys &amp; Tutorials 11</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep reinforcement learning</term>
					<term>deep Q-learning</term>
					<term>networking</term>
					<term>communications</term>
					<term>spectrum access</term>
					<term>rate control</term>
					<term>security</term>
					<term>caching</term>
					<term>data offloading</term>
					<term>data collection Proactive caching Data offloading Network security Connectivity preservation Traffic routing Resource scheduling Data collection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, deep reinforcement learning, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks such as 5G and beyond. Furthermore, we present applications of deep reinforcement learning for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying deep reinforcement learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Reinforcement learning <ref type="bibr" target="#b0">[1]</ref> is one of the most important research directions of machine learning which has significant impacts to the development of Artificial Intelligence (AI) over the last 20 years. Reinforcement learning is a learning process in which an agent can periodically make decisions, observe the results, and then automatically adjust its strategy to achieve the optimal policy. However, this learning process, even though proved to converge, takes a lot of time to reach the best policy as it has to explore and gain knowledge of an entire system, making it unsuitable and inapplicable to large-scale networks. Consequently, applications of reinforcement learning are very limited in practice. Recently, deep learning <ref type="bibr" target="#b1">[2]</ref> has been introduced as a new breakthrough technique. It can overcome the limitations of reinforcement learning, and thus open a new era for the development of reinforcement learning, namely Deep Reinforcement Learning (DRL). DRL embraces the advantage of Deep Neural Networks (DNNs) to train the learning process, thereby improving the learning speed and the performance of reinforcement learning algorithms. As a result, DRL has been adopted in numerous applications of reinforcement learning in practice such as robotics, computer vision, speech recognition, and natural language processing <ref type="bibr" target="#b1">[2]</ref>. One of the most famous applications of DRL is AlphaGo <ref type="bibr" target="#b2">[3]</ref>, the first computer program which can beat a human professional without handicaps on a full-sized 19×19 board.</p><p>In the areas of communications and networking, DRL has been recently used as an emerging tool to effectively address various problems and challenges. In particular, modern networks such as Internet of Things (IoT), Heterogeneous Networks (HetNets), and Unmanned Aerial Vehicle (UAV) network become more decentralized, ad-hoc, and autonomous in nature. Network entities such as IoT devices, mobile users, and UAVs need to make local and autonomous decisions, e.g., spectrum access, data rate selection, transmit power control, and base station association, to achieve the goals of different networks including, e.g., throughput maximization and energy consumption minimization. Under uncertain and stochastic environments, most of the decision-making problems can be modeled by a so-called Markov Decision Process (MDP) <ref type="bibr" target="#b3">[4]</ref>. Dynamic programming <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and other algorithms such as value iteration, as well as reinforcement learning techniques can be adopted to solve the MDP. However, the modern networks are large-scale and complicated, and thus the computational complexity of the techniques rapidly becomes unmanageable. As a result, DRL has been developing to be an alternative solution to overcome the challenge. In general, the DRL approaches provide the following advantages:</p><p>• DRL can obtain the solution of sophisticated network optimizations. Thus, it enables network controllers, e.g., base stations, in modern networks to solve non-convex and complex problems, e.g., joint user association, computation, and transmission schedule, to achieve the optimal solutions without complete and accurate network information.</p><p>• DRL allows network entities to learn and build knowl-edge about the communication and networking environment. Thus, by using DRL, the network entities, e.g., a mobile user, can learn optimal policies, e.g., base station selection, channel selection, handover decision, caching and offloading decisions, without knowing channel model and mobility pattern.</p><p>• DRL provides autonomous decision-making. With the DRL approaches, network entities can make observation and obtain the best policy locally with minimum or without information exchange among each other. This not only reduces communication overheads but also improves security and robustness of the networks. • DRL improves significantly the learning speed, especially in the problems with large state and action spaces. Thus, in large-scale networks, e.g., IoT systems with thousands of devices, DRL allows network controller or IoT gateways to control dynamically user association, spectrum access, and transmit power for a massive number of IoT devices and mobile users.</p><p>• Several other problems in communications and networking such as cyber-physical attacks, interference management, and data offloading can be modeled as games, e.g., the non-cooperative game. DRL has been recently used as an efficient tool to solve the games, e.g., finding the Nash equilibrium, without the complete information.</p><p>Although there are some surveys related to machine learning, they do not discuss the applications of DRL in communications and networking. Specifically, there are surveys that discuss the applications of DRL such as <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>, but they are specifically for computer vision and natural language processing. Also, there are surveys discussing the applications of machine learning for networking such as <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, and <ref type="bibr" target="#b12">[13]</ref>. However, they mostly focus on deep learning approaches. In particular, the survey in <ref type="bibr" target="#b8">[9]</ref> discusses deep learning approaches for network cybersecurity, the survey in <ref type="bibr" target="#b9">[10]</ref> reviews deep learning approaches for network traffic control, the survey in <ref type="bibr" target="#b10">[11]</ref> presents deep learning approaches for physical layer modulation, network access/resource allocation, and network routing, and the survey in <ref type="bibr" target="#b11">[12]</ref> presents deep learning approaches for emerging issues including edge caching and computing, multiple radio access and interference management. In summary, the existing surveys either consider applications of DRL for computer vision and natural language processing or discuss applications of deep learning for networking. There is no survey specifically discussing the applications of DRL for communications and networking. This motivates us to deliver the survey with the tutorial of DRL and the comprehensive literature review on the applications of DRL to address issues in communications and networking. For convenience, the related works in this survey are classified based on issues in communications and networking as shown in Fig. <ref type="figure">2</ref>. The major issues include network access, data rate control, wireless caching, data offloading, network security, connectivity preservation, traffic routing, and data collection. Also, the percentages of DRL related works for different networks and different issues in the networks are shown in Figs. <ref type="figure">1(a</ref>) and 1(b), respectively. From the figures, Fig. <ref type="figure">2:</ref> A taxonomy of the applications of deep reinforcement learning for communications and networking.</p><p>The rest of this paper is organized as follows. Section II presents the introduction of reinforcement learning and discusses DRL techniques as well as their extensions. Section III reviews the applications of DRL for dynamic network access and adaptive data rate control. Section IV discusses the applications of DRL for wireless caching and data offloading. Section V presents DRL related works for network security and connectivity preservation. Section VI considers how to use DRL to deal with other issues in communications and networking. Important challenges, open issues, and future research directions are outlined in Section VII. Section VIII concludes the paper. The list of abbreviations commonly appeared in this paper is given in Table <ref type="table">I</ref>. Note that DRL consists of two different algorithms which are Deep Q-Learning (DQL) and policy gradients <ref type="bibr" target="#b13">[14]</ref>. In particular, DQL is mostly used for the DRL related works. Therefore, in the rest of the paper, we use "DRL" and "DQL" interchangeably to refer to the DRL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP REINFORCEMENT LEARNING: AN OVERVIEW</head><p>In this section, we first present fundamental knowledge of Markov decision processes, reinforcement learning, and deep learning techniques which are important branches of machine learning theory. We then discuss DRL technique that can capitalize on the capability of the deep learning to improve efficiency and performance in terms of the learning rate for reinforcement learning algorithms. Afterward, advanced DRL models and their extensions are reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Markov Decision Processes</head><p>MDP <ref type="bibr" target="#b3">[4]</ref> is a discrete time stochastic control process. MDP provides a mathematical framework for modeling decisionmaking problems in which outcomes are partly random and under control of a decision maker or an agent. MDPs are useful for studying optimization problems which can be solved by dynamic programming and reinforcement learning techniques. Typically, an MDP is defined by a tuple (S, A, p, r) where S is a finite set of states, A is a finite set of actions, p is a transition probability from state s to state s after action a is executed, and r is the immediate reward obtained after action a is performed. We denote π as a "policy" which is a mapping from a state to an action. The goal of an MDP is to find an optimal policy to maximize the reward function. An MDP can be finite or infinite time horizon. For the finite time horizon MDP, an optimal policy π * to maximize the expected total reward is defined by max π E T t=0 r t (s t , π(s t )) , where a t = π(s t ). For the infinite time horizon MDP, the objective can be to maximize the expected discounted total reward or to maximize the average reward. The former is defined by max π E T t=0 γr t (s t , π(s t )) , while the latter is expressed by</p><formula xml:id="formula_0">lim inf T →∞ max π E T t=0</formula><p>r t (s t , π(s t )) , where γ ∈ [0, 1] is the discount factor. The discount factor γ determines the importance of future rewards compared with the current reward. If γ = 0, the agent is "myopic", i.e., it only considers to maximize its current reward, i.e., immediate reward. In contrast, if γ approaches one, the agent will strive for a long-term higher reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Partially Observable Markov Decision Process:</head><p>In MDPs, we assume that the system state is fully observable by the agent. However, in many cases, the agent only can observe a part of the system state, and thus Partially Observable Markov Decision Processes (POMDPs) <ref type="bibr" target="#b14">[15]</ref> can be used to model the decision-making problems. A typical POMDP model is defined by a 6-tuple (S, A, p, r, Ω, O), where S, A, p, r are defined the same as in the MDP model, Ω and O are defined as the set of observations and observation probabilities, respectively. At each time epoch, the agent is at state s, selects an action a based on its belief about the current state s, i.e., b(s), and observes the immediate reward r and current observation o. Based on the observation o and its belief about the current state b(s), the agent then updates its belief about the new state s , i.e., b(s ), as follows <ref type="bibr" target="#b14">[15]</ref> ,</p><p>where O(o|s, a, s ) is the probability that the agent obtains observation o after action a is taken at state s and the agent moves to state s . p(s |s, a) is defined the same as that of the MDP model, i.e., the transition probability from state s to state s after action a is performed at state s. Finally, the agent receives an immediate reward r that is equal to r(s, a) in the MDP. Similar to the MDP model, the agent in POMDP also aims to find the optimal policy π * in order to maximize its expected long-term discounted reward ∞ t=0 γr t (s t , π * (s t )). Fig. <ref type="figure" target="#fig_1">3</ref> illustrates and compares an MDP and a POMDP. 2) Markov Games: In game theory, a Markov game, or a stochastic game <ref type="bibr" target="#b15">[16]</ref>, is a dynamic game with probabilistic transitions played by multiple players, i.e., agents. A typical Markov game model is defined by a tuple (I, S, {A i } i∈I , p, {r i } i∈I ), where</p><p>• I {1, . . . , i, . . . , I} is a set of agents, • S {S 1 , . . . , S i , . . . , S I } is the global state space of all agents with S i being the state space of agent i, • {A i } i∈I are sets of action spaces of the agents with A i being the action space of agent i, <ref type="bibr" target="#b0">1]</ref> is the transition probability function of the system. • {r i } i∈I are payoff functions of the agents with r i S × A 1 × • • • × A I → R, i.e., the payoff of agent i obtained after all actions of the agents are executed.</p><formula xml:id="formula_2">• p S×A 1 ×• • •×A I → [0,</formula><p>In a Markov game, the agents start at some initial state s 0 ∈ S. After observing the current state, all the agents simultaneously select their actions a = {a 1 , . . . , a I } and they will receive their corresponding rewards together with their own new observations. At the same time, the system will transit to a new state s ∈ S with probability p(s |s, a). The procedure is repeated at the new state and continues for a finite or infinite number of stages. In this game, all the agents try to find their optimal policies to maximize their own expected long-term average rewards, i.e., ∞ t=0 γ i r i t (s t , π * i (s t )), ∀i. The set of all optimal policies of this game, i.e., {π * 1 , . . . , π * I } is known to be the equilibrium of this game. If there is a finite number of players and the sets of states and actions are finite, then the Markov game always has a Nash equilibrium <ref type="bibr" target="#b16">[17]</ref> under a finite number of stages. The same is true for Markov games with infinite stages, but the total payoff of agents is the discounted sum <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reinforcement Learning</head><p>Reinforcement learning, an important branch of machine learning, is an effective tool and widely used in the literature to address MDPs <ref type="bibr" target="#b0">[1]</ref>. In a reinforcement learning process, an agent can learn its optimal policy through interaction with its environment. In particular, the agent first observes its current state, and then takes an action, and receives its immediate reward together with its new state as illustrated in Fig. <ref type="figure" target="#fig_2">4(a)</ref>. The observed information, i.e., the immediate reward and new state, is used to adjust the agent's policy, and this process will be repeated until the agent's policy approaches to the optimal policy. In reinforcement learning, Q-learning is the most effective method and widely used in the literature. In the following, we will discuss the Q-learning algorithm and its extensions for advanced MDP models.</p><p>1) Q-Learning Algorithm: In an MDP, we aim to find an optimal policy π * : S → A for the agent to maximize the expected long-term reward function for the system. Accordingly, we first define value function V π : S → R that represents the expected value obtained by following policy π from each state s ∈ S. The value function V for policy π quantifies the goodness of the policy through an infinite horizon and discounted MDP that can be expressed as follows:</p><formula xml:id="formula_3">V π (s) = E π ∞ t=0</formula><p>γr t (s t , a t )|s 0 = s = E π r t (s t , a t ) + γV π (s t+1 )|s 0 = s .</p><p>(2)</p><p>Since we aim to find the optimal policy π * , an optimal action at each state can be found through the optimal value function expressed by V * (s) = max at E π r t (s t , a t )+γV π (s t+1 ) .</p><p>If we denote Q * (s, a) r t (s t , a t ) + γE π V π (s t+1 ) as the optimal Q-function for all state-action pairs, then the optimal value function can be written by V * (s) = max a Q * (s, a) . Now, the problem is reduced to find optimal values of Qfunction, i.e., Q * (s, a), for all state-action pairs, and this can be done through iterative processes. In particular, the Qfunction is updated according to the following rule:</p><formula xml:id="formula_4">Q t+1 (s, a) =Q t (s, a)+ α t r t (s, a) + γ max a Q t (s, a ) -Q t (s, a) .</formula><p>(3) The core idea behind this update is to find the Temporal Difference (TD) between the predicted Q-value, i.e., r t (s, a)+ γmax a Q t (s, a ) and its current value, i.e., Q t (s, a). In <ref type="bibr" target="#b2">(3)</ref>, the learning rate α t is used to determine the impact of new information to the existing Q-value. The learning rate can be chosen to be a constant, or it can be adjusted dynamically during the learning process. However, it must satisfy Assumption 1 to guarantee the convergence for the Q-learning algorithm. Assumption 1. The step size α t is deterministic, nonnegative and satisfies the following conditions:</p><formula xml:id="formula_5">α t ∈ [0, 1], ∞ t=0 α t = ∞,</formula><p>and</p><formula xml:id="formula_6">∞ t=0 (α t ) 2 &lt; ∞ .</formula><p>The step size adaptation α t = 1 t is one of the most common examples used in reinforcement learning. More discussions for selecting an appropriate step size can be found in <ref type="bibr" target="#b17">[18]</ref>. The details of the Q-learning algorithm are then provided in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Q-learning algorithm</head><p>Input: For each state-action pair (s, a), initialize the table entry Q(s, a) arbitrarily, e.g., to zero. Observe the current state s, initialize a value for the learning rate α and the discount factor γ. for t := 1 to T do From the current state-action pair (s, a), execute action a and obtain the immediate reward r and a new state s . Select an action a based on the state s and then update the table entry for Q(s, a) as follows:</p><formula xml:id="formula_7">Q t+1 (s, a) ← Q t (s, a) + α t r t (s, a)+ γ max a Q t (s , a ) -Q t (s, a)<label>(4)</label></formula><p>Replace s ← s . end for Output: π * (s) = arg max a Q * (s, a).</p><p>Once either all Q-values converge or a certain number of iterations is reached, the algorithm will terminate. The algorithm then yields the optimal policy indicating an action to be taken at each state such that Q * (s, a) is maximized for all states in the state space, i.e., π * (s) = arg max a Q * (s, a). Under the assumption of the step size (i.e., Assumption 1), it is proved in <ref type="bibr" target="#b18">[19]</ref> that the Q-learning algorithm converges to the optimum action-values with probability one.</p><p>It is worth noting that unlike value function V π , Q-function is an example of model-free learning algorithm that does not require the agent to know the system model parameters, e.g., the state-transition and reward models, in advance to estimate the pairs of state-action values. Specifically, the core idea of Q-function is to approximate the values of state-action pairs through samples obtained during the interactions with the environment. Furthermore, while the value function takes the expectation of all actions according to the policy π, the Q-function only focuses on a particular action at a particular state. As a result, learning algorithms using Q-function are less complex than those of using the value function. However, from the sampling perspective, the dimension of Q-function is higher than that of value function, and thus Q-function might be more difficult to get enough samples, i.e., state-action pairs, to learn. Therefore, if the system model is available in advance, the value function is usually preferable.</p><p>2) SARSA: An Online Q-Learning Algorithm: Although the Q-learning algorithm can find the optimal policy for the agent without requiring knowledge about the environment, this algorithm works in an offline fashion. In particular, Algorithm 1 can obtain the optimal policy only after all Q-values converge. Therefore, this section presents an alternative online learning algorithm, i.e., the SARSA algorithm, which allows the agent to approach the optimal policy in an online fashion.</p><p>Different from the Q-learning algorithm, the SARSA algorithm is an online algorithm which allows the agent to choose optimal actions at each time step in a real-time fashion without waiting until the algorithm converges. In the Qlearning algorithm, the policy is updated according to the maximum reward of available actions regardless of which policy is applied, i.e., an off-policy method. In contrast, the SARSA algorithm interacts with the environment and updates the policy directly from the actions taken, i.e., an on-policy method. Note that the SARSA algorithm updates Q-values from the quintuple Q(s, a, r, s , a ).</p><p>3) Q-Learning for Markov Games: To apply Q-learning algorithm to the Markov game context, we first define the Q-function for agent i by Q i (s, a i , a -i ), where a -i {a 1 , . . . , a i-1 , a i+1 , . . . , a I } denotes the set of actions of all agents except i. Then, the Nash Q-function of agent i is defined by:</p><formula xml:id="formula_8">Q * i (s, a i , a -i ) = r i (s, a i , a -i )+ β s ∈S p(s |s, a i , a -i )V i (s , π * 1 , . . . , π * I ),<label>(5)</label></formula><p>where (π * 1 , . . . , π * I ) is the joint Nash equilibrium strategy, r i (s, a i , a -i ) is agent i's immediate reward in state s under the joint action (a i , a -i ), and V i (s , π * 1 , . . . , π * I ) is the total discounted reward over an infinite time horizon starting from state s given that all the agents follow the equilibrium strategies.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, the authors propose a multi-agent Q-learning algorithm for general-sum Markov games which allows the agents to perform updates based on assuming Nash equilibrium behavior over the current Q-values. In particular, agent i will learn its Q-values by forming an arbitrary guess from starting time of the game. At each time step t, agent i observes the current state and takes an action a i . Then, it observes its immediate reward r i , actions taken by others a -i , others' immediate rewards, and the new system state s . After that, agent i calculates a Nash equilibrium (π 1 (s ), . . . , π I (s )) for the state game (Q t 1 (s ), . . . , Q t I (s )), and updates its Q-values according to:</p><formula xml:id="formula_9">Q t+1 i (s, a i , a -i ) = (1 -α t )Q t i (s, a i , a -i ) + α t [r i t + γN i t (s )],<label>(6)</label></formula><p>where α t ∈ (0, 1) is the learning rate and</p><formula xml:id="formula_10">N i t (s ) Q t i (s ) × π 1 (s ) × • • • × π I (s ).</formula><p>In order to calculate the Nash equilibrium, agent i needs to know (Q t 1 (s ), . . . , Q t I (s )). However, the information about other agents' Q-values is not given, and thus agent i must learn this information too. To do so, agent i will set estimations about others' Q-values at the beginning of the game, e.g., Q j 0 (s, a i , a -i ) = 0, ∀j, s. As the game proceeds, agent i observes other agents' immediate rewards and previous actions. That information can then be used to update agent i's conjectures on other agents' Q-functions. Agent i updates its beliefs about agent j's Q-function, according to the same updating rule in <ref type="bibr" target="#b5">(6)</ref>. Then, the authors prove that under some highly restrictive assumptions on the form of the state games during learning, the proposed multi-agent Q-learning algorithm is guaranteed to be converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Learning</head><p>Deep learning <ref type="bibr" target="#b1">[2]</ref> is composed of a set of algorithms and techniques that attempt to find important features of data and to model its high-level abstractions. The main goal of deep learning is to avoid manual description of a data structure (like hand-written features) by automatic learning from the data. Its name refers to the fact that typically any neural network with two or more hidden layers is called DNN. Most deep learning models are based on an Artificial Neural Network (ANN), even though they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in Deep Belief Networks and Deep Boltzmann Machines.</p><p>An ANN is a computational nonlinear model based on the neural structure of the brain that is able to learn to perform tasks such as classification, prediction, decision-making, and visualization. An ANN consists of artificial neurons and is organized into three interconnected layers: input, hidden, and output as illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>(b). The input layer contains input neurons that send information to the hidden layer. The hidden layer sends data to the output layer. Every neuron has weighted inputs (synapses), an activation function, and one output. Synapses are the adjustable parameters that convert a neural network to a parameterized system. The activation function of a node defines the outputs of that node given the inputs. In particular, the activation function will map the input values into target ranges depending on the selected activation function. For example, the logistic activation function will map all inputs in the real number domain into the range of 0 to 1.</p><p>During the training phase, ANNs use backpropagation as an effective learning algorithm to compute quickly a gradient descent with respect to the weights. Backpropagation is a special case of automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weights of neurons by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.  A DNN is defined as an ANN with multiple hidden layers. There are two typical DNN models, i.e., Feedforward Neural Network (FNN) and Recurrent Neural Network (RNN). In the FNN, the information moves in only one direction, i.e., from the input nodes, through the hidden nodes and to the output nodes, and there are no cycles or loops in the network as shown in Fig. <ref type="figure" target="#fig_3">5</ref>. In FNNs, Convolutional Neural Network (CNN) is the most well known model with a wide range of applications especially in image and speech recognition. The CNN contains one or more convolutional layers, pooling or fully connected, and uses a variation of multilayer perceptrons discussed above. In general, CNNs have two main components, i.e., feature extraction and classification, as illustrated in Fig. <ref type="figure" target="#fig_4">6</ref>. The feature extraction component is placed at the hidden layers with the aim of performing a series of convolutions and pooling operations during which the features are being detected. After that, the classification component, placed at the fully connected layers, will assign a probability for the object, e.g., on the image, to what we need to predict.</p><p>Unlike FNNs, the RNN is a variant of a recursive artificial neural network in which connections between neurons make directed cycles. It means that an output depends not only on its immediate inputs, but also on the previous further step's neuron state. The RNNs are designed to utilize sequential data, when the current step has some relation with the previous steps. This makes the RNNs ideal for applications with a time component, e.g., time-series data, and natural language processing. However, all RNNs have feedback loops in the recurrent layer. This lets RNNs maintain information in memory over time. Nevertheless, it can be difficult to train standard RNNs to solve problems that require learning longterm temporal dependencies. The reason is that the gradient of the loss function decays exponentially with time, which is called the vanishing gradient problem. Thus, Long Short-Term Memory (LSTM) is often used in RNNs to address this issue. LSTMs are designed to model temporal sequences and their long-range dependencies are more accurate than conventional RNNs. In particular, LSTMs provide a solution by incorporating memory units that allow the network to learn when to forget previous hidden states and when to update hidden states given new information. Usually, LSTM units are implemented in "blocks" which have three or four "gates", e.g., input gate, forget gate, output gate and input modulation gate, as illustrated in Fig. <ref type="figure" target="#fig_5">7</ref> [20] to control information flow drawing on the logistic function. Unlike the RNN, the LSTM memory cell is composed of three components, i.e., the previous memory cell c t , current input x t and previous hidden state h t-1 . Here, the input gate and forget gate will be used to selectively forget its previous memory or consider its current input. Likewise, the output gate learns how much of the memory cell to transfer to the hidden state. These additional blocks enable the LSTM to learn extremely complex and longterm temporal dynamics that the RNN is unable to do.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deep Q-Learning</head><p>The Q-learning algorithm can efficiently obtain an optimal policy when the state space and action space are small. However, in practice, with complicated system models, these spaces are usually large. As a result, the Q-learning algorithm may not be able to find the optimal policy. Thus, Deep Q-Learning (DQL) algorithm is introduced to overcome this shortcoming. Intuitively, the DQL algorithm implements a Deep Q-Network (DQN), i.e., a DNN, instead of the Q-table to derive an approximate value of Q * (s, a) as shown in Fig. <ref type="figure" target="#fig_2">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c).</head><p>As stated in <ref type="bibr" target="#b20">[21]</ref>, the average reward obtained by reinforcement learning algorithms may not be stable or even diverge when a nonlinear function approximator is used. This stems from the fact that a small change of Q-values may greatly affect the policy. Thus, the data distribution and the correlations between the Q-values and the target values R + γ max a Q(s , a ) are varied. To address this issue, two mechanisms, i.e., experience replay and target Q-network, can be used.</p><p>• Experience replay mechanism: The algorithm first initializes a replay memory D, i.e., the memory pool, with transitions (s t , a t , r t , s t+1 ), i.e., experiences, generated randomly, e.g., through using -greedy policy. Then, the algorithm randomly selects samples, i.e., minibatches, of transitions from D to train the DNN. The Q-values obtained by the trained DNN will be used to obtain new experiences, i.e., transitions, and these experiences will then be stored in the memory pool D. This mechanism allows the DNN trained more efficiently by using both old and new experiences. In addition, by using the experience replay, the transitions are more independent and identically distributed, and thus the correlations between observations can be removed. • Fixed target Q-network: In the training process, the Qvalue will be shifted. Thus, the value estimations can be out of control if a constantly shifting set of values is used to update the Q-network. This leads to the destabilization of the algorithm. To address this issue, the target Q-network is used to update frequently but slowly the primary Q-networks' values. In this way, the correlations between the target and estimated Qvalues are significantly reduced, thereby stabilizing the algorithm. The DQL algorithm with experience replay and fixed target Q-network is presented in Algorithm 2. DQL inherits and promotes advantages of both reinforcement and deep learning techniques, and thus it has a wide range of applications in practice such as game development <ref type="bibr" target="#b2">[3]</ref>, transportation <ref type="bibr" target="#b21">[22]</ref>, and robotics <ref type="bibr" target="#b22">[23]</ref>. Table <ref type="table" target="#tab_2">II</ref> summarizes how different the DQL approach and other techniques, i.e., reinforcement learning, deep learning, and traditional combinatorial optimization methods, solve optimization problems.</p><p>E. Advanced Deep Q-Learning Models 1) Double Deep Q-Learning: In some stochastic environments, the Q-learning algorithm performs poorly due to the large over-estimations of action values <ref type="bibr" target="#b23">[24]</ref>. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value as shown in Eq. ( <ref type="formula" target="#formula_7">4</ref>). The reason is that the same samples are used to decide which action is the best, i.e., with highest expected reward, and the same samples are also used to estimate that action-value. Thus, to overcome the over-estimation problem of the Q-learning algorithm, the authors in <ref type="bibr" target="#b24">[25]</ref> introduce a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic programming</head><p>Used to address complex problem by breaking it down into a set of simpler subproblems over multiple steps, solving each of those subproblems just once at at a time, and storing their solutions in a memory. Thus, in the future, if the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time.</p><p>Reinforcement Learning (RL) This is a branch of machine learning used to help an agent to learn the optimal policy when the agent has no information about the surrounding environment. Specifically, the agent first observes its current state, and then takes an action, and receives its immediate reward together with its new state. The observed information, i.e., the immediate reward and new state, is used to adjust the agent's policy, and this process is repeated until the agent's policy approaches the optimal policy.</p><p>Deep Learning (DL) This is a branch of machine learning used to help an agent to learn the optimal policy when the agent has some information about surrounding environment in advance. In particular, based on the obtained information (i.e., stored in a database), the agent will train the neural network and find the optimal parameters for the network. The trained neural network will be then implemented on the agent to help the agent make decisions in an online fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Reinforcement Learning (DRL)</head><p>This is an advanced model of reinforcement learning technique in which deep learning is utilized as an effective tool to improve learning rate for reinforcement learning algorithms. In particular, during real-time learning process, the obtained experiences will be stored and used as the data to train the neural network. The trained neural network will be then used to help the agent make optimal decisions in a real-time manner. Note that unlike deep learning technique, the neural network in the DRL will be trained frequently based on new experiences obtained during the real-time interactions with surrounding environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2</head><p>The DQL Algorithm with Experience Replay and Fixed Target Q-Network With probability select a random action a t , otherwise select a t = arg max Q * (s t , a t , θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Perform action a t and observe immediate reward r t and next state s t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Store transition (s t , a t , r t , s t+1 ) in D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Select randomly samples c(s j , a j , r j , s j+1 ) from D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>The weights of the neural network then are optimized by using stochastic gradient descent with respect to the network parameter θ to minimize the loss:</p><formula xml:id="formula_11">r j + γ max aj+1 Q(s j+1 , a j+1 ; θ ) -Q(s j , a j ; θ) 2 .<label>(7) 10:</label></formula><p>Reset Q = Q after every a fixed number of steps. 11: end for solution using two Q-value functions, i.e., Q 1 and Q 2 , to simultaneously select and evaluate action values. In particular, the selection of an action is still due to the online weights θ 1 . This means that, as in Q-learning, we are still estimating the value of the greedy policy according to the current values, as defined by θ 1 . However, the second set of weights θ 2 is used to evaluate fairly the value of this policy. This second set of weights can be updated symmetrically by switching the roles of θ 1 and θ 2 . Inspired by this idea, the authors in <ref type="bibr" target="#b24">[25]</ref> then develop Double Deep Q-Learning (DDQL) model <ref type="bibr" target="#b25">[26]</ref> using a Double Deep Q-Network (DDQN) with the loss function updated as follows:</p><formula xml:id="formula_12">r j + γ Q s j+1 , arg max aj+1 Q s j+1 , a j+1 ; θ ; θ<label>(8)</label></formula><p>-Q(s j , a j ; θ)</p><formula xml:id="formula_13">2 .</formula><p>Unlike double Q-learning, the weights of the second network θ 2 are replaced with the weights of the target networks θ for the evaluation of the current greedy policy as shown in Eq. <ref type="bibr" target="#b7">(8)</ref>. The update to the target network stays unchanged from DQN, and remains a periodic copy of the online network. Due to the effectiveness of DDQL, there are some applications of DDQL introduced recently to address dynamic spectrum access problems in multichannel wireless networks <ref type="bibr" target="#b26">[27]</ref> and resource allocation in heterogeneous networks <ref type="bibr" target="#b27">[28]</ref>.</p><p>2) Deep Q-Learning with Prioritized Experience Replay: Experience replay mechanism allows the reinforcement learning agent to remember and reuse experiences, i.e., transitions, from the past. In particular, transitions are uniformly sampled from the replay memory D. However, this approach simply replays transitions at the same frequency as that the agent was originally experienced, regardless of their significance. Therefore, the authors in <ref type="bibr" target="#b28">[29]</ref> develop a framework for prioritizing experiences, so as to replay important transitions more frequently, and therefore learn more efficiently. Ideally, we want to sample more frequently those transitions from which there is much to learn. In general, the DQL with the Prioritized Experience Replay (PER) samples transitions with a probability related to the last encountered absolute error <ref type="bibr" target="#b28">[29]</ref>. New transitions are inserted into the replay buffer with maximum priority, providing a bias towards recent transitions. Note that stochastic transitions may also be favoured, even when there is little left to learn about them. Through real experiments on many Atari games, the authors demonstrate that DQL with PER outperforms DQL with uniform replay on 41 out of 49 games. However, this solution is only appropriate to implement when we can find and define the important experiences in the replay memory D.</p><p>3) Dueling Deep Q-Learning: The Q-values, i.e., Q(s, a), used in the Q-learning algorithm, i.e., Algorithm 1, are to express how good it is to take a certain action at a given state. The value of an action a at a given state s can actually be decomposed into two fundamental values. The first value is the state-value function, i.e., V (s), to estimate the importance of being in a particular state s. The second value is the action-value function, i.e., A (a), to estimate the importance of selecting an action a compared with other actions. As a result, the Q-value function can be expressed by two fundamental value functions as follows: Q(s, a) = V (s) + A (a).</p><p>Stemming from the fact that in many MDPs, it is unnecessary to estimate both values, i.e., action and state values of Q-function Q(s, a), at the same time. For example, in many racing games, moving left or right matters if and only if the agent meets the obstacles or enemies. Inspired by this idea, the authors in <ref type="bibr" target="#b29">[30]</ref> introduce an idea of using two streams, i.e., two sequences, of fully connected layers instead of using a single sequence with fully connected layers for the DQN. The two streams are constructed such that they are able to provide separate estimations on the action and state value functions, i.e., V (s) and A (a). Finally, the two streams are combined to generate a single output Q(s, a) as follows:</p><formula xml:id="formula_14">Q(s, a; α, β) = V (s; β) + A (s, a; α) -a A (s, a ; α) |A| ,<label>(9)</label></formula><p>where β and α are the parameters of the two streams V (s; β) and A (s, a ; α), respectively. Here, |A| is the total number of actions in the action space A. Then, the loss function is derived in the similar way to <ref type="bibr" target="#b6">(7)</ref>. Through the simulation, the authors show that the proposed dueling DQN can outperform DDQN <ref type="bibr" target="#b25">[26]</ref> in 50 out of 57 learned Atari games. However, the proposed dueling architecture only clearly benefits for MDPs with large action spaces. For small state spaces, the performance of dueling DQL is even not as good as that of double DQL as shown in simulation results in <ref type="bibr" target="#b29">[30]</ref>.</p><p>4) Asynchronous Multi-step Deep Q-Learning: Most of the Q-learning methods such as DQL and dueling DQL rely on the experience replay method. However, such kind of method has several drawbacks. For example, it uses more memory and computation resources per real interaction, and it requires off-policy learning algorithms that can update from data generated by an older policy. This limits the applications of DQL. Therefore, the authors in <ref type="bibr" target="#b30">[31]</ref> introduce a method using multiple agents to train the DNN in parallel. In particular, the authors propose a training procedure which utilizes asynchronous gradient decent updates from multiple agents at once. Instead of training one single agent that interacts with its environment, multiple agents are interacting with their own version of the environment simultaneously. After a certain amount of timesteps, accumulated gradient updates from an agent are applied to a global model, i.e., the DNN. These updates are asynchronous and lock free. In addition, to tradeoff between bias and variance in the policy gradient, the authors adopt n-step updates method <ref type="bibr" target="#b0">[1]</ref> to update the reward function. In particular, the truncated n-step reward function can be defined by r</p><formula xml:id="formula_15">(n) t = n-1 k=0</formula><p>γ (k) r t+k+1 . Thus, the alternative loss for each agent will be derived by:</p><formula xml:id="formula_16">r (n) j + γ (n) j max a Q(s j+n , a ; θ ) -Q(s j , a j ; θ) 2 . (10)</formula><p>The effects of training speed and quality of the proposed asynchronous DQL with multi-step learning are analyzed for various reinforcement learning methods, e.g., 1-step Qlearning, 1-step SARSA, and n-step Q-learning. They show that asynchronous updates have a stabilizing effect on policy and value updates. Also, the proposed method outperforms the current state-of-the-art algorithms on the Atari games while training for half of the time on a single multi-core CPU instead of a GPU. As a result, some recent applications of asynchronous DQL have been developed for handover control problems in wireless systems <ref type="bibr" target="#b31">[32]</ref> 5) Distributional Deep Q-learning: All aforementioned methods use the Bellman equation to approximate the expected value of future rewards. However, if the environment is stochastic in nature and the future rewards follow multimodal distribution, choosing actions based on expected value may not lead to the optimal outcome. For example, we know that the expected transmission time of a packet in a wireless network is 20 minutes. However, this information may not be so meaningful because it may overestimate the transmission time most of the time. For example, the expected transmission time is calculated based on the normal transmissions (without collisions) and the interference transmissions (with collisions). Although the interference transmissions are very rare to happen, but it takes a lot of time. Then, the estimation about the expected transmission is overestimated most of the time. This makes estimations not useful for the DQL algorithms.</p><p>Thus, the authors in <ref type="bibr" target="#b32">[33]</ref> introduce a solution using distributional reinforcement learning to update Q-value function based on its distribution rather than its expectation. In particular, let Z(s, a) be the return obtained by starting from state s, executing action a, and following the current policy, then</p><formula xml:id="formula_17">Q(s, a) = E[Z(s, a)].</formula><p>Here, Z represents the distribution of future rewards, which is no longer a scalar quantity like Qvalues. Then we obtain the distributional version of Bellman equation as follows: Z(s, a) = r + γZ(s , a ). Although the proposed distributional deep Q-learning is demonstrated to outperform the conventional DQL <ref type="bibr" target="#b20">[21]</ref> on many Atari 2600 Games (45 out of 57 games), its performance relies much on the distribution function Z. If Z is well defined, the performance of distributional deep Q-learning is much more significant than that of the DQL. Otherwise, its performance is even worse than that of the DQL.</p><p>6) Deep Q-learning with Noisy Nets: In <ref type="bibr" target="#b33">[34]</ref>, the authors introduce Noisy Net, a type of neural network whose bias and weights are iteratively perturbed during training by a parametric function of the noise. This network basically adds the Gaussian noise to the last (fully-connected) layers of the network. The parameters of this noise can be adjusted by the model during training, which allows the agent to decide when and in what proportion it wants to introduce the uncertainty to its weights. In particular, to implement the noisy network, we first replace the -greedy policy by a randomized action-value function. Then, the fully connected layers of the value network are parameterized as a noisy network, where the parameters are drawn from the noisy network parameter distribution after every replay step. For replay, the current noisy network parameter sample is held fixed across the batch. Since the DQL takes one step of optimization for every action step, the noisy network parameters are re-sampled before every action.</p><p>Through experimental results, the authors demonstrate that by adding the Gaussian noise layer to the DNN, the performance of conventional DQL <ref type="bibr" target="#b20">[21]</ref>, dueling DQL <ref type="bibr" target="#b29">[30]</ref>, and asynchronous DQL <ref type="bibr" target="#b30">[31]</ref> can be significantly improved for a wide range of Atari games. However, the impact of noise to the performance of the deep DQL algorithms is still under debating in the literature, and thus analysis on the impact of noise layer requires further investigations.</p><p>7) Rainbow Deep Q-learning: In <ref type="bibr" target="#b34">[35]</ref>, the authors propose a solution which integrates all advantages of seven aforementioned solutions (including DQL) into a single learning agent, called Rainbow DQL. In particular, this algorithm first defines the loss function based on the asynchronous multi-step and distributional DQL. Then, the authors combine the multi-step distributional loss with double Q-learning by using the greedy action in s t+n selected according to the Q-network as the bootstrap action a * t+n , and evaluate the action by using the target network.</p><p>In standard proportional prioritized replay <ref type="bibr" target="#b28">[29]</ref> technique, the absolute TD-error is used to prioritize the transitions. Here, TD-error at a time slot is the error in the estimate made at the time slot. However, in the proposed Rainbow DQL algorithm, all distributional Rainbow variants prioritize transitions by the Kullbeck-Leibler (KL) loss because this loss may be more robust to noisy stochastic environment. Alternatively, the dueling architecture of DNNs is presented in <ref type="bibr" target="#b29">[30]</ref>. Finally, the Noisy Net layer <ref type="bibr" target="#b34">[35]</ref> is used to replace all linear layers in order to reduce the number of independent noise variables. Through simulation, the authors show that this is the most advanced technique which outperforms almost all current DQL algorithms in the literature over 57 Atari 2600 games.</p><p>In Table <ref type="table" target="#tab_4">III</ref>, we summarize the DQL algorithms and their performance under the parameter settings used in <ref type="bibr" target="#b34">[35]</ref>. As observed in Table <ref type="table" target="#tab_4">III</ref>, all of the DQL algorithms have been developed by Google DeepMind based on the original work in <ref type="bibr" target="#b20">[21]</ref>. So far, through experimental results on Atari 2600 games, the Rainbow DQL presents very impressive results over all other DQL algorithms. However, more experiments need to be further conducted in different domains to confirm the real efficiency of the Rainbow DQL algorithm.</p><p>F. Deep Q-Learning for Extensions of MDPs 1) Deep Deterministic Policy Gradient Q-Learning for Continuous Action: Although DQL algorithm can solve problems with high-dimensional state spaces, it can only handle discrete and low-dimensional action spaces. However, systems in many applications have continuous, i.e., real values, and high dimensional action spaces. The DQL algorithms cannot be straightforwardly applied to continuous actions since they rely on choosing the best action that maximizes the Q-value function. In particular, a full search in a continuous action space to find the optimal action is often infeasible.</p><p>In <ref type="bibr" target="#b35">[36]</ref>, the authors introduce a model-free off-policy actorcritic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces.</p><p>The key idea is based on the deterministic policy gradient (DPG) algorithm proposed in <ref type="bibr" target="#b36">[37]</ref>. In particular, the DPG algorithm maintains a parameterized actor function µ(s; θ µ ) with parameter vector θ which specifies the current policy by deterministically mapping states to a specific action. The critic Q(s, a) is learned by using the Bellman equation as in Q-learning. The actor is updated by applying the chain rule to the expected return from the start distribution with respect to the actor parameters.</p><p>Based on this update rule, the authors then introduce Deep DPG (DDPG) algorithm which can learn competitive policies by using low-dimensional observations, e.g. cartesian coordinates or joint angles, under the same hyper-parameters and network structure. The algorithm makes a copy of the actor and critic networks Q (s, a; θ Q ) and µ (s; θ µ ), respectively, to calculate the target values. The weights of these target networks are then updated with slowly tracking on the learned networks, i.e., θ ← τ θ + (1 -τ )θ with τ 1. This means that the target values are constrained to change slowly, greatly improving the stability of learning. Note that the major challenge of learning in continuous action spaces is exploration. Therefore, in the proposed algorithm, the exploration policy µ is constructed by adding noise sampled from a noise process N to the actor policy.</p><p>2) Deep Recurrent Q-Learning for POMDPs: To tackle problems with partially observable environments by deep reinforcement learning, the authors in <ref type="bibr" target="#b37">[38]</ref> propose a framework called Deep Recurrent Q-Learning (DRQN) in which an LSTM layer was used to replace the first post-convolutional fully-connected layer of the conventional DQN. The recurrent structure is able to integrate an arbitrarily long history to better estimate the current state instead of utilizing a fixedlength history as in DQNs. Thus, DRQNs estimate the function Q(o t , h t-1 ; θ) instead of Q(s t , a t ); θ), where θ denotes the parameters of entire network, h t-1 denotes the output of the LSTM layer at the previous step, i.e., h t = LST M (h t-1 , o t ). DRQN matches DQN's performance on standard MDP problems and outperforms DQN in partially observable domains. Regarding the training process, DRQN only considers the convolutional features of the observation history instead of explicitly incorporating the actions. Through the experiments, the authors demonstrate that DRQN is capable of handling partial observability, and recurrency confers benefits when the quality of observations changes during evaluation time.</p><p>3) Deep SARSA Learning: In <ref type="bibr" target="#b38">[39]</ref>, the authors introduce a DQL technique based on SARSA learning to help the agent determine optimal policies in an online fashion. In this algorithm, given the current state s, the CNN is used to obtain the current state-action value Q(s, a). Then, the current action a is selected by the -greedy algorithm. After that, the immediate reward r and the next state s can be observed.</p><p>To estimate the current Q(s, a), the next state-action value Q(s , a ) is obtained. Here, when the next state s is used as the input of the CNN, Q(s , a ) can be obtained as the output. Then, a label vector related to Q(s, a) is defined as Q(s , a ) which represents the target vector. The two vectors only have one different component, i.e., r + γQ(s , a ) → Q(s, a). It should be noted that during the training phase, the next action a for estimating the current state-action value is never greedy. On the contrary, there is a small probability that a random action is chosen for exploration.</p><p>4) Deep Q-Learning for Markov Games: In <ref type="bibr" target="#b39">[40]</ref>, the authors introduce the general notion of sequential prisoner's dilemma (SPD) to model real world prisoner's dilemma (PD) problems. Since SPD is more complicated than PD, existing approaches addressing learning in matrix PD games cannot be directly applied in SPD. Thus, the authors propose a multiagent DRL approach for mutual cooperation in SDP games. The deep multi-agent reinforcement learning towards mutual cooperation consists of two phases, i.e., offline and online phases. The offline phase generates policies with varying cooperation degrees. Since the number of policies with different cooperation degrees is infinite, it is computationally infeasible to train all the policies from scratch. To address this issue, the algorithm first trains representative policies using actorcritic until it converges, i.e., cooperation and defection baseline policy. Second, the algorithm synthesizes the full range of policies from the above baseline policies. Another task is to detect effectively the cooperation degree of the opponent. The algorithm divides this task into two steps. First, the algorithm trains an LSTM-based cooperation degree detection network offline, which will be then used for real-time detection during the online phase. In the online phase, the agent plays against the opponents by reciprocating with a policy of a slightly higher cooperation degree than that of the opponent. On one hand, intuitively the algorithm is cooperation-oriented and seeks for mutual cooperation whenever possible. On the other hand, the algorithm is also robust against selfish exploitation and resorts to defection strategy to avoid being exploited whenever necessary.</p><p>Unlike <ref type="bibr" target="#b39">[40]</ref> which considers a repeated normal form game with complete information, in <ref type="bibr" target="#b40">[41]</ref>, the authors introduce an application of DRL for extensive form games with imperfect information. In particular, the authors in <ref type="bibr" target="#b40">[41]</ref> introduce Neural Fictitious Self-Play (NFSP), a DRL method for learning approximate Nash equilibria of imperfect-information games. NFSP combines FSP with neural network function approximation. An NFSP agent has two neural networks. The first network is trained by reinforcement learning from memorized experience of play against fellow agents. This network learns an approximate best response to the historical behaviour of other agents. The second network is trained by supervised learning from memorized experience of the agent's own behaviour. This network learns a model that averages over the agent's own historical strategies. The agent behaves according to a mixture of its average strategy and best response strategy.</p><p>In the NSFP, all players of the game are controlled by separate NFSP agents that learn from simultaneous play against each other, i.e., self-play. An NFSP agent interacts with its fellow agents and memorizes its experience of game transitions and its own best response behaviour in two memories, M RL and M SL . NFSP treats these memories as two distinct datasets suitable for DRL and supervised classification, respectively. The agent trains a neural network, Q(s, a; θ Q ), to predict action values from data in M RL using off-policy reinforcement learning. The resulting network defines the agent's approximate best response strategy, β = -greedy(Q), which selects a random action with probability and otherwise chooses the action that maximizes the predicted action values. The agent trains a separate neural network Π(s, a; θ Π ) to imitate its own past best response behavior by using supervised classification on the data in M SL . NFSP also makes use of two technical innovations in order to ensure the stability of the resulting algorithm as well as to enable simultaneous self-play learning. Through experimental results, the authors show that the NFSP can converge to approximate Nash equilibria in a small poker game.</p><p>Summary: In this section, we have presented the basics of reinforcement learning, deep learning, and DQL. Furthermore, we have discussed various advanced DQL techniques and their extensions. Different DQL techniques can be used to solve different problems in different network scenarios. In the next sections, we review DQL related works for various problems in communications and networking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NETWORK ACCESS AND RATE CONTROL</head><p>Modern networks such as IoT become more decentralized and ad-hoc in nature. In such networks, entities such as sensors and mobile users need to make independent decisions, e.g., channel and base station selections, to achieve their own goals, e.g., throughput maximization. However, this is challenging due to the dynamic and the uncertainty of network status. Learning algorithms such as DQL allow to learn and build knowledge about the networks that are used to enable the network entities to make their optimal decisions. In this section, we review the applications of DQL for the following issues:</p><p>• Dynamic spectrum access: Dynamic spectrum access allows users to locally select channels to maximize their throughput. However, the users may not have full observations of the system, e.g., channel states. Thus, DQL can be used as an effective tool for dynamic spectrum access. • Joint user association and spectrum access: User association is implemented to determine which user to be assigned to which Base Station (BS). The joint user association and spectrum access problems are studied in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b42">[43]</ref>. However, the problems are typically combinatorial and non-convex which require nearly complete and accurate network information to obtain the optimal strategy. DQL is able to provide distributed solutions which can be effectively used for the problems. • Adaptive rate control: This refers to bitrate/data rate control in dynamic and unpredictable environments such as Dynamic Adaptive Streaming over HTTP (DASH). Such a system allows clients or users to independently choose video segments with different bitrates to download. The client's objective is to maximize its Quality of Experience (QoE). DQL can be adopted to effectively solve the problem instead of dynamic programming which has high complexity and demands complete information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sink</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Access</head><p>This section discusses how to use DQL to solve the spectrum access and user association in networks.</p><p>1) Dynamic Spectrum Access: The authors in <ref type="bibr" target="#b43">[44]</ref> propose a dynamic channel access scheme of a sensor based on the DQL for IoT. The model is shown in Fig. <ref type="figure" target="#fig_6">8</ref>. At each time slot, the sensor selects one of M channels for transmitting its packet. The channel state is either in low interference, i.e., successful transmission, or in high interference, i.e., transmission failure. Since the sensor only knows the channel state after selecting the channel, the sensor's optimization decision problem can be formulated as a POMDP. In particular, the action of sensor is to select one of M channels. The sensor receives a positive reward "+1" if the selected channel is in low interference, and a negative reward "-1" otherwise. The objective is to find an optimal policy which maximizes the sensor's expected accumulated discounted reward over time slots. In fact, the objective can be obtained by the myopic policy <ref type="bibr" target="#b44">[45]</ref>. However, the myopic policy requires the prior knowledge of the system transition matrix which is hard to obtain. DQL allows the sensor to find the optimal policy from its experiences, and thus it can be adopted to solve the sensor's problem. In particular, the DQL uses a DQN<ref type="foot" target="#foot_0">1</ref> with experience replay <ref type="bibr" target="#b45">[46]</ref>. The input of the DQN is a state of the sensor which is the combination of actions and observations, i.e., the rewards, in the past time slots. The output includes Q-values corresponding to the actions. To balance the exploration of the current best Q-value with the exploration of the better one, the -greedy policy is adopted for the action selection mechanism. The simulation results based on the real data from <ref type="bibr" target="#b46">[47]</ref> show that the proposed scheme achieves an average reward of 4.4 that is close to the myopic policy <ref type="bibr" target="#b44">[45]</ref> with an average reward of 4.5. Note that the myopic policy requires the knowledge of the system transition matrix.</p><p>[44] can be considered to be a pioneer work using the DQL for the channel access. However, the DQL keeps following the learned policy over time slots and stops learning a suitable policy. Actual IoT environments are dynamic, and the DQN in the DQL needs to be re-trained. An adaptive DQL scheme is proposed in <ref type="bibr" target="#b47">[48]</ref> which evaluates the accumulated reward of the current policy for every period. When the reward is reduced by a given threshold, the DQN is re-trained to find a new good policy. The simulation results <ref type="bibr" target="#b47">[48]</ref> show that when the states of the channels change, the adaptive DQL scheme can detect the change and start re-learning to obtain the high reward.</p><p>The models in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b47">[48]</ref> are constrained to only one sensor. Consider a multi-sensor scenario, the authors in <ref type="bibr" target="#b48">[49]</ref> address the joint channel selection and packet forwarding using the DQL. The model is shown in Fig. <ref type="figure" target="#fig_7">9</ref> in which one sensor as a relay forwards packets received from its neighboring sensors to the sink. The sensor is equipped with a buffer to store the received packets. At each time slot, the sensor selects a set of channels for the packet forwarding so as to maximize its utility, i.e., the ratio of the number of transmitted packets to the transmit power. Similar to <ref type="bibr" target="#b43">[44]</ref>, the sensor's problem can be formulated as an MDP. The action is to select a set of channels, the number of packets transmitted on the channels, and a modulation mode. To avoid packet loss, the state is defined as the combination of the buffer state and channel state. The MDP is then solved by the DQL in which the input is the state and the output is the action selection. The DQL uses the stacked autoencoder to reduce the massive calculation and storage in the Q-learning phase. The sensor's utility function is proved to be bounded which can guarantee the convergence of the algorithm. The analysis shows that the computational complexity of the proposed algorithm is O(KM (J + 1)) that is lower than that of the strategy iteration algorithm <ref type="bibr" target="#b49">[50]</ref> with the computational complexity of O(KM (J + 1))(L + 1) K C M ), where K is the number of buffers, L is the buffer length, M and C respectively are the numbers of channels and channel states, and J is the number of possible transmission modes. The simulation results show that the proposed scheme significantly improves the system utility compared with the random action selection scheme. In particular, the average system utility of the proposed scheme is 0.63, while that obtained by the random policy is 0.37. However, as the packet arrival rate increases, the system utility of the proposed scheme decreases since the sensor needs to consume more power to transmit all packets.</p><p>Consuming more power leads to poor sensor's performance due to its energy constraint, i.e., a shorter IoT system lifetime. The channel access problem in the energy harvesting-enabled IoT system is investigated in <ref type="bibr" target="#b50">[51]</ref>. The model consists of one BS and energy harvesting-based sensors (see Fig. <ref type="figure" target="#fig_8">10</ref>). The BS as a controller allocates channels to the sensors. However, the uncertainty of ambient energy availability at the sensors may make the channel allocation inefficient. For example, the channel allocated to the sensor with low available energy may not be fully utilized since the sensor cannot communicate later.</p><p>Therefore, the BS's problem is to predict the sensors' battery states and select sensors for the channel access so as to maximize the total rate. To solve the BS's problem, the optimal approaches such as the uplink resource allocation scheme <ref type="bibr" target="#b51">[52]</ref> can be adopted. However, the scheme requires the BS to have perfect non-causal knowledge of all the random processes. The perfect knowledge may not be available since the sensors are distributed randomly over a geographical area. Thus, the DQL is used to solve the problem of the BS, i.e., the agent. The DQL uses a DQN consisting of two LSTM-based neural network layers. The first layer generates the predicted battery states of sensors, and the second layer uses the predicted states along with Channel State Information (CSI) to determine the channel access policy. The state space consists of (i) channel access scheduling history, (ii) the history of predicted battery information, (iii) the history of the true battery information, and (iv) the current CSI of the sensors. The action space contains all sets of sensors to be selected for the channel access, and the reward is the difference between the total rate and the prediction error. As shown in the simulation results, the proposed scheme is close to the optimal approach <ref type="bibr" target="#b51">[52]</ref> and outperforms the myopic policy <ref type="bibr" target="#b44">[45]</ref> in terms of total rate. In particular, the total rates obtained by the proposed scheme, the myopic policy, and the optimal scheme are 6.8, 6.5, and 7.0 kbps, respectively. Moreover, the battery prediction error obtained from the proposed scheme is close to zero.</p><p>The above schemes, e.g., <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b50">[51]</ref>, focus on the rate maximization. In IoT systems such as Vehicle-to-Vehicle (V2V) communications, latency also needs to be considered due to the mobility of V2V transmitters/receivers and vital applications in the traffic safety. One of the problems of each V2V transmitter is to select a channel and a transmit power level to maximize its capacity under a latency constraint. Given the decentralized network, a DQN is adopted to make optimal decisions as proposed in <ref type="bibr" target="#b52">[53]</ref>. The model consists of V2V transmitters, i.e., agents, which share a set of channels. The actions of each V2V transmitter include choosing channels and transmit power levels. The reward is a function of the V2V transmitter's capacity and latency. The state observed by the V2V transmitter consists of (i) the instantaneous CSI of the corresponding V2V link, (ii) the interference to the V2V link in the previous time slot, (iii) the channels selected by the V2V transmitter' neighbors in the previous time slot, and (iv) the remaining time to meet the latency constraint. The state is also an input of the DQN. The output includes Q-values corresponding to the actions. As shown in the simulation results, by dynamically adjusting the power and channel selection when V2V links are likely to violate the latency constraint, the proposed scheme has more V2V transmitters meeting the latency constraint compared with the random channel allocation.</p><p>To reduce spectrum cost, the above IoT systems often use unlicensed channels. However, this may cause the interference to existing networks, e.g., WLANs. The authors in <ref type="bibr" target="#b53">[54]</ref> propose to use the DQN to jointly address the dynamic channel access and interference management. The model consists of Small Base Stations (SBSs) which share unlicensed channels in an LTE network (see Fig. <ref type="figure" target="#fig_9">11</ref>). At each time slot, the SBS selects one of channels for transmitting its packet. However, there may be WLAN traffics on the selected channel, and thus the SBS accesses the selected channel with a probability. The actions of the SBS include pairs of channel selection and channel access probability. The problem of the SBS is to determine an action vector so as to maximize its total throughput, i.e., its utility, over all channels and time slots. The resource allocation problem can be formulated as a noncooperative game, and the DQN using LSTM can be adopted to solve the game. The input of the DQN is the history traffic of the SBSs and the WLAN on the channels. The output includes predicted action vectors of the SBSs. The utility function of each SBS is proved to be convex, and thus the DQN-based algorithm converges to a Nash equilibrium of the game. The analysis shows that the computational complexity per time step of the proposed scheme is O(n 2 c + n c n i + n c n 0 + n c ), where n c , n i , and n o are the numbers of memory cells, input units, and output units, respectively. The simulation results based on real traffic data from <ref type="bibr" target="#b54">[55]</ref> show that the proposed scheme can improve the average throughput up to 28% compared with the standard Q-learning. Moreover, deploying more SBSs in the LTE network does not allow more airtime fraction for the network. This implies that the proposed scheme can avoid causing performance degradation to the WLAN. However, the proposed scheme requires synchronization between the SBSs and the WLAN which is challenging in real networks.</p><p>In the same cellular network context, the authors in <ref type="bibr" target="#b26">[27]</ref> address the dynamic spectrum access problem for multiple users sharing K channels. At a time slot, the user selects a channel with a certain attempt probability or chooses not to transmit at all. The state is the history of the user's actions and its local observations, and the user's strategy is mapping from the history to an attempt probability. The problem of the user is to find a vector of the strategies, i.e., the policy, over time slots to maximize its expected accumulated discounted data rate of the user.</p><p>The above problem is solved by training a DQN. The input of the DQN includes past actions and the corresponding observations. The output includes estimated Q-values of the actions. To avoid the overestimation in the Q-learning, the DDQN <ref type="bibr" target="#b24">[25]</ref> is used. Moreover, the dueling DQN <ref type="bibr" target="#b55">[56]</ref> is employed to improve the estimated Q-value. The DQN is then offline trained at a base station. Similar to <ref type="bibr" target="#b53">[54]</ref>, the multichannel random access is modeled as a non-cooperative game. As proved in <ref type="bibr" target="#b26">[27]</ref>, the game has a subgame perfect equilibrium. Note that some users can keep increasing their attempt probability to increase their rates. This makes the equilibrium point inefficient, and thus the strategy space of the users is restricted to avoid the situation. The simulation results show that the proposed scheme can achieve twice the channel throughput compared with the slotted-Aloha <ref type="bibr" target="#b56">[57]</ref>. The reason is that in the proposed scheme, each user only learns from its local observation without an online coordination or carrier sensing. However, the proposed scheme requires the central unit which may raise the message exchanges as the training is frequently updated.</p><p>In the aforementioned models, the number of users is fixed in all time slots, and the arrival of new users is not considered. The authors in <ref type="bibr" target="#b57">[58]</ref> address the channel allocation to new arrival users in a multibeam satellite system. The multibeam satellite system generates a geographical footprint subdivided into multiple beams which provide services to ground User Terminals (UTs). The system has a set of channels. If there exist available channels, the system allocates a channel to the new arrived UT, i.e., the new service is satisfied. Otherwise, the service is blocked. The system's problem is to find a channel allocation decision to minimize the total service blocking probability of the new UT over time slots without causing the interference to the current UTs.</p><p>The system's problem can be viewed as a temporal correlated sequential decision-making optimization problem which is effectively solved by the DQN. Here, the satellite system is the agent. The action is an index indicating which channel is allocated to the new arrived UT. The reward is positive when the new service is satisfied and is negative when the service is blocked. The state includes the set of current UTs, the current channel allocation matrix, and the new arrived UT. Note that the state has the spatial correlation feature due to the co-channel interference, and thus it can be represented in an image-like fashion, i.e., an image tensor. Therefore, the DQN adopts the CNN to extract useful features of the state. The simulation results show that the proposed DQN algorithm converges after a certain number of training steps. Also, by allocating available channels to the new arrived UTs, the proposed scheme can improve the system traffic up to 24.4% compared with the fixed channel allocation scheme. However, as the number of current UTs increases, the number of available channels is low or even zero. Therefore, the dynamic channel allocation decisions of the proposed scheme become meaningless, and the performance difference between the two schemes becomes insignificant. For the future work, a joint channel and power allocation algorithm based on the DQL can be investigated.</p><p>2) Joint User Association and Spectrum Access: Joint user association and spectrum access problems are typically nonconvex. To solve the problems, traditional approaches such as linear programming <ref type="bibr" target="#b58">[59]</ref> are developed to obtain the optimal solution. However, the approaches require nearly complete and accurate network information that is usually not available. Learning techniques such as Q-learning can be used. However, it is challenging to obtain an optimal solution due to the large state and action spaces of the joint optimization problems. By combining DNN with Q-learning, DQL is effectively used to solve the joint optimization problems as proposed in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b59">[60]</ref>.</p><p>The authors in <ref type="bibr" target="#b27">[28]</ref> consider a HetNet which consists of multiple users and BSs including macro base stations and femto base stations (see Fig. <ref type="figure" target="#fig_10">12</ref>). The BSs share a set of orthogonal channels, and the users are randomly located in the network. The problem of each user is to select one BS and a channel to maximize its data rate while guaranteeing that the Signal-to-Interference-plus-Noise Ratio (SINR) of the user is higher than a minimum Qualtiy of Service (QoS) requirement. The DQL is adopted to solve the problem in which each user is an agent, and its state is a vector including QoS states of all users, i.e., the global state. Here, the QoS state of the user refers to whether its SINR exceeds the minimum QoS requirement or not. At each time slot, the user takes an action. If the QoS is satisfied, the user receives utility as its immediate reward. Otherwise, it receives a negative reward, i.e., an action selection cost. Note that the cumulative reward of one user depends on actions of other users, then the user's problem can be defined as an MDP. Similar to <ref type="bibr" target="#b26">[27]</ref>, the DDQN and the dueling DQN are used to learn the optimal policy, i.e., the joint BS and channel selections, for the user to maximize its cumulative reward. The simulation results from <ref type="bibr" target="#b27">[28]</ref> show that the proposed scheme outperforms the Q-learning implemented in <ref type="bibr" target="#b18">[19]</ref> in terms of convergence speed and system capacity. The simulation comparisons imply that the DQN can be effectively used to solve the complex problems, e.g., joint optimization problems, in the large-scale systems such as HetNets and IoT.</p><p>The scheme proposed in <ref type="bibr" target="#b27">[28]</ref> is considered to be the first work using the DQL for the joint user association and spectrum access problem. Inspired by this work, the authors in <ref type="bibr" target="#b59">[60]</ref> propose to use the DQL for a joint user association, spectrum access, and content caching problem. The network model is an LTE network which consists of UAVs serving ground users. The UAVs are equipped with storage units and can act as cached-enabled LTE-BSs. The UAVs are able to access both licensed and unlicensed bands in the network. The UAVs are controlled by a cloud-based server, and the transmissions from the cloud to the UAVs are implemented by using the licensed cellular band. The problem of each UAV is to determine (i) its optimal user association, (ii) the bandwidth allocation indicators on the licensed band, (iii) the time slot indicators on the unlicensed band, and (iv) a set of popular contents that the users can request to maximize the number of users with stable queue, i.e., users satisfied with content transmission delay. The UAV's problem is combinatorial and non-convex, and the DQL can be used to solve it. The UAVs do not know the users' content requests, and thus the Liquid State Machine approach (LSM) <ref type="bibr" target="#b60">[61]</ref> is adopted to predict the content request distribution of the users and to perform resource allocation. In particular, predicting the content request distribution is implemented at the cloud based on an LSM-based prediction algorithm. Then, given the request distributions, each UAV as an agent uses an LSM-based learning algorithm to find its optimal users association. Specifically, the input of the LSMbased learning algorithm consists of actions, i.e., UAV-user association schemes, that other UAVs take, and the output includes the expected numbers of users with stable queues corresponding to actions that the UAV can take. After the user association is done, the optimal content caching is determined based on the results of <ref type="bibr" target="#b61">[62,</ref><ref type="bibr">Theorem 2]</ref>, and the optimal spectrum allocation is done by using linear programming. Based on the Gordon's Theorem <ref type="bibr" target="#b62">[63]</ref>, the proposed DQL is proved to converge with probability one. The simulation results using the content request data from <ref type="bibr" target="#b63">[64]</ref> show that the proposed DQL can converge within 400 iterations. Compared with the Qlearning, the proposed DQN improves the convergence time up to 33%. Moreover, the proposed DQL significantly improves the number of users with stable queues up to 50% compared with the Q-learning without cache. In fact, energy efficiency is also important for the UAVs, and thus applying the DQL for a joint user association, spectrum access, and power allocation problem needs to be investigated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Rate Control</head><p>Dynamic Adaptive Streaming over HTTP (DASH) becomes the dominant standard for video streaming <ref type="bibr" target="#b64">[65]</ref>. DASH is able to leverage existing content delivery network infrastructure and is compatible with a multitude of client-side applications. A general DASH system is shown in Fig. <ref type="figure" target="#fig_11">13</ref> in which the videos are stored in servers as multiple segments, i.e., chunks. Each segment is encoded at different compression levels to generate representations with different bitrates, i.e., different video visual quality. At each time slot, the client chooses a representation, i.e., a segment with a certain bitrate, to download. The client's problem is to find an optimal policy which maximizes its QoE such as maximizing average bitrate and minimizing rebuffering, i.e., the time which the video playout freezes.</p><p>As presented in <ref type="bibr" target="#b65">[66]</ref>, the above problem can be modeled as an MDP in which the agent is the client and the action is choosing a representation to download. To maximize the QoE, the reward is defined as a function of (i) visual quality of the video, (ii) video quality stability, (iii) rebuffering event, and (iv) buffer state. Given the reward formulation, the state of the client should include (i) the video quality of the last downloaded segment, (ii) the current buffer state, (iii) the rebuffering time, and (iv) the channel capacities experienced during downloading of segments in the past time slots. The MDP can be solved by using dynamic programming, but the computational complexity rapidly becomes unmanageable as the size of the problem increases. Thus, the authors in <ref type="bibr" target="#b65">[66]</ref> adopt the DQL to solve the problem. Similar to <ref type="bibr" target="#b50">[51]</ref>, the LSTM networks are used in which the input is the state of the client, and the output includes Q-values corresponding to the client's possible actions. To improve the performance of the standard LSTM, peephole connections are added into the LSTM networks. The simulation results based on the dataset from <ref type="bibr" target="#b66">[67]</ref> show that the proposed DQL algorithm can converge much faster than Q-learning. In particular, the DQL algorithm converges in around 3 episodes, while the Q-learning converges in around 180 episodes. The fast convergence suggests that DQL can provide efficient solution to problems in realtime applications. Moreover, the proposed DQL improves the video quality and reduces the rebuffering since it is able to dynamically manage the buffer by considering the buffer state and channel capacity.</p><p>The network model and the optimization problem in <ref type="bibr" target="#b65">[66]</ref> are also found in <ref type="bibr" target="#b67">[68]</ref>. However, different from <ref type="bibr" target="#b65">[66]</ref>, the authors in <ref type="bibr" target="#b67">[68]</ref> adopt the Asynchronous Advantage Actor-Critic (A3C) method <ref type="bibr" target="#b30">[31]</ref> for the DQL to further enhance and speed up the training. As presented in Section II-F1, A3C includes two neural networks, namely, actor network and critic network. The actor network is to choose bitrates for the client, and the critic network helps train the actor network. For the actor network, the input is the client's state, and the output is a policy, i.e., a probability distribution over possible actions given states that the client can take. Here, the action is choosing the next representation, i.e., the next segment with a certain bitrate, to download. For the critic network, the input is the client's state, and the output is the expected total reward when following the policy obtained from the actor network. The simulation results based on the mobile dataset from <ref type="bibr" target="#b68">[69]</ref> show that the proposed DQL can improve the average QoE up to 25% compared with the bitrate control scheme <ref type="bibr" target="#b69">[70]</ref>. Also, by having sufficient buffer to handle the network's throughput fluctuations, the proposed DQL reduces the rebuffering around 32.8% compared with the baseline scheme.</p><p>In practice, the DQL algorithm proposed in <ref type="bibr" target="#b67">[68]</ref> can be easily deployed in a multi-client network since A3C is able to support parallel training for multiple agents. Accordingly, each client, i.e., an agent, is configured to observe its reward. Then, the client sends a tuple including its state, action, and reward to a server. The server uses the actor-critic algorithm to update its actor network model. The server then pushes the newest model to the agent. This update process can happen asynchronously among all agents which improves quality and speeds up the training. Although the parallel training scheme may incur a Round-Trip Time (RTT) between the clients and the server, the simulation results in <ref type="bibr" target="#b67">[68]</ref> show that the RTT between the clients and the server reduces the average QoE by only 3.5%. The performance degradation is small, and thus the proposed DQL can be implemented in real network systems.</p><p>In <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b67">[68]</ref>, the input of the DQL, i.e., the client's state, includes the video quality of the last downloaded video segment. The video segment is raw which may cause "state explosion" to the state space <ref type="bibr" target="#b70">[71]</ref>. To reduce the state space and to improve the QoE, the authors in <ref type="bibr" target="#b70">[71]</ref> propose to use a video quality prediction network. The prediction network extracts useful features from the raw video segments using CNN and RNN. Then, the output of the prediction network, i.e., the predicted video quality, is used as one of the inputs of the DQL which is proposed in <ref type="bibr" target="#b67">[68]</ref>. Simulation results based on the broadband dataset from <ref type="bibr">[72]</ref> show that the proposed DQL can improve the average QoE up to 25% compared with the Google Hangout, i.e., a communication platform developed by Google. Moreover, the proposed DQL can reduce the average latency of video transmission around 45% due to the small state space. This means that in the scenarios that the state space is large, the CNN should be used to improve the user QoE and the convergence time.</p><p>Apart from the DASH systems, the DQL can be effectively used for the rate control in High Volume Flexible Time (HVFT) applications. HVFT applications use cellular networks to deliver IoT traffic as shown in Fig. <ref type="figure" target="#fig_12">14</ref>. The HVFT applications have a large volume of traffic, and the traffic scheduling, e.g., data rate control, in the HVFT applications is necessary. One common approach is to assign static priority classes per traffic type, and then traffic scheduling is based on its priority class. However, such an approach does not evolve to accommodate new traffic classes. Thus, learning methods such as DQL should be used to provide adaptive rate control mechanisms as proposed in <ref type="bibr" target="#b71">[73]</ref>. The network model is a single cell including one BS as a central controller and multiple mobile users. The problem at the BS is to find a proper policy, i.e., data rate for the users, to maximize the amount of transmitted HVFT traffic while minimizing performance degradation to existing data traffics. It is shown in <ref type="bibr" target="#b71">[73]</ref> that the problem can be formulated as an MDP. The agent is the BS, and the state includes the current network state and the useful features extracted from network states in the past time slots. The network state at a time slot includes (i) the congestion metric, i.e., the cell's traffic load, at the time slot, (ii) the total number of network connections, and (iii) the cell efficiency, i.e., the cell quality. The action that the BS takes is a combination of the traffic rate for the users. To achieve the BS' objective, the reward is defined as a function of (i) the sum of HVFT traffic, (ii) traffic loss to existing applications due to the presence of the HVFT traffic, and (iii) the amount of bytes served below desired minimum throughput. The DQL using the actor and critic networks with LSTM is then adopted. By using the real network data collected in Melbourne, the simulation results show that the proposed DQL scheme increases the HVFT traffic up to 2 times compared with the heuristic control scheme. The proposed DQL is thus expected to be applied in modern networks in large-scale cities with a large population growth.</p><p>In the aforementioned approaches, the maximum number of objectives is constrained, e.g., to 3 in <ref type="bibr" target="#b72">[74]</ref>. The authors in <ref type="bibr" target="#b73">[75]</ref> show that the DQL can be used for the rate control to achieve multiple objectives in complex communication systems. The network model is a future space communication system which is expected to operate in unpredictable environments, e.g., orbital dynamics, atmospheric and space weather, and dynamic channels. In the system, the transmitter needs to be configured with several transmit parameters, e.g., symbol rate and encoding rate, to achieve multiple conflict objectives, e.g., low Bit Error Rate (BER), throughput improvement, power and spectral efficiency. The adaptive coding and modulation schemes, i.e., <ref type="bibr" target="#b74">[76]</ref>, can be used. However, the methods allow to achieve only limited number objectives. Learning algorithms such as the DQL can be thus used. The agent is the transmitter in the system. The action is a combination of (i) symbol rate, (ii) energy per symbol, (iii) modulation mode, (iv) number of bits per symbol, and (v) encoding rate. The objective is to maximize the system performance. Thus, the reward is defined as a fitness function of performance parameters including (i) BER estimated at the receiver, (ii) throughput, (iii) spectral efficiency, (iv) power consumption, and (v) transmit power efficiency. The state is the system performance measured by the transmitter, and thus the state is the reward. To achieve multiple objectives, the DQL is implemented by using a set of multiple neural networks in parallel. The input of the DQL is the current state and the channel conditions, and the output is the predicted action. The neural networks are trained by using the Levenberg-Marquardt backpropagation algorithm <ref type="bibr" target="#b75">[77]</ref>. The simulation results show that the proposed DQL can achieve the fitness score, i.e., the weighted sum of different objectives, close to the ideal, i.e., the exhaustive search approach. This implies that the DQL is able to select near-optimal actions and learn the relationship between rewards and actions given dynamic channel conditions.</p><p>Summary: This section reviews applications of DQL for the dynamic network access and adaptive rate control. The reviewed approaches are summarized along with the references in Table <ref type="table" target="#tab_5">IV</ref>. We observe that the problems are mostly modeled as an MDP. Moreover, DQL approaches for the IoT and DASH systems receive more attentions than other networks. Future networks, e.g., 5G networks, involve multiple network entities with multiple conflicting objectives, e.g., provider's revenue versus users' utility maximization. This poses a number of challenges to the traditional resource management mechanisms that deserve in-depth investigation. In the next section, we review the adoption of DQL for the emerging services, i.e., offloading and caching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CACHING AND OFFLOADING</head><p>As one of the key features of information-centric networking, in-network caching can efficiently reduce duplicated content transmissions. The studies on wireless caching has shown that access delays, energy consumption, and the total amount of traffic can be reduced significantly by caching contents in wireless devices. Big data analytics <ref type="bibr" target="#b76">[78]</ref> also demonstrate that with limited cache size, proactive caching at network edge nodes can achieve 100% user satisfaction while offloading 98% of the backhaul traffic. Joint content caching and offloading can address the gap between the mobile users' large data demands and the limited capacities in data storage and processing. This motivates the study on Mobile Edge Computing (MEC). By deploying both computational resources and caching capabilities close to end users, MEC significantly improves energy efficiency and QoS for applications that require intensive computations and low latency. A unified study on caching, offloading, networking, and transmission control in MEC scenarios involves very complicated system analysis because of strong couplings among mobile users with heterogeneities in application demand, QoS provisioning, mobility pattern, radio access interface, and wireless resources. A learning-based and model-free approach becomes a promising candidate to manage huge state space and optimization variables, especially by using DNNs. In this section, we review the modeling and optimization of caching and offloading policies in wireless networks by leveraging the DRL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wireless Proactive Caching</head><p>Wireless proactive caching has attracted great attentions from both academia and industry. Statistically, a few popular contents are usually requested by many users during a short time span, which accounts for most of the traffic load. Therefore, proactively caching popular contents can avoid the heavy traffic burden of the backhaul links. In particular, this technique aims at pre-caching the contents from the remote content servers at the edge devices or BSs that are close to the end users. If the requested contents are already cached locally, the BS can directly serve the end users with small delay. Otherwise, the BS requests these contents from the original content server and updates the local cache based on the caching policy, which is one of the main design problem for wireless proactive caching.</p><p>1) QoS-Aware Caching: Content popularity is the key factor used to solve the content caching problem. With a large number of contents and their time-varying popularities, DQL is an attractive strategy to tackle this problem with high-dimensional state and action spaces. The authors in <ref type="bibr" target="#b76">[78]</ref> present a DQL scheme to improve the caching performance. The system model consists of a single BS with a fixed cache size. For each request, the BS as an agent makes a decision on whether or not to store the currently requested content in the cache. If the new content is kept, the BS determines which local content will be replaced. The state is the feature space of the cached contents and the currently requested content. The feature space consists of the total number of requests for each content in a specific short-, medium-, and long-term. There are two types of actions: (i) to find a pair of contents and exchange the cache states of the two contents and (ii) to keep the cache states of the contents unchanged. The aim of the BS is to maximize the long-term cache hit rate, i.e., reward.</p><p>The DQL scheme in <ref type="bibr" target="#b76">[78]</ref> trains the policy by using the DDPG method <ref type="bibr" target="#b77">[79]</ref> and employs Wolpertinger architecture <ref type="bibr" target="#b78">[80]</ref> to reduce the size of the action space and avoid missing an optimal policy. The Wolpertinger architecture consists of three main parts: an actor network, K-Nearest Neighbors (K-NN), and a critic network. The actor network is to avoid a large action space. The critic network is to correct the decision made by the actor network. The DDPG method is applied to update both critic and actor networks. K-NN can help to explore a set of actions to avoid poor decisions. The actor and critic networks are then implemented by using FNNs. The simulation results show that the proposed DQL scheme outperforms the first-in first-out scheme in terms of long-term cache hit rate. In particular, the cache hit rate obtained by the DQL scheme is 0.5, while that obtained by the first-in first-out scheme is 0.4. The performance comparisons demonstrate that the proposed DQL scheme can achieve competitive cache hit rates while effectively reducing the run-time. This makes the proposed framework efficient and suitable for handling largescale data.</p><p>Maximizing the long-term cache hit rate in <ref type="bibr" target="#b76">[78]</ref> implies that the cache stores the most popular contents. In a dynamic environment, contents stored in a cache have to be replaced according to the users' dynamic requests. An optimization of the placement or replacement of cached contents is studied in <ref type="bibr" target="#b79">[81]</ref> by a deep learning method. The optimization algorithm is trained by a DNN in advance and then used for realtime caching or scheduling with minimum delay. The authors in <ref type="bibr" target="#b80">[82]</ref> propose an optimal caching policy to learn the cache expiration times, i.e., Time-To-Live (TTL), for dynamically changing requests in content delivery networks. The system includes a cloud database server and multiple mobile devices that can issue queries and update entries in a single database. The query results can be cached for a specified time interval at server-controlled caches. All cached queries will become invalid if one of the cached records has been updated. A large TTL will strain cache capacities while a small TTL increases latencies significantly if the database server is physically remote.</p><p>Unlike the DDPG approach used in <ref type="bibr" target="#b76">[78]</ref>, the authors in <ref type="bibr" target="#b80">[82]</ref> propose to utilize Normalized Advantage Functions (NAFs) for continuous DQL scheme to learn optimal cache expiration duration. The key problem in continuous DQL is to select an action maximizing the Q-function, while avoiding performing a costly numerical optimization at each step. The use of NAFs obviates a second actor network that needs to be trained separately. Instead, a single neural network is used to output both a value function and an advantage term. The DQL agent at the cloud database uses an encoding of a query itself and the query miss rates as the system states, which allows for an easier generalization. The system reward is linearly proportional to the current load, i.e., the number of cached queries divided by the total capacity. This reward function can encourage longer TTLs when fewer queries are cached, and shorter TTLs when the load is close to the system capacity. Considering incomplete measurements for rewards and next-states at runtime, the authors introduce the Delayed Experience Injection (DEI) approach that allows the DQL agent to keep track of incomplete transitions when measurements are not immediately available. The authors evaluate the learning algorithm by Yahoo! cloud serving benchmark with customized web workloads <ref type="bibr" target="#b81">[83]</ref>. The simulation results verify that the learning approach based on NAFs and DEI outperforms a statistical estimator.</p><p>2) Joint Caching and Transmission Control: The caching policies determine where to store and retrieve the requested content efficiently, e.g., by learning the contents' popularities <ref type="bibr" target="#b76">[78]</ref> and cache expiration time <ref type="bibr" target="#b80">[82]</ref>. Another important aspect of caching design is the transmission control of the content delivery from caches to end users, especially for wireless systems with dynamic channel conditions. To avoid mutual interference in multi-user wireless networks, the transmission control decides which cached contents can be transmitted concurrently as well as the most appropriate control parameters, e.g., transmit power, precoding, data rate, and channel allocation. Hence, the joint design of caching and transmission control is required to enable efficient content delivery in multi-user wireless networks.</p><p>Recently, some approaches, e.g., <ref type="bibr" target="#b82">[84]</ref>, are proposed for the joint caching and interference alignment in wireless systems. However, most of the approaches assume that the channel state information is invariant that may not be the case in dynamic wireless systems. The authors in <ref type="bibr" target="#b83">[85]</ref>- <ref type="bibr" target="#b85">[87]</ref> propose to use the DQL framework to address the joint caching and interference alignment to tackle mutual interference in multi-user wireless networks. The authors consider an MIMO system with limited backhaul capacity and the caches at the transmitter. The precoding design for interference alignment requires the global CSI at each transmitter. A central scheduler is responsible for collecting CSI and cache status from each user via the backhaul, scheduling the users' transmission, and optimizing the resource allocation. By enabling content caching at individual transmitters, we can decrease the demand for data transfer and thus save more backhaul capacity for realtime CSI update and sharing. Using the DQL-based approach at the central scheduler can reduce the explicit demand for CSI and the computational complexity in matrix optimization, especially with time-varying channel conditions. The DQL agent implements the DNN to approximate the Q-function with experience replay in training. To make the learning process more stable, the target Q-network parameter is updated by the Q-network for every a few time instants. The collected information is assembled into a system state and sent to the DQL agent, which feeds back an optimal action for the current time instant. The action indicates which users to be active, and the resource allocation among active users. The system reward represents the total throughput of multiple users. An extended work of <ref type="bibr" target="#b83">[85]</ref> and <ref type="bibr" target="#b84">[86]</ref> with a similar DQL framework is presented in <ref type="bibr" target="#b85">[87]</ref>, in which a CNN-based DQN is adopted and evaluated in a more practical conditions with imperfect or delayed CSI. Simulation results show that the performance of the MIMO system is significantly improved compared with the baseline scheme <ref type="bibr" target="#b82">[84]</ref> in terms of the total throughput and energy efficiency. In particular, at SN R = 15 dB, the total rate obtained by the proposed DQN scheme is 240 Mbps, while that obtained by the baseline scheme is 200 Mbps.</p><p>Interference management is an important requirement of wireless systems. The application-related QoS or user experience is also an essential metric. Different from <ref type="bibr" target="#b83">[85]</ref>- <ref type="bibr" target="#b85">[87]</ref>, the authors in <ref type="bibr" target="#b86">[88]</ref> propose a DQL approach to maximize Quality of Experience (QoE) of IoT devices by jointly optimizing the cache allocation and transmission rate in content-centric wireless networks. The system state is specified by the nodes' caching conditions, e.g., the service information and cached contents, as well as the transmission rates of the cached contents. The aim of the DQL agent is to minimize continuously the network cost or maximize the QoE. The proposed DQL framework is further enhanced with the use of PER and DDQN. PER replays important transitions more frequently so that DQN can learn from samples more efficiently. The use of DDQN can stabilize the learning by providing two value functions in separated neural networks. This avoids an overestimation of the DQN with the increasing number of actions. These two neural networks are not completely decoupled as the target network is a periodic copy of estimation network. A discrete simulator ccnSim <ref type="bibr" target="#b87">[89]</ref> is used to model the caching behavior in various graph structures. The output data trace of the simulator is then imported to Matlab and used to evaluate the learning algorithm. As shown in the simulation results, the proposed DQL framework can achieve a QoE value of 4 that is double to that of the standard penetration test scheme. Moreover, the computational complexity of the DDQN is O(lg n) that is lower than that of the standard penetration test scheme with the computational complexity of O(lsn 3 ), where n is the number of content-centric computing nodes, s is the number of service nodes, and l is the number of transmission rate values.</p><p>The QoE can be used to characterize the users' perception of Virtual Reality (VR) services. The authors in <ref type="bibr" target="#b88">[90]</ref> address the joint content caching and transmission strategy in a wireless VR network, where UAVs capture videos on live games and transmit them to small-cell BSs servicing the VR users. Millimeter wave (mmWave) downlink backhaul links are used for VR content transmission from the UAVs to BSs. The BSs can also cache the popular contents that may be requested frequently by end users. The joint content caching and transmission problem is formulated as an optimization to maximize the users' reliability, i.e., the probability that the content transmission delay satisfies the instantaneous delay target. The maximization involves the control of transmission format, users' association, the set and format of cached contents. A DQL framework combining the Liquid State Machine (LSM) and Echo State Network (ESN) is proposed for each BS to find the optimal transmission and caching strategies. As a randomly generated spiking neural network <ref type="bibr" target="#b89">[91]</ref>, LSM can store information about the network environment over time and adjust the users' association policy, cached contents and formats according to the users' content requests. It has been used in <ref type="bibr" target="#b90">[92]</ref> to predict the users' content request distribution while having only limited information regarding the network and different users. Conventional LSM uses FNNs as the output function, which demands high complexity in training due to the computation of gradients for all of the neurons. Conversely, the proposed DQL framework uses an ESN as the output function, which uses historical information to find the relationship between the users' reliability, caching, and content transmission. It also has a lower complexity in training and a better memory for network information. Simulation results show that the proposed DQL framework can yield 25.4% gain in terms of users' reliability compared to the baseline Q-learning.</p><p>3) Joint Caching, Networking, and Computation: Caching and transmission control will become more involved in a Het-Net that integrates different communication technologies, e.g., cellular system, device-to-device network, vehicular network, and networked UAVs, to support various application demands. The network heterogeneity raises the problem of complicated system design that needs to address challenging issues such as mutual interference, differentiated QoS provisioning, and resource allocation, hopefully in a unified framework. Obviously this demands a joint optimization far beyond the extent of joint caching and transmission control.</p><p>Accordingly, the authors in <ref type="bibr" target="#b91">[93]</ref> propose a DQL framework for energy-efficient resource allocation in green wireless networks, jointly considering the couplings among networking, in-network caching and computation. The system consists of a Software-Defined Network (SDN) with multiple virtual networks and mobile users requesting for video on-demand files that require a certain amount of computational resource at either the content server or at local devices. In each virtual network, an authorized user issues a request to download files from a set of available SBSs in its neighborhood area. The wireless channels between each mobile user and the SBSs are characterized as Finite-State Markov Channels (FSMC). The states are the available cache capacity at the SBSs, the channel conditions between mobile users and SBSs, the computational capability of the content servers and mobile users. The DQL agent at each SBS decides an association between each mobile user and SBS, where to perform the computational task, and how to schedule the transmissions of SBSs to deliver the required data. The objective is to minimize the total energy consumption of the system from data caching, wireless transmission, and computation. Simulation results show that the total energy consumption in different testing scenarios is very high at the beginning of the learning process and gradually decreases a stable value when the learning converges. Moreover, the energy consumption of the unified DRL framework considering caching, networking, and computing is significantly lower than those of other DRL frameworks that only focus on part of the control variables.</p><p>The DQL scheme proposed in <ref type="bibr" target="#b91">[93]</ref> has been applied to improve the performance of Vehicular Ad doc NETworks (VANETs) in <ref type="bibr" target="#b92">[94]</ref>- <ref type="bibr" target="#b94">[96]</ref>. The network model includes multiple BSs, Road Side Units (RSUs), MEC servers, and content servers. All devices are controlled by a mobile virtual network operator. The vehicles request for video contents that can be cached at the BSs or retrieved from remote content servers. The authors in <ref type="bibr" target="#b92">[94]</ref> formulate the resource allocation problem as a joint optimization of caching, networking, and computing, e.g., compressing and encoding operations of the video contents. The system states include the CSI from each BS, the computational capability, and cache size of each MEC/content server. The network operator feeds the system state to the FNN-based DQN and gets the optimal policy that determines the resource allocation for each vehicle. To exploit spatial correlations in learning, the authors in <ref type="bibr" target="#b93">[95]</ref> enhance Q-learning by using CNNs in DQN. This makes it possible to extract high-level features from raw input data. Two schemes have been introduced in <ref type="bibr" target="#b94">[96]</ref> to improve stability and performance of the ordinary DQN method. Firstly, DDQN is designed to avoid over-estimation of Q-value in ordinary DQN. Hence, the action can be decoupled from the target Q-value generation. This makes the training process faster and more reliable. Secondly, the dueling DQN approach is also integrated in the design with the intuition that it is not always necessary to estimate the reward by taking some action. The state-action Qvalue in dueling DQN is decomposed into one value function representing the reward in the current state, and the advantage function that measures the relative importance of a certain action compared with other actions. Simulation results show that the proposed DQL scheme outperforms the existing static scheme in terms of total utility. In particular, the total utility obtained by the DQL scheme is 8000, while that obtained by the existing static scheme is 5000.</p><p>Considering the huge action space and high complexity with the vehicle's mobility and service delay deadline d , a multitime scale DQN framework is proposed in <ref type="bibr" target="#b95">[97]</ref> to minimize the system cost by the joint design of communication, caching and computing in VANET. The policy design accounts for limited storage capacities and computational resources at the vehicles and the RSUs. The small timescale DQN is for every time slot and aims to maximize the exact immediate reward. Additionally, the large timescale DQN is designed for every T d time slots within the service delay deadline, and used to estimate the reward considering the vehicle's mobility in a large timescale. Simulation results show that the proposed framework can reduce the cost up to 30% compared with the random resource allocation scheme.</p><p>The aforementioned DQL framework for VANETs, e.g., <ref type="bibr" target="#b92">[94]</ref>- <ref type="bibr" target="#b94">[96]</ref>, has also been generalized to smart city applications in <ref type="bibr" target="#b96">[98]</ref>, which necessitates dynamic orchestration of networking, caching, and computation to meet different servicing requirements. Through Network Function Virtualization (NFV) <ref type="bibr" target="#b97">[99]</ref>, the physical wireless network in smart cities can be divided logically into several virtual ones by the network operator, which is responsible for network slicing and resource scheduling, as well as allocation of caching and computing capacities. The use cases in smart cities are presented in <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>, which apply the generalized DQL framework to improve the security and efficiency for trustbased data exchange, sharing, and delivery in mobile social networks through the resource allocation and optimization of MEC allocation, caching, and D2D (Device-to-Device) Cache Cellular BS Small cell BS Cloud Joint design of caching, networking, and transmission control strategies Fig. <ref type="figure" target="#fig_3">15</ref>: Joint caching, networking, and transmission control to optimize cache hit rate <ref type="bibr" target="#b76">[78]</ref>, cache expiration time <ref type="bibr" target="#b80">[82]</ref>, interference alignment <ref type="bibr" target="#b83">[85]</ref>- <ref type="bibr" target="#b85">[87]</ref>, Quality of Experience <ref type="bibr" target="#b86">[88]</ref>, <ref type="bibr" target="#b88">[90]</ref>, energy efficiency <ref type="bibr" target="#b91">[93]</ref>, resource allocation <ref type="bibr" target="#b92">[94]</ref>- <ref type="bibr" target="#b94">[96]</ref>, traffic latency, or redundancy <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b98">[100]</ref>. networking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data and Computation Offloading</head><p>With limited computation, memory and power supplies, IoT devices such as sensors, wearable devices, and handheld devices become the bottleneck to support advanced applications such as interactive online gaming and face recognition. To address such a challenge, IoT devices can offload the computational tasks to nearby MEC servers, integrated with the BSs, Access Points (APs), and even neighboring Mobile Users (MUs). As a result, data and computation offloading can potentially reduce the processing delay, save the battery energy, and even enhance security for computation-intensive IoT applications. However, the critical problem in the computation offloading is to determine the offloading rate, i.e., the amount of computational workload, and choose the MEC server from all available servers. If the chosen MEC server experiences heavy workloads and degraded channel conditions, it may take even longer time for the IoT devices to offload data and receive the results from the MEC server. Hence, the design of an offloading policy has to take into account the time-varying channel conditions, user mobility, energy supply, computation workload and the computational capabilities of different MEC servers. Some optimal offloading approaches, e.g., the dynamic programming based offloading algorithm and heuristic offloading algorithm <ref type="bibr" target="#b100">[102]</ref>, are proposed. However, the approaches assume that the mobility patterns of the mobile users are given in advance. Without knowing the mobility pattern in advance, the DQL can be used for each mobile user to learn the optimal offloading policy from past experiences such as the approaches proposed in <ref type="bibr" target="#b101">[103]</ref> and <ref type="bibr" target="#b102">[104]</ref>. The authors in <ref type="bibr" target="#b101">[103]</ref> focus on minimizing the mobile user's cost and energy consumption by offloading cellular traffic to WLAN. Each mobile user can either access the cellular network, or the complimentary WLAN as illustrated in Fig. <ref type="figure" target="#fig_13">16</ref>(a), but with different monetary costs. The mobile user also has to pay a penalty if the data transmission does not finish before the deadline. The mobile user's data offloading decision can be modeled as an MDP. The system state includes the mobile user's location and the remaining file size of all data flows. The mobile user will choose to transmit data through either WLAN or cellular network, and decide how to allocate channel capacities to concurrent flows. CNNs are employed in the DQL to predict a continuous value of the mobile user's remaining data. Simulation results reveal that the DQN-based scheme generally outperforms the dynamic programming algorithm for the MDP in terms of cost and energy consumption. In particular, the DQL-based scheme can reduce the energy consumption up to 500 Joules compared with that of the dynamic programming algorithm. The reason is that the DQN can learn from experience while the dynamic programming algorithm cannot obtain the optimal policy with incorrect transition probability.</p><p>The allocation of limited computational resources at the MEC server is critical for cost energy minimization. The authors in <ref type="bibr" target="#b102">[104]</ref> consider an cellular system, in which multiple mobile users can offload their computational tasks via wireless channels to one MEC server, co-located with the cellular BS as shown in Fig. <ref type="figure" target="#fig_13">16(b)</ref>. Each mobile user has a computational-intensive task, characterized by the required computational resources, CPU cycles, and the maximum tolerable delay. The capacity of the MEC server is limited to accommodate all mobile users' task loads. The bandwidth sharing between different mobile users' offloading also affects the overall delay performance and energy consumptions. The DQL is used to minimize the cost of delay and power consumptions for all mobile users, by jointly optimizing the offloading decision and computational resource allocation. The system states include the sum of cost of the entire system and the available computational capacity of the MEC server. The action of BS is to determine the resource allocation and offloading decision for each mobile user. To limit the size of action space, a pre-classification step is proposed to check the mobile users' feasible set of actions. Simulation results show that the proposed scheme can reduce the sum cost up to 55% compared with the static allocation strategies.</p><p>In contrast to <ref type="bibr" target="#b102">[104]</ref>, multiple BSs in an ultra-dense network is considered in <ref type="bibr" target="#b103">[105]</ref> and <ref type="bibr" target="#b104">[106]</ref>, as shown in Fig. <ref type="figure" target="#fig_13">16(c)</ref>, with the objective of minimizing the long-term cost of delay in computation offloading. All computational tasks are offloaded to the shared MEC server via different BSs. Besides the allocation of computational resources and transmission control, the offloading policy also has to optimize the association between mobile users and the BSs. With dynamic network conditions, the mobile users' decision-making can be formulated as an MDP. The system states are the channel conditions between the mobile user and the BSs, the states of energy and task queues. The cost function is defined as a weighted sum of the execution delay, the handover delay and the computational task dropping cost. The authors in <ref type="bibr" target="#b104">[106]</ref> firstly propose a DDQN-based DQL algorithm to learn the optimal offloading policy without knowing the network dynamics. By leveraging the additive structure of the utility function, the Q-function decomposition combined with the DDQN further leads to a novel online SARSA-based DRL algorithm. Numerical experiments show that the new algorithm achieves a significant improvement in computation offloading performance compared with the baseline policies, e.g., the DQN-based DQL algorithm and some heuristic offloading strategies without learning. The high density of SBSs can relieve the data offloading pressure in peak traffic hours but consume a large amount of energy in off-peak time. Therefore, the authors in <ref type="bibr" target="#b105">[107]</ref>, <ref type="bibr" target="#b106">[108]</ref>, and <ref type="bibr" target="#b107">[109]</ref> propose a DQL-based strategy for controlling the (de)activation of different SBSs to minimize the energy consumption without compromising the quality of provisioning. In particular, in <ref type="bibr" target="#b105">[107]</ref>, the on/off decision framework uses a DQL scheme to approximate both the policy and value functions in an actor-critic method. The reward of the DQL agent is defined as a cost function relating to energy consumption, QoS degradation, and the switching cost of SBSs. The DDPG approach is also employed together with an action refinement scheme to expedite the training process. Through extensive numerical simulations, the proposed scheme is shown to greatly outperform other baseline methods in terms of both energy and computational efficiency.</p><p>With a similar model to that in <ref type="bibr" target="#b104">[106]</ref>, computation offloading finds a proper application for cloud-based malware detection in <ref type="bibr" target="#b108">[110]</ref>. A review of the threat models and the RLbased solutions for security and privacy protection in mobile offloading and caching are discussed in <ref type="bibr" target="#b109">[111]</ref>. With limited energy supply, computational resources, and channel capacity, mobile users cannot always update the local malware database and process all application data in time and thus are vulnerable to zero-day attacks <ref type="bibr" target="#b110">[112]</ref>. By leveraging the remote MEC server, all mobile users can offload their application data and detection tasks via different BSs to the MEC/security server with larger and more sophisticated malware database, more computational capabilities, and powerful security services. This can be modeled by a dynamic malware detection game in which multiple mobile users interact with each other in resource competition, e.g., the allocation of wireless channel capacities and the computational capabilities of the MEC/security server. A DQL scheme is proposed for each mobile user to learn its offloading data rate to the MEC/security server. The system states include the channel state and the size of application traces. The objective is to optimize the detection accuracy of the security server, which is defined as a concave function in the total amount of malware samples. The Q-value is estimated by using a CNN in the DQL framework. The authors also propose the hotbooting Q-learning technique that provides a better initialization for Q-learning by exploiting the offloading experiences in similar scenarios. It can save exploration time at the initial stage and accelerate the learning speed compared with a standard Q-learning algorithm with all-zero initialization of the Q-value <ref type="bibr" target="#b111">[113]</ref>. The proposed DQL scheme not only improves the detection speed and accuracy, but also increases the mobile users' battery life. The simulation results reveal that compared with the hotbooting Q-learning and standard Q-learning schemes, the DQL-based malware detection has faster learning rate, higher accuracy, and lower detection delay. For example, the detection delay of the proposed DQL scheme reduces by 24.6% and 35.3% at time slot 2000, respectively, compared with those of the hotbooting Q-learning and the standard Q-learning schemes.</p><p>Multiple MEC servers have been considered in <ref type="bibr" target="#b112">[114]</ref>, <ref type="bibr" target="#b113">[115]</ref>, as illustrated in Fig. <ref type="figure" target="#fig_13">16(d</ref>). The authors in <ref type="bibr" target="#b112">[114]</ref> aim to design optimal offloading policy for IoT devices with energy harvesting capabilities. The system consists of multiple MEC servers, such as and APs, with different capabilities in computation and communications. The IoT devices are equipped with energy storage and energy harvesters. They can execute computational tasks locally and offload the tasks to the MEC servers. The IoT device's offloading decision can be formulated as an MDP. The system states include the battery status, the channel capacity, and the predicted amount of harvested energy in the future. The IoT device evaluates the reward based on the overall delay, energy consumption, the task drop loss and the data sharing gains in each time slot. Similar to <ref type="bibr" target="#b108">[110]</ref>, the authors in <ref type="bibr" target="#b112">[114]</ref> enhance Q-learning by the hotbooting technique to save the random exploration time at the beginning of learning. The authors also propose a fast DQL offloading scheme that uses hotbooting to initialize the CNN and accelerates the learning speed. The authors in <ref type="bibr" target="#b113">[115]</ref> view the MEC-enabled BSs as different physical machines constituting a part of the cloud resources. The cloud optimizes the MUs' computation offloading to different virtual machines residing on the physical machines. A two-layered DQL algorithm is proposed for the offloading problem to maximize the utilization of cloud resources. The system state relates to the waiting time of each computational task and the number of virtual machines. The first layer is implemented by a CNN-based DQL framework to estimate an optimal cluster for each computational task. Different clusters of physical machines are generated based on the K-NN algorithm. The second layer determines the optimal serving physical machine within the cluster by Q-learning method.</p><p>The aforementioned works all focus on data or computation offloading in cellular system via BSs to remote MEC servers, e.g., <ref type="bibr" target="#b101">[103]</ref>- <ref type="bibr" target="#b104">[106]</ref>, <ref type="bibr" target="#b108">[110]</ref>, <ref type="bibr" target="#b112">[114]</ref>, <ref type="bibr" target="#b113">[115]</ref>. In <ref type="bibr" target="#b115">[117]</ref> and <ref type="bibr" target="#b114">[116]</ref>, the authors study QoS-aware computation offloading in an ad-hoc mobile network. By making a certain payment, the mobile user can offload its computational tasks to nearby mobile users constituting a mobile cloudlet, as shown in Fig. <ref type="figure" target="#fig_13">16(d)</ref>. Each mobile user has a first-in-first-out queue with limited buffer size to store the arriving tasks arriving as a Poisson process. The mobile user selects nearby cloudlets within D2D communication range for task offloading. The offloading decision depends on the states including the number of remaining tasks, the quality of the links between mobile users and the cloudlet, and the availability of the cloudlet's resources. The objective is to maximize a composite utility function, subject to the mobile user's QoS requirements, e.g., energy consumption and processing delay. The utility function is firstly an increasing function of the total number of tasks that have been processed either locally or remotely by the cloudlets. It is also related to the user's benefit such as energy efficiency and payment for task offloading. This problem is formulated as an MDP and solved by linear programming and Q-learning approaches, depending on the availability of information about the state transition probabilities. This work (a) Offloading cellular traffic to WLAN <ref type="bibr" target="#b101">[103]</ref>, (b) Offloading to a single MEC-enabled BS <ref type="bibr" target="#b102">[104]</ref>, (c) Offloading to one shared MEC server via multiple BSs <ref type="bibr" target="#b103">[105]</ref>, <ref type="bibr" target="#b104">[106]</ref>, <ref type="bibr" target="#b108">[110]</ref>, (d) Offloading to multiple MEC-enabled BSs <ref type="bibr" target="#b112">[114]</ref>, <ref type="bibr" target="#b113">[115]</ref> and mobile cloudlets <ref type="bibr" target="#b114">[116]</ref>, <ref type="bibr" target="#b115">[117]</ref>.</p><p>is further enhanced by leveraging DNN or DQN to learn the decision strategy more efficiently. A similar model is studied in <ref type="bibr">[118]</ref>, where the computation offloading is formulated as an MDP to minimize the cost of computation offloading. The solution to the MDP can be used to train a DNN by supervised learning. The well-trained DNN is then applied to unseen network conditions for real-time decision-making. Simulation results show that the use of deep supervised learning achieves significant performance gain in offloading accuracy and cost saving.</p><p>Data and computation offloading is also used in fog computing. The mobile application demanding a set of data and computational resources can be hosted in a container, e.g., virtual machine of a fog node. With user's mobility, the container has to be migrated or offloaded to other nodes and dynamically consolidated. With the container migration, some nodes with low resource utilization can be switched off to reduce power consumption. The authors in <ref type="bibr" target="#b117">[119]</ref> model the container migration as a multi-dimensional MDP, which is solved by the DQL. The system states consist of the delay, the power consumption and the migration cost. The action includes the selection policy that selects the containers to be emigrated from each source node, and the allocation policy that determines the destination node of each container. The action space can be optimized for more efficient exploration by dividing fog nodes into under-utilization, normal-utilization, and over-utilization groups. By powering off under-utilization nodes, all their containers will be migrated to other nodes to reduce power consumption. The training process is also optimized by using DDQN and PER which assigns different priorities to the transitions in experience memory. This helps the DQL agent at each fog node to perform better in terms of faster learning speed and more stability. The analysis presents that the proposed scheme can be executed in polynomial time. Simulation results reveal that the DQL scheme achieves fast decision-making and outperforms the existing baseline approaches significantly in terms of delay, power consumption, and migration cost.</p><p>Summary: This section reviews the applications of the DQL for wireless caching and data/computation offloading, which are inherently coupled with networking and allocation of channel capacity, computational resources, and caching capabilities, etc. We observe that the DQL framework for caching is typically centralized and mostly implemented at the network controller, e.g., the BS, service provider, and central scheduler, which is more powerful in information collection and cross-layer policy design. On the contrary, the end users have more control over their offloading decisions, and hence we observe more popular implementation of the DQL agent at local devices, e.g., mobile users, IoT devices, and fog nodes. Though an orchestration of networking, caching, data and computation offloading in one unified DQL framework is promising for network performance maximization, we face many challenges in designing highly-stable and fastconvergent learning algorithms, due to excessive delay and unsynchronized information collection from different network entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NETWORK SECURITY AND CONNECTIVITY PRESERVATION</head><p>Future networks become more decentralized and ad-hoc in nature which are vulnerable to various attacks such as Denialof-Service (DoS) and cyber-physical attack. Recently, the DQL has been used as an effective solution to avoid and prevent the attacks. In this section, we review the applications of DQL in addressing the following security issues:</p><p>• Jamming attack: In the jamming attack, attackers as jammers transmit Radio Frequency (RF) jamming signals with high power to cause interference to the legitimate communication channels, thus reducing the SINR at legitimate receivers. Anti-jamming techniques such as the frequency hopping <ref type="bibr" target="#b118">[120]</ref> and user mobility, i.e., moving out from the heavy jamming area, have been commonly used. However, without being aware of the radio channel model and the jamming methods, it is for the users to choose an appropriate frequency channel as well as to determine how to leave and avoid the attack. enables the users to learn an optimal policy based on their past observations, and thus DQL can be used to address the above challenge.</p><p>• Cyber-physical attack: The cyber-physical attack is an integrity attack in which an attacker manipulates data to alter control signals in the system. This attack often happens in autonomous systems such as Intelligent Transportation Systems (ITSs) and increases the risk of accidents to Autonomous Vehicles (AVs). The DQL allows the AVs to learn optimal actions based on their time-varying observations of the attacker' activities. Thus, Fig. <ref type="figure" target="#fig_5">17</ref>: Jamming attack in cognitive radio network <ref type="bibr" target="#b119">[121]</ref>.</p><p>the DQL can be used to robust and dynamic control of the AV to the attacks. • Connectivity preserving: This refers to maintaining the connectivity among the robots, e.g., UAVs, to support the communication and exchange of information among them. The system and network environment is generally dynamic and complex, and thus the DQL which allows each robot to make dynamic decisions based on its state can be effectively used to preserve the connectivity in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Security</head><p>This section discusses the applications of DQL to address the jamming attack and the cyber-physical attack.</p><p>1) Jamming Attack: A pioneer work using the DQL for the anti-jamming is <ref type="bibr" target="#b119">[121]</ref>. The network model is a Cognitive Radio Network (CRN) as shown in Fig. <ref type="figure" target="#fig_5">17</ref> which consists of one Secondary User (SU), multiple Primary Users (PUs), and multiple jammers. The network has a set of frequency channels for hopping. At each time slot, each jammer can arbitrarily select one of the channels to send its jamming signal, and the SU, i.e., the agent, needs to choose a proper action based on the SU's current state. The action is (i) selecting one of the channels to send its signals or (ii) leaving the area to connect to another BS. The jammers are assumed to avoid causing interference to the PUs, and thus the SU's current state consists of the number of PUs and the discretized SINR of the SU signal at the last time slot. The objective of the SU is to maximize its expected discounted utility over time slots. Note that when the SU chooses to leave the area to connect to another BS, it spends a mobility cost. Thus, the utility is defined as a function of the SINR of the SU signal and the mobility cost. Since the number of frequency channels may be large that results in a large action set, the CNN is used for the DQL to quickly learn the optimal policy. As shown in the simulation results, the proposed DQL has a faster convergence speed than that of the Q-learning algorithm. In particular, the utility of the SU increases from 2.73 at the beginning to 3.39 at time slot 1000 that is 8.3% higher than that of the Qlearning algorithm. Moreover, considering the scenario with two jammers, the proposed DQL outperforms the frequencyhopping method in terms of the SINR and the mobility cost.</p><p>The model in <ref type="bibr" target="#b119">[121]</ref> is constrained to two jammers. As the number of jammers in the network increases, the proposed Composite utility related to delay, power consumption, and migration cost Fog computing scheme may not be effective. The reason is that it becomes hard for the SU to find good actions when the number of jammed channels increases. An appropriate solution, as proposed in <ref type="bibr" target="#b120">[122]</ref>, allows the receiver of the SU to leave its current location. Since the leaving incurs the mobility cost, the receiver, i.e., the agent, needs an optimal policy, i.e., staying at or leaving the current location, to maximize its utility. In this scenario, the DQL based on CNN can be used for the receiver to find the optimal action to maximize its expected utility. Here, the utility and state of the receiver are essentially defined similarly to that of the agent in <ref type="bibr" target="#b119">[121]</ref>. In particular, the state includes the discretized SINR of the signal measured by the receiver at the last time slot. Simulation results show that the proposed DQL converges to SINR and utility values that are higher than those obtained by the Q-learning and random schemes. In particular, the SINR value obtained by the proposed DQL is 3.4, while those obtained by the Q-learning and random schemes are 3.3 and 2.8, respectively.</p><p>The above approaches, i.e., in <ref type="bibr" target="#b119">[121]</ref> and <ref type="bibr" target="#b120">[122]</ref>, define states of the agents based on raw SINR values of the signals. In practical wireless environments, the number of SINR values may be large and even infinite. Moreover, the raw SINR can be inaccurate and noisy. To cope with the challenge of the infinite number of states, the DQL can use a recursive Convolutional Neural Network (RCNN) as proposed in <ref type="bibr" target="#b121">[123]</ref>. By using the pre-processing layer and recursive convolution layers, the RCNN is able to remove noise from the network environment and extract useful features of the SINR, i.e., discrete spectrum sample values greater than a noise threshold, thus reducing the computational complexity. The network model and the problem formulation considered in <ref type="bibr" target="#b121">[123]</ref> are similar to those in <ref type="bibr" target="#b119">[121]</ref>. However, instead of directly using the raw SINR, the state of the SU is the extracted features of the SINR. Also, the action of the SU includes only frequency-hopping decision.</p><p>The simulation results show that the proposed DQL based on the RCNN can converge in both fixed and dynamic jamming scenarios while the Q-learning cannot converge in the dynamic jamming one. Furthermore, the proposed DQL can achieve the average throughput close to that of the optimal scheme, i.e., an anti-jamming scheme with completely known jamming actions.</p><p>Instead of finding the frequency-hopping decisions, the authors in <ref type="bibr" target="#b122">[124]</ref> propose the use of DQL to find an optimal power control policy for the anti-jamming. The model is an IoT network including IoT devices and one jammer. The jammer can observe the communications of the transmitter and chooses a jamming strategy to reduce the SINR at the receiver. Thus, the transmitter chooses an action, i.e., transmit power level, to maximize its utility. Here, the utility is the difference between the SINR and the energy consumption cost due to the transmission. Note that choosing the transmit power impacts the future jamming strategy, and thus the interaction between the transmitter and the jammer can be formulated as an MDP. The transmitter is the agent, and the state is SINR measured at its receiver at the last time slot. The DQN using the CNN is then adopted to find an optimal power control policy for the transmitter to maximize its expected accumulated discounted reward, i.e., the utility, over time slots. The simulation results show that the proposed DQL can improve the utility of the transmitter up to 17.7% compared with the Q-learning. Also, the proposed DQL reduces the utility of the jammer around 18.1% compared with the Q-learning. Moreover, the proposed DQL has faster convergence speed than that of the Q-learning. Specifically, the proposed DQL converges at time slot 210, while the Q-learning converges at time slot 240.</p><p>To prevent the jammer's observations of communications, the transmitter can change its communication strategy, e.g., by using relays that are far from the jamming area. The relays can be UAVs as proposed in <ref type="bibr" target="#b123">[125]</ref>. The model consists of one UAV, i.e., a relay, one jammer, one mobile user and its serving BS (see Fig. <ref type="figure" target="#fig_15">18</ref>). The mobile user transmits messages to its server via the serving BS. In the case that the serving BS is heavily jammed, the UAV helps the mobile user to relay the messages to the server through a backup BS. In particular, depending on the SINR and Bit Error Rate (BER) values sent from the serving BS, the UAV as an agent decides the relay power level to maximize its utility, i.e., the difference between the SINR and the relay cost. The relay power level can be considered to be the UAV's actions, and the SINR and BER are its states. As such, the next state observed by the UAV is independent of all the past states and actions. The problem is formulated as an MDP. To quickly achieve the optimal relay policy for the UAV, the DQL based on CNN is then adopted. The simulation results in <ref type="bibr" target="#b123">[125]</ref> show that the proposed DQL scheme takes only 200 time slots to converge to the optimal policy, which is 83.3% less than that of the relay scheme based on Q-learning <ref type="bibr" target="#b124">[126]</ref>. Moreover, the proposed DQL scheme reduces the BER of the user by 46.6% compared with the hill climbing-based UAV relay scheme <ref type="bibr" target="#b125">[127]</ref>.</p><p>The scheme proposed in <ref type="bibr" target="#b123">[125]</ref> assumes that the relay UAV is sufficiently far from the jamming area. However, as illustrated in Fig. <ref type="figure" target="#fig_15">18</ref>, the attacker can use a compromised UAV close to the relay UAV to launch the jamming attack to the relay UAV. In such a scenario, the authors in <ref type="bibr" target="#b126">[128]</ref> show that the DQL can still be used to address the attack. The system model is based on physical layer security and consists of one UAV and one attacker. The attacker is assumed to be "smarter" than that in the model in <ref type="bibr" target="#b123">[125]</ref>. This means that the attacker can observe channels that the UAV uses to communicate with the BS in the past time slots and then chooses jamming power levels on the target channels. Therefore, the UAV needs to find a power allocation policy, i.e., transmit power levels on the channels, to maximize the secrecy capacity of the UAV-BS communication. Similar to <ref type="bibr" target="#b123">[125]</ref>, the DQL based on CNN is used which enables the UAV to choose its actions, i.e., transmit power levels on the channels, based on its state, i.e., the attacker's jamming power level in the last time slot. The reward is the difference between the secrecy capacity of the UAV and BS and the energy consumption cost.</p><p>The simulation results in <ref type="bibr" target="#b126">[128]</ref> show that the proposed DQL can improve the UAV's utility up to 13% compared with the baseline scheme <ref type="bibr" target="#b127">[129]</ref> which uses the Win or Learn Faster-Policy Hill Climbing (WoLF-PHC) to prevent the attack. Also, the safe rate of the UAV, i.e., the probability that the UAV is attacked, obtained by the proposed DQL is 7% higher than that of the baseline. However, the proposed DQL has higher computational complexity and takes longer time to make a decision in each time epoch compared with the WoLH-PHC. Thus, the proposed DQL is applied only to a single-UAV system. For the future work, scenarios with multiple UAVs need to be considered. In such a scenario, more computational overhead is expected and multi-agent DQL algorithms can be applied. 2) Cyber-Physical Attack: In autonomous systems such as ITSs, the attacker can seek to inject faulty data to information transmitted from the sensors to the AVs. The AVs which receive the injected information may inaccurately estimate the safe spacing among them. This increases the risk of AV accidents. Vehicular communication security algorithms, e.g., <ref type="bibr" target="#b128">[130]</ref>, can be used to minimize the spacing deviation. However, the attacker's actions in these algorithms are assumed to be stable which may not be applicable in practical systems. The DQL that enables the AVs to learn optimal actions based on their time-varying observations of the attacker' actions can be thus used.</p><p>The first work using the DQL for the cyber-physical attack in an ITS can be found in <ref type="bibr" target="#b129">[131]</ref>. The system is a carfollowing model <ref type="bibr" target="#b130">[132]</ref> of the General Motors as shown in Fig. <ref type="figure" target="#fig_16">19</ref>. In the model, each AV updates its speed based on measurement information received from the closest road smart sensors. The attacker attempts to inject faulty data to the measurement information. However, the attacker cannot the measurements of different sensors equally due to its resource constraint. Thus, the AV can choose less-faulty measurements by selecting a vector of measurement weights. The objective of the attacker is to maximize the deviation, i.e., the utility, from the safe spacing between the AV and its nearby AV while that of the AV is to minimize the deviation. The interaction between the attacker and the AV can be modeled as a zero-sum game. The authors in <ref type="bibr" target="#b129">[131]</ref> show that the DQL can be used to find the equilibrium strategies. In particular, the action of the AV is to choose a weight vector. Its state includes the past actions, i.e., the weight vectors, and the past deviation values. Since the actions and deviations have continuous values, the state space is infinite. Thus, LSTM units that are able to extract useful features are adopted for the DQL to reduce the state space. The simulation results show that by using the past actions and deviations for learning the attacker's action, the proposed DQL scheme can guarantee a lower steady-state deviation than the Kalmar filter-based scheme <ref type="bibr" target="#b128">[130]</ref>. Moreover, by using the LSTM units, the results show that the proposed DQL scheme can converge much faster than the baseline scheme.</p><p>Another work that uses the LSTM to extract useful features from the measurement information to detect the cyber-physical attack is proposed in <ref type="bibr" target="#b131">[133]</ref>. The model is an IoT system including a cloud and a set of IoT devices. The IoT devices The algorithm proposed in <ref type="bibr" target="#b131">[133]</ref> is also called dynamic watermarking <ref type="bibr" target="#b132">[134]</ref> which is able to detect the cyber-physical attack and to prevent eavesdropping attacks. However, the algorithm requires large computational resources at the cloud for the IoT device signal authentication. Consequently, the cloud can only authenticate a limited number of vulnerable IoT devices. The cloud can choose the vulnerable IoT devices by observing their security status. However, this can be impractical since the IoT devices may not report their security status. Thus, the authors in <ref type="bibr" target="#b133">[135]</ref> propose to use the DQL that enables the cloud to decide which IoT devices to authenticate with the incomplete information. Since IoT devices with more valuable data are likely to be attacked, the reward is defined as a function of data values of IoT devices. The cloud's state includes attack actions of the attacker on the IoT devices in the past time slots. The actions of the attacker on the IoT devices can be obtained by using the dynamic watermarking algorithm in <ref type="bibr" target="#b131">[133]</ref> (see Fig. <ref type="figure" target="#fig_17">20</ref>). The DQL then uses an LSTM unit to find the optimal policy. The input of the LSTM unit is the state of the cloud, and the output includes probabilities of attacking the IoT devices. By using a real dataset from the accelerometers, the simulation results show that the proposed DQL can improve the cloud's utility up to 30% compared with the case in which the cloud chooses the IoT devices with equal probability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Connectivity Preservation</head><p>Multi-robot systems such as multi-UAV cooperative networks have been widely applied in many fields such as military, e.g., enemy detecting. In the cooperative multi-robot system, the connectivity among the robots, e.g., UAVs in Fig <ref type="figure" target="#fig_18">21</ref>, is required to enable the communication and exchange of information. To tackle the connectivity preservation problem, the Artificial Potential Field (APF) algorithm <ref type="bibr" target="#b134">[136]</ref> is used. However, the algorithm cannot be directly adopted when the robots are undertaking missions in dynamic and complex environments. The DQL which allows each robot to make dynamic decisions based on its own state can be effectively applied to preserve the connectivity in the multirobot system. Such an approach is proposed in <ref type="bibr" target="#b135">[137]</ref>.</p><p>The model in <ref type="bibr" target="#b135">[137]</ref> consists of two robots or UAVs, i.e., one leader robot and one follower robot. In the model, a central control, i.e., a ground BS, adjusts the velocity of the follower such that the follower stays in the communication range of the leader at all time (see <ref type="bibr">Fig 21)</ref>. The connectivity preservation problem can be thus formulated as an MDP. The agent is the BS, and the states are the relative position and the velocity of the leader with respect to the follower. The action space consists of possible velocity values of the follower. Taking an action returns a reward which is +1 if the follower is in the range of the leader, and -1 otherwise. A DQN using is used which enables the BS to learn an optimal policy to maximize the expected discounted cumulative reward. The input of the DQN includes the states of the two robots, and the output is the action space of the follower. The simulation show that for different locations of the leader and the follower, the score obtained by the proposed scheme is always 100, while the score of the APF method can be occasionally less than 100. This means that the proposed scheme achieves better connectivity between the two robots than that of the APF method. However, a general scenario with more than one leader and one follower needs to be investigated.</p><p>Considering the general scenario, the authors in <ref type="bibr" target="#b136">[138]</ref> address the connectivity preservation between multiple leaders and multiple followers. The robot system is definitely connected if any two robots are connected via a direct link or multi-hop link. To express the connectivity in such a robot system, the authors introduce the concept of algebraic connectivity <ref type="bibr" target="#b137">[139]</ref> which is the second smallest eigenvalue of a Laplacian matrix. The robot system is connected if the algebraic connectivity of the system is positive. Thus, the problem is to adjust the velocity of the followers such that the algebraic connectivity is positive over time slots. This problem can be formulated as an MDP in which the agent is the ground BS, the state is a combination of the states of all robots, the action is a set of possible velocity values for the followers. The reward is +1 if the algebraic connectivity of the system increases or holds, and becomes a penalty of -1 if the algebraic connectivity decreases. Similar to <ref type="bibr" target="#b135">[137]</ref>, a DQN is adopted. Due to the large action space of the followers, the actor-critic neural network <ref type="bibr" target="#b30">[31]</ref> is used. The simulation results show that the followers always follow the motion of the leaders even if the leaders' trajectory dynamically changes. This demonstrates the capability of DQN to tackle the connectivity preservation problem for multi-robot systems. However, the proposed DQN requires more time to converge than that in <ref type="bibr" target="#b135">[137]</ref> because of the presence of more followers.</p><p>The proposed schemes in <ref type="bibr" target="#b135">[137]</ref> and <ref type="bibr" target="#b136">[138]</ref> do not consider a minimum distance between the leaders and followers. The leaders and followers can collide with each other if the distance between them is too short. Thus, the BS needs to guarantee the minimum distance between them. One solution is to have the minimum distance in the reward as proposed in <ref type="bibr" target="#b138">[140]</ref>. In particular, if the leader is too close to its follower, the reward of the system is penalized regarding the minimum distance. The DQL algorithm proposed in <ref type="bibr" target="#b136">[138]</ref> is then used such that the BS learns proper actions, e.g., turning left and right, to maximize the cumulative reward.</p><p>When BSs are densely deployed, the UAVs or mobile users need to trigger a frequent handover to preserve the connectivity. The frequent handover increases communication overhead and energy consumption of the mobile users, and interrupts data flows. Thus, it is essential to maintain an appropriate handover rate. The authors in <ref type="bibr" target="#b31">[32]</ref> address the handover decision problem in an ultra-density network. The network model consists of multiple mobile users, SBSs, and one central controller. At each time slot, the user needs to decide its serving SBS. The handover decision process can be modeled as an MDP, and the DQL is adopted to find an optimal handover policy for each user to minimize the number of handover occurrences while ensuring certain throughput. The state of the user, i.e., the agent, includes reference signal quality received from candidate SBSs and the last action of the user. The reward is defined as the difference between the data rate of the user and its energy consumption for the handover process. Given a high density of users, the DQL using A3C and LSTM is adopted to find the optimal policy in short training time. The simulation results show that the proposed DQL can achieve higher throughput and lower handover rate than those of the upper confidence bandit algorithm <ref type="bibr" target="#b139">[141]</ref> with similar training time. Specifically, the throughput and the handover rate of the DQL are bit/s/Hz and 0.0003, respectively, while those of the upper confidence bandit algorithm are 0.67 bit/s/Hz and 0.00049, respectively.</p><p>To enhance the reliability of the communication between the SBSs and the mobile users, the SBSs should be able to handle network faults and failure automatically as selfhealing. The DQL can be applied as proposed in <ref type="bibr" target="#b140">[142]</ref> to make optimal parameter adjustments based on the observation of the network performance. The model is the 5G network including one MBS. The MBS as an agent needs to handle network faults such as transmit diversity faults and antenna azimuth change, e.g., because of wind. These faults are represented as the MBS's state that is the number of active alarms. Based on the alarms, the MBS can take actions including (i) enabling the transmit diversity and (ii) setting the antenna azimuth to default value. The reward that the MBS receives is the scores, e.g., -1, 0, and +1, depending on the number of faults happening. The DQL is used to learn the optimal policy. The simulation results show that the proposed DQL can achieve network throughput close to that of the oracle-based selfhealing, i.e., the upper-performance bound, but incurs less fault message passing overhead. In particular, the message passing complexity of the proposed DQL is O(N ), and that of the oracle-based self-healing is O(N 2 ), where N is the number of SBSs in the network.</p><p>Summary: This section reviews applications of DQL for the network security and connectivity preservation. The reviewed approaches are summarized along with the references in Table VI. We observe that the CNN is mostly used for the DQL to enhance the network security. Moreover, DQL approaches for the anonymous system such as robot systems and ITS receive more attentions than other networks. However, the applications of DQL for the cyber-physical security are relatively few and need to be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MISCELLANEOUS ISSUES</head><p>In previous sections, we have reviewed and analyzed the applications of DRL framework in different aspects (with different technological focuses) of communications and networking, i.e., network access and rate control, caching and offloading, security and connectivity preservation. However, the optimization of communications and networking actually involves cross-layer co-design and interactions among different network entities. For example, the traffic engineering problem may require joint optimization of routing, transmit power control, channel access control, resource allocation and so on. The problem can become more complicated for the emerging communications systems, e.g., UAV and vehicular networks. In this section, we review the other uses of DRL in communications and networking. These issues include (i) traffic engineering and routing, (ii) resource sharing and scheduling, and (iii) crowdsensing and social networking. Due to its model-free nature, DRL provides a flexible tool for dynamic and diversified applications, typically involving high-dimensional, cross-layer, and multi-agent interactions. All these imply a huge space of state transitions and actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traffic Engineering and Routing</head><p>Traffic Engineering (TE) in communication networks refers to Network Utility Maximization (NUM) by optimizing a path to forward the data traffic, given a set of network flows from source to destination nodes. Traditional NUM problems are mostly model-based. However, with the advances of wireless communication technologies, the network environment becomes more complicated and dynamic, which makes it hard to model, predict, and control. The recent development of DQL methods provides a feasible and efficient way to design experience-driven and model-free schemes that can learn and adapt to the dynamic wireless network from past observations. Routing optimization is one of the major control problems in traffic engineering. The authors in <ref type="bibr" target="#b141">[143]</ref> present the first attempt to use the DQL for the routing optimization. Through the interaction with the network environment, the DQL agent at the network controller determines the paths for all sourcedestination pairs. The system state is represented by the bandwidth request between each source-destination pair, and the reward is a function of the mean network delay. The DQL agent leverages the actor-critic method for solving the routing problem that minimizes the network delay, by adapting routing configurations automatically to current traffic conditions. The DQL agent is trained using the traffic information generated by a gravity model <ref type="bibr" target="#b142">[144]</ref>. The routing solution is then evaluated by OMNet+ discrete event simulator <ref type="bibr" target="#b143">[145]</ref>. Simulation results show that the well-trained DQL agent can produce a nearoptimal routing configuration in a single step and thus the agent is agile for real-time network control. The proposed approach is attractive as the traditional optimization-based techniques require a large number of steps to produce a new configuration. The authors in <ref type="bibr" target="#b144">[146]</ref> consider a similar network model with multiple end-to-end communication sessions. Each source-destination pair has a set of candidate paths that can transport the traffic load. Experimental results show that the conventional DDPG method does not work well for the continuous control problem in <ref type="bibr" target="#b144">[146]</ref>. One possible explanation is that DDPG utilizes uniform sampling for experience replay, which ignores different significance of the transition samples.</p><p>The authors in <ref type="bibr" target="#b144">[146]</ref> also combine two new techniques to optimize DDPG particularly for traffic engineering problems, i.e., TE-aware exploration and actor-critic-based PER methods. The TE-aware exploration leverages the shortest path algorithm and NUM-based solution as the baseline during exploration. The PER method is conventionally used in DQL, e.g., <ref type="bibr" target="#b86">[88]</ref> and while the authors in <ref type="bibr" target="#b144">[146]</ref> integrate the PER method with the actor-critic framework for the first time. The proposed scheme assigns different priorities to transitions in the experience replay. Based on the priority, the proposed scheme samples the transitions in each epoch. The system state consists of throughput and delay performance of each communication session. The action specifies the amount of traffic load going through each of the paths. By learning the dynamics of network environment, the DQL agent aims to maximize the total utility of all the communication sessions, which is defined based on end-to-end throughput and delay <ref type="bibr" target="#b145">[147]</ref>. Packet-level simulations using NS-3 <ref type="bibr" target="#b146">[148]</ref>, tested on well-known network topologies as well as random topologies generated by BRITE <ref type="bibr" target="#b147">[149]</ref>, reveal that the proposed DQL scheme significantly reduces the end-to-end delay and improves the network utility, compared with the baseline schemes including DDPG and the NUM-based solutions.</p><p>The networking and routing optimization become more </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enabling transmit diversity and changing antenna azimuth</head><p>Score -1, 0, +1, and +5</p><p>Selforganization network complicated in the UAV-based wireless communications. The authors in <ref type="bibr" target="#b138">[140]</ref> model autonomous navigation of one single UAV in a large-scale unknown complex environment as a POMDP, which can be solved by actor-critic-based DRL method. The system state includes its distances and orientation angles to nearby obstacles, the distance and angle between its present position and the destination. The UAV's action is to turn left or right or keep ahead. The reward is composed of four parts: an exponential penalty term if it is too close to any obstacles, a linear penalty term to encourage minimum time delay, the transition and direction rewards if the UAV is getting close to the target position in a proper direction. Instead of using conventional DDPG for continuous control, the Recurrent Deterministic Policy Gradient (RDPG) is proposed for the POMDP by approximating the actor and critic using RNNs. Considering that RDPG is not suitable for learning using memory replay, the authors in <ref type="bibr" target="#b138">[140]</ref> propose the fast-RDPG method by utilizing the actor-critic framework with function approximation <ref type="bibr" target="#b148">[150]</ref>. The proposed method derives policy update for POMDP by directly maximizing the expected long-term accumulated discounted reward. Path planning for multiple UAVs connected via cellular systems is studied in <ref type="bibr" target="#b149">[151]</ref> and <ref type="bibr" target="#b150">[152]</ref>. Each UAV aims to achieve a tradeoff between maximizing energy efficiency and minimizing both latency and interference caused to the ground network along its path. The network state observable by each UAV includes its distances and orientation angles to cellular BSs, the orientation angle to its destination, and the horizontal coordinates of all UAVs. The action of each UAV includes an optimal path, transmit power, and cell association along its path. The interaction among UAVs is cast as a dynamic game and solved by a multi-agent DRL framework. The use of ESN in the DRL framework allows each UAV to retain previous memory states and make a decision for unseen network states, based on the reward obtained from previous states. ESN is a new type of RNNs with feedback connections, consisting of the input, recurrent, and output weight matrices. ESN training is typically quick and computationally efficient compared with other RNNs. Deep ESNs can exploit the advantages of a hierarchical temporal feature representation at different levels of abstraction, hence disentangling the difficulties in modeling complex tasks. The analysis shows that the computational complexity of the proposed DRL is O(A 3 ), where A is the number of discretized areas. Simulation results show that the proposed scheme improves the tradeoff between energy efficiency, wireless latency, and the interference caused to the ground network. Results also show that each UAV's altitude is a function of the ground network density and the UAV's objective function is an important factor in achieving the UAV's target.</p><p>Besides networked UAVs, vehicle-to-infrastructure also constitutes an important part and provides rich application implications in 5G ecosystem. The authors in <ref type="bibr" target="#b151">[153]</ref> adopt the DQL to achieve an optimal control policy in communication-based train control system, which is supported by bidirectional trainground communications. The control problem aims to optimize the handoff decision and train control policy, i.e., accelerate or decelerate, based on the states of stochastic channel conditions and real-time information including train position, speed, measured SNR from APs, and handoff indicator. The objective of the DQL agent is to minimize a weighted combination of operation profile tracking error and energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Resource Sharing and Scheduling</head><p>System capacity is one of the most important performance metrics in wireless communication networks. System capacity enhancements can be based on the optimization of resource sharing and scheduling among multiple wireless nodes. The integration of DRL into 5G systems would revolutionize the resource sharing and scheduling schemes from model-based to model-free approaches and meet various application demands by learning from the network environment.</p><p>The authors in <ref type="bibr" target="#b152">[154]</ref> study the user scheduling in a multiuser massive MIMO system. User scheduling is responsible for allocating resource blocks to BSs and mobile users, taking into account the channel conditions and QoS requirements. Based on this user scheduling strategy, a DRL-based coverage and capacity optimization is proposed to obtain dynamically the scheduling parameters and a unified threshold of QoS metric. The performance indicators are calculated as the average spectrum efficiency of all the users. The system state is an indicator of the average spectrum efficiency. The action of the scheduler is a set of scheduling parameters to maximize the reward as a function of the average spectrum efficiency. The DRL scheme uses policy gradient method to learn a policy function (instead of a Q-function) directly from trajectories generated by the current policy. The policy network is trained with a variant of the REINFORCE algorithm <ref type="bibr" target="#b148">[150]</ref>. The simulation results in <ref type="bibr" target="#b152">[154]</ref> show that compared with the optimization-based algorithms that suffer from incomplete network information, the policy gradient method achieves much better performance in terms of network coverage and capacity.</p><p>In <ref type="bibr" target="#b153">[155]</ref>, the authors focus on dynamic resource allocation in a cloud radio access network and present a DQL-based framework to minimize the total power consumption while fulfilling mobile users' QoS requirements. The system model contains multiple Remote Radio Heads (RRHs) connected to a cloud BaseBand Unit (BBU). The information of RRHs can be shared in a centralized manner. The system state contains information about the mobile users' demands and the RRHs' working states, e.g., active or sleep. According to the system state and the result of last execution, the DQL agent at the BBU decides whether to turn on or off certain RRH(s), and how to allocate beamforming weight for each active RRH. The objective is to minimize the total expected power consumption. The authors propose a two-step decision framework to reduce the size of action space. In the first step, the DQL agent determines the set of active RRHs by Q-learning and DNNs. In the second step, the BBU derives the optimal resource allocation for the active RRHs by solving a convex optimization problem. Through the combination of DQL and optimization techniques, the proposed framework results in a relatively small action space and low online computational complexity. Simulation results show that the framework achieves significant power savings while satisfying user demands and is robust in highly dynamic network environment. The aforementioned works mostly focus on simulations and numerical comparisons. With one step further, the authors in <ref type="bibr" target="#b154">[156]</ref> implement a multi-objective DQL framework as the radio-resource-allocation controller for space communications. The implementation uses modular software architecture to encourage re-use and easy modification for different algorithms, which is integrated into the real spaceground system developed by NASA Glenn Research Center.</p><p>In emerging and future wireless networks, BSs are deployed with a high density that introduces a number of challenges to resource management such as the power allocation and interference management. Traditional power allocation approaches, e.g., the iterative algorithm based on closed-form factional programming <ref type="bibr" target="#b155">[157]</ref> can be used. However, the approaches always require full CSI that may be not available as the networks become larger and more complicated. Learning algorithms such as DRL can be used for the power allocation. The authors in <ref type="bibr" target="#b156">[158]</ref> propose to use a DQL scheme which allows the BSs to learn their optimal power control policy. In the proposed scheme, each BS is an agent, the action is choosing power levels, and the state includes interference that the BS caused to its neighbors in the last time slot. The objective is to maximize the BS's data rate. The DQN using FNN is then adopted to implement the DQL algorithm. The DQN using FNN is then adopted to implement the DQL algorithm. In the simulation, the authors model the channel variations in the Jake's model. With random CSI and delay, the DQL algorithm is shown to achieve near-optimal power allocation in real time. In certain scenarios, it even outperforms the centralized algorithms. This verifies that the DQL algorithm can be fast and competitive, especially for the scenarios with inaccurate CSI and nonnegligible delay. For the future work, a joint power control and channel selection can be considered.</p><p>Network slicing <ref type="bibr" target="#b157">[159]</ref> and NFV <ref type="bibr" target="#b97">[99]</ref> are two emerging concepts for resource allocation in the 5G ecosystem to provide cost-effective services with better performance. The network infrastructure, e.g., cache, computation, and radio resources, is comparatively static while the upper-layer Virtualized Network Functions (VNFs) are dynamic to support time-varying application-specific service requests. The concept of network slicing is to divide the network resources into multi-layer slices, managed by different service renderers independently with minimal conflicts. The concept of Service Function Chaining (SFC) is to orchestrate different VNFs to provide required functionalities and QoS provisioning.</p><p>The authors in <ref type="bibr" target="#b158">[160]</ref> propose a DQL scheme for QoS/QoEaware SFC in NFV-enabled 5G systems. Typical QoS metrics are bandwidth, delay, throughput, etc. The evaluation of QoE normally involves the end-user's participation in rating the service based on direct user perception. The authors quantify QoE by measurable QoS metrics without end-user involvements, according to the Weber-Fechner Law (WFL) <ref type="bibr" target="#b159">[161]</ref> and exponential interdependency of QoE and QoS hypothesis <ref type="bibr" target="#b160">[162]</ref>. These two principles actually define nonlinear relationship between QoE and QoS. The system state represents the network environment including network topology, QoS/QoE status of the VNF instances, and the QoS requirements of the SFC request. The DQL agent selects a certain direct successive VNF instance as an action. The reward is a composite function of the QoE gain, the QoS constraint penalty, and the OPEX penalty. A DQL based on CNNs is implemented to approximate the action-value function. The authors in <ref type="bibr" target="#b161">[163]</ref> review the application of a DQL framework in two typical resource management scenarios using network slicing. For radio resource slicing, the authors simulate a scenario containing one single BS with different types of services. Due to limited spectrum resource, the radio resource slicing aims to optimize the allocation of resource blocks to each slice. The reward can be defined as a weighted sum of spectrum efficiency and QoE. For prioritybased core network slicing, the authors simulate a scenario with 3 SFCs demanding different computational resources and waiting time. The aim is to minimize the scheduling delay by optimizing the common or dedicated VNFs. The reward can be defined as a weighted sum of spectrum efficiency and QoE. For priority-based core network slicing, the authors simulate a scenario with 3 SFCs demanding different computational resources and waiting time. The reward is the sum of waiting time in different SFCs. Simulation results in both scenarios show that the DQL framework could exploit more implicit relationship between user activities and resource allocation in resource constrained scenarios, and enhance the effectiveness and agility for network slicing.</p><p>Mobile data traffic is anticipated to significantly increase that may cause congestion at SBSs in 5G wireless networks. The conventional technique adjusts the uplink-downlink ratio based on the current network traffic condition and lacks knowledge of future traffic patterns. The conventional technique is thus prone to repeated congestion. The deep learning technique based on LSTM can be used as proposed in <ref type="bibr" target="#b162">[164]</ref> that allows the SBS to make local prediction of its traffic load based on the past and current traffic load. The input of the LSTM is the traffic information of the SBS in the past and current time slots, and the output is the traffic load predictions in the next time slots. Based on the prediction, the SBS proactively adjusts the uplink-downlink ratio to avoid the congestion. The simulation results show that the proposed scheme outperforms the conventional methods in terms of network throughput and packet loss rate.</p><p>Resource allocation and scheduling problems are also important for computer clusters or database systems. This usually leads to an online decision-making problem depending on the information of workload and environment. The authors in <ref type="bibr" target="#b163">[165]</ref> propose a DRL-based solution, DeepRM, by employing policy gradient methods <ref type="bibr" target="#b148">[150]</ref> to manage resources in computer systems directly from experience. The same policy gradient method is also used in <ref type="bibr" target="#b152">[154]</ref> for user scheduling and resource management in wireless systems. DeepRM is a multi-resource cluster scheduler that learns to optimize various objectives such as minimizing average job slowdown or completion time. The system state is the current allocation of cluster resources and the resource profiles of jobs in the queue. The action of the scheduler is to decide how to schedule the pending jobs. By simulations with synthetic dataset, DeepRM is shown to perform comparably or better than state-of-the-art heuristics, e.g., Shortest-Job-First (SJF). It adapts to different conditions and converges quickly, without any prior knowledge of system behavior. In <ref type="bibr" target="#b164">[166]</ref>, the authors use the actor-critic method to address the scheduling problem in a general-purpose distributed data stream processing systems, which deal with processing of continuous data flow in real time or near-real-time. The system model contains multiple threads, processes, and machines. The system state consists of the current scheduling decision and the workload of each data source. The scheduling problem is to assign each thread to a process of a machine. The agent at the scheduler determines the assignment of each thread, with the objective of minimizing the average processing time. The DRL framework includes three components, i.e., an actor network, an optimizer producing a K-NN set of the actor network's output action, and the critic network predicting the Q-value for each action in the set. The action is selected from the K-NN set with the maximum Q-value. The use of optimizer may avoid unstable learning and divergence problems in conventional actor-critic methods <ref type="bibr" target="#b165">[167]</ref>. Simulation results show that the proposed scheme can reduce the average processing time by 25.1% compared to the default scheduler and 9% compared to the model-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Power Control and Data Collection</head><p>With the prevalence of IoT and smart mobile devices, mobile crowdsensing becomes a cost-effective solution for network information collection to support more intelligent operations of wireless systems. The authors in <ref type="bibr" target="#b166">[168]</ref> consider spectrum sensing and power control in non-cooperative cognitive radio networks. There is no information exchange between PUs and SUs. As such, the SU outsources the sensing task to a set of spatially distributed sensing devices to collect information about the PU's power control strategy. The SU's power control can be formulated as an MDP. The system state is determined by the Received Signal Strength (RSS) at individual sensing devices. The SU chooses its transmit power from the set of pre-specified power levels based on the current state. A reward is obtained if both primary and SUs can fulfill their SNR requirements. Considering the randomness in RSS measurements, the authors propose a DQL scheme for the SU to learn and adjust its transmit power. The DQL is then implemented by a DQN by using FNN. The simulation results show that the proposed DQL scheme is able to converge to a close-to-optimal solution.</p><p>In massive MIMO networks, each base station is able to serve a number of users simultaneously. Thus, one critical problem is to allocate power to the users. To address the problem, the authors in <ref type="bibr" target="#b167">[169]</ref> propose to use the deep learning. The model consists of multiple base stations that serve multiple users. The problem is to determine a power allocation vector for the users to maximize the global energy efficiency of the network. The global energy efficiency is generally proportional to the data rates of the users and inversely proportionally to their total power consumption. The FNN is trained in which the input includes the signals received at the users, and the output includes the predicted power allocation vector. The loss function is formulated from the predicted power allocation vector and the optimal power allocation vector. Here, the optimal power allocation vector is obtained off-line by using the Sequential Fractional Programming (SFP) algorithm <ref type="bibr" target="#b168">[170]</ref>. The simulation results show that the proposed deep learning scheme can achieve the energy efficiency close to that of the SFP algorithm while using significantly less CPU time.</p><p>Max-min fairness is also a performance metric in the massive MIMO network to enhance the channel capacity of the users. The authors in <ref type="bibr" target="#b169">[171]</ref> adopt a deep learning model for the power allocation to achieve the objective. The model consists of multiple cells, and each cell consists of a base station that serves the base station's users. The problem of the base station is to determine the power allocation vector for its serving users to achieve the max-min fairness. Here, the max-min fairness is a max-min optimization problem in which the objective function is the ergodic channel capacity of each user. The FNN is trained in which the input includes geographical positions of the users in the whole network and the output includes the predicted power allocation vector of the users served by the base station. The loss function is a function of the optimal power allocation vector and the predicted power allocation vector. In particular, the optimal power allocation vector is obtained by offline solving the base station's optimization problem based on the traditional bisection approach <ref type="bibr" target="#b170">[172]</ref>. The simulation results show that the proposed deep learning scheme matches well the Minimum Mean Square Error (MMSE) scheme <ref type="bibr" target="#b171">[173]</ref> that is optimal but has high computational complexity.</p><p>The authors in <ref type="bibr" target="#b172">[174]</ref> leverage the DQL framework for sensing and control problems in a Wireless Sensor and Actor Network (WSAN), which is a group of wireless devices with the ability to sense events and to perform actions based on the sensed data shared by all sensors. The system state includes processing power, mobility abilities, and functionalities of the actors and sensors. The mobile actor can choose its moving direction, networking, sensing and actuation policies to maximize the number of connected actor nodes and the number of sensing events.</p><p>The authors in <ref type="bibr" target="#b173">[175]</ref> focus on mobile crowdsensing paradigm, where data inference is incorporated to reduce sensing costs while maintaining the quality of sensing. The target sensing area is split into a set of cells. The objective of a sensing task is to collect data, e.g., temperature and air quality, in all the cells. Here, the sensed data from different cells of the target sensing area may has different quality. The problem for each sensor in the mobile crowdsensing paradigm is how to choose cells to collect data to minimize the sensing cost while guaranteeing the sensing data quality. The optimal cell selection algorithms, e.g., <ref type="bibr" target="#b174">[176]</ref>, need to know the true data of each cell in advance that is usually impossible in practice. DQL is able to provide optimal decisions without a complete knowledge of the true data. Thus, it can be adopted for the cell selection of the mobile sensors to decide which cell is better to perform sensing tasks. The system state includes the selection matrices for a few past decision epochs. The reward function is determined by the sensing quality and cost in the chosen cells. To extract temporal correlations in learning, the authors propose the DRQN that uses LSTM layers in DQL to capture the hidden patterns in state transitions. Considering inter-data correlations, the authors use the transfer learning method to reduce the amount of data in training. That is, the cell selection strategy learned for one task can benefit another correlated task. Hence, the parameters of DRQN can be initialized by another DRQN with rich training data. Simulations are conducted based on two real-life datasets collected from sensor networks, i.e., the Sensor-Scope dataset <ref type="bibr" target="#b175">[177]</ref> in the EPFL campus and the U-Air dataset of air quality readings in Beijing <ref type="bibr" target="#b176">[178]</ref>. The experiments verify that DRQN reduces up to 15% of the sensed cells with the same inference quality guarantee. The authors in <ref type="bibr" target="#b177">[179]</ref> combine UAV and unmanned vehicle in mobile crowdsensing for smart city applications. The UAV cruises in the above of the target region for city-level data collection. Meanwhile, the unmanned vehicle carrying mobile charging stations moves on the ground and can charge the UAV at a preset charging point.</p><p>The target region is divided into multiple subregions and each subregion has a different sample priority. The authors in <ref type="bibr" target="#b177">[179]</ref> propose a DQL-based control framework for the unmanned vehicle to schedule its data collection, constrained by limited energy supply. The system state includes information about the sample priority of each subregion, the location of charging point, and the moving trace of the UAV and unmanned vehicle. The UAV and unmanned vehicle can choose the moving direction. The DQL framework utilizes CNNs for extracting the correlation of adjacent subregions, which can increase the convergence speed in training. The DQL algorithm can be enhanced by using a feasible control solution as the baseline during exploration. The PER method is also used in DQL to assign higher priorities to important transitions so that the DQL agent can learn from samples more efficiently. The proposed scheme is evaluated by using real dataset of taxi traces in Rome <ref type="bibr" target="#b178">[180]</ref>. Simulation results reveal that the proposed DQL algorithm can obtain the highest data collection rate compared with the MDP and other heuristic baselines.</p><p>Mobile crowdsensing is vulnerable to faked sensing attacks, as selfish users may report faked sensing results to save their sensing costs and avoid compromising their privacy. The authors in <ref type="bibr" target="#b179">[181]</ref> formulate the interactions between the server and a number of crowdsensing users as a Stackelberg game. The server is the leader that sets and broadcasts its payment policy for different sensing accuracy. In particular, the higher payment is set for more sensing accuracy. Based on the server's sensing policy, each user as a follower then chooses its sensing effort and thus the sensing accuracy to receive the payment. The payment motivates the users to put in sensing efforts, and thus the payment decision process can be modeled as an MDP. The server can apply Q-learning to optimize payment policy without requiring the sensing model. However, in the presence of a large state space, Q-learning has a slow learning rate. Therefore, the server can use the DQL to obtain the optimal payment to maximize its utility, based on the system state consisting of the past sensing quality and the payment policy. The DQL uses a deep CNN to accelerate the learning process and improve the crowdsensing performance against selfish users. Simulation results show that the DQLbased scheme produces a higher sensing quality, lower attack rate, and higher utility of the server, exceeding those of both the Q-learning and the random payment strategies. Moreover, the DQL-based scheme can converge at 200 time slots that is 225% faster than that of the Q-learning.</p><p>Social networking is an important component of smart city applications. The authors in <ref type="bibr" target="#b180">[182]</ref> aim to extract useful information by observing and analyzing the users' behaviors in social networking. One of the main difficulties is that the social behaviors are usually fuzzy and divergent. The authors model pervasive social networking as a monopolistically competitive market, which contains different users as data providers selling information at a certain price. Given the market model, the DQL can be used to estimate the users' behavior patterns and find the market equilibrium. Considering the costly deep learning structure, the authors in <ref type="bibr" target="#b180">[182]</ref> propose a Decentralized DRL (DDRL) framework that decomposes the costly deep component from the RL algorithms at individual users. The deep component can be a feature extractor integrated with the network infrastructure and provide mutual knowledge for all individuals. Multiple RL agents can purchase the most desirable data from the mutual knowledge. The authors combine well-known RL algorithms, i.e., Q-learning and learning automata, to estimate users' patterns which are described by vectors of probabilities representing the users' preferences or altitudes to different information. In social networking and smart city applications with human involvement, there can be both labeled and unlabeled data and hence a semisupervised DRL framework can be designed, by combining the strengths of DNNs and statistical modeling to improve the performance and accuracy in learning. Then, the authors in <ref type="bibr" target="#b181">[183]</ref> introduce the semi-supervised DRL framework that utilizes variational auto-encoders <ref type="bibr" target="#b182">[184]</ref> as an inference engine to infer the classification of unlabeled data. As a case study, the proposed DRL framework is customized to provide indoor localization based on the RSS from Bluetooth devices. The positioning environment contains a set of positions. Each position is associated with the set of RSS values from the set of anchor devices with known positions. The system state includes a vector of RSS values, the current location, and the distance to the target. The DQL agent, i.e., the positioning algorithm itself, chooses a moving direction to minimize the error distance to the target point. Simulations tested on realworld dataset show an improvement of 23% in terms of the error distance to the target compared with the supervised DRL scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Direction-of-Arrival (DoA) Estimation</head><p>Massive MIMO will be deployed for 5G to achieve highspeed communications at Gbps. For this, the Direction-of-Arrival (DoA) estimation is the prerequisite for realizing massive MIMO. However, the required DoA estimation is very challenging since it is difficult to exploit the characteristics of the channel and sparsity of the massive MIMO systems. Deep learning is able to learn useful features, and it can be used for the DoA estimation as proposed in <ref type="bibr" target="#b183">[185]</ref>. In particular, the DNN is trained based on a training dataset including DoAs and received signals. The DoAs are randomly generated, and the received signals are obtained by using different wireless channel models. The input of the DNN includes the DoAs and the received signals, and the output includes the estimated DoAs. The simulation results show that the high accuracy of the DoA estimation can be achieved with a mean square error of 10 -3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Signal Detection</head><p>Apart from improving the accuracy of the DoA estimation, enhancing the accuracy of the symbol detection needs to be considered to reduce Bit Error Ratio (BER). The authors in <ref type="bibr" target="#b184">[186]</ref> propose to use the deep learning for the symbol detection in OFDM systems. Specifically, the DNN is used in which the input includes each pair of pilot symbol and the corresponding received signal. The output of the DNN includes the predicted pilot symbol. The simulation results show that the proposed scheme outperforms the least square technique in terms of BER. These results imply that the DNN can learn the characteristics of the wireless channel accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. User Association and Load Balancing</head><p>In the complicated future networks such as HetNets and massive MIMO, user association is one of critical problems to assign users to base stations to maximize the total data rate. The existing approaches generally incur a considerable complexity overhead that impairs the real-time user association. Thus, deep learning can be used to perform the user association in real-time in realistic massive MIMO networks as proposed in <ref type="bibr" target="#b185">[187]</ref>. The model is similar to that in <ref type="bibr" target="#b169">[171]</ref>. The problem is to determine the user-association vector to maximize the total data rate of all users in the network. The FNN is trained to output the user association vector given the user positions. The training set consists of multiple pairs of user positions and optimal user association vector. Here, the optimal association vector is obtained offline by using the branch-and-cut algorithm <ref type="bibr" target="#b186">[188]</ref>. Simulation results show that the total data rate obtained by the deep learning scheme is close to that obtained by the optimal user association scheme from <ref type="bibr" target="#b171">[173]</ref>. This implies that the structure of the optimal association can be learned by the deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. User Localization</head><p>Channel State Information (CSI) has been recently used for the user localization since the CSI values contain information related to locations of the users. However, in the complicated wireless networks such as massive MIMO, measuring wireless channel characteristics, i.e., the CSI values, is challenging. Deep learning that is able to learn useful features from complicated environments can be used for the user localization as proposed in <ref type="bibr" target="#b187">[189]</ref>. The deep learning algorithm consists of two phases, i.e., the offline training phase and online localization phase. In the offline training phase, a mobile device transmits packets from known locations to a base station. At each location, the base station measures the corresponding CSI values. The measured CSI values as a fingerprint of the location are used to train the FNN. The input of the FNN includes the measured CSI values, and the output includes the reconstructed CSI values. The gready learning algorithm <ref type="bibr" target="#b188">[190]</ref> is used to train the FNN that minimizes the error between the measured CSI values and the reconstructed CSI values. In the online localization phase, the reconstructed CSI values are used to estimate the location of the mobile device based on the probabilistic methods. Simulation results show that with the proposed deep learning algorithm, 60% of the positions in the test have errors under 1m, while the baseline algorithm <ref type="bibr" target="#b189">[191]</ref> only ensures that 25% of the tested positions have errors under 1m.</p><p>The FNN can work well as the number of CSI values is small. However, it may be computationally intractable with a large number of CSI values. To address the issue, CNNs can be to learn the features of the wireless channels for the user localization as proposed in <ref type="bibr" target="#b190">[192]</ref>. However, CNN is suitable to process inputs with grid-like structures, e.g., an image. Therefore, the CSI values must be first transformed into socalled channel snapshots. Then, the CNN is trained to output the user locations given the channel snapshots. The analysis shows that the overall complexity of the proposed CNN algorithm is O(K 2 M LN F S 1 S 2 ), where K is the number of convolution Kernels, L is the number of convolutionalactivation-pooling layers, M is the number of the base station's antennas, N F is the number of frequency points, and S 1 and S 2 are the sizes of the Kernels, respectively. As such, the complexity does not depend on the training set size that is one main advantage of using the CNN for the user localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Access Device Detection</head><p>Massive Machine-Type Communication (mMTC) is expected to be one of key technologies in 5G. mMTC enables tens of billions of machine-type devices to communicate with each other with high availability, low latency, and high reliability. However, the mMTC systems only allow the devices to transmit small packets with minimum signaling overhead. Thus, it is challenging for the base stations to detect and identify active devices, i.e., devices to access the base stations. The traditional detection algorithms such as iterative algorithm <ref type="bibr" target="#b191">[193]</ref> can be used. However, they fail to consider the time constraints. Deep learning that is able to provide online decisions can be an alternative solution as proposed in <ref type="bibr" target="#b192">[194]</ref>. The system model consists of a base station and multiple machine-type devices. Each device is assigned with a unique pilot sequence. The signal received at the base station is the sum of signals transmitted from active users. Given the received signal, the base station needs to determine which users are active. The base station uses the FNN in which the input is the received signal, and the output contains the user activity matrix. The FNN is trained in which the training data is randomly generated and the loss function is formulated from the received signal, the user activity matrix and the channel estimations of the users, and the pilot matrix of the users. Simulation results show that the proposed deep learning scheme outperforms the traditional iterative algorithm from <ref type="bibr" target="#b191">[193]</ref> in terms of active user detection success rate.</p><p>Summary: In this section, we review miscellaneous uses of DRL in wireless and networked systems. DRL provides a flexible tool in rich and diversified applications, conventionally involving dynamic system modeling and multi-agent interactions. All these imply a huge space of state transitions and actions. These approaches are summarized along with the references in Table <ref type="table" target="#tab_8">VII</ref>. We observe that the NUM problems in 5G ecosystem for traffic engineering and resource allocation face very diversified control variables, including discrete indicators, e.g., for BS (de)activation, user/cell association, and path selection, as well as continuous variables such as bandwidth allocation, transmit power, and beamforming optimization. Hence, both DQL and policy gradient methods are used extensively for discrete and continuous control problems, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CHALLENGES, OPEN ISSUES, AND FUTURE RESEARCH DIRECTIONS</head><p>Different approaches reviewed in this survey evidently show that DRL can effectively address various emerging issues in communications and networking. There are existing challenges, open issues, and new research directions which are discussed as follows.</p><p>A. Challenges 1) State Determination in Density Networks: The DRL approaches, e.g., <ref type="bibr" target="#b27">[28]</ref>, allow the users to find an optimal access policy without having complete and/or accurate network information. However, the DRL approaches often require the users to report their local states at every time slot. To observe the local state, the user needs to monitor Received Signal Strength Indicators (RSSIs) from its neighboring BSs, and then it temporarily connects to the BS with the maximum RSSI. However, the future networks will deploy a high density of the BSs, and the RSSIs from different BSs may not be different. Thus, it is challenging for the users to determine the temporary BS <ref type="bibr" target="#b193">[195]</ref>.</p><p>2) Knowledge of Jammers' Channel Information: The DRL approach for wireless security as proposed in <ref type="bibr" target="#b126">[128]</ref> enables the UAV to find optimal transmit power levels to maximize the security capacity of the UAV and the BS. However, to formulate the reward of the UAV, a perfect knowledge of channel information of the jammers is required. This is challenging and even impossible in practice.</p><p>3) Multi-agent DRL in Dynamic HetNets: Most of the existing works focus on the customizations of DRL framework for individual network entities, based on locally observed or exchanged network information. Hopefully, the network environment is relatively static to ensure convergent learning results and stable policies. This requirement may be challenged in a dynamic heterogenous 5G network, which consists of hierarchically nested IoT devices/networks with fast changing service requirements and networking conditions. In such a situation, the DQL agents for individual entities have to be light-weighted and agile to the change of network conditions. This implies a reduce to the state and action spaces in learning, which however may compromise the performance of the convergent policy. The interactions among multiple agents also complicate the network environment and cause a considerable increase to the state space, which inevitably slows down the learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Training and Performance Evaluation of DRL Frame-</head><p>The DRL framework requires large amounts of data for both training and performance evaluation. In wireless systems, such data is not easily accessible as we rarely have referential data pools as other deep learning scenarios, e.g., computer vision. Most of the existing works rely on simulated dataset, which undermines the confidence of the DRL framework in practical system. The simulated data set is usually generated by a specific stochastic model, which is a simplification of the real system and may overlook the hidden patterns. Hence, a more effective way for generating simulation data is required to ensure that the training and performance evaluation of the DRL framework are more consistent with practical system. B. Open Issues 1) Distributed DRL Framework in Wireless Networks: The DRL framework requires large amounts of training for DNNs. This may be implemented at a centralized network controller, which has sufficient computational capacity and the capability for information collection. However, for massive end users with limited capabilities, it becomes a meaningful task to design distributed implementation for the DRL framework that decomposes resource-demanding basic functionalities, e.g., information collection, sharing, and DNN training, from reinforcement learning algorithms at individual devices. The basic functionalities can be integrated with the network controller. It remains an open issue for the design of network infrastructure that supports these common functionalities for distributed DRL. The overhead of information exchange between end users and network controller also has to be well controlled.</p><p>2) Balance between Information Quality and Learning Performance: The majority of the existing works consider the orchestration of networking, transmission control, offloading, and caching decisions in one DRL framework to derive the optimal policy, e.g., <ref type="bibr" target="#b91">[93]</ref>- <ref type="bibr" target="#b96">[98]</ref>, <ref type="bibr" target="#b98">[100]</ref>, <ref type="bibr" target="#b99">[101]</ref>. However, from a practical viewpoint, the network system will have to pay substantially increasing cost for information gathering. The cost is incurred from large delay, pre-processing of asynchronous information, excessive energy consumption, reduced learning speed, etc. Hence, an open issue is to find the optimal balance between information quality and learning performance so that the DQL agent does not consume too much resources only to achieve insignificantly marginal increase in the learning performance.</p><p>C. Future Research Directions 1) DRL for Channel Estimation in Wireless Systems: We expect that the combination of Wireless Power Transfer (WPT) and Mobile Crowd Sensing (MCS), namely Wireless-Powered Crowd Sensing (WPCS) will be a promising technique for the emerging IoT services. To this end, a higher power transfer efficiency of WPT is very critical to enable the deployment of WPCS in low-power wide area network. A "large-scale array antenna based WPT" will achieve this goal of higher WPT efficiency, but the channel estimation should be performed with minimal power consumption at a sensor node. This is because of that the sensor must operate with self-powering via WPT from the dedicated energy source, e.g., power beacon, Wi-Fi or small-cell access point, and/or ambient RF sources, e.g., TV tower, Wi-Fi AP and cellular BS. In this regard, the channel estimation based on the receive power measurements at the sensor node is one viable solution, because the receive power can be measured by the passive-circuit power meter with negligible power consumption. DRL can be used for the time-varying wireless channels with temporal correlations over time by taking the receive power measurements from the sensor node as the input for DRL, which will enable the channel estimation for WPT efficiently.</p><p>2) DRL for Crowdsensing Service Optimization: In MCS, mobile users contribute sensing data to a crowdsensing service provider and receive an incentive in return. However, due to limited resources, e.g., bandwidth and energy, the mobile user has to decide on whether and how much data to be uploaded to the provider. Likewise, the provider aiming to maximize its profit has to determine the amount of incentive to be given. The provider's decision depends on the actions of the mobile users. For example, with many mobile users contributing data to the crowdsensing service provider, the provider can lower the incentive. Due to a large state space of a large number of users and dynamic environment, DRL can be applied to obtain an optimal crowdsensing policy similar to <ref type="bibr" target="#b194">[196]</ref>.</p><p>3) DRL for Cryptocurrency Management in Wireless Networks: Pricing and economic models have been widely applied to wireless networks <ref type="bibr" target="#b195">[197]</ref>, <ref type="bibr" target="#b196">[198]</ref>. For example, wireless users pay money to access radio resources or mobile services.</p><p>Alternatively, the users can receive money if they contribute to the networks, e.g., offering a relay or cache function. However, using real money and cash in such scenarios faces many issues related to accounting, security, and privacy. Recently, the concept of cryptocurrency based on the blockchain technology has been introduced and adopted in wireless networks, e.g., <ref type="bibr" target="#b197">[199]</ref>, which has been shown to be a secure and efficient solution. However, the value of cryptocurrency, i.e., token or coin, can be highly dynamic depending on many market factors. The wireless users possessing the tokens can decide to keep or spend the tokens, e.g., for radio resource access and service usage or exchange into real money. In the random cryptocurrency market environment, DRL can be applied to achieve the maximum long-term reward of the cryptocurrency management for wireless users as in <ref type="bibr" target="#b198">[200]</ref>.</p><p>4) DRL for Auction: An auction has been effectively used for radio resource management, e.g., spectrum allocation <ref type="bibr" target="#b199">[201]</ref>. However, obtaining the solution of the auction, e.g., a winner determination problem, can be complicated and intractable when the number of participants, i.e., bidders and sellers, become very large. Such a scenario is typical in nextgeneration wireless networks such as 5G highly-dense heterogeneous networks. DRL appears to be an efficient approach for solving different types of auctions such as in <ref type="bibr" target="#b200">[202]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>This paper has presented a comprehensive survey of the applications of deep reinforcement learning to communications and networking. First, we have presented an overview of reinforcement learning, deep learning, and deep reinforcement learning. Then, we have introduced various deep reinforcement learning techniques and their extensions. Afterwards, we have provided detailed reviews, analyses, and comparisons of the deep reinforcement learning to solve different issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, connectivity preservation, traffic routing, and data collection. Finally, we have outlined important challenges, open issues as well as future research directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>: b(s ) = O(o|s, a, s ) s∈S p(s |s, a)b(s) s ∈S O(o|s, a, s ) s∈S p(s |s, a)b(s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: An illustration of MDP and POMDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (a) Reinforcement learning, (b) Artificial neural network, and (c) Deep Q-learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: FNN and RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: An illustration of CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: An illustration of a basic (a) LSTM and (b) RNN cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Channel selection based on DQL in IoT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Joint channel selection and packet forwarding in IoT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Channel access in energy harvesting-enabled IoT systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Unlicensed channel access in LTE networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Joint user association and channel selection based on DQL in HetNets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: A dynamic adaptive streaming system based on HTTP standard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Data rate control based on DQL in HVFT applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Data/computation offloading models in cellular networks:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 :</head><label>18</label><figDesc>Fig.18: Anti-jamming scheme based on UAV<ref type="bibr" target="#b123">[125]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 :</head><label>19</label><figDesc>Fig. 19: Car-following model with cyber-physical attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 20 :</head><label>20</label><figDesc>Fig. 20: Cyber-physical detection in IoT systems using DQL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 21 :</head><label>21</label><figDesc>Fig. 21: Connectivity preservation of a multi-UAV network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparisons among optimization techniques Non-linear programming Used to address static optimization problems, i.e., optimizes the objective function for one time instant only. To address this optimization, we can analyze the objective function and adopt suitable techniques. For example, if the objective function is quadratic and the constraints are linear, quadratic programming techniques can be used.</figDesc><table><row><cell>Techniques</cell><cell>Problem-solving</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance comparison among DQL algorithms by Google DeepMind.</figDesc><table><row><cell>DQL</cell><cell>Key</cell><cell>Pros</cell><cell>Cons</cell><cell>Apps</cell></row><row><cell>algorithms</cell><cell>features</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DQL [21]</cell><cell>Utilize the</cell><cell>Fast</cell><cell>Over</cell><cell>Suitable to</cell></row><row><cell></cell><cell>DNN to train</cell><cell>convergence</cell><cell>estimation</cell><cell>implement on</cell></row><row><cell></cell><cell>Q-value</cell><cell>and easy to</cell><cell>on action values</cell><cell>MDPs with small</cell></row><row><cell></cell><cell>function</cell><cell>implement</cell><cell></cell><cell>number of actions</cell></row><row><cell>DDQL [26]</cell><cell>Use 2 Q-value</cell><cell>Easy to</cell><cell>Without</cell><cell>Applicable</cell></row><row><cell></cell><cell>functions to</cell><cell>implement and</cell><cell>considering</cell><cell>to almost MDPs</cell></row><row><cell></cell><cell>simultaneously</cell><cell>faster</cell><cell>specially features</cell><cell></cell></row><row><cell></cell><cell>select and evaluate</cell><cell>convergence</cell><cell>of MDPs</cell><cell></cell></row><row><cell></cell><cell>action values</cell><cell>than DQL</cell><cell></cell><cell></cell></row><row><cell>Prioritized</cell><cell>Prioritize</cell><cell>Faster</cell><cell>Require information</cell><cell>Especially</cell></row><row><cell>DDQL [29]</cell><cell>experiences</cell><cell>convergence</cell><cell>about important</cell><cell>effective for MDPs</cell></row><row><cell></cell><cell>in replay</cell><cell>than DQL</cell><cell>experiences in</cell><cell>with prioritized</cell></row><row><cell></cell><cell>memory</cell><cell>and DDQL</cell><cell>replay memory</cell><cell>experiences</cell></row><row><cell>Dueling</cell><cell>Using 2 DNNs to</cell><cell>Much faster</cell><cell>High complexity</cell><cell>Especially</cell></row><row><cell>DDQL [30]</cell><cell>simultaneously</cell><cell>than DQL,</cell><cell>and less efficient</cell><cell>effective to deal</cell></row><row><cell></cell><cell>estimate the</cell><cell>DDQL and</cell><cell>on MDPs with</cell><cell>with MDPs</cell></row><row><cell></cell><cell>action and state</cell><cell>prioritized</cell><cell>small action</cell><cell>with large action</cell></row><row><cell></cell><cell>value functions</cell><cell>DDQL</cell><cell>and state spaces</cell><cell>and state spaces</cell></row><row><cell>Asynchronous</cell><cell>Using multiple</cell><cell>Learning speed</cell><cell>High complexity</cell><cell>Especially</cell></row><row><cell>DQL [31]</cell><cell>agents to train</cell><cell>is extremely</cell><cell>with intensive</cell><cell>effective for</cell></row><row><cell></cell><cell>the DNN in</cell><cell>faster by using</cell><cell>requirements on</cell><cell>MDPs with very</cell></row><row><cell></cell><cell>parallel</cell><cell>multiple agents</cell><cell>hardware devices</cell><cell>large state and</cell></row><row><cell></cell><cell></cell><cell>for training</cell><cell>for training</cell><cell>action spaces</cell></row><row><cell>Distributional</cell><cell>Use a distribution</cell><cell>More</cell><cell>Need to know</cell><cell>Suitable to</cell></row><row><cell>DQL [33]</cell><cell>function (instead</cell><cell>accuracy in</cell><cell>the distribution</cell><cell>implement on</cell></row><row><cell></cell><cell>of expectation)</cell><cell>evaluating</cell><cell>of reward function</cell><cell>MDPs with available</cell></row><row><cell></cell><cell>to update</cell><cell>Q-value</cell><cell>over state and</cell><cell>distribution</cell></row><row><cell></cell><cell>Q-value function</cell><cell>function</cell><cell>action spaces</cell><cell>reward function</cell></row><row><cell>Noisy Nets</cell><cell>Adding the</cell><cell>Improve</cell><cell>Efficiency of</cell><cell>Especially</cell></row><row><cell>DQL [34]</cell><cell>Gaussian noise</cell><cell>efficiency in</cell><cell>adding Gaussian</cell><cell>effective for</cell></row><row><cell></cell><cell>layer to the DNN</cell><cell>exploring the</cell><cell>noise layer is</cell><cell>MDPs with very</cell></row><row><cell></cell><cell>for training</cell><cell>environment</cell><cell>still debating</cell><cell>large state and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>action spaces</cell></row><row><cell>Rainbow [35]</cell><cell>Combine</cell><cell>Inherit all</cell><cell>Extremely high</cell><cell>Only suitable for</cell></row><row><cell></cell><cell>features of all</cell><cell>advantages of</cell><cell>complexity with</cell><cell>MDPs with large</cell></row><row><cell></cell><cell>aforementioned</cell><cell>aforementioned</cell><cell>many requirements</cell><cell>state/action spaces</cell></row><row><cell></cell><cell>algorithms</cell><cell>algorithms</cell><cell>of MDPs</cell><cell>and some properties</cell></row><row><cell></cell><cell></cell><cell></cell><cell>in advance</cell><cell>known in advance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>A summary of approaches using DQL for network access and adaptive rate control.</figDesc><table><row><cell cols="3">ISSUES REF. MODEL</cell><cell>LEARNING ALGORITHMS</cell><cell>AGENT</cell><cell>STATES</cell><cell>ACTIONS</cell><cell>REWARDS</cell><cell>NETWORKS</cell></row><row><cell></cell><cell>[44]</cell><cell>POMDP</cell><cell>DQN using FNN</cell><cell>Sensor</cell><cell>Past channel selections and observations</cell><cell>Channel selection</cell><cell>Score +1 or -1</cell><cell>IoT</cell></row><row><cell></cell><cell>[49]</cell><cell>MDP</cell><cell>DQN using FNN</cell><cell>Sensor</cell><cell>Current buffer state and channel state</cell><cell>Channel, packets, mode selection and modulation</cell><cell>Ratio of number of to transmit power transmitted packets</cell><cell>IoT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Channel access history,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>[51]</cell><cell>MDP</cell><cell>DQN with LSTM</cell><cell>Base station</cell><cell>predicted and true battery information history, and</cell><cell>Sensor selection for channel access</cell><cell>Total rate and prediction error</cell><cell>IoT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>current CSI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>access Network</cell><cell>[53] [54]</cell><cell>MDP Game</cell><cell>DQN with LSTM DQN with LSTM</cell><cell>V2V transmitter Small base station</cell><cell>Current CSI, past interference, past channel selections, and remaining time to meet the latency constraints Traffic history of small base stations and the WLAN</cell><cell>Channel and transmit power selection Channel selection and channel access probability</cell><cell>Capacity and latency Throughput</cell><cell>IoT LTE network</cell></row><row><cell></cell><cell>[27]</cell><cell>Game</cell><cell>DDQN and dueling DQN</cell><cell>Mobile user</cell><cell>Past channel selections and observations</cell><cell>Channel selection</cell><cell>Data rate</cell><cell>CRN</cell></row><row><cell></cell><cell>[58]</cell><cell>MDP</cell><cell>DQN with CNN</cell><cell>Satellite system</cell><cell>Current user terminals, channel allocation matrix, and the new arrival user</cell><cell>Channel selection</cell><cell>Score +1 or -1</cell><cell>Satellite system</cell></row><row><cell></cell><cell>[28]</cell><cell>MDP</cell><cell>DDQN and dueling DQN</cell><cell>Mobile user</cell><cell>QoS states</cell><cell>Base station and channel selection</cell><cell>Utility</cell><cell>HetNet</cell></row><row><cell></cell><cell>[60]</cell><cell>Game</cell><cell>DQN with LSM</cell><cell>UAV</cell><cell>Content request distribution</cell><cell>Base station selection</cell><cell>Users with stable queues</cell><cell>LTE network</cell></row><row><cell></cell><cell>[66]</cell><cell>MDP</cell><cell>DQN with LSTM and peephole connections</cell><cell>Client</cell><cell>Last segment quality, current buffer state, rebuffering time, and channel capacities</cell><cell>Bitrate selection for segment</cell><cell>Video quality, rebuffering even, and buffer state</cell><cell>DASH system</cell></row><row><cell></cell><cell>[68]</cell><cell>MDP</cell><cell>DQN with A3C</cell><cell>Client</cell><cell>Last segment quality, current buffer state, rebuffering time, and channel capacities</cell><cell>Bitrate selection for segment</cell><cell>Video quality, rebuffering even, and buffer state</cell><cell>DASH system</cell></row><row><cell>Rate control</cell><cell>[71] [73]</cell><cell>MDP MDP</cell><cell>DQN with CNN and RNN DQN using A3C and LSTM</cell><cell>Client Base station</cell><cell>Predicted video quality, current buffer state, rebuffering time, and channel capacities Congestion metric, current network connections, and cell efficiency</cell><cell>Bitrate selection for segment Traffic rate decisions for mobile users</cell><cell>Video quality, rebuffering even, and buffer state HVFT traffic, traffic loss to existing applications, and the amount of served</cell><cell>DASH system HVFT application</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bytes</cell><cell></cell></row><row><cell></cell><cell>[75]</cell><cell>MDP</cell><cell>DQN using FNN</cell><cell>Base station</cell><cell>Measurements of BER, throughput, spectral efficiency, power consumption, and transmit power efficiency</cell><cell>Symbol rate, encoding rate energy per symbol, modulation mode, number of bits per symbol, and</cell><cell>Same as the state</cell><cell>Space com-munication system</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>A summary of approaches using DQL for caching and offloading.</figDesc><table><row><cell cols="3">ISSUES REF. MODEL</cell><cell>LEARNING ALGORITHMS</cell><cell>AGENT</cell><cell>STATES</cell><cell>ACTIONS</cell><cell>REWARDS</cell><cell>NETWORKS</cell></row><row><cell></cell><cell>[78]</cell><cell>MDP</cell><cell>DQN using actor-critic, DDPG</cell><cell>Base station</cell><cell>Cached contents and requested content</cell><cell>Replace selected content or not</cell><cell>Cache hit rate (score 1 or 0)</cell><cell>CRN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>User association,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>[93]</cell><cell>MDP</cell><cell>DQN using FNN</cell><cell>Base station</cell><cell>Channel states and computational capabilities</cell><cell>computational unit, content</cell><cell>Energy consumption</cell><cell>CRN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>delivery</cell><cell></cell><cell></cell></row><row><cell>Wireless proactive caching</cell><cell>[82] [85] [86] [87] [88] [90]</cell><cell>MDP MDP MDP MDP MDP</cell><cell>DQN using NAFs DQN using FNN DQN using CNN DDQN DQN using LSM and ESN</cell><cell>Cloud database Central scheduler Central scheduler Service provider Base station</cell><cell>Encoding of a query, query cache miss rate Channel coefficients, cache state Channel coefficients, cache state Conditions of cache nodes, transmission rates of content chunks Historical content request</cell><cell>Cache expiration times Active users and resource allocation Active users and resource allocation The content to cache and to remove User association, cached contents and formats</cell><cell>Cache hit rates, CDN utilization Network throughput Network throughput Network cost, QoE Reliability</cell><cell>Cloud database MU MIMO system MU MIMO system Content centric IoT Cellular system</cell></row><row><cell></cell><cell>[95] [98]</cell><cell>MDP</cell><cell>DQN using CNN</cell><cell>Service provider</cell><cell>Available BS, MEC, and cache</cell><cell>User association, caching, and offloading</cell><cell>Composite revenue</cell><cell>Vehicular ad hoc network</cell></row><row><cell></cell><cell>[94]</cell><cell>MDP</cell><cell>DQN using FNN</cell><cell>Service provider</cell><cell>Available BS, MEC, and cache</cell><cell>User association, caching, and offloading</cell><cell>Composite revenue</cell><cell>Vehicular ad hoc network</cell></row><row><cell></cell><cell>[96]</cell><cell>MDP</cell><cell>DDQN and dueling DQN</cell><cell>Service provider</cell><cell>Available BS, MEC, and cache</cell><cell>User association, caching, and offloading</cell><cell>Composite revenue</cell><cell>Vehicular ad hoc network</cell></row><row><cell></cell><cell>[100]</cell><cell>MDP</cell><cell>DQN using CNN</cell><cell>Base station</cell><cell>Channel state, computational capability, content/version indicator, and the trust value</cell><cell>User association, caching, and offloading</cell><cell>Revenue</cell><cell>Mobile social network</cell></row><row><cell></cell><cell>[103]</cell><cell>MDP</cell><cell>DQN using CNN</cell><cell>Mobile user</cell><cell>User's location and remaining file size</cell><cell>Idle, transmit via WLAN or cellular network</cell><cell>Total data rate</cell><cell>Cellular system</cell></row><row><cell>Data and computation offloading</cell><cell>[104] [105] [106] [110]</cell><cell>MDP MDP MDP Game</cell><cell>Q-learning DQN using FNN DQN using FNN DDQN, SARSA DQN using CNN, hotbooting</cell><cell>Base station Mobile user Mobile user Mobile user</cell><cell>Sum of cost and computational capacity of the MEC server Channel qualities, states of energy and task queues Channel qualities, states of energy and task queues Channel states, size of App traces</cell><cell>Offloading decision and resource allocation Offloading and resource allocation Offloading decision and computational resource allocation Offloading rate</cell><cell>the transmission cost Sum of cost of delay and energy consumption Long term cost function Long term cost function Utility related to detection accuracy, response speed, and</cell><cell>Cellular system Cellular system Cellular system Cellular system</cell></row><row><cell></cell><cell>[119]</cell><cell>MDP</cell><cell>DDQN</cell><cell>Fog node</cell><cell>Delay, container's location and resource allocation</cell><cell>Container's next location</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>A summary of approaches using DQL for network security and connectivity preservation.</figDesc><table><row><cell cols="3">ISSUES REF. MODEL</cell><cell>LEARNING ALGORITHMS</cell><cell>AGENT</cell><cell>STATES</cell><cell>ACTIONS</cell><cell>REWARDS</cell><cell>NETWORKS</cell></row><row><cell></cell><cell>[121]</cell><cell>Game</cell><cell>DQN using CNN</cell><cell>Secondary user</cell><cell>Number of PUs and signal SINR</cell><cell>Channel selection and leaving decision</cell><cell>SINR and mobility cost</cell><cell>CRN</cell></row><row><cell>security</cell><cell>[122] [123]</cell><cell>Game MDP</cell><cell>DQN using CNN DQN using RCNN</cell><cell>Receiving transducer SU</cell><cell>Signal SINR Signal SINR</cell><cell>Staying and leaving decisions Channel selection</cell><cell>SINR and mobility cost SINR and mobility cost</cell><cell>Underwater acoustic network CRN</cell></row><row><cell>Network</cell><cell>[124] [125]</cell><cell>MDP MDP</cell><cell>DQN using CNN DQN using CNN</cell><cell>Transmit IoT device Relay UAV</cell><cell>Signal SINR Signal SINR and BER</cell><cell>Channel selection Relay power</cell><cell>SINR and energy consumption cost SINR and relay cost</cell><cell>IoT UAV</cell></row><row><cell></cell><cell>[128]</cell><cell>MDP</cell><cell>DQN using CNN</cell><cell>Transmit UAV</cell><cell>Jamming power</cell><cell>Transmit power</cell><cell>Secrecy capacity and cost energy consumption</cell><cell>UAV</cell></row><row><cell></cell><cell>[131]</cell><cell>Game</cell><cell>DQN using LSTM units</cell><cell>Autonomous vehicle</cell><cell>Deviation values</cell><cell>Measurement weight selection</cell><cell>Safe spacing deviation</cell><cell>ITS</cell></row><row><cell></cell><cell>[135]</cell><cell>Game</cell><cell>DQN using LSTM units</cell><cell>Cloud</cell><cell>Attack actions on IoT devices</cell><cell>IoT device set selection</cell><cell>IoT devices' data values</cell><cell>IoT</cell></row><row><cell>Connectivity preservation</cell><cell>[137] [138] [140] [32]</cell><cell>MDP MDP POMDP MDP</cell><cell>DQN using FNN DQN using A3C DQN using A3C DQN using A3C and LSTM</cell><cell>Ground base station Ground base station Ground base station Mobile users</cell><cell>Relative positions and the velocity of robots Relative positions and the velocity of robots Information of distances among robots Reference signal received quality and the last action</cell><cell>Velocity decision Velocity decision Turning left and turning right decisions Serving SBS selection</cell><cell>Sore +1 and -1 Sore +1 and -1 Sore +1 and -1 Data rate and energy consumption</cell><cell>Robot system Robot system Robot system Ultra-dense network</cell></row><row><cell></cell><cell>[142]</cell><cell>MDP</cell><cell>DQN using CNN</cell><cell>MBS</cell><cell>The number of active alarms</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>A summary of applications of DQL for traffic engineering, resource scheduling, and data collection.</figDesc><table><row><cell cols="3">ISSUES REF. MODEL</cell><cell>LEARNING ALGORITHMS</cell><cell>AGENT</cell><cell>STATES</cell><cell>ACTIONS</cell><cell>REWARDS</cell><cell>SCENARIOS</cell></row><row><cell>Traffic engineering and routing</cell><cell>[143] [146] [140] [151] [152] [153]</cell><cell>MDP NUM POMDP Game MDP</cell><cell>DQN using actor-critic networks DQN using actor-critic networks DQN using actor-critic networks DQN using ESN DQN using FNN</cell><cell>Network controller Network controller UAV UAV Train scheduler</cell><cell>Bandwidth request of each node pair Throughput and delay performance Local sensory information, e.g., distances and angles Coordinates, distances, and orientation angles Channel conditions, train position, speed, SNR, and handoff indicator</cell><cell>Traffic load split on different paths Traffic load split on different paths Turn left or right Path, transmit power, and cell association Making handoff of connection, or accelerate or decelerate the train</cell><cell>Mean network delay α-fairness utility Composite reward Weighted sum of energy efficiency, latency, and interference Tracking error and energy consumption</cell><cell>5G network 5G network UAV navigation Cellular-connected UAVs Vehicle-to-infrastructure system</cell></row><row><cell>sharing and scheduling Resource</cell><cell>[155] [160] [163] [166]</cell><cell>MDP MDP MDP MDP</cell><cell>DQN using FNN DQN with CNN DQN using FNN DQN using actor-critic networks</cell><cell>Cloud baseband unit Network controller Network controller Central scheduler</cell><cell>MUs' demands and the RRHs' working states Network topology, QoS/QoE status, and the QoS requirements The number of arrived packets/the priority and time-stamp of flows Current scheduling decision and the workload</cell><cell>Turn on or off certain RRH(s), and beamforming allocation Successive VNF instance Bandwidth/SFC allocation Assignment of each thread</cell><cell>Expected power consumption Composite function of QoE gain, QoS constraints penalty, and OPEX penalty Weighted sum of spectrum efficiency and QoE/waiting time in SFCs Average processing time</cell><cell>Cloud RAN Cellular system 5G network Distributed stream data processing</cell></row><row><cell></cell><cell>[168]</cell><cell>MDP</cell><cell>DQN using FNN</cell><cell>Secondary user</cell><cell>Received signal strength at individual sensors</cell><cell>Transmit power</cell><cell>Fixed reward if QoS satisfied</cell><cell>CRN</cell></row><row><cell>Data collection</cell><cell>[175] [179] [181]</cell><cell>MDP MDP Game</cell><cell>DRQN, LSTM, transfer learning DQN using CNN DQN using CNN</cell><cell>Mobile sensors UAV and unmanned vehicle Crowdsensing server</cell><cell>Cell selection matrices Subregions' sample priority, charging point's location, and trace of the UAV and unmanned vehicle Previous sensing quality and payment policy</cell><cell>Next cell for sensing Moving direction of the UAV and unmanned vehicle Current payment policy</cell><cell>A function of the sensing quality and cost Fixed reward related to subregions' sample priority Utility</cell><cell>WSN UAV and vehicle Mobile crowdsens-ing</cell></row><row><cell></cell><cell>[182]</cell><cell>Game</cell><cell>DDQN</cell><cell>Mobile users</cell><cell>Current preferences</cell><cell>Positive or negative altitude</cell><cell>-</cell><cell>Mobile social network</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Remind that DQN is the core of the DQL algorithms.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing RGANS1906, WASP/NTU M4082187 (4080), Singapore MOE Tier 1 under Grant 2017-T1-002-007 RG122/17, MOE Tier 2 under Grant MOE2014-T2-2-015 ARC4/15, Singapore NRF2015-NRF-ISF001-2277, and Singapore EMA Energy Resilience under Grant NRF2017EWT-EP003-041, in part by the National Research Foundation of Korea (NRF) Grant funded by the Korean Government under Grants 2014R1A5A1011478 and 2017R1A2B2003953, in part by the National Natural Science Foundation of China under Grants 61631005, U1801261, and 61571100, and in part by NSFC under Grant 61601449 and the Shenzhen Talent Peacock Plan Program under Grant KQTD2015071715073798.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Google achieves ai &quot;breakthrough&quot; by beating go champion</title>
		<ptr target="https://www.bbc.com/news/technology-35420579" />
	</analytic>
	<monogr>
		<title level="j">BBC</title>
		<imprint>
			<date type="published" when="2016-01">2016. Jan.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic programming and optimal control</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena scientific Belmont</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>MA</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dynamic programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Courier Corporation</publisher>
			<pubPlace>Mineola, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: An overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A brief survey of deep reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine learning and deep learning methods for cybersecurity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">State-of-the-art deep learning: Evolving machine intelligence toward tomorrow&apos;s intelligent network traffic control systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fadlullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Akashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mizutani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2432" to="2455" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for intelligent wireless networks: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2595" to="2621" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Machine learning for wireless networks with artificial intelligence: A tutorial on neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02913</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on applications of model-free strategy learning in cognitive wireless networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kwasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1717" to="1757" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">State of the art-a survey of partially observable markov decision processes: theory, models, and algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Monahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic games</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1953">1953</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adaptive step-sizes for reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Dabney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An efficient deep reinforcement learning model for urban traffic control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01876</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3389" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Issues in using function approximation for reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Connectionist Models Summer School</title>
		<meeting>Connectionist Models Summer School<address><addrLine>Hillsdale, NJ. Lawrence Erlbaum</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Double q-learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2094" to="2100" />
			<date type="published" when="2016-02">Feb. 2016</date>
			<pubPlace>Phoenix, AZ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep multi-user reinforcement learning for dynamic spectrum access in multichannel wireless networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Naparstek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02613</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for user association and resource allocation in heterogeneous networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE GLOBECOM</title>
		<meeting><address><addrLine>Abu Dhabi, UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prioritized experience replay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York City, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Handover control in wireless systems via asynchronous multi-user deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02077</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06887</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Noisy networks for exploration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<pubPlace>San Juan, Puerto Rico, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep recurrent q-learning for partially observable mdps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno>CoRR, abs/1507.06527</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with experience replay based on sarsa</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence (SSCI)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards cooperation in sequential prisoner&apos;s dilemmas: a deep multiagent reinforcement learning approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00162</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from self-play in imperfect-information games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01121</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint resource allocation and user association for heterogeneous wireless cellular networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fooladivanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="257" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimizing user association and spectrum allocation in hetnets: A utility perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1025" to="1039" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dynamic multichannel access</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computing, Networking and Communications (ICNC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On myopic sensing for multichannel opportunistic access: structure, optimality, and performance</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5431" to="5440" />
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1312.5602</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Tutornet: A low power wireless iot testbed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
		<ptr target="http://anrg.usc.edu/www/tutornet/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dynamic multichannel access in wireless networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive Communications and Networking</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A new deep-q-learningbased transmission scheduling mechanism for the cognitive internet of things</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Strategy iteration algorithms for games and markov decision processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fearnley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>University of Warwick</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Reinforcement learning based multi-access control and battery prediction with energy harvesting in iot systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05929</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Optimal resource allocation in wireless powered communication networks with user cooperation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7936" to="7949" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for resource allocation in v2v communications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00968</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Proactive resource management in lte-u systems: A deep learning perspective</title>
		<author>
			<persName><forename type="first">U</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07031</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ibm watson research center</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Castro</surname></persName>
		</author>
		<ptr target="https://crawdad.org/ibm/watson/20030219" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multiagent learning for aloha-like spectrum access in cognitive radio systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Wireless Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning based dynamic channel allocation algorithm in multibeam satellite systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACCESS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">742</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Resource allocation and inter-cell interference management for dual-access small cells</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Elsherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1082" to="1096" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Liquid state machine learning for resource allocation in a network of cache-enabled lte-u uavs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Liquid state machines: motivation, theory, and applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computability in context: computation and logic in the real world</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="275" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Caching in the sky: Proactive deployment of cache-enabled unmanned aerial vehicles for optimized quality-of-experience</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1046" to="1061" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reinforcement learning with echo state networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gyenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lőrincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="830" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Tyouku of china network video index</title>
		<ptr target="http://index.youku.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dynamic adaptive streaming over http-: standards and design principles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stockhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second annual ACM conference on Multimedia systems</title>
		<meeting>the second annual ACM conference on Multimedia systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">D-dash: A deep q-learning framework for dash video streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gadaleta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chiariotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zanella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="703" to="718" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Evalvid-a framework for video transmission and quality evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klaue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rathke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolisz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on modelling techniques and tools for computer performance evaluation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="255" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural adaptive video streaming with pensieve</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the ACM Special Interest Group on Data Communication</title>
		<meeting>the Conference of the ACM Special Interest Group on Data Communication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Commute path bandwidth traces from 3g networks: analysis and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Riiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vigmostad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Griwodz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Multimedia Systems Conference</title>
		<meeting>the 4th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="114" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A control-theoretic approach for dynamic adaptive video streaming over http</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinopoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="338" />
			<date type="published" when="2015">2015</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Qarc: Video quality aware rate control for real-time video streaming based on deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02482</idno>
	</analytic>
	<monogr>
		<title level="m">Measuring fixed broadband report</title>
		<imprint>
			<date type="published" when="2016">2018. 2016</date>
			<biblScope unit="volume">72</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cellular network traffic scheduling with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chinchali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sachin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Cache-enabled dynamic rate allocation via deep self-transfer reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11334</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning for cognitive satellite communications using deep neural network ensembles</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paffenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Wyglinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Bilén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Reinhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mortensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Adaptive coding and modulation techniques for next generation hand-held mobile satellite communications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Corazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vanelli-Coralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE ICC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="4504" to="4508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Training feedforward networks with the marquardt algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Menhaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="993" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">A deep reinforcement learning-based framework for content caching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velipasalar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08132</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>abs/1509.02971</idno>
		<ptr target="http://arxiv.org/abs/1509.02971" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Reinforcement learning in large discrete action spaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coppin</surname></persName>
		</author>
		<idno>abs/1512.07679</idno>
		<ptr target="http://arxiv.org/abs/1512.07679" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A deep learning approach for optimizing content delivering in cache-enabled HetNet</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatzinotas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Sym. Wireless Commun. Systems (ISWCS)</title>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page" from="449" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Learning runtime parameters in computer systems with delayed experience injection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gessert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yoneki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09903</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in proc. 1st ACM Sym. Cloud Comput</title>
		<imprint>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">On the benefits of edge caching for mimo interference alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deghel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bastug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="655" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Cache-enabled wireless networks with opportunistic interference alignment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09024</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Optimization of cache-enabled opportunistic interference alignment wireless networks: A big data deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep-reinforcement-learning-based optimization for cacheenabled opportunistic interference alignment wireless networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">445</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Green resource allocation based on deep reinforcement learning in contentcentric iot</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computing</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">CodingCache: Multipath-aware CCN cache with network coding</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Workshop on Information-centric Networking</title>
		<imprint>
			<biblScope unit="page" from="41" to="42" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Echo-liquid state deep learning for 360 content transmission and caching in wireless vr networks with cellular-connected uavs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03284</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Machine learning for wireless networks with artificial intelligence: A tutorial on neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<idno>abs/1710.02913</idno>
		<ptr target="http://arxiv.org/abs/1710.02913" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Liquid state machine learning for resource allocation in a network of cache-enabled LTE-U UAVs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE GLOBECOM</title>
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A big data deep reinforcement learning approach to next generation green wireless networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE GLOBECOM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Resource allocation in software-defined and information-centric vehicular networks with mobile edge computing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vehicular Technology Conference (VTC-Fall)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning (drl)-based resource management in software-defined and virtualized vehicular ad hoc networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boukerche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications</title>
		<meeting>the 6th ACM Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Integrated networking, caching, and computing for connected vehicles: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Mobility-aware edge caching and computing framework in vehicle networks: A deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Software-defined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Network function virtualization: Challenges and opportunities for innovations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Secure social networks in 5g systems with mobile edge computing, caching and device-to-device (d2d) communications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="103" to="109" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Trust-based social networks with computing, caching and communications: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cost-and energy-aware multi-flow mobile data offloading using markov decision process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Communications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning based approach for cost-and energy-aware multiflow mobile data offloading</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Communications</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning based computation offloading and resource allocation for mec</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tiejun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yueming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE WCNC</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Performance optimization in mobile-edge computing via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00514</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Optimized computation offloading performance in virtual edge computing systems via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">DRAG: Deep reinforcement learning based base station activation in heterogeneous networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02159</idno>
		<imprint>
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Deep q-learning based dynamic resource allocation for self-powered ultra-dense networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICC (ICC Workshops)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deepnap: Data-driven base station sleeping operations through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Reinforcement learning based mobile offloading for cloud-based malware detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Security in mobile edge caching with reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
		<idno>abs/1801.05915</idno>
		<ptr target="http://arxiv.org/abs/1801.05915" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Malware detection on mobile devices using distributed machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Shamili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alpcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. Int&apos;l Conf. Pattern Recognition</title>
		<meeting>Int&apos;l Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="4348" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Mobile cloud offloading for malware detections with learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM Workshops</title>
		<imprint>
			<date type="published" when="2015-04">Apr. 2015</date>
			<biblScope unit="page" from="197" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Learning-based computation offloading for iot devices with energy harvesting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08768</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A novel two-layered reinforcement learning for task offloading with tradeoff between physical machine utilization rate and delay</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Quality of service aware computation offloading in an ad-hoc mobile cloud</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning based offloading scheme in ad-hoc mobile clouds</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE INFOCOM IECCO Workshop</title>
		<meeting>IEEE INFOCOM IECCO Workshop<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">apr 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Computation offloading for mobile edge computing: A deep learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Langar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PIMRC</title>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Migration modeling and learning algorithms for containers in fog computing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Strategies for adaptive frequency hopping in the unlicensed bands</title>
		<author>
			<persName><forename type="first">P</forename><surname>Popovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Two-dimensional anti-jamming communication based on deep reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Anti-jamming underwater transmission with mobility and learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Letters</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Anti-jamming communications using spectrum waterfall: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anpalagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Letters</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Dqn-based power control for iot transmission against jamming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 87th Vehicular Technology Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
		<respStmt>
			<orgName>VTC Spring</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Uav-aided 5g communications with deep reinforcement learning against jamming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06628</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Uav relay in vanets against smart jamming with reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4087" to="4097" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Anti-jamming power control game in unmanned aerial vehicle networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE GLOBECOM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">User-centric view of unmanned aerial vehicle transmission against smart attacks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3420" to="3430" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Multiagent learning using a variable learning rate</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Cyber-physical attacks with control objectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1418" to="1425" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Robust deep reinforcement learning for security and safety in autonomous vehicle systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferdowsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Mandayam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00983</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Car-following: a historical review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brackstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part F: Traffic Psychology and Behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="181" to="196" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Deep learning-based dynamic watermarking for secure signal authentication in the internet of things</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferdowsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICC</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Dynamic watermarking: Active defense of networked cyber-physical systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Satchidanandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="219" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Deep learning for signal authentication and security in massive internet of things systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferdowsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00916</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Evolutionary artificial potential fields and their application in real time robot path planning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vadakkepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ming-Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 Congress on Evolutionary Computation</title>
		<meeting>the 2000 Congress on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Deep q-learning to preserve connectivity in multi-robot systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Signal Processing Systems</title>
		<meeting>the 9th International Conference on Signal Processing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning approach to preserve connectivity for multi-robot systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress on Image and Signal Processing</title>
		<imprint>
			<publisher>CISP-BMEI</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Collisionfree formation control with decentralized connectivity preservation for nonholonomic-wheeled mobile robots</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Poonawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Satici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Spong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on control of Network Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="130" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Autonomous navigation of uav in large-scale unknown complex environment with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<editor>IEEE GlobalSIP</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="858" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A non-stochastic learning approach to energy efficient mobility management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3854" to="3868" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Deep q-learning for self-organizing networks fault management and radio performance improvement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brian</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.02329" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">A deep-reinforcement learning approach for software-defined networking routing optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stampa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez-Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Muntes-Mulero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cabellos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07080</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">the synthesis of internet traffic matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roughan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.02913" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="93" to="96" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">An overview of the OMNeT++ simulation environment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hornig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. Int&apos;l Conf. Simulation Tools and Techniques for Communications</title>
		<meeting>Int&apos;l Conf. Simulation Tools and Techniques for Communications</meeting>
		<imprint>
			<publisher>Networks and Systems &amp; Workshops</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Experience-driven networking: A deep reinforcement learning based approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05757</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">TCP ex Machina: Computergenerated congestion control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Winstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Modeling and Tools for Network Simulation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>ch. The ns-3 Network Simulator</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">BRITE: an approach to universal topology generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lakhina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MASCOTS</title>
		<imprint>
			<date type="published" when="2001-08">Aug. 2001</date>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in proc. 12th Int&apos;l Conf. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1057" to="1063" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for interference-aware path planning of cellular connected uavs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bettstetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICC</title>
		<meeting><address><addrLine>Kansas City, MO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Cellular-connected uavs over 5g: Deep reinforcement learning for interference management</title>
		<author>
			<persName><forename type="first">U</forename><surname>Challita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bettstetter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05500</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Communication-based train control system performance optimization using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10" to="705" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Decco: Deep-learning enabled coverage and capacity optimization for massive mimo systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning based framework for power-efficient resource allocation in cloud rans</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gursoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Implementation of a space communications cognitive engine</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Bilén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Wyglinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Reinhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Communications for Aerospace Applications Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Fractional programming for communication systemsÑpart i: Power control and beamforming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2616" to="2630" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for distributed dynamic power allocation in wireless networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00490</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Network slicing in 5g: Survey and challenges</title>
		<author>
			<persName><forename type="first">X</forename><surname>Foukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Patounas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmokashfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Marina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Reinforcement learning based qos/qoe-aware service function chaining in software-driven 5g slices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02099</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">The logarithmic nature of qoe and the role of the weber-fechner law in qoe assessment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Reichl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'alconzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICC</title>
		<meeting><address><addrLine>Cape Town, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">A generic quantitative relationship between quality of experience and quality of service</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hossfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tran-Gia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="36" to="41" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for network slicing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06591</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">A deep-learning-based radio resource assignment technique for 5g ultra dense networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Fadlullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Resource management with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM Workshop on Hot Topics in Networks</title>
		<meeting>the 15th ACM Workshop on Hot Topics in Networks</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Model-free control for distributed stream data processing using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning in large discrete action spaces</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coppin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07679</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Intelligent power control for spectrum sharing in cognitive radios: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07365</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Online energy-efficient power control in wireless networks by deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zappone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Globally optimal energy-efficient power control and receiver design in wireless networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zappone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Björnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jorswieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2844" to="2859" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Deep learning power allocation in massive mimo</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zappone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03640</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">A bisection method for computing the h • norm of a transfer matrix and related problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kabamba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Massive mimo has unlimited capacity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Björnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoydis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sanguinetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="590" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Design and implementation of a simulation system based on deep q-network for mobile actor node control in wireless sensor and actor networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Obukata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Information Networking and Applications Workshops</title>
		<imprint>
			<publisher>WAINA</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Cell selection with deep reinforcement learning in sparse mobile crowdsensing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07047</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Ccs-ta: quality-guaranteed online task allocation in compressive crowdsensing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="683" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">SensorScope: Application-specific sensor network for environmental monitoring</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ingelrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barrenetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Couach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parlange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Sensor Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">U-Air: when urban air quality inference meets big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Int&apos;l Conf. Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Learningbased energy-efficient data collection by unmanned vehicles in smart cities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1666" to="1676" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Bracciale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loreti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabuffi</surname></persName>
		</author>
		<ptr target="http://crawdad.org/roma/taxi/20140717" />
		<title level="m">CRAWDAD dataset roma/taxi</title>
		<imprint>
			<date type="published" when="2014-07-17">2014. Jul. 2014-07-17</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A secure mobile crowdsensing game with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="47" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Social behavior study under pervasive social networking based on decentralized deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="72" to="81" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Semisupervised deep reinforcement learning in support of iot and smart city services</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Fuqaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="624" to="635" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Deep learning for super-resolution channel estimation and doa estimation based massive mimo system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Power of deep learning for channel estimation and signal detection in ofdm systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Communications Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="117" />
			<date type="published" when="2018-02">Feb 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<title level="m" type="main">User association and load balancing for massive mimo through deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zappone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06905</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Branch-price-and-cut algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lübbecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Operations Research and Management Science</title>
		<meeting><address><addrLine>Chichester</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Csi-based fingerprinting for indoor localization: A deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="763" to="776" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Greedy layerwise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Fifs: Fine-grained indoor fingerprinting system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCCN</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for massive mimo fingerprint-based positioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leitinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sarajlic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tufvesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Symposium on Personal, Indoor, and Mobile Radio Communications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsaig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1094" to="1121" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Deep learning based fast multiuser detection for massive machine-type communication</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00967</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Aif: An artificial intelligence framework for smart wireless network management</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="403" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Crowdsensing game with demand uncertainties: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Resource management in cloud networking using economic analysis and pricing models: a survey</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="954" to="1001" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Data collection and wireless communication in internet of things (iot) using economic analysis and pricing models: A survey</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2546" to="2590" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Oppay: Design and implementation of a payment system for opportunistic data services</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Distributed Computing Systems</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="1618" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Cryptocurrency portfolio management with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems Conference (IntelliSys)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="905" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Applications of economic and pricing models for resource management in 5g wireless networks: A survey</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Nguyen Cong Luong received the B.E. degree in electronic and telecommunication engineering from the Hanoi</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00259</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep reinforcement learning for sponsored search real-time bidding. His research interest includes the Internet of Things. IoT</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">He has been the Guest Lead Editor for a special issue in IEEE Transactions on Cognitive Communications and Networking. He received the MAC and cross-layer design Best Paper Award in IEEE WCNC 2019. His research interests include IoT, mobile edge computing, wireless communications and networking. Dusit Niyato (M&apos;09-SM&apos;15-F&apos;17) is currently a Professor in the School of Computer Science and Engineering</title>
		<author>
			<persName><forename type="first">Thai</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He is an Exemplary Reviewer of IEEE Transactions on Communications in 2018 and an Exemplary Reviewer of IEEE Transactions on Wireless Communications in 2017 and 2018. Currently, he is an Editor of IEEE Wireless Communications Letters and IEEE Transactions on Cognitive Communications and Networking</title>
		<meeting><address><addrLine>Singapore; Wuhan, China; Singapore; Shenzhen, China; Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Communications Society (ComSoc) Green Communications &amp; Computing Technical Committee</publisher>
			<date type="published" when="2007">2016. 2008 and 2012. 2014. 2008. 2008. 2007. 2012</date>
		</imprint>
		<respStmt>
			<orgName>University of Technology Sydney, Australia. He received his Ph.D. in Computer Science and Engineering from the Nanyang Technological University ; Nanyang Technological University ; Sun Yat-sen University ; Nanyang Technological University ; Electrical Engineering and Computer Science, York University, Canada. Before that, she was with Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>Her current research interests include resource allocation in multimedia wireless networks, cloud computing, and smart grid. She was a co-recipient of the Best Paper Awards. TCGCC) in 2018. She has been serving as an Associate Editor for several journals including the IEEE Transactions on Wireless Communications, the EURASIP Journal on Wireless Communications and Networking, and the International Journal of Ultra Wideband Communications and Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
