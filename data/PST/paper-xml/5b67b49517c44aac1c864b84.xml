<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on Automatic Image Caption Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-25">May 25, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shuang</forename><surname>Bai</surname></persName>
							<email>shuangb@bjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shan</forename><surname>An</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<addrLine>No.3 Shang Yuan Cun</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hai Dian District</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Jingdong Shangke Information Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey on Automatic Image Caption Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-25">May 25, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">7BCB7B631E0062666ABCFEBEB99DE433</idno>
					<idno type="DOI">10.1016/j.neucom.2018.05.080</idno>
					<note type="submission">Received date: 5 May 2017 Revised date: 13 April 2018 Accepted date: 19 May 2018 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image captioning</term>
					<term>Sentence template</term>
					<term>Deep neural networks</term>
					<term>Multimodal embedding</term>
					<term>Encoder-decoder framework</term>
					<term>Attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image captioning means automatically generating a caption for an image. As a recently emerged research area, it is attracting more and more attention. To achieve the goal of image captioning, semantic information of images needs to be captured and expressed in natural languages. Connecting both research communities of computer vision and natural language processing, image captioning is a quite challenging task. Various approaches have been proposed to solve this problem. In this paper, we present a survey on advances in image captioning research. Based on the technique adopted, we classify image captioning approaches into different categories. Representative methods in each category are summarized, and their strengths and limitations are talked about. In this paper, we first discuss methods used in early work which are mainly retrieval and template based. Then, we focus our main attention on neural network based methods, which give state of the art results. Neural network based methods are further divided into subcategories based on the specific framework they use. Each subcategory of neural network based methods are discussed in detail. After that, state of the art methods are compared on benchmark datasets. Following that, discussions on future research directions are presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans are able to relatively easily describe the environments they are in. Given an image, it is natural for a human to describe an immense amount of details about this image with a quick glance <ref type="bibr" target="#b0">[1]</ref>. This is one of humans' basic abilities. Making computers imitate humans' ability to interpret the visual world has been a long standing goal of researchers in the field of artificial intelligence.</p><p>Although great progress has been made in various computer vision tasks, such as object recognition <ref type="bibr" target="#b1">[2]</ref> [3], attribute classification <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b4">[5]</ref>, action classification <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref>, image classification <ref type="bibr" target="#b7">[8]</ref> and scene recognition <ref type="bibr" target="#b8">[9]</ref> [10], it is a relatively new task to let a computer use a human-like sentence to automatically describe an image that is forwarded to it.</p><p>Using a computer to automatically generate a natural language description for an image, which is defined as image captioning, is challenging. Because connecting both research communities of computer vision and natural language processing, image captioning not only requires a high level understanding of the semantic contents of an image, but also needs to express the information in a human-like sentence. Determination of presences, attributes and relationships of objects in an image is not an easy task itself. Organizing a sentence to describe such information makes this task even more difficult.</p><p>Since much of human communication depends on natural languages, whether written or spoken, enabling computers to describe the visual world will lead to a great number of possible applications, such as producing natural human robot interactions, early childhood education, information retrieval, and visually impaired assistance, and so on.</p><p>As a challenging and meaningful research field in artificial intelligence, image captioning is attracting more and more attention and is becoming increasingly important.</p><p>Given an image, the goal of image captioning is to generate a sentence that is linguistically plausible and semantically truthful to the content of this image. So there are two basic questions involved in image captioning, i.e. visual understanding and linguistic processing. To ensure generated sentences are grammatically and semantically correct, techniques of computer vision and natural language processing are supposed to be adopted to deal with problems arising from the corresponding modality and integrated appropriately. To this end, various approaches have been proposed.</p><p>Originally, automatic image captioning is only attempted to yield simple descriptions for images taken under extremely constrained conditions. For example, Kojima et al. <ref type="bibr" target="#b10">[11]</ref> used concept hierarchies of actions, case structures and verb patterns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>to generate natural languages to describe human activities in a fixed office environment. Hede et al. used a dictionary of objects and language templates to describe images of objects in backgrounds without clutters <ref type="bibr" target="#b11">[12]</ref>. Apparently, such methods are far from applications to describing images that we encounter in our everyday life.</p><p>It is not until recently that work aiming to generate descriptions for generic real life images is proposed <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b15">[16]</ref>. Early work on image captioning mainly follows two lines of research, i.e. retrieval based and template based. Because these methods accomplish the image captioning task either by making use of existing captions in the training set or relying on hard-coded language structures, the disadvantage of methods adopted in early work is that they are not flexible enough. As a result, expressiveness of generated descriptions by these methods is, to a large extent, limited.</p><p>Despite the difficult nature of the image captioning task, thanks to recent advances in deep neural networks <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref> [19] <ref type="bibr" target="#b19">[20]</ref> [21] <ref type="bibr" target="#b21">[22]</ref>, which are widely applied to the fields of computer vision <ref type="bibr" target="#b22">[23]</ref>  <ref type="bibr" target="#b23">[24]</ref> [25] <ref type="bibr" target="#b25">[26]</ref> and natural language processing <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b27">[28]</ref> [29] <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b30">[31]</ref>, image captioning systems based on deep neural networks are proposed. Powerful deep neural networks provide efficient solutions to visual and language modelling. Consequently, they are used to augment existing systems and design countless new approaches. Employing deep neural networks to tackle the image captioning problem has demonstrated state of the art results <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b32">[33]</ref> [34] <ref type="bibr" target="#b34">[35]</ref> [36] <ref type="bibr" target="#b36">[37]</ref>.</p><p>With the recent surge of research interest in image captioning, a large number of approaches have been proposed. To facilitate readers to have a quick overview of the advances of image captioning, we present this survey to review past work and envision future research directions. Although there exist several research topics that also involve both computer vision and natural language processing, such as visual question answering <ref type="bibr" target="#b37">[38]</ref> [39] <ref type="bibr" target="#b39">[40]</ref> [41] <ref type="bibr" target="#b41">[42]</ref>, text summarization <ref type="bibr" target="#b42">[43]</ref>  <ref type="bibr" target="#b43">[44]</ref> and video description <ref type="bibr" target="#b44">[45]</ref>  <ref type="bibr" target="#b45">[46]</ref> [47] <ref type="bibr" target="#b47">[48]</ref>, because each of them has its own focus, in this survey we mainly focus on work that aims to automatically generate descriptions for generic real life images.</p><p>Based on the technique adopted in each method, we classify image captioning approaches into different categories, which are summarized in Table <ref type="table" target="#tab_0">1</ref>. Representative methods in each category are listed. Methods in early work are mainly retrieval and template based, in which hard coded rules and hand engineered features are utilized. Outputs of such methods have obvious limitations. We review early work relatively briefly in this survey. With the great progress made in research of deep neural networks, approaches that employ neural networks for image captioning are proposed and demonstrate state of the art results. Based on the framework used in each deep neural network based method, we further classify these methods into subcategories. In this survey, we will focus our main attention on neural network based methods. The framework used in each subcategory will be introduced, and the corresponding representative methods will be discussed in more detail.</p><p>This paper is organized as follows. In Sections 2 and 3, we first review retrieval based and template based image captioning methods, respectively. Section 4 is about neural network based methods, in this section we divide neural network based image captioning methods into subcategories, and discuss representative methods in each subcategory, respectively. State of art methods will be compared on benchmark datasets in Section 5. After that, we will envision future research directions of image captioning in Section 6. The conclusion will be given in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Retrieval based image captioning</head><p>One type of image captioning methods that are common in early work is retrieval based. Given a query image, retrieval based methods produce a caption for it through retrieving one or a set of sentences from a pre-specified sentence pool. The generated caption can either be a sentence that has already existed or a sentence composed from the retrieved ones. First, let us investigate the line of research that directly uses retrieved sentences as captions of images.</p><p>Farhadi et al. establish a ob ject, action, scene meaning space to link images and sentences. Given a query image, they map it into the meaning space by solving a Markov Random Field, and use Lin similarity measure <ref type="bibr" target="#b76">[77]</ref> to determine the semantic distance between this image and each existing sentence parsed by Curran &amp; Clark parser <ref type="bibr" target="#b77">[78]</ref>. The sentence closest to the query image is taken as its caption <ref type="bibr" target="#b12">[13]</ref>.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, to caption an image Ordonez et al. first employ global image descriptors to retrieve a set of images from a webscale collection of captioned photographs. Then, they utilize semantic contents of the retrieved images to perform re-ranking and use the caption of the top image as the description of the query.</p><p>Hodosh et al. frame image captioning as a ranking task <ref type="bibr" target="#b31">[32]</ref>. The authors employ the Kernel Canonical Correlation Analysis technique <ref type="bibr" target="#b78">[79]</ref>  <ref type="bibr" target="#b79">[80]</ref> to project image and text items into a common space, where training images and their corresponding captions are maximally correlated. In the new common space, cosine similarities between images and sentences are calculated to select top ranked sentences to act as descriptions of query images.</p><p>To alleviate impacts of noisy visual estimation in methods that depend on image retrieval for image captioning, Mason and Charniak first use visual similarity to retrieve a set of captioned images for a query image <ref type="bibr" target="#b48">[49]</ref>. Then, from the captions of the retrieved images, they estimate a word probability density conditioned on the query image. The word probability density is used to score the existing captions to select the one with the largest score as the caption of the query.</p><p>The above methods have implicitly assumed that given a query image there always exists a sentence that is pertinent to it. This assumption is hardly true in practice. Therefore, instead of using retrieved sentences as descriptions of query images directly, in the other line of retrieval based research, retrieved sentences are utilized to compose a new description for a query image.</p><p>Provided with a dataset of paired images and sentences,  In order to generate a description for a query image, image retrieval is first performed based on global image features to retrieve a set of images for the query. Then, a model trained to predicate phrase relevance is used to select phrases from the ones associated with retrieved images. Finally a description sentence is generated based on the selected relevant phrases <ref type="bibr" target="#b15">[16]</ref>. With a similar idea, Kuznetsova et al. propose a tree based method to compose image descriptions by making use of captioned web images <ref type="bibr" target="#b49">[50]</ref>. After performing image retrieval and phrase extraction, the authors take extracted phrases as tree fragments and model description composition as a constraint optimization problem, which is encoded by using Integer Linear Programming <ref type="bibr" target="#b80">[81]</ref> [82] and solved by using the CPLEX solver <ref type="foot" target="#foot_1">2</ref> . Before this paper, the same authors have reported a similar method in <ref type="bibr" target="#b82">[83]</ref>.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>Disadvantages of retrieval based image captioning methods are obvious. Such methods transfer well-formed human-written sentences or phrases for generating descriptions for query images. Although the yielded outputs are usually grammatically correct and fluent, constraining image descriptions to sentences that have already existed can not adapt to new combinations of objects or novel scenes. Under certain conditions, generated descriptions may even be irrelevant to image contents. Retrieval based methods have large limitations to their capability to describe images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Template based image captioning</head><p>In early image captioning work, another type of methods that are commonly used is template based. In template based methods, image captions are generated through a syntactically and semantically constrained process. Typically, in order to use a template based method to generate a description for an image, a specified set of visual concepts need to be detected first. Then, the detected visual concepts are connected through sentence templates or specific language grammar rules or combinatorial optimization algorithms <ref type="bibr" target="#b83">[84]</ref>  <ref type="bibr" target="#b52">[53]</ref> to compose a sentence.</p><p>A method to use a sentence template for generating image descriptions is presented in <ref type="bibr" target="#b13">[14]</ref> by <ref type="bibr">Yang et al.</ref>, where a quadruplet (Nouns-Verbs-Scenes-Prepositions) is utilized as a sentence template. To describe an image, the authors first use detection algorithms <ref type="bibr" target="#b1">[2]</ref> [85] to estimate objects and scenes in this image. Then, they employ a language model <ref type="bibr" target="#b85">[86]</ref> trained over the Gigaword corpus <ref type="foot" target="#foot_2">3</ref> to predicate verbs, scenes and prepositions that may be used to compose the sentence. With probabilities of all elements computed, the best quadruplet is obtained by using Hidden Markov Model inference. Finally, the image description is generated by filling the sentence structure given by the quadruplet.</p><p>Kulkarni et al. employ Conditional Random Field to determine image contents to be rendered in the image caption <ref type="bibr">[87] [51]</ref>. In their method, nodes of the graph correspond to objects, object attributes and spatial relationships between objects, respectively. In the graph model, unary potential functions of nodes are obtained by using corresponding visual models, while pairwise potential functions are obtained by making statistics on a collection of existing descriptions. Image contents to be described are determined by performing Conditional Random Field inference. Outputs of the inference is used to generate a description based on a sentence template.</p><p>Li et al. use visual models to perform detections in images for extracting semantic information including objects, attributes and spatial relationships <ref type="bibr" target="#b51">[52]</ref>. Then, they define a triplet of the format ad j1, ob j1 , prep, ad j2, ob j2 for encoding recognition results. To generate a description with the triplet, webscale n-gram data, which is able to provide frequency counts of possible n-gram sequences, is resorted to for performing phrase selection, so that candidate phrases that may compose the triplet can be collected. After that, phrase fusion is implemented to use dynamic programming to find the optimal compatible set of phrases to act as the description of the query image.</p><p>Mitchell et al. employ computer vision algorithms to process an image and represent this image by using ob jects, actions, spatialrela triplets <ref type="bibr" target="#b52">[53]</ref>. After that, they formulate image description as a tree-generating process based on the visual recognition results. Trough object nouns clustering and ordering, the authors determine image contents to describe. Then sub-trees are created for object nouns, which are further used for creating full trees. Finally, a trigram language model <ref type="bibr" target="#b87">[88]</ref> is used to select a string from the generated full trees as the description of the corresponding image.</p><p>Methods mentioned above use visual models to predicate individual words from a query image in a piece-wise manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Then, predicted words such as objects, attributes, verbs and prepositions are connected in later stages to generate humanlike descriptions. Since phrases are combinations of words, compared to individual words, phrases carry bigger chunks of information <ref type="bibr" target="#b88">[89]</ref>. Sentences yielded based on phrases tend to be more descriptive. Therefore, methods utilizing phrases under the template based image captioning framework are proposed.</p><p>Ushiku et al. present a method called Common Subspace for Model and Similarity to learn phrase classifiers directly for captioning images <ref type="bibr" target="#b53">[54]</ref>. Specifically, the authors extract continuous words <ref type="bibr" target="#b83">[84]</ref> from training captions as phrases. Then, they map image features and phrase features into the same subspace, where similarity based and model based classification are integrated to learn a classifier for each phrase. In the testing stage, phrases estimated from a query image are connected by using multi-stack beam search <ref type="bibr" target="#b83">[84]</ref> to generate a description.</p><p>Template based image captioning can generate syntactically correct sentences, and descriptions yielded by such methods are usually more relevant to image contents than retrieval based ones. However, there are also disadvantages for template based methods. Because description generation under the template based framework is strictly constrained to image contents recognized by visual models, with the typically small number of visual models available, there are usually limitations to coverage, creativity, and complexity of generated sentences. Moreover, compared to human-written captions, using rigid templates as main structures of sentences will make generated descriptions less natural.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep neural network based image captioning</head><p>Retrieval based and template based image captioning methods are adopted mainly in early work. Due to great progress made in the field of deep learning <ref type="bibr" target="#b17">[18]</ref> [90], recent work begins to rely on deep neural networks for automatic image captioning. In this section, we will review such methods. Even though deep neural networks are now widely adopted for tackling the image captioning task, different methods may be based on different frameworks. Therefore, we classify deep neural network based methods into subcategories on the basis of the main framework they use and discuss each subcategory respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Retrieval and template based methods augmented by neural networks</head><p>Encouraged by advances in the field of deep neural networks, instead of utilizing hand-engineered features and shallow models like in early work, deep neural networks are employed to perform image captioning. With inspiration from retrieval based methods, researchers propose to utilize deep models to formulate image captioning as a multi-modality embedding <ref type="bibr" target="#b90">[91]</ref> and ranking problem.</p><p>To retrieve a description sentence for a query image, Socher et al. propose to use dependency-tree recursive neural networks to represent phrases and sentences as compositional vectors. They use another deep neural network <ref type="bibr" target="#b91">[92]</ref> as visual model to extract features from images <ref type="bibr" target="#b54">[55]</ref>. Obtained multimodal features are mapped into a common space by using a max-margin objective function. After training, correct image and sentence pairs in the common space will have larger inner products and vice versa. At last, sentence retrieval is performed based on similarities between representations of images and sentences in the common space.</p><p>Karpathy et al. propose to embed sentence fragments and image fragments into a common space for ranking sentences for a query image <ref type="bibr" target="#b36">[37]</ref>. They use dependency tree relations <ref type="bibr" target="#b92">[93]</ref> of a sentence as sentence fragments and use detection results of the Region Convolutional Neural Network method <ref type="bibr" target="#b2">[3]</ref> in an image as image fragments. Representing both image fragments and sentence fragments as feature vectors, the authors design a structured max-margin objective, which includes a global ranking term and a fragment alignment term, to map visual and textual data into a common space. In the common space, similarities between images and sentences are computed based on fragment similarities, as a result sentence ranking can be conducted at a finer level.</p><p>In order to measure similarities between images and sentences with different levels of interactions between them taken into consideration, Ma et al. propose a multimodal Convolutional Neural Network <ref type="bibr" target="#b55">[56]</ref>. Ma's framework includes three kinds of components, i.e. image CNNs to encode visual data <ref type="bibr" target="#b93">[94]</ref>  <ref type="bibr" target="#b94">[95]</ref>, matching CNNs to jointly represent visual and textual data <ref type="bibr" target="#b95">[96]</ref> [97] and multilayer perceptions to score compatibility of visual and textual data. The authors use different variants of matching CNNs to account for joint representations of images and words, phrases and sentences. The final matching score between an image and a sentence is determined based on an ensemble of multimodal Convolutional Neural Networks.</p><p>Yan and Mikolajczyk propose to use deep Canonical Correlation Analysis <ref type="bibr" target="#b97">[98]</ref> to match images and sentences <ref type="bibr" target="#b56">[57]</ref>. They use a deep Convolutional Neural Network <ref type="bibr" target="#b7">[8]</ref> to extract visual features from images and use a stacked network to extract textual features from Frequency-Inverse Document Frequency represented sentences. The Canonical Correlation Analysis objective is employed to map visual and textual features to a joint latent space with correlation between paired features maximized. In the joint latent space, similarities between an image feature and a sentence feature can be computed directly for sentence retrieval.</p><p>Besides using deep models to augment retrieval based image captioning methods, utilizing deep models under the template based framework is also attempted. Lebret et al. leverage a kind of soft-template to generate image captions with deep models <ref type="bibr" target="#b57">[58]</ref>. In this method, the authors use the SENNA software <ref type="foot" target="#foot_3">4</ref> to extract phrases from training sentences and make statistics on the extracted phrases. Phrases are represented as high-dimensional vectors by using a word vector representation approach <ref type="bibr" target="#b30">[31]</ref> [99] <ref type="bibr" target="#b99">[100]</ref>, and images are represented by using a deep Convolutional Neural Network <ref type="bibr" target="#b93">[94]</ref>. A bilinear model is trained as a metric between image features and phrase features, so that given a query image, phrases can be inferred from it. Phrases inferred from an image are used to generate a sentence under the guidance of statistics made in the early stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>With the utilization of deep neural networks, performances of image captioning methods are improved significantly. However, introducing deep neural networks into retrieval based and template based methods does not overcome their disadvantages. Limitations of sentences generated by these methods are not removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image captioning based on multimodal learning</head><p>Retrieval based and template based image captioning methods impose limitations on generated sentences. Thanks to powerful deep neural networks, image captioning approaches are proposed that do not rely on exiting captions or assumptions about sentence structures in the caption generation process. Such methods can yield more expressive and flexible sentences with richer structures. Using multimodel neural networks is one of the attempts that rely on pure learning to generate image captions.</p><p>General structure of multimodal learning based image captioning methods is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In such kind of methods, image features are first extracted by using a feature extractor, such as deep convolutional neural networks. Then, the obtained image feature is forwarded to a neural language model, which maps the image feature into the common space with the word features and perform word predication conditioned on the image feature and previously generated context words. Kiros et al. propose to use a neural language model which is conditioned on image inputs to generate captions for images <ref type="bibr" target="#b58">[59]</ref>. In their method, log-bilinear language model <ref type="bibr" target="#b29">[30]</ref> is adapted to multimodal cases. In a natural language processing problem, a language model is used to predicate the probability of generating a word w t conditioned on previously generated words w 1 , • • • , w t-1 , which is shown below:</p><formula xml:id="formula_1">P(w t | w 1 , • • • , w t-1 ). (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>The authors make the language model become dependent on images through two different ways, i.e. adding an image feature as an additive bias to the representation of the next predicted word and gating the word representation matrix by using the image feature. Consequently, in the multimodal case the probability of generating a word w t is as follows:</p><formula xml:id="formula_3">P(w t | w 1 , • • • , w t-1 , I). (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where I is an image feature. In their method, images are represented by a deep Convolutional Neural Network, and joint image-text feature learning is implemented by back propagating gradients from the loss function through the multimodal neural network model. By using this model, an image caption can be generated word by word, with the generation of each word conditioned on previously generated words and the image feature.</p><p>To generate novel captions for images, Mao et al. adapt a Recurrent Neural Network language model to multimodal cases for directly modelling the probability of generating a word conditioned on a given image and previously generated words <ref type="bibr">[60] [35]</ref>. Under their framework, a deep Convulutional Neural Network <ref type="bibr" target="#b7">[8]</ref> is used to extract visual features from images, and a Recurrent Neural Network <ref type="bibr" target="#b100">[101]</ref> with a multimodal part is used to model word distributions conditioned on image features and context words. For the Recurrent Neural Network language model, each unit consists of an input word layer w, an recurrent layer r and an output layer y. At the t th unit of the Recurrent Neural Network language model, the calculation performed by these three layers is shown as follows:</p><formula xml:id="formula_5">x(t) = [w(t) r(t -1)],<label>(3)</label></formula><formula xml:id="formula_6">r(t) = f (U • x(t)),<label>(4)</label></formula><formula xml:id="formula_7">y(t) = g(V • r(t)),<label>(5)</label></formula><p>where f (•) and g(•) are element-wise non-linear functions, and U and V are matrices of weights to be learned. The multimodal part calculates its layer activation vector m(t) by using the equation below:</p><formula xml:id="formula_8">m(t) = g m (V w • w(t) + V r • r(t) + V I • I),<label>(6)</label></formula><p>where g m is a non-linear function. I is the image feature. V w , V r and V I are matrices of weights to be learned. The multimodal part fuses image features and distributed word representations by mapping and adding them. To train the model, a perplexity based cost function is minimized based on back propagation. Karpathy and Fei-Fei present an approach to align image regions represented by a Convolutional Neural Network and sentence segments represented by a Bidirectional Recurrent Neural Network <ref type="bibr" target="#b101">[102]</ref> to learn a multimodal Recurrent Neural Network model to generate descriptions for image regions <ref type="bibr" target="#b60">[61]</ref>. In their method, after representing image regions and sentence segments by using corresponding neural networks, a structured objective is used to map visual and textual data into a common space and associate each region feature to the textual feature that describes the region. The aligned two modalities are then employed to train a multimodal Recurrent Neural Network model, that can be used to predicate the probability of generating the next word given an image feature and context words.</p><p>Recurrent Neural Networks are known to have difficulties in learning long term dependencies <ref type="bibr" target="#b102">[103]</ref>  <ref type="bibr" target="#b103">[104]</ref>. To alleviate this weakness in image captioning, Chen and Zitnick propose to dynamically build a visual representation of an image as a caption is being generated for it, so that long term visual concepts can be remembered during this process <ref type="bibr" target="#b61">[62]</ref>. To this end, a set of latent variables U t-1 are introduced to encode visual interpretation of words W t-1 that have already been generated. With these latent variables, the probability of generating a word</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T w t is given below: P(w t , V | W t-1 , U t-1 ) = P(w t | V, W t-1 , U t-1 )P(V | W t-1 , U t-1 ),<label>(7)</label></formula><p>where V denotes observed visual features, and W t-1 denotes generated words (w 1 , • • • , w t-1 ). The authors realize the above idea through adding recurrent visual hidden layer u into the Recurrent Neural Networks. The recurrent layer u is helpful for both reconstructing the visual features V from previous words W t-1 and predicting the next word w t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image captioning based on the encoder-decoder framework</head><p>Inspired by recent advances in neural machine translation <ref type="bibr" target="#b27">[28]</ref> [105] <ref type="bibr" target="#b105">[106]</ref>, the encoder-decoder framework is adopted to generate captions for images. General structure of encoderdecoder based image captioning methods is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. This framework is originally designed to translate sentences from one language into another language. Motivated by the neural machine translation idea, it is argued that image captioning can be formulated as a translation problem, where the input is an image, while the output is a sentence <ref type="bibr" target="#b62">[63]</ref>. In image captioning methods under this framework, an encoder neural network first encodes an image into an intermediate representation, then a decoder recurrent neural network takes the intermediate representation as input and generate a sentence word by word. </p><p>where I is an input image and θ is the model parameter. Since a sentence S equals to a sequence of words (S 0 , . . . , S T +1 ), with chain rule Eqn.8 is reformulated below:  It has demonstrated promising results to use the encoderdecoder framework to tackle the image captioning problem. Encouraged by the success, approaches aiming to augment this framework for obtaining better performances are proposed.</p><formula xml:id="formula_11">S =</formula><formula xml:id="formula_12">h t+1 = f (h t , x t ),<label>(10)</label></formula><p>Aiming to generate image descriptions that are closely related to image contents, Jia et al. extract semantic information from images and add the information to each unit of the Long Short-Term Memory Recurrent Neural Networks during the process of sentence generation <ref type="bibr" target="#b64">[65]</ref>. The original forms of the memory cell and gates of a LSTM unit <ref type="bibr" target="#b108">[109]</ref> are defined as follows:</p><formula xml:id="formula_13">i l = σ(W ix x l + W im m l-1 ), (<label>11</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">f l = σ(W f x x l + W f m m l-1 ), (<label>12</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">o l = σ(W ox x l + W om m l-1 ), (<label>13</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">c l = f l c l-1 + i l h(W cx x l + W cm m l-1 ), (<label>14</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">m l = o l c l ,<label>(15)</label></formula><p>where σ(•) and h(•) are non-linear functions, variables i l , f l and o l stand for input gate, forget gate, output gate of a LSTM cell, respectively, c l and m l stand for the state and hidden state of the memory cell unit, x l is the input,</p><formula xml:id="formula_22">W [•][•]</formula><p>are model parameters, and denotes an element-wise multiplication operation.</p><p>With the addition of semantic information to an LSTM unit, the forms of the memory cell and gates are changed to be as follows:</p><formula xml:id="formula_23">i l = σ(W ix x l + W im m l-1 + W ig g), (<label>16</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">f l = σ(W f x x l + W f m m l-1 + W f g g), (<label>17</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">o l = σ(W ox x l + W om m l-1 + W og g), (<label>18</label></formula><formula xml:id="formula_28">) A C C E P T E D M A N U S C R I P T c l = f l c l-1 + i l h(W cx x l + W cm m l-1 + W cg g), (<label>19</label></formula><formula xml:id="formula_29">)</formula><formula xml:id="formula_30">m l = o l c l , (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>where g is the representation of semantic information, which can be from any sources as long as it can provide guidance for image captioning. Given an image, approaches introduced above seek to directly derive a description from its visual features. In order to utilize high-level semantic information for image captioning, Wu et al. incorporate visual concepts into the encoderdecoder framework <ref type="bibr" target="#b65">[66]</ref>. To this end, the authors first mined a set of semantic attributes from the training sentences. Under the region-based multi-label classification framework <ref type="bibr" target="#b109">[110]</ref>, a Convolutional Neural Network based classifier is trained for each attribute. With trained semantic attribute classifiers, an image can be represented as a prediction vector V att (I) giving the probability of each attribute to be present in the image. After encoding an image I as V att (I), a Long Short-Term Memory network <ref type="bibr" target="#b106">[107]</ref> is employed as a decoder to generate a sentence describing the contents of the image based on the representation. Under this condition, the image captioning problem can be reformulated below:</p><formula xml:id="formula_32">S = arg max S P(S | V att (I); θ) (<label>21</label></formula><formula xml:id="formula_33">)</formula><p>where I is the input image. S is a sentence. θ is the model parameter.</p><p>Because in practical applications, there may be far less captioned images than uncaptioned ones, semi-supervised learning of image captioning models is of significant practical values. To obtain an image captioning system by leveraging the vast quantity of uncaptioned images available, Pu et al. propose a semi-supervised learning method under the encoder-decoder framework to use a deep Convolutional Neural Network to encode images and a Deep Generative Deconvolutional Network to decode latent image features for image captioning <ref type="bibr" target="#b66">[67]</ref>. The system uses the deep Convolutional Neural Network to provide an approximation to the distribution of the latent features of the Deep Generative Deconvolutional Network and link the latent features to generative models for captions. After training, given an image, the caption can be generated by averaging across the distribution of latent features of Deep Generative Deconvolutional Network .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Attention guided image captioning</head><p>It is well-known that images are rich in information they contain, while in image captioning it is unnecessary to describe all details of a given image. Only the most salient contents are supposed to be mentioned in the description. Motivated by the visual attention mechanism of primates and humans <ref type="bibr" target="#b110">[111]</ref> [112], approaches that utilize attention to guide image description generation are proposed. By incorporating attention to the encoder-decoder image captioning framework, sentence generation will be conditioned on hidden states that are computed based on attention mechanism. General structure of attention guided image captioning methods is given Fig. <ref type="figure" target="#fig_2">3</ref>. In such methods, attention mechanism based on various kinds of cues from the input image is incorporated into the encoder-decoder framework to make the decoding process focus on certain aspects of the input image at each time step to generate a description for the input image. Encouraged by successes of other tasks that employ attention mechanism <ref type="bibr" target="#b112">[113]</ref> [114] <ref type="bibr" target="#b114">[115]</ref>, Xu et al. propose an attentive encoder-decoder model to be able to dynamically attend salient image regions during the process of image description generation <ref type="bibr" target="#b67">[68]</ref>. Forwarding an image to a deep Convolutional Neural Network and extracting features from a lower convolutional layer of the network, the authors encode an image as a set of feature vectors which is shown as follows:</p><formula xml:id="formula_34">a = (a 1 , • • • a N ), a i ∈ R D ,<label>(22)</label></formula><p>where a i is a D-dimensional feature vector that represents one part of the image. As a result, an image is represented by N vectors. In the decoding stage, a Long Short-Term Memory network is used as the decoder. Different from previous LSTM versions, a context vector z l is utilized to dynamically represent image parts that are relevant for caption generation at time l. Consequently, the memory cell and gates of a LSTM unit become the forms given below:</p><formula xml:id="formula_35">i l = σ(W ix x l + W im m l-1 + W iz z l ), (<label>23</label></formula><formula xml:id="formula_36">)</formula><formula xml:id="formula_37">f l = σ(W f x x l + W f m m l-1 + W f z z l ),<label>(24)</label></formula><formula xml:id="formula_38">o l = σ(W ox x l + W om m l-1 + W oz z l ),<label>(25)</label></formula><formula xml:id="formula_39">c l = f l c l-1 + i l h(W cx x l + W cm m l-1 + W cz z l ),<label>(26)</label></formula><formula xml:id="formula_40">m l = o l c l .<label>(27)</label></formula><p>Attention is imposed to the decoding process by using the context vector z l , which is a function of image region vectors (a 1 , • • • a N ) and weights associated with them (α 1 , • • • α N ):</p><formula xml:id="formula_41">z l = φ({a i }, {α i }).<label>(28)</label></formula><p>With different function forms, different attention mechanisms can be applied. In <ref type="bibr" target="#b67">[68]</ref>, Xu et al. proposed a stochastic hard attention and a deterministic soft attention for image captioning. In each time step, the stochastic hard attention mechanism selects a visual feature from one of the N locations as the context vector to generate a word, while the deterministic soft attention mechanism combines visual features from all N locations to obtain the context vector to generate a word.</p><p>Specifically, in the stochastic hard attention mechanism, at time step l, for each location i, the positive weight α l,i associated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>with it is taken as the probability for this location to be focused on for generating the corresponding word. The context vector z l is calculated as follows:</p><formula xml:id="formula_42">z l = N i s l,i a i .<label>(29)</label></formula><p>where s l,i is an indicator variable, which is set to 1, if the visual feature a i from the i th location out of N is attended at time step l, otherwise 0. The distribution of the variable s l,i is treated as a multinouli distribution parametrized by {α l,i }, and its value is determined based on sampling. Contrarily, in the deterministic soft attention mechanism, the positive weight α l,i associated with location i at time step l is used to represent the relative importance of the corresponding location in blending visual features from all N locations to calculate the context vector z l , which is formulated below:</p><formula xml:id="formula_43">z l = N i α l,i a i .<label>(30)</label></formula><p>Finding that both bottom-up <ref type="bibr" target="#b12">[13]</ref> [87] <ref type="bibr" target="#b115">[116]</ref> and top-down <ref type="bibr" target="#b33">[34]</ref> [61] <ref type="bibr" target="#b61">[62]</ref> image captioning approaches have certain limitations, You et al. propose a semantic attention model to take advantages of the complimentary properties of both types of approaches <ref type="bibr" target="#b68">[69]</ref>. To achieve this goal, the authors use a deep Convolutional Neural Network and a set of visual attribute detectors to extract a global feature v and a list of visual attributes {A i } from an input image, respectively. With each attribute corresponding to one entry of the used vocabulary, words to generate and attributes to detect share the same vocabulary. Under the encoder-decoder framework, the global visual feature v is only forwarded to the encoder at the initial step. In the decoding stage, using an input attention function φ(•), certain cognitive visual cues in the attribute list {A i } will be attended with a probability distribution:</p><formula xml:id="formula_44">{α i t } = φ(y t-1 , {A i }),<label>(31)</label></formula><p>where α i t is the weight assigned to an attribute in the list, and y t-1 is the previously generated word. These weights are used to calculate input vector x t to the t th unit of a Long Short-Term Memory neural network. With an output attention function ϕ(•), the attention on all the attributes will be modulated by using the weights given below:</p><formula xml:id="formula_45">{β i t } = ϕ(m t , {A i }),<label>(32)</label></formula><p>where β i t is the weight assigned to an attribute. m t is the hidden state of t th unit of the Long Short-Term Memory neural network. The obtained weights are further used to predicate probability distribution of the next word to be generated.</p><p>Arguing that attentive encoder-decoder models lack global modelling abilities due to their sequential information processing manner, Yang et al. propose a review network to enhance the encoder-decoder framework <ref type="bibr" target="#b69">[70]</ref>. To overcome the abovementioned problem, a reviewer module is introduced to perform review steps on the hidden states of the encoder and give a thought vector at each step. During this process, attention mechanism is applied to determine weights assigned to hidden states. Through this manner, information encoded by the encoder can be reviewed and learned by the thought vectors which can capture global properties of the input. Obtained thought vectors are used by the decoder for word predication. Specifically, the authors use the VGGNet <ref type="bibr" target="#b93">[94]</ref>, which is a commonly used deep Convolutional Neural Network to encode an image as a context vector c and a set of hidden states H = {h t }. A Long-Short Term Memory neural network is used as reviewer to produce thought vectors. A thought vector f t at the t th LSTM unit is calculated as follows:</p><formula xml:id="formula_46">f t = g t (H, f t-1 ), (<label>33</label></formula><formula xml:id="formula_47">)</formula><p>where g t is a function performed by a reviewer with attention mechanism applied. After obtaining thought vectors F = {f t }, a Long-Short Term Memory neural network decoder can predicate word probability distribution based on them as given below:</p><formula xml:id="formula_48">y t = g t (F, s t-1 , y t-1 ), (<label>34</label></formula><formula xml:id="formula_49">)</formula><p>where s t is the hidden state of the t th LSTM unit in the decoder. y t is the t th word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Compositional architectures for image captioning</head><p>In Section 4, we focus on image captioning methods that are based on deep neural networks. Most of the approaches in previous subsections are based on end-to-end frameworks, whose parameters can be trained jointly. Such methods are neat and efficient. However, believing that each type of approaches have their own advantages and disadvantages, architectures composed of independent building blocks are proposed for image captioning. In this subsection, we will talk about compositional image captioning architectures that are consisted of independent functional building blocks that may be used in different types of methods.</p><p>General structure of compositional image captioning methods is given Fig. <ref type="figure" target="#fig_3">4</ref>. In contrast to end-to-end image captioning framework, compositional image captioning methods integrate independent building blocks into a pipeline to generate captions for input images. Generally, compositional image captioning methods use a visual model to detect visual concepts appearing in the input image. Then, detected visual concepts are forwarded to a language model to generate candidate descriptions, which are then post-processed to select one of them as the caption of the input image. Fang et al. propose a system that is consisted of visual detectors, language models and multimodal similarity models for automatic image captioning <ref type="bibr" target="#b32">[33]</ref>. The authors first detect a vocabulary of words that are most common in the training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>captions. Then, corresponding to each word, a visual detector is trained by using a Multiple Instance Learning approach <ref type="bibr" target="#b116">[117]</ref>. Visual features used by these detectors are extracted by a deep Convolutional Neural Network <ref type="bibr" target="#b7">[8]</ref>. Given an image, conditioned on the words detected from it, a maximum entropy language model <ref type="bibr" target="#b117">[118]</ref> is adopted to generate candidate captions. During this process, left-to-right beam search <ref type="bibr" target="#b118">[119]</ref> with a stack of pre-specified length of l is performed. Consequently, l candidate captions are obtained for this image. Finally, a deep multimodal similarity model, which maps images and text fragments into a common space for similarity measurement is used to re-rank the candidate descriptions.</p><p>Based on Fang's work <ref type="bibr" target="#b32">[33]</ref>, Tran et al. presented a system for captioning open domain images <ref type="bibr" target="#b70">[71]</ref>. Similar to <ref type="bibr" target="#b32">[33]</ref>, the authors use a deep residual network based vision model to detect a broad range of visual concepts <ref type="bibr" target="#b119">[120]</ref>, a maximum entropy language model for candidate description generation, and a deep multimodal similarity model for caption ranking. What's more, the authors added detection for landmarks and celebrities and a confidence model for dealing with images that are difficult to describe.</p><p>To exploit parallel structures between images and sentences for image captioning, Fu et al. propose to align the word generation process to visual perception of image regions <ref type="bibr" target="#b71">[72]</ref>. Furthermore, the authors introduce scene-specific contexts to capture high-level semantic information in images for adapting word generation to specific scene types. Given an image, Fu et al. first use the selective search method <ref type="bibr" target="#b120">[121]</ref> to extract a large number of image regions. Then, based on the criterion of being semantically meaningful, non-compositional and contextually rich, a small number of them are selected for further processing. Each selected region is represented as a visual feature by using the ResNet network <ref type="bibr" target="#b119">[120]</ref>. These features are dynamically attended by an attention-based decoder, which is a Long-Short Term Memory neural network <ref type="bibr" target="#b106">[107]</ref>. Finally, to exploit semantic-contexts in images for better captioning, Latent Dirichlet Allocation <ref type="bibr" target="#b121">[122]</ref> and a multilayer perceptron are used to predicate a context vector for an image to bias the word generation in the Long-Short Term Memory neural network.</p><p>To be able to produce detailed descriptions about image contents, Ma and Han propose to use structural words for image captioning <ref type="bibr" target="#b72">[73]</ref>. Their method consists of two-stages, i.e. structural word recognition and sentence translation. The authors first employ a multi-layer optimization method to generate a hierarchical concepts to represent an image as a tetrad &lt; ob jects, attributes, activities, scenes &gt;. The tetrad plays the role of structural words. Then, they utilize an encoder-decoder machine translation model, which is based on the Long-Short Term Memory neural network, to translate the structural words into sentences.</p><p>Oruganti et al. present a fusion based model which consists of an image processing stage, a language processing stage and a fusion stage <ref type="bibr" target="#b73">[74]</ref>. In their method, images and languages are independently processed in their corresponding stages based on a Convolutional Neural Network and a Long-Short Term Memory network, respectively. After that, the outputs of these two stages are mapped into a common vector space, where the fu-sion stage associate these two modalities and make predications. Such a method is argued to be able to make the system more flexible and mitigate the shortcomings of previous approaches on their inability to accommodate disparate inputs.</p><p>A parallel-fusion RNN-LSTM architecture is presented in <ref type="bibr" target="#b74">[75]</ref> by <ref type="bibr">Wang et al.</ref> to take advantages of the complementary properties of simple Recurrent Neural Networks and Long-Short Term Memory networks for improving the performance of image captioning systems. In their method, inputs are mapped to hidden states by Recurrent Neural Network units and Long-Short Term Memory units in parallel. Then, the hidden states in these two networks are merged with certain ratios for word predication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Generating descriptions for images with novelties</head><p>So far, all of the introduced image captioning methods are limited to pre-specified and fixed word dictionaries and are not enabled to generate descriptions for concepts that are not trained with paired image-sentence training data. Humans have the ability to recognize, learn and use novel concepts in various visual understanding tasks. And in practical image description applications, it is quite possible to come across situations where there are novel objects which are not in the pre-specified vocabulary or have not been trained with paired image-sentence data. It is undesirable to retrain the whole system every time when a few images with novel concepts appear. Therefore, it is a useful ability for image captioning systems to adapt to novelties appearing in images for generating image descriptions efficiently. In this subsection, we talk about approaches that can deal with novelties in images during image captioning.</p><p>In order to learn novel visual concepts without retraining the whole system, Mao et al. propose to use linguistic context and visual features to hypothesize semantic meanings of new words and use these words to describe images with novelties <ref type="bibr" target="#b75">[76]</ref>. To accomplish the novelty learning task, the authors build their system by making two modifications to the model proposed in <ref type="bibr" target="#b34">[35]</ref>. First, they use a transposed weight sharing strategy to reduce the number of parameters in the model, so that the over fitting problem can be prevented. Second, they use a Long-Short Term Memory (LSTM) layer <ref type="bibr" target="#b106">[107]</ref> to replace the recurrent layer to avoid the gradient explosion and vanishing problem.</p><p>With the aim of describing novel objects that are not present in the training image-sentence pairs, Hendricks et al. propose the Deep Compositional Captioner method <ref type="bibr" target="#b35">[36]</ref>. In this method, large object recognition datasets and external text corpora are leveraged, and novel object description is realised based on knowledges transferred between semantically similar concepts. To achieve this goal, Hendricks et al. first train a lexical classifier and a language model over image datasets and text corpora, respectively. Then, they trained a deep multimodal caption model to integrate the lexical classifier and the language model. Particularly, as a linear combination of affine transformation of image and language features, the caption model enables easy transfer of semantic knowledge between these two modalities, which allows predication of novel objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">State of the art method comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image captioning evaluation metrics</head><p>In this section, we will compare image captioning methods that give state of the art results. Being plagued by the complexity of the outputs, image captioning methods are difficult to evaluate. In order to compare image captioning systems as for their capability to generate human-like sentences with respect to linguistic quality and semantic correctness, various evaluation metrics have been designed. For state of the art method comparison, we need to introduce the commonly used evaluation metrics first.</p><p>In fact, the most intuitive way to determine how well a generated sentence describes the content of an image is by direct human judgements. However, because human evaluation requires large amounts of un-reusable human efforts, it is difficult to scale up. Furthermore, human evaluation is inherently subjective making it suffer from user variances. Therefore, in this paper we report method comparison based on automatic image captioning evaluation metrics. The used automatic evaluation metrics include BLEU <ref type="bibr" target="#b122">[123]</ref>, ROUGE-L <ref type="bibr" target="#b123">[124]</ref>, METEOR <ref type="bibr" target="#b124">[125]</ref> and CIDEr <ref type="bibr" target="#b125">[126]</ref>. BLEU, ROUGE-L and METEOR are originally designed to judge the quality of machine translation. Because the evaluation process of image captioning is exactly the same as machine translation, in which generated sentences are compared against ground truth sentences, these metrics are widely used for image captioning evaluation.</p><p>BLEU <ref type="bibr" target="#b122">[123]</ref> is to use variable lengths of phrases of a candidate sentence to match against reference sentences written by human to measure their closeness. In other words, BLEU metrics are determined by comparing a candidate sentence with reference sentences in n-grams. Specifically, to determine BLEU-1, the candidate sentence is compared with reference sentences in unigram, while for calculating BLEU-2, bigram will be used for matching. A maximum order of four is empirically determined to obtain the best correlation with human judgements. For BLEU metrics, the unigram scores account for the adequacy, while higher n-gram scores account for the fluency. ROUGE-L <ref type="bibr" target="#b123">[124]</ref> is designed to evaluate the adequacy and fluency of machine translation. This metric employs the longest common subsequence between a candidate sentence and a set of reference sentences to measure their similarity at sentencelevel. The longest common subsequence between two sentences only requires in-sequence word matches, and the matched words are not necessarily consecutive. Determination of the longest common subsequence is achieved by using dynamic programming technique. Because this metric automatically includes longest in-sequence common n-grams, sentence level structure can be naturally captured.</p><p>METEOR <ref type="bibr" target="#b124">[125]</ref> is an automatic machine translation evaluation metric. It first performs generalized unigram matches between a candidate sentence and a human-written reference sentence, then computes a score based on the matching results. The computation involves precision, recall and alignments of the matched words. In the case of multiple reference sentences, the best score among all independently computed ones is taken as the final evaluation result of the candidate. Introduction of this metric is for addressing weakness of the BLEU metric, which is derived only based on the precision of matched n-grams.</p><p>CIDEr <ref type="bibr" target="#b125">[126]</ref> is a paradigm that uses human consensus to evaluate the quality of image captioning. This metric measures the similarity of a sentence generated by the image captioning method to the majority of ground truth sentences written by human. It achieves this by encoding the frequency of the n-grams in the candidate sentence to appear in the reference sentences, where a Term Frequency Inverse Document Frequency weighting for each n-gram is used. This metric is designed to evaluate generated sentences in aspects of grammaticality, saliency, importance and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison on benchmark datasets</head><p>Three benchmark datasets that are widely used to evaluate image captioning methods are employed as the testbed for method comparison. The datasets are Flickr8K <ref type="bibr" target="#b31">[32]</ref>, Flickr30k <ref type="bibr" target="#b126">[127]</ref> and Microsoft COCO Caption dataset <ref type="bibr" target="#b127">[128]</ref>.</p><p>Flickr8K <ref type="bibr" target="#b31">[32]</ref> contains 8, 000 images extracted from Flickr. The images in this dataset mainly contain human and animals. Each image is annotated by five sentences based on crowdsourcing service from Amazon Mechanical Turk. During image annotation, the Amazon Mechanical Turk workers are instructed to focus on the images and describe their contents without considering the context in which the pictures are taken.</p><p>Flickr30k <ref type="bibr" target="#b126">[127]</ref> is a dataset that is extended from the Flickr 8k dataset. There are 31, 783 annotated images in Flickr30k. Each image is associated to five sentences purposely written for it. The images in this dataset are mainly about humans involved in everyday activities and events.</p><p>Microsoft COCO Caption dataset <ref type="bibr" target="#b127">[128]</ref> is created by gathering images of complex everyday scenes with common objects in their natural context. Currently, there are 123, 287 images in total, of which 82, 783 and 40, 504 are used for training and validation, respectively. For each image in the training and validation set, five human written captions are provided. Captions of test images are unavailable publicly. This dataset poses great challenges to the image captioning task.</p><p>The comparison is based on an experiment protocol that is commonly adopted in previous work. For datasets Flickr8k and Flickr30k, 1, 000 images are used for validation and testing respectively, while all the other images are used for training. For the Microsoft COCO Caption dataset, since the captions of the test set are unavailable, only training and validation sets are used. All images in the training set are used for training, while 5, 000 validation images are used for validation, and another 5, 000 images from the validation set are used for testing. Under the experiment setting described above, image captioning comparison on datasets Flcikr8k and Flick30k is shown in Table 2, and comparison results on the Microsoft COCO Caption dataset are shown in Table <ref type="table" target="#tab_5">3</ref>.</p><p>In the method Karpathy and Fei-Fei <ref type="bibr" target="#b60">[61]</ref>, a multimodal Recurrent Neural Network is trained to align image regions and sentence fragments for image captioning. The authors report their results on the benchmark datasets Flcikr8k, Flick30k and Microsoft COCO Caption dataset in Table <ref type="table" target="#tab_4">2</ref> and<ref type="table" target="#tab_5">3</ref>  Another multimodal learning based image captioning method is Mao et al. <ref type="bibr" target="#b34">[35]</ref>, where a deep Convulutional Neural Network is used to extract visual features from images, and a Recurrent Neural Network with a multimodal part is used to model word distributions conditioned on image features and context words. In their method, words are generated one by one for captioning images. They evaluate their method on all three benchmark datasets, with respect to BLEU-n metrics. Their method outperforms Karpathy and Fei-Fei <ref type="bibr" target="#b60">[61]</ref> on all three benchmarks. The results show that multimodal learning based image captioning method that generates image descriptions word by word can outperform the one using language fragments due to its flexibility.</p><p>After the encoder-decoder framework is introduced to solve the image captioning problem, it becomes a popular paradigm, and promising performances are demonstrated. Donahue et al. adopt a deep Convolutional Neural Network for encoding and a Long Short-Term Memory Recurrent Network for decoding to generate sentence descriptions for input images <ref type="bibr" target="#b33">[34]</ref>. In Donahue's method, both image feature and context word feature are provided to the sequential model at each time step. On the Flickr30k dataset, the achieved BLEU-n scores are 0.587, 0.391, 0.251 and 0.165, respectively. On the Microsoft COCO Caption dataset, the achieved BLEU-n scores are 0.669, 0.489, 0.349 and 0.249, respectively. The results are superior to Karpathy and Fei-Fei <ref type="bibr" target="#b60">[61]</ref>, but a little bit inferior to Mao et al. <ref type="bibr" target="#b34">[35]</ref>.</p><p>With the same encoder-decoder framework, Vinyals et al. <ref type="bibr" target="#b63">[64]</ref> outperform Donahue et al. <ref type="bibr" target="#b33">[34]</ref> by feeding image features to the decoder network at only the initial time step. In Vinyals' method, inputs to the decoder at the following time steps are features of previously predicated context words. They report BLUE-1, BLUE-2 and BLUE-3 scores on the Flickr8k and Flickr30k datasets and report BLUE-4, METEOR and CIDEr scores on the MSCOCO dataset. As for the reported results, they outperform multimodal learning based image captioning methods <ref type="bibr" target="#b34">[35]</ref>[61] and the other encoder-decoder based method <ref type="bibr" target="#b33">[34]</ref>. The results show that compared to multimodal learning based image captioning framework, the encoder-decoder framework is more effective for image captioning.</p><p>Following the encoder-decoder paradigm, Jia et al. <ref type="bibr" target="#b64">[65]</ref> propose to extract semantic information from images and add the information to each unit of the Long Short-Term Memory Recurrent Neural Network during the process of sentence generation for generating image descriptions that are closely related to image contents. Through this manner, the BLEU-n scores on the Flickr8k dataset are improved to 0.647, 0.459, 0.318 and 0.216, respectively. And the BLEU-n scores on the Flickr30k dataset are improved to 0.646, 0.446, 0.305 and 0.206, respectively. The METEOR scores on the Flickr8k and Flickr30k are 0.202 and 0.179, respectively. Compared to the basic encoderdecoder framework, results achieved by their method are much higher. And scores reported by the authors on the MSCOCO dataset are also competitive with other methods.</p><p>With the encoder-decoder framework, Xu et al. <ref type="bibr" target="#b67">[68]</ref> propose to add the attention mechanism to the model, so that the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>attentive encoder-decoder model is able to dynamically attend salient image regions during the process of image description generation. Xu et al. reported their BLEU-n and METEOR scores on all three benchmark datasets. Their results are comparable to Jia et al. <ref type="bibr" target="#b64">[65]</ref>.</p><p>To take advantages of the complimentary properties of bottomup and top-down image captioning approaches, You et al. <ref type="bibr" target="#b68">[69]</ref> propose a semantic attention model to incorporate cognitive visual cues into the decoder as attention guidance for image captioning. Their method is evaluated on the Flickr30k and MSCOCO dataset, with BLEU-n and METEOR scores reported. The experiment results show that their method can improve the scores further compared to Xu et al. <ref type="bibr" target="#b67">[68]</ref> and Jia et al. <ref type="bibr" target="#b64">[65]</ref>. The results show that appropriate modifications to the basic encoder-decoder framework by introducing attention mechanism can improve the image captioning performances effectively.</p><p>A compositional architecture is used by Fu et al. <ref type="bibr" target="#b71">[72]</ref> to integrate independent building blocks for generating captions for input images. In their method, the word generation process is aligned to visual perception of image regions, and scenespecific contexts are introduced to capture high-level semantic information in images for adapting word generation to specific scene types. The authors report their experiment results on all three benchmark datasets with respect to evaluation metrics BLEU-n, METEOR and CIDEr. Most of the reported results can outperform other methods. However, although methods based on compositional architectures can utilize information from different sources and take advantages of strengths of various methods to give better results than most of the other methods, they are usually much more complex and relatively hard to implement.</p><p>To ensure consistency in evaluation of image captioning methods, a test server is hosted by the MSCOCO team <ref type="bibr" target="#b127">[128]</ref>. For method evaluation, this server allows researchers to forward captions generated by their own models to it for computing several popular metric scores. The computed metric scores include BLEU, METEOR, ROUGE and CIDEr. The evaluation on the server is on the "test 2014" test set of the Microsoft COCO Caption dataset, whose ground truth captions are unavailable publicly. With each image in the test set accompanied by 40 human-written captions, two types of metrics can be computed for caption evaluation, i.e. c5 and c40, which means to compare one caption against 5 reference captions and 40 reference captions for metric score computation, respectively. Evaluation results of previous methods on the test server are summarized in Table <ref type="table" target="#tab_6">4</ref>.</p><p>From Table <ref type="table" target="#tab_6">4</ref>, it can be seen that image captioning evaluation metric scores computed based on c40 are higher than the ones computed based on c5. This is because the evaluation metrics are computed based on the consistency between the generated description and the reference descriptions. Therefore, more references can usually lead to higher probability of matching, resulting higher metric scores.</p><p>From Table <ref type="table" target="#tab_5">3</ref> and Table <ref type="table" target="#tab_6">4</ref>, it can be seen that although image captioning evaluation metric scores computed on the MSCOCO test server are different from the ones computed under the com-monly used protocol, the tendency of the performances of the methods are similar. The method Mao et al. <ref type="bibr" target="#b34">[35]</ref>, which is multimodal learning based, is outperformed by encoder-decoder based image captioning methods Donahue et al. <ref type="bibr" target="#b33">[34]</ref> and Vinyals et al. <ref type="bibr" target="#b63">[64]</ref>. Although both methods Donahue et al. <ref type="bibr" target="#b33">[34]</ref> and Vinyals et al. <ref type="bibr" target="#b63">[64]</ref> are based on the encoder-decoder framework, with different decoding mechanisms, like in Table <ref type="table" target="#tab_4">2</ref> and Table <ref type="table" target="#tab_5">3</ref>, Vinyals et al. <ref type="bibr" target="#b63">[64]</ref> achieve higher scores than Donahue et al. <ref type="bibr" target="#b33">[34]</ref>, with respect to all used evaluation metrics.</p><p>Incorporating additional information into the encoder-decoder framework can improve the image captioning performance further. For example, by using the attention mechanism, Xu et al. <ref type="bibr" target="#b67">[68]</ref> give superior performances to Donahue et al. <ref type="bibr" target="#b33">[34]</ref>. By incorporating visual concepts into the encoder-decoder framework, Wu et al. <ref type="bibr" target="#b65">[66]</ref> outperform Xu et a. <ref type="bibr" target="#b67">[68]</ref>. By using a semantic attention model, You et al. <ref type="bibr" target="#b68">[69]</ref> achieve superior performances to nearly all the other methods.</p><p>These results show that various kinds of cues from the images can be utilized to improve image captioning performances of the encoder-decoder framework. And effectiveness of different information may be different for improving the image captioning performance. And even with the same structure, when information are fed to the framework in different ways, quite different results may be achieved.</p><p>On MSCOCO test server image captioning methods based on compositional architectures can usually give relatively good results. Fu et al. <ref type="bibr" target="#b71">[72]</ref>, which is a compositional architecture, achieve image captioning scores comparable to You et al. <ref type="bibr" target="#b68">[69]</ref>, and another compositional method Fang et al. <ref type="bibr" target="#b32">[33]</ref> can also outperform multimodal based method Mao et al. <ref type="bibr" target="#b34">[35]</ref> and encoderdecoder based method Donahue et al. <ref type="bibr" target="#b33">[34]</ref> and Xu et al. <ref type="bibr" target="#b67">[68]</ref>.</p><p>In summary, from Table <ref type="table" target="#tab_6">4</ref>, it can be observed that when using the MSCOCO test server for image captioning method evaluation, image captioning methods based on the encoderdecoder framework <ref type="bibr" target="#b33">[34]</ref> [64] outperform the multimodal learning image captioning method <ref type="bibr" target="#b34">[35]</ref>, noticeably. When semantic information or attention mechanisms are used <ref type="bibr" target="#b65">[66]</ref> [69], the performance can be improved further. Currently, the best results on the MSCOCO test server are achieved by image captioning methods that utilize attention mechanisms to augment the encoder-decoder framework <ref type="bibr" target="#b68">[69]</ref>  <ref type="bibr" target="#b69">[70]</ref>, which outperform the compositional method <ref type="bibr" target="#b71">[72]</ref> slightly (Accessed in March, 2017).</p><p>Finally, in Fig 5 <ref type="figure">we</ref> show examples of image captioning results obtained based on different approaches to give readers a straightforward impression for different kinds of image caption methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future research directions</head><p>Automatic image captioning is a relatively new task, thanks to the efforts made by researchers in this field, great progress has been made. In our opinion there is still much room to improve the performance of image captioning. First, with the fast development of deep neural networks, employing more powerful network structures as language models and/or visual models will undoubtedly improve the performance of image descrip-   tion generation. Second, because images are consisted of objects distributed in space, while image captions are sequences of words, investigation on presence and order of visual concepts in image captions are important for image captioning. Furthermore, since this problem fits well with the attention mechanism and attention mechanism is suggested to run the range of AIrelated tasks <ref type="bibr" target="#b128">[129]</ref>, how to utilize attention mechanism to generate image cations effectively will continue to be an important research topic. Third, due to the lack of paired image-sentence training set, research on utilizing unsupervised data, either from images alone or text alone, to improve image captioning will be promising. Fourth, current approaches mainly focus on generating captions that are general about image contents. However, as pointed by Johnson et al. <ref type="bibr" target="#b129">[130]</ref>, to describe images at a human level and to be applicable in real-life environments, image description should be well grounded by the elements of the images. Therefore, image captioning grounded by image regions will be one of the future research directions. Fifth, so far, most of previous methods are designed to image captioning for generic cases, while task-specific image captioning is needed in certain cases. Research on solving image captioning problems in various special cases will also be interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a survey on image captioning. Based on the technique adopted in each method, we classify image captioning approaches into different categories. Represen-tative methods in each category are summarized, and strengths and limitations of each type of work are talked about. We first discuss early image captioning work which are mainly retrieval based and template based. Then, our main attention is focused on neural network based methods, which give state of the art results. Because different frameworks are used in neural network based methods, we further divided them into subcategories and discussed each subcategory, respectively. After that, state of the art methods are compared on benchmark datasets. Finally, we present a discussion on future research directions of automatic image captioning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General structure of multimodal learning based image captioning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: General structure of encoder-decoder based image captioning methods. Kiros et al. introduce the encoder-decoder framework into image captioning research to unify joint image-text embedding models and multimodal neural language models, so that given an image input, a sentence output can be generated word by word [63] like language translation. They use Long Short-Term Memory (LSTM) Recurrent Neural Networks to encode textual data [107] and a deep Convolutional Neural Network to encode visual data. Then, through optimizing a pairwise ranking loss, encoded visual data is projected into an embedding space spanned by LSTM hidden states that encode textual data. In the embedding space, a structure-content neural language model is used to decode visual features conditioned on context word feature vectors, allowing for sentence generation word by word. With the same inspiration from neural machine translation, Vinyals et al. use a deep Convolutional Neural Network as an encoder to encode images and use Long Short-Term Memory (LSTM) Recurrent Neural Networks to decode obtained image features into sentences [64] [108]. With the above framework, the authors formulate image captioning as predicating the probability of a sentence conditioned on an input image: S = arg max S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: General structure of attention guided image captioning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: General structure of compositional image captioning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, respec-A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of image captioning results obtained based on different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of image captioning methods.Ordonez et al.[15], Gupta et al.<ref type="bibr" target="#b15">[16]</ref>, Hodosh et al.<ref type="bibr" target="#b31">[32]</ref>, Mason and Charniak [49], Kuznetsova et al. [50]. Template based Yang et al. [14], Kulkarni et al. [51], Li et al. [52], Mitchell et al. [53], Ushiku et al. [54]. Karpathy et al. [37], Ma et al. [56], Yan and Mikolajczyk [57], Lebret et al. [58]. Multimodal learning Kiros et al. [59], Mao et al. [60], Karpathy and Fei-Fei [61], Chen and Zitnick [62]. Encoder-decoder framework Kiros et al. [63], Vinyals et al. [64], Donahue et al. [34], Jia et al. [65] Wu et al. [66], Pu et al. [67].</figDesc><table><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Gupta et al. use Stanford CoreNLP toolkit1 to process sentences in the dataset to derive a list of phrases for each image.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>where x t is the input to the Long Short-Term Memory neural network. In the first unit, x t is an image feature, while in other units x t is a feature of previously predicated context words. The model parameter θ is obtained by maximizing the likelihood of sentence image pairs in the training set. With the trained model, possible output word sequences can be predicted by either sampling or beam search. Similar to Vinyals's work [64] [108], Donahue et al. also adopt a deep Convolutional Neural Network for encoding and Long Short-Term Memory Recurrent Networks for decoding to generate a sentence description for an input image [34]. The difference is that instead of inputting image features to the system only at the initial stage, Donahue et al. provide both image feature and context word feature to the sequential model at each time step.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Method comparison on datasets Flcikr8k and Flick30k. In this table, B-n, MT, RG, CD stand for Bleu-n, METEOR, ROUGE-L and CIDEr, respectively.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>B-1</cell><cell>B-2</cell><cell>B-3</cell><cell>Flcikr8k B-4</cell><cell>MT</cell><cell>RG</cell><cell>CD</cell><cell>B-1</cell><cell>B-2</cell><cell>B-3</cell><cell>Flcikr30k B-4</cell><cell>MT</cell><cell>RG</cell><cell>CD</cell></row><row><cell></cell><cell>Karpathy and Fei-Fei[61]</cell><cell>0.579</cell><cell>0.383</cell><cell>0.245</cell><cell>0.160</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.573</cell><cell>0.369</cell><cell>0.240</cell><cell>0.157</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Multimodal learning</cell><cell>Mao et al. [35]</cell><cell>0.565</cell><cell>0.386</cell><cell>0.256</cell><cell>0.170</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.600</cell><cell>0.410</cell><cell>0.280</cell><cell>0.190</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Kiros et al.[59]</cell><cell>0.656</cell><cell>0.424</cell><cell>0.277</cell><cell>0.177</cell><cell>0.173</cell><cell>-</cell><cell>-</cell><cell>0.600</cell><cell>0.380</cell><cell>0.254</cell><cell>0.171</cell><cell>0.169</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Donahue et al. [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.587</cell><cell>0.391</cell><cell>0.251</cell><cell>0.165</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Encoder-decoder framework</cell><cell>Vinyals et al. [64]</cell><cell>0.630</cell><cell>0.410</cell><cell>0.270</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.670</cell><cell>0.450</cell><cell>0.300</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Jia et al. [65]</cell><cell>0.647</cell><cell>0.459</cell><cell>0.318</cell><cell>0.216</cell><cell>0.202</cell><cell>-</cell><cell>-</cell><cell>0.646</cell><cell>0.446</cell><cell>0.305</cell><cell>0.206</cell><cell>0.179</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>You et al. [69]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.647</cell><cell>0.460</cell><cell>0.324</cell><cell>0.230</cell><cell>0.189</cell><cell>-</cell><cell>-</cell></row><row><cell>Attention guided</cell><cell>Xu et al. [68]</cell><cell>0.670</cell><cell>0.457</cell><cell>0.314</cell><cell>0.213</cell><cell>0.203</cell><cell>-</cell><cell>-</cell><cell>0.669</cell><cell>0.439</cell><cell>0.296</cell><cell>0.199</cell><cell>0.185</cell><cell>-</cell><cell>-</cell></row><row><cell>Compositional architectures</cell><cell>Fu et al. [72]</cell><cell>0.639</cell><cell>0.459</cell><cell>0.319</cell><cell>0.217</cell><cell>0.204</cell><cell>0.470</cell><cell>0.538</cell><cell>0.649</cell><cell>0.462</cell><cell>0.324</cell><cell>0.224</cell><cell>0.194</cell><cell>0.451</cell><cell>0.472</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Method comparison on Microsoft COCO Caption dataset under the commonly used protocol. In this table, B-n, MT, RG, CD stand for Bleu-n, METEOR, ROUGE-L and CIDEr, respectively.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>B-1</cell><cell>B-2</cell><cell>B-3</cell><cell>MSCOCO B-4</cell><cell>MT</cell><cell>RG</cell><cell>CD</cell></row><row><cell>Multimodal learning</cell><cell>Karpathy and Fei-Fei[61] Mao et al. [35]</cell><cell>0.625 0.670</cell><cell>0.450 0.490</cell><cell>0.321 0.350</cell><cell>0.230 0.250</cell><cell>0.195 -</cell><cell>--</cell><cell>0.660 -</cell></row><row><cell></cell><cell>Donahue et al. [34]</cell><cell>0.669</cell><cell>0.489</cell><cell>0.349</cell><cell>0.249</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Encoder-decoder framework</cell><cell>Jia et al. [65] Vinyals et al. [64]</cell><cell>0.670 -</cell><cell>0.491 -</cell><cell>0.358 -</cell><cell>0.264 0.277</cell><cell>0.227 0.237</cell><cell>--</cell><cell>0.813 0.855</cell></row><row><cell></cell><cell>Wu et al. [66]</cell><cell>0.74</cell><cell>0.56</cell><cell>0.42</cell><cell>0.31</cell><cell>0.26</cell><cell>-</cell><cell>0.94</cell></row><row><cell>Attention guided</cell><cell>Xu et al. [68] You et al. [69]</cell><cell>0.718 0.709</cell><cell>0.504 0.537</cell><cell>0.357 0.402</cell><cell>0.250 0.304</cell><cell>0.230 0.243</cell><cell>--</cell><cell>--</cell></row><row><cell>Compositional architectures</cell><cell>Fang et al. [33] Fu et al. [72]</cell><cell>-0.724</cell><cell>-0.555</cell><cell>-0.418</cell><cell>0.257 0.313</cell><cell>0.236 0.248</cell><cell>-0.532</cell><cell>-0.955</cell></row><row><cell cols="2">tively. On Flcikr8k, the achieved BLEU-1, BLEU-2, BLEU-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3 and BLEU-4 scores are 0.579, 0.383, 0.245 and 0.160, re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">spectively. Similar results are achieved on the Flick30k dataset,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">which are 0.573, 0.369, 0.240 and 0.157, respectively. Higher</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">scores are achieved by their method on the Microsoft COCO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Caption dataset for all the BLEU-n evaluation metrics. Further-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">more, on this dataset, METEOR and CIDEr scores are reported,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>which are 0.195 and 0.660, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Automatic metric scores on the MSCOCO test server. In this table, B-n, MT, RG, CD stand for Bleu-n, METEOR, ROUGE-L and CIDEr, respectively.</figDesc><table><row><cell></cell><cell>Category</cell><cell></cell><cell>Method</cell><cell>B-1</cell><cell>B-2</cell><cell>B-3</cell><cell>MSCOCO c5 B-4</cell><cell>MT</cell><cell>RG</cell><cell>CD</cell><cell>B-1</cell><cell>B-2</cell><cell>B-3</cell><cell>MSCOCO c40 B-4</cell><cell>MT</cell><cell>RG</cell><cell>CD</cell></row><row><cell cols="2">Multimodal learning</cell><cell></cell><cell>Mao et al. [35]</cell><cell>0.680</cell><cell>0.506</cell><cell>0.369</cell><cell>0.272</cell><cell>0.225</cell><cell>0.499</cell><cell>0.791</cell><cell>0.865</cell><cell>0.760</cell><cell>0.641</cell><cell>0.529</cell><cell>0.304</cell><cell>0.640</cell><cell>0.789</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Donahue et al. [34]</cell><cell>0.700</cell><cell>0.530</cell><cell>0.380</cell><cell>0.280</cell><cell>0.240</cell><cell>0.520</cell><cell>0.870</cell><cell>0.870</cell><cell>0.770</cell><cell>0.650</cell><cell>0.530</cell><cell>0.320</cell><cell>0.660</cell><cell>0.890</cell></row><row><cell cols="3">Encoder-decoder framework</cell><cell>Vinyals et al. [64]</cell><cell>0.713</cell><cell>0.542</cell><cell>0.407</cell><cell>0.309</cell><cell>0.254</cell><cell>0.530</cell><cell>0.943</cell><cell>0.895</cell><cell>0.802</cell><cell>0.694</cell><cell>0.587</cell><cell>0.346</cell><cell>0.682</cell><cell>0.946</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Wu et al. [66]</cell><cell>0.730</cell><cell>0.560</cell><cell>0.410</cell><cell>0.310</cell><cell>0.250</cell><cell>0.530</cell><cell>0.920</cell><cell>0.890</cell><cell>0.800</cell><cell>0.690</cell><cell>0.580</cell><cell>0.330</cell><cell>0.670</cell><cell>0.930</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Xu et al. [68]</cell><cell>0.705</cell><cell>0.528</cell><cell>0.383</cell><cell>0.277</cell><cell>0.241</cell><cell>0.516</cell><cell>0.865</cell><cell>0.881</cell><cell>0.779</cell><cell>0.658</cell><cell>0.537</cell><cell>0.322</cell><cell>0.654</cell><cell>0.893</cell></row><row><cell cols="2">Attention guided</cell><cell></cell><cell>You et al. [69]</cell><cell>0.731</cell><cell>0.565</cell><cell>0.424</cell><cell>0.316</cell><cell>0.250</cell><cell>0.535</cell><cell>0.943</cell><cell>0.9</cell><cell>0.815</cell><cell>0.709</cell><cell>0.599</cell><cell>0.335</cell><cell>0.682</cell><cell>0.958</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Yang et al. [70]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.597</cell><cell>0.347</cell><cell>0.686</cell><cell>0.969</cell></row><row><cell cols="2">Compositional architectures</cell><cell></cell><cell>Fang et al. [33] Fu et al. [72]</cell><cell>0.695 0.722</cell><cell>-0.556</cell><cell>-0.418</cell><cell>0.291 0.314</cell><cell>0.247 0.248</cell><cell>0.519 0.530</cell><cell>0.912 0.939</cell><cell>0.880 0.902</cell><cell>-0.817</cell><cell>-0.711</cell><cell>0.567 0.601</cell><cell>0.331 0.336</cell><cell>0.662 0.680</cell><cell>0.925 0.946</cell></row><row><cell>"</cell><cell>#</cell><cell>$</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(</cell><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>!!</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>'</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://nlp.stanford.edu/software/corenlp.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>ILOG CPLEX: High-performance software for mathematical programming and optimization. http://www.ilog.com/products/cplex/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://catalog.ldc.upenn.edu/LDC2003T05</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Available at http://ml.nec-labs.com/senna/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was Supported by National Natural Science Foundation of China (61602027).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What do we perceive in a glance of a real-world scene?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3177" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining semantic affordances of visual object categories</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4259" to="4267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>European Confrence on Computer Vision</publisher>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language description of human activities from video images based on concept hierarchy of actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International of Vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic generation of natural language descriptions for images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moellic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bourgeoys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Recherche Dinformation Assistee Par Ordinateur</title>
		<meeting>Recherche Dinformation Assistee Par Ordinateur</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Choosing linguistics over vision to describe images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep hierarchical visual feature coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2212" to="2225" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Darrell, Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caffe</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Research on point-wise gated deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1210" to="1221" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-tuning deep belief networks using harmony search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="875" to="885" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid deep neural network model for human action recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ijjina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="936" to="952" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feedforward kernel neural networks, generalized least learning machine, and its deep learning with application to image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Growing random forest on deep convolutional neural networks for scene categorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="279" to="287" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Merrinboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<idno>arXiv preprint:1406.1078</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing:deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: data, models and evaluation metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R R M S</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<title level="m">Vqa: Visual question answering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ask your neurons:a neuralbased approach to answering questions about images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="2296" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual turing test for computer vision systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="3618" to="3623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic caption generation for news images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A context-driven extractive framework for generating realistic image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="2712" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence to sequence -video to text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving lstmbased video description with linguistic knowledge mined from text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nonparametric method for data driven image captioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Composition and compression of trees for image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Treetalk</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transaction of Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="351" to="362" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Composing simple image descriptions using web-scale n-grams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter</title>
		<meeting>the 13th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Common subspace for model and similarity: Phrase learning for caption generation from images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2668" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multimodal convolutional neural networks for matching image and sentences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lifeng</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Phrase-based image captioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1090</idno>
		<title level="m">Explain images with multimodal recurrent neural networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Guiding the long-short term memory model for image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2407" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems?</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2361" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Aligning where to see and what to tell: Image captioning with region-based attention and scene-specific contexts</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Describing images by feeding lstm with structural words</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Image description through fusion based recurrent multi-modal learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Oruganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3613" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A parallel-fusion rnn-lstm architecture for image caption generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning like a child: Fast novel visual concept learning from sentence descriptions of images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2533" to="2541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale nlp with cc and boxer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Kernel independent component analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Conference on Computational Natural Language Learning</title>
		<meeting>of the Annual Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression an integer linear programming approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="339" to="429" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Meeting of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Efficient image annotation for automatic sentence generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia</title>
		<meeting>the 20th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MT Summit</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Phrasal recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2854" to="2865" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><surname>Devise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermannet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Architec-A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<title level="m">A convolutional neural network for modelling sentences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficien testimation of word representations in vector space</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<title level="m">Lstm: A search space odyssey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Singlelabel to multi-label</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A feedback model of visual attention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spratling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="237" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Image description using visual dependency representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1419" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Trainable methods for surface natural language generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
	<note>North American chapter of the Association for Computational Linguistics conference</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting on Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Meeting on Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: new similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: data collection and evaluation server</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Densecap</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
