<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beckman Institute for Advanced Science and Tech-nology</orgName>
								<orgName type="department" key="dep2">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B8CE093A6D1BAE1FE6ABF560860090D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relevance Feedback: A Power Tool for Interactive Content-Based Image Retrieval Yong Rui, Thomas S. Huang, Fellow, IEEE, Michael Ortega, and Sharad Mehrotra Abstract-Content-based image retrieval (CBIR) has become one of the most active research areas in the past few years. Many visual feature representations have been explored and many systems built. While these research efforts establish the basis of CBIR, the usefulness of the proposed approaches is limited. Specifically, these efforts have relatively ignored two distinct characteristics of CBIR systems: 1) the gap between high-level concepts and low-level features, and 2) subjectivity of human perception of visual content.</p><p>This paper proposes a relevance feedback based interactive retrieval approach, which effectively takes into account the above two characteristics in CBIR. During the retrieval process, the user's high-level query and perception subjectivity are captured by dynamically updated weights based on the user's feedback. The experimental results over more than 70 000 images show that the proposed approach greatly reduces the user's effort of composing a query, and captures the user's information need more precisely.</p><p>Index Terms-Content-based image retrieval, interactive multimedia processing, relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the advances in computer technologies and the advent of the World-Wide Web, there has been an explosion in the amount and complexity of digital data being generated, stored, transmitted, analyzed, and accessed. Much of this information is multimedia in nature, including digital images, video, audio, graphics, and text data. In order to make use of this vast amount of data, efficient and effective techniques to retrieve multimedia information based on its content need to be developed. Among the various media types, images are of prime importance. Not only it is the most widely used media type besides text, but it is also one of the most widely used bases for representing and retrieving videos and other multimedia information. This paper deals with the retrieval of images based on their contents, even though the approach is readily generalizable to other media types.</p><p>Keyword annotation is the traditional image retrieval paradigm. In this approach, the images are first annotated manually by keywords. They can then be retrieved by their corresponding annotations. However, there are three main difficulties with this approach, i.e., the large amount of manual effort required in developing the annotations, the differences in interpretation of image contents, and the inconsistency of the keyword assignments among different indexers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>. As the size of image repositories increases, the keyword annotation approach becomes infeasible.</p><p>To overcome the difficulties of the annotation-based approach, an alternative mechanism, content-based image retrieval (CBIR), was proposed in the early 1990's. Besides using human-assigned keywords, CBIR systems use the visual content of the images, such as color, texture, and shape features, as the image index. This greatly alleviates the difficulties of the pure annotation-based approach since the feature extraction process can be made automatic and the image's own content is always consistent. Since its advent, CBIR has attracted great research attention, ranging from government <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, industry <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, to universities <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Even ISO/IEC has launched a new work item, MPEG-7 <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, to define a standard multimedia content description interface. Many special issues from leading journals have been dedicated to CBIR <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b43">[44]</ref>, and many CBIR systems, both commercial <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and academic <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref>, have been developed recently.</p><p>Despite the extensive research effort, the retrieval techniques used in CBIR systems lag behind the corresponding techniques in today's best text search engines, such as Inquery <ref type="bibr" target="#b5">[6]</ref>, Alta Vista, Lycos, etc. At the early stage of CBIR, research primarily focused on exploring various feature representations, hoping to find a "best" representation for each feature. For example, for the texture feature alone, almost a dozen representations have been proposed <ref type="bibr" target="#b17">[18]</ref>, including Tamura <ref type="bibr" target="#b10">[11]</ref>, MSAR <ref type="bibr" target="#b26">[27]</ref>, Word decomposition <ref type="bibr" target="#b20">[21]</ref>, Fractal <ref type="bibr" target="#b7">[8]</ref>, Gabor filter <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b22">[23]</ref>, Wavelets <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b23">[24]</ref>, etc. The corresponding system design strategy for early CBIR systems is to first find the "best" representations for the visual features. Then we have the following.</p><p>• During the retrieval process, the user selects the visual feature(s) that he or she is interested in. In the case of multiple features, the user needs to also specify the weights for each of the features. • Based on the selected features and specified weights, the retrieval system tries to find similar images to the user's query. We refer to such systems as computer-centric systems. While this approach establishes the basis of CBIR, the performance is not satisfactory due to the following two reasons.</p><p>• The gap between high level concepts and low level features.</p><p>The assumption that the computer-centric approach makes is that the high-level concepts to low-level features mapping is easy for the user to do. While, in some cases, the assumption is true, e.g., mapping a high-level concept (fresh apple) to low-level features (color and shape), in other cases, this may not be true. One example is to map an ancient vase with sophisticated design to an equivalent representation using low-level features. A gap exists between the two levels. • The subjectivity of human perception. Different persons, or the same person under different circumstances, may perceive the same visual content differently. This is called human perception subjectivity <ref type="bibr" target="#b39">[40]</ref>. The subjectivity exists at various levels. For example, one person may be more interested in an image's color feature, while another may be more interested in the texture feature. Even if both people are interested in texture, the way they perceive the similarity of texture may be quite different. This is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Among the above three texture images, some may say that (a) and (b) are more similar if they do not care for the intensity contrast, while others may say that (a) and (c) are more similar if they ignore the local property on the seeds. No single texture representation can capture everything. Different representations capture the visual feature from different perspectives. In the computer-centric approach, the "best" features and representations and their corresponding weights are fixed, which cannot effectively model high-level concepts and user's perception subjectivity. Furthermore, specification of weights imposes a great burden on the user as it requires the user to have a comprehensive knowledge of the low-level feature representations used in the retrieval system, which is normally not the case.</p><p>Motivated by the limitations of the computer-centric approach, recent research focus in CBIR has moved to an interactive mechanism that involves a human as part of the retrieval process <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Examples include interactive region segmentation <ref type="bibr" target="#b8">[9]</ref>, interactive image database annotation <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b35">[36]</ref>, usage of supervised learning before the retrieval <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and interactive integration of keywords and high-level concepts to enhance image retrieval performance <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p><p>In this paper, to address the difficulties faced by the computer-centric approach, we present a relevance-feedbackbased approach to CBIR, in which a human and a computer interact to refine high-level queries to representations based on low-level features. Relevance feedback is a powerful technique used in traditional text-based information retrieval systems. It is the process of automatically adjusting an existing query using the information fed back by the user about the relevance of previously retrieved objects such that the adjusted query is a better approximation to the user's information need <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In the relevance-feedback-based approach <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, the retrieval process is interactive between the computer and the human. Under the assumption that high-level concepts can be captured by low-level features, the relevance feedback technique tries to establish the link between highlevel concepts and low-level features from the user's feedback. Furthermore, the burden of specifying the weights is removed from the user. The user only needs to mark which images he or she thinks are relevant to the query. The weights embedded in the query object are dynamically updated to model the high-level concepts and perception subjectivity.</p><p>The rest of the paper is organized as follows. Section II introduces a multimedia object model which supports multiple features, multiple representations, and their corresponding weights. The weights are essential in modeling high-level concepts and perception subjectivity. Section III discusses how the weights are dynamically updated based on the relevance feedback to track the user's information need. Sections IV and V discuss the normalization procedure and dynamic weight updating process, the two bases of the retrieval algorithm. Extensive experimental results over more than 70 000 images for testing both the efficiency and the effectiveness of the retrieval algorithm are given in Section VI. Concluding remarks are given in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE MULTIMEDIA OBJECT MODEL</head><p>Before we describe how the relevance feedback technique can be used for CBIR, we first need to formalize how an image object is modeled <ref type="bibr" target="#b39">[40]</ref>. An image object is represented as <ref type="bibr" target="#b0">(1)</ref> • is the raw image data, e.g., a JPEG image. • is a set of low-level visual features associated with the image object, such as color, texture, and shape.</p><p>• is a set of representations for a given feature e.g., both color histogram and color moments are representations for the color feature <ref type="bibr" target="#b50">[51]</ref>. Note that each representation itself may be a vector consisting of multiple components, i.e., <ref type="bibr" target="#b1">(2)</ref> where is the length of the vector. In contrast to the computer-centric approach's single representation and fixed weights, the proposed object model supports multiple representations with dynamically updated weights to accommodate the rich content in the image objects. Weights exist at various levels. and are associated with features representations and components respectively. The goal of relevance feedback, described in the next section, is to find the appropriate weights to model the user's information need.</p><p>Further, note that a query has the same model as that of the image objects since it is also an image object in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INTEGRATING RELEVANCE FEEDBACK IN CBIR</head><p>An image object model together with a set of similarity measures specifies a CBIR model The similarity measures are used to determine how similar or dissimilar two objects are. Different similarity measures may be used for different feature representations. For example, Euclidean is used for comparing vector-based representations, while histogram intersection is used for comparing color histogram representations.</p><p>Based on the image object model and the set of similarity measures, the retrieval process is described below and also illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>1) Initialize the weights to which is a set of no-bias weights. That is, every entity is initially of the same importance</p><formula xml:id="formula_0">(3) (4) (5)</formula><p>where is the number of features in set is the number of representations for feature and is the length of the presentation vector 2) The user's information need, represented by the query object is distributed among different features according to their corresponding weights 3) Within each feature the information need is further distributed among different feature representations according to the weights .</p><p>4) The objects' similarity to the query, in terms of is calculated according to the corresponding similarity measure and the weights 7) The objects in the database are ordered by their overall similarity to The most similar ones are returned to the user, where is the number of objects the user wants to retrieve. 8) For each of the retrieved objects, the user marks it as highly relevant, relevant, no opinion, nonrelevant, or highly nonrelevant, according to his information need and perception subjectivity. 9) The system updates the weights (described in Section V) according to the user's feedback such that the adjusted is a better approximation to the user's information need. 10) Go to step 2) with the adjusted and start a new iteration of retrieval. In Fig. <ref type="figure" target="#fig_1">2</ref>, the information need embedded in flows up, while the content of 's flows down. They meet at the dashed line, where the similarity measures are applied to calculate the similarity values 's between and 's. Following the information retrieval theories <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b45">[46]</ref>, the objects stored in the database are considered objective, and their weights are fixed. Whether the query is considered objective or subjective, and whether its weights can be updated distinguishes the proposed relevance feedback approach from the computer-centric approach. In the computer-centric approach, a query is considered objective, the same as the objects stored in the database, and its weights are fixed. Because of the fixed weights, this approach cannot effectively model highlevel concepts and human perception subjectivity. It requires the user to specify a precise set of weights at the query stage, which is normally not possible. On the other hand, queries in the proposed approach are considered as subjective. That is, during the retrieval process, the weights associated with the query can be dynamically updated via relevance feedback to reflect the user's information need. The burden of specifying the weights is removed from the user.</p><p>Note that in the proposed retrieval algorithm, both and are linear combinations of their corresponding lower level similarities. The basis of the linear combination is that the weights are proportional to the entities' relative importance <ref type="bibr" target="#b11">[12]</ref>. For example, if a user cares twice as much about one feature (color) as he does about another feature (shape), the overall similarity would be a linear combination of the two individual similarities with the weights being 2/3 and 1/3, respectively <ref type="bibr" target="#b11">[12]</ref>. Furthermore, because of the nature of linearity, these two levels can be combined into one, i.e., <ref type="bibr" target="#b8">(9)</ref> where 's are now redefined to be the weights by which the information need in is distributed directly into 's. Note that it is not possible to absorb into since the calculation of can be a nonlinear function of 's, such as Euclidean or histogram intersection.</p><p>In the next two sections, we will discuss two key components of this retrieval algorithm, i.e., normalization and weight updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NORMALIZATION</head><p>In the retrieval algorithm described in the previous section, we have assumed that the similarity values of each representation 's are of the same dynamic range, say, from 0 to 1. Otherwise, the linear combination of 's to form (9) becomes meaningless. One may overshadow the others just because its magnitude is large. For the same reason, when calculating 's, the vector components 's should also be normalized before applying the similarity measure We refer to the normalization of 's as intranormalization and the normalization of 's as inter-normalization <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intranormalization</head><p>This normalization process puts equal emphasis on each component within a representation vector To see the importance of this, note that different components within a vector may be of totally different physical quantities. Their magnitudes can vary drastically, thereby biasing the similarity measure.</p><p>To simplify the notations, define That is, every representation vector will go through the following normalization procedure.</p><p>Assume that there are images in the database, and let be the image id index; then <ref type="bibr" target="#b9">(10)</ref> is the representation vector for image where is the length of vector</p><p>If we put all 's into a matrix form, we have a matrix <ref type="bibr" target="#b10">(11)</ref> where is the th component in vector Now, the th column of matrix is a length-sequence. Denote this sequence as</p><p>Our goal is to normalize the entries in each column to the same range so as to ensure that each individual component receives equal emphasis when calculating the similarity between two vectors. One way of normalizing the sequence is to find the maximum and minimum values of and normalize the sequence to [0, 1] as follows: <ref type="bibr" target="#b11">(12)</ref> where and refer to the smallest and the largest values in the sequence Although simple, this is not a desirable normalization procedure. Let us consider a sequence 1.0, 1.1, 1.2, 1.3, 100.0 If we use <ref type="bibr" target="#b11">(12)</ref> to normalize the sequence, most of the [0, 1] range will be taken away by a single entry 100.0, and most of the information in 1.0, 1.1, 1.2, 1.3 is warped into a very narrow range.</p><p>A better approach is to use the Gaussian normalization. Assuming the sequence to be a Gaussian sequence, we compute the mean and standard deviation of the sequence. We then normalize the original sequence to a sequence as follows:</p><p>(13)</p><p>It is easy to prove that, after the normalization according to <ref type="bibr" target="#b12">(13)</ref>, the probability of an entry's value being in the range of <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b0">1]</ref> is 68%. If we use in the denominator, according to the 3-rule, the probability of an entry's value being in the range of <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b0">1]</ref> is approximately 99%. In practice, we can consider all of the entry values to be within the range of <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b0">1]</ref> by mapping the out-of-range values to either 1 or 1. The advantage of this normalization process over <ref type="bibr" target="#b11">(12)</ref> is that the presence of a few abnormally large or small values, such as the 100.0 entry in the example sequence, does not bias the importance of a component in computing the similarity between vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Internormalization</head><p>The intranormalization procedure ensures the equal emphasis of each component within a representation vector On the other hand, the internormalization procedure ensures equal emphasis of each individual similarity value within the overall similarity value Depending on the similarity measure used, the values of 's can be of quite different dynamic ranges. In order to ensure that no single will overshadow the others only because it has a larger magnitude, internormalization should be applied. This procedure is summarized as follows.</p><p>1) For any pair of images and in the image collection, compute their similarity As explained in the intranormalization subsection, this Gaussian normalization procedure ensures that 99% of all of the values will be within the range of An additional shift will guarantee that 99% of similarity values are within [0, 1] (17</p><formula xml:id="formula_1">)</formula><p>After this shift, in practice, we can consider all of the values to be within the range of [0, 1] since an image whose distance from the query is greater than 1 is very dissimilar and can be disregarded without affecting the retrieval results. In the above normalization process, the first two steps are done off line to obtain and The last two steps are done on line to convert the unnormalized value to normalized ones by using the precalculated statistics and The above-described normalization process assumes that is large enough, such that and calculated based on similarity values approximate the true mean and standard deviation of the distribution of all possible 's by the law of large number (LLN) <ref type="bibr" target="#b49">[50]</ref>. This assumption is important since it ensures that we can use <ref type="bibr" target="#b15">(16)</ref> to normalize the similarity value between an image and a query where the query is arbitrary and may not be one of the images in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WEIGHT UPDATING</head><p>After the intra-and internormalization procedures discussed above, the components within a vector as well as 's within the overall similarity are of equal emphasis. This objective equality allows us to meaningfully associate subjectively unequal intra-and interweights for a particular query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Update of (Interweight)</head><p>The 's associated with the 's reflect the user's different emphasis of a representation in the overall similarity. The support of different weights enables the user to specify his or her information need more precisely. We will next discuss how to update 's according to user's relevance feedback. Let be the set of the most similar objects according to the overall similarity value <ref type="bibr" target="#b17">(18)</ref> Let be the set containing the relevance scores fed back by the user for 's (see Section III):</p><formula xml:id="formula_2">if highly relevant (19) if relevant (20) if no opinion (21) if nonrelevant (22) if highly nonrelevant (<label>23</label></formula><formula xml:id="formula_3">)</formula><p>The choice of 3, 1, 0, and 3 as the scores is arbitrary. Experimentally, we find that the above scores capture the semantic meaning of highly relevant, relevant, etc. In ( <ref type="formula">19</ref>)-( <ref type="formula" target="#formula_2">23</ref>), we provide the user with five levels of relevance. Although more levels result in more accurate feedback, it is less convenient for the user to interact with the system. Experimentally, we find that five levels is a good tradeoff between convenience and accuracy.</p><p>For each let be the set containing the most similar objects to the query according to the similarity values <ref type="bibr" target="#b23">(24)</ref> To calculate the weight for first initialize and then use the following procedure:</p><formula xml:id="formula_4">if is in (25) if is not in (26)<label>(27)</label></formula><p>Here, we consider all of the images outside as marked with no opinion and have a score of 0. After this procedure, if set it to 0. Let be the total weights. The raw weights obtained by the above procedure are then normalized by the total weight to make the sum of the normalized weight equal to 1 <ref type="bibr" target="#b27">(28)</ref> As we can see, the more the overlap of relevant objects between and the larger the weight of That is, if a representation reflects the user's information need, it receives more emphasis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Update of (Intraweight)</head><p>The 's associated with 's reflect the different contributions of the components to the representation vector For example, in the wavelet texture representation, we know that the mean of a subband may be corrupted by the lighting condition, while the standard deviation of a subband is independent of the lighting condition. Therefore, more weight should be given to the standard deviation component, and less weight to the mean component. The support of different weights for 's enables the system to have more reliable feature representation, and thus better retrieval performance.</p><p>A standard-deviation-based weight updating approach has been proposed in our previous work <ref type="bibr" target="#b36">[37]</ref>. Out of the returned objects, for those objects that are marked highly relevant or relevant by the user, stack their representation vector 's to form a matrix, where is the number of objects marked with highly relevant or relevant. In this way, each column of the matrix is a length-sequence of 's. Intuitively, if all of the relevant objects have similar values for the component it means that the component is a good indicator of the user's information need. On the other hand, if the values for the component are very different among the relevant objects, then is not a good indicator.</p><p>Based on this analysis, the inverse of the standard deviation of the sequence is a good estimation of the weight for component That is, the smaller the variance, the larger the weight and vice versa <ref type="bibr" target="#b28">(29)</ref> where is the standard deviation of the length-sequence of 's. Here, we assume that the user will mark at least one image, besides the query image, as relevant or highly relevant, such that will not be zero. The assumption is valid since, otherwise, the user would restart a new query if nothing relevant is retrieved. Furthermore, just as in <ref type="bibr" target="#b27">(28)</ref>, we need to normalize 's in the same way <ref type="bibr" target="#b29">(30)</ref> where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Summary</head><p>Based on the description of the relevance feedback algorithm in Sections III-V, we briefly summarize the properties of the algorithm.</p><p>• Multimodality: The proposed image object model, and therefore the retrieval model, supports multiple features and multiple representations. In contrast to a computercentric approach's attempt of finding the single "best" universal feature representation, the proposed approach concentrates on how to organize the multiple feature representations, such that appropriate feature representations are invoked (emphasized) at the right place and right time.</p><p>The multimodality approach allows the system to better model user's perception subjectivity. • Interactivity: In contrast to a computer-centric approach's automated system, the proposed approach is interactive in nature. The interactivity allows the system to make use of the ability both from computer and from human. • Dynamic: In contrast to a computer-centric approach's fixed query weights, the proposed approach dynamically updates the query weights via relevance feedback. The advantages are twofold.</p><p>-Remove burden from the user: The user is no longer required to specify a precise set of weights at the query formulation stage. Instead, the user interacts with the system, indicating which returns he or she thinks are relevant. Based on the user's feedback, query weights are dynamically updated. -Remove burden from the computer: The computer is no longer required to understand the high-level concept. Based on user's feedback, the high-level concept embedded in the query weights automatically gets refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>To address the challenging research issues involved in CBIR, a multimedia analysis and retrieval system (MARS) project is ongoing at the University of Illinois <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>. MARS-1 is accessible via the Internet at http://jadzia.ifp.uiuc.edu:8000. The relevance feedback architecture proposed in this paper is currently being integrated into MARS-2.</p><p>In the experiments reported here, we test our proposed approach over two image collections. The first image collection is provided by the Fowler Museum of Cultural History at the University of California, Los Angeles. It contains 286 ancient African and Peruvian artifacts, and is part of the Museum Educational Site Licensing Project (MESL), sponsored by the Getty Information Institute. The second image collection is obtained from Corel Corporation. It contains more than 70 000 images covering a wide range of more than 500 categories. The 120 80 resolution images are available at http://corel.digitalriver.com/commerce/photostudio/catalog. htm.</p><p>We have chosen these two test sets because they provide complementary properties to each other. The size of the MESL test set is relatively small, but it allows us to explore all of the color, texture, and shape features simultaneously in a meaningful way. On the other hand, although the heterogeneity of the Corel test set makes the extraction of some features, such as shape, difficult, it has the advantages of large size and wide coverage. We believe that testing our proposed approach on both sets will provide a fair evaluation of its performance.</p><p>For the MESL test set, the visual features used are color, texture, and shape of the objects in the image. That is color, texture, shape</p><p>The representations used are color histogram and color moments <ref type="bibr" target="#b50">[51]</ref> for the color feature, Tamura <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b51">[52]</ref> and cooccurrence matrix <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref> texture representations for the texture feature, and a Fourier descriptor and Chamfer shape descriptor <ref type="bibr" target="#b40">[41]</ref> for the shape feature color histogram, color moments, Tamura, cooccurrence matrix, Fourier descriptor Chamfer shape descriptor For the Corel test set, the visual features used are color and texture. That is color, texture</p><p>The representations used are color histogram and color moments <ref type="bibr" target="#b50">[51]</ref> for the color feature, and cooccurrence matrix <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref> texture representation for the texture feature color histogram, color moments, cooccurrence matrix</p><p>Our proposed relevance feedback architecture is an open retrieval architecture. Other visual features or feature representations can be easily incorporated if needed. The similarity measures used for the corresponding representations are the following. Color histogram intersection <ref type="bibr" target="#b50">[51]</ref> is used for the color histogram representation, weighted Euclidean is used for the color moments, Tamura texture, cooccurrence matrix, and Fourier shape descriptor <ref type="bibr" target="#b40">[41]</ref> representations, and Chamfer matching <ref type="bibr" target="#b40">[41]</ref> is used for the chamfer shape representation.</p><p>There are two sets of experiments reported here. The first set of experiments is on the efficiency of the retrieval algorithm, i.e., how fast the retrieval results converge to the true results. The second set of experiments is on the effectiveness of the retrieval algorithm, i.e., how good the retrieval results are subjectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Efficiency of the Algorithm</head><p>The ultimate goal of the relevance feedback technique is to help the user retrieve what he or she wants. Because of this, it is very important to verify that the above proposed relevance feedback retrieval algorithm converges to the user's true information need fast.</p><p>The only assumption that we make in the experiments is that the user is consistent when doing relevance feedback. That is, the user does not change his or her information need during the feedback process, such that the feedback process can be simulated by a computer.</p><p>As we have discussed in Sections II and III, the image object is modeled by the combinations of representations with their corresponding weights. If we fix the representations, then a query can be completely characterized by the set of weights embedded in the query object Let set be the highly relevant set, set be the relevant set, set be the no-opinion set, set be the nonrelevant set, and set be the highly nonrelevant set. The testing procedure is described as follows.</p><p>1) Retrieval results of the ideal case: Let be the set of weights associated with the query object</p><p>The retrieval results based on are the ideal case, and serve as the baseline for comparing other nonideal cases. a) Specify a set of weights to the query object. b) Set to c) Invoke the retrieval algorithm (see Section III). d) Obtain the best returns e) From find the sizes of sets , 's are marked by a human for testing purposes. f) Calculate the ideal weighted relevant count as <ref type="bibr" target="#b32">(33)</ref> Note that 3 and 1 are the scores of the highly relevant and relevant sets, respectively (see Section V). Therefore, is the maximal achievable weighted relevant count, and serves as the baseline for comparing other nonideal cases.</p><p>2) Retrieval results of relevance feedback case: In the real retrieval situation, neither the user nor the computer knows the specified weights However, the proposed retrieval algorithm will move the initial weights to the ideal weights via relevance feedback. a) Set b) Set the maximum number of iterations of relevance feedback, c) Initialize the iteration counter, d) Invoke the retrieval algorithm, and get back the best returns (see Section III). e) Compute the weighted relevant count for the current iteration <ref type="bibr" target="#b33">(34)</ref> where and are the number of highly relevant and relevant objects in These two numbers can be determined by comparing against f) Compute the convergence ratio for the current iteration <ref type="bibr" target="#b34">(35)</ref> g) Set If quit; otherwise, continue. h) Feed back the current five sets to the retrieval system. i) Update the weights according to ( <ref type="formula">25</ref>)- <ref type="bibr" target="#b29">(30)</ref>. Go to Step 2d). There are three parameters that affect the behavior of the retrieval algorithm, i.e., number of feedback iterations number of returns and specified query weights For the more relevance feedback iterations, the better the retrieval performance. However, we cannot expect the user to do relevance feedback forever. In the experiments reported here, we set to study the convergence behavior of the first three iterations. The experiments show that the greatest increase occurs in the first iteration of feedback, which is a very desirable property.</p><p>In all of the experiments reported here, for both the MESL and the Corel test sets, 100 randomly selected images are used as the query images, and the values of listed in the tables are the averages of the 100 cases.</p><p>1) as a Function of : In the experiments here, we will concentrate on the effect of The effect of has been studied in our previous research in <ref type="bibr" target="#b36">[37]</ref>. Specifically, only is specified for in the experiments. In the MESL test set, there are six 's as described at the beginning of this section. Therefore, both and have six components. In addition <ref type="bibr" target="#b35">(36)</ref> where each entry in the vector is the weight for its corresponding representation.</p><p>In the Corel test set, there are three 's as described at the beginning of this section. Therefore, both and have three components. In addition <ref type="bibr" target="#b36">(37)</ref> where each entry in the vector is the weight for its corresponding representation.</p><p>Obviously, the retrieval performance is affected by the offset of the specified weights from the initial weights We classify into two categories, i.e., moderate offset and      To better the process of convergence, we redraw the average convergence ratio of the MESL test set and the Corel test set in Fig. <ref type="figure" target="#fig_5">3</ref>.</p><p>Based on the tables and figures, some observations can be made.</p><p>• In all of the cases, increases the most in the first iteration. Later iterations only result in a minor increase in This is a very desirable property, which ensures that the user gets reasonable results after only one iteration of feedback. No feedbacks are needed if time is a concern. • is affected by the degree of offset. The less the offset, the higher the final absolute However, the more the offset, the higher the relative increase of . • Although the final absolute is higher for the MESL test set than that for the Corel test set, the final relative increase of is comparable for both test sets (around 10-20%). The convergence process is more challenging for the Corel test set because of its much bigger size and fewer feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>as a Function of : is related to the size of the test data set. Normally, only 2-5% of the whole data set is needed. For the MESL test set, we test For the Corel test set, we test . The experimental results are listed in Tables <ref type="table" target="#tab_4">V</ref> and<ref type="table" target="#tab_5">VI</ref>. Some observations can be made based on the experiments.</p><p>• The first iteration's increases the most when is large. This is because, the larger the number of returns, the more the fed-back information, and thus the better the retrieval performance. • In the second and third iterations, is almost independent of different 's. This is because, after the first iteration's feedback, most of the desired objects have been found, and later performance is almost independent of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effectiveness of the Algorithm</head><p>The previous subsection's experiments focus on the convergence of the algorithm. This subsection will focus on how good the returns are subjectively. The only way of performing subjective tests is to ask the user to evaluate the retrieval system subjectively. Extensive experiments have been carried out. Users from various disciplines, such as  computer vision, art, library science, etc., as well as users from industry, have been invited to compare the retrieval performance between the proposed interactive approach and the computer-centric approach. All of the users rated the proposed approach much higher than the computer-centric approach in terms of capturing their perception subjectivity and information need.</p><p>A typical retrieval process on the MESL test set is given in Figs. <ref type="figure" target="#fig_6">4</ref> and<ref type="figure" target="#fig_7">5</ref>.</p><p>The user can browse through the image database. Once he or she finds an image of interest, that image is submitted as a query. Alternating to this query-by-example mode, the user can also submit images outside the database as queries. In Fig. <ref type="figure" target="#fig_6">4</ref>, the query image is displayed at the upper left corner, and the best 11 retrieved images, with are displayed in order from top to bottom and from left to right. The retrieved results are obtained based on their overall similarities to the query image, which are computed from all of the features and all of the representations. Some retrieved images are similar to the query image in terms of shape feature, while others are similar to the query image in terms of the color or texture feature.</p><p>Assume that the user's true information need is to "retrieve similar images based on their shapes." In the proposed retrieval approach, the user is no longer required to explicitly map his information need to low-level features, but rather, he or she can express his intended information need by marking the relevance scores of the returned images. In this example, images 247, 218, 228, and 164 are marked highly relevant. Images 191, 168, 165, and 78 are marked highly nonrelevant. Images 154, 152, and 273 are marked no opinion.</p><p>Based on the information fed back by the user, the system dynamically adjusts the weights, putting more emphasis on the shape feature, possibly even more emphasis on one of the two shape representations which matches the user's perception subjectivity of shape. The improved retrieval results are displayed in Fig. <ref type="figure" target="#fig_7">5</ref>. Note that our shape representations are invariant to translation, rotation, and scaling. Therefore, images 164 and 96 are relevant to the query image.</p><p>Similarly, a typical retrieval process over the Corel test set is given in Figs. <ref type="figure" target="#fig_8">6</ref> and<ref type="figure" target="#fig_9">7</ref>.</p><p>In Fig. <ref type="figure" target="#fig_8">6</ref>, the top left image is the query image (a glacier). Before any feedback, several human-constructed structures appear in the retrieval results. After the user feeds back his interests in ice/water-related images, in the next iteration (Fig. <ref type="figure" target="#fig_9">7</ref>), many more ice/water-related images are returned. Note that the retrieval of such images is based on both the color and the texture features.</p><p>Unlike the computer-centric approach, where the user has to precisely decompose his information need into different features and representations and precisely specify all of the weights associated with them, the proposed interactive approach allows the user to submit a coarse initial query, and continuously refine his information need via relevance feedback. This approach greatly reduces the user's effort of composing a query, and captures the user's information need more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>CBIR has emerged as one of the most active research areas in the past few years. Most of the early research effort focused on finding the "best" image feature representations. Retrieval was performed as the summation of similarities of individual feature representations with fixed weights. While this computer-centric approach establishes the basis of CBIR, the usefulness of such systems was limited due to the difficulty in representing high-level concepts using low-level features and human perception subjectivity.</p><p>In this paper, we introduce a human-computer interaction approach to CBIR based on relevance feedback. Unlike the computer-centric approach, where the user has to precisely decompose his information need into different feature representations and precisely specify all of the weights associated with them, the proposed interactive approach allows the user to submit a coarse initial query, and continuously refine his information need via relevance feedback. This approach greatly reduces the user's effort of composing a query, and captures the user's information need more precisely. Furthermore, the efficiency and effectiveness of the proposed approach have been validated by a large number of experiments.</p><p>Although the proposed retrieval model is for CBIR, it can be easily expanded to handle other media types, such as video and audio. The proposed model also has a close relationship to MPEG-7, as discussed in our previous MPEG-7 proposal <ref type="bibr" target="#b37">[38]</ref>. Furthermore, the proposed model provides a natural way of combining keyword features with visual features. We envision the importance of supporting keywords with visual features, and are currently expanding our system to handle this.</p><p>One of the future research directions of this approach is to explore optimal or suboptimal weight updating strategies. Currently, the weight updating strategy is heuristic based, and may not be the best solution. Techniques, such as expectation maximization (EM), are promising techniques worth</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Subjectivity in perceiving the texture feature.</figDesc><graphic coords="2,41.28,59.58,254.64,81.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The retrieval process.</figDesc><graphic coords="3,50.88,59.58,235.44,179.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 6 ) 5 ) 7 ) 6 )</head><label>6576</label><figDesc>Each representation's similarity values are then combined into a feature's similarity value (The overall similarity is obtained by combining indi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 14 ) 2 ) 15 ) 4 )</head><label>142154</label><figDesc>Since there are images, there are possible similarity values between any pair of images. Treat them as a data sequence, and find the mean and standard deviation of the sequence. Store and in the database to be used in later normalization.3) When a query is presented, compute the raw (unnormalized) similarity values between and the images in the database (Normalize the raw similarity values as follows:<ref type="bibr" target="#b15">(16)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>significant offset, by considering how far away they are from the initial weights For the MESL test set, the six moderate offset testing weights are The six significant offset testing weights are For the Corel test set, the three moderate offset testing weights are The three significant offset testing weights are The experimental results for these cases are summarized in Tables I-IV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convergence ratio curves. (a) MESL test set. (b) Corel test set.</figDesc><graphic coords="8,318.30,451.26,226.56,166.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Initial retrieval results.</figDesc><graphic coords="9,307.56,59.55,249.00,230.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Retrieval results after the relevance feedback.</figDesc><graphic coords="9,307.20,328.57,249.00,228.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Retrieval results before the relevance feedback.</figDesc><graphic coords="10,44.70,59.60,248.00,179.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Retrieval results after the relevance feedback.</figDesc><graphic coords="10,306.72,59.60,250.00,175.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MESL</head><label>I</label><figDesc>MODERATE OFFSET CONVERGENCE RATIO WITH N RT = 12</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MESL</head><label>II</label><figDesc>SIGNIFICANT OFFSET CONVERGENCE RATIO WITH N RT = 12</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COREL</head><label>III</label><figDesc>MODERATE OFFSET CONVERGENCE RATIO WITH N RT = 1000</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COREL</head><label>IV</label><figDesc>SIGNIFICANT OFFSET CONVERGENCE RATIO WITH N RT = 1000</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V CONVERGENCE</head><label>V</label><figDesc>RATIO FOR MESL TEST SET WITH W 3 7</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI CONVERGENCE</head><label>VI</label><figDesc>RATIO FOR COREL TEST SET WITH W 3 4</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank their colleagues, P. Kriengkrai and H. Yu, for their help in implementing the system. The authors also would like to thank the anonymous reviewers for their valuable comments in improving the quality of this paper. The example images used in this paper are from two sources. One set of images is used with permission from the Fowler Museum of Cultural History at the University of California, Los Angeles. These images were part of an image database delivery project called the Museum Educational Site Licensing Project (MESL), sponsored by the Getty Information Institute. This goal of the two-year MESL project was to test the licensing and delivery of digital images and meta data from seven U.S. museums to seven U.S. universities. The University of Illinois was selected as a participant in the MESL project. The second set of images was obtained from the Corel collection, and used in accordance with their copyright statement.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by NSF/DARPA/NASA DLI Program under Cooperative Agreement 94-11318, in part by ARL Cooperative Agreement DAAL01-96-2-0003, and in part by NSF CISE Research Infrastructure Grant CDA-9624396. The work of Y. Rui was also supported in part by a CSE Fellowship, the University of Illinois. The work of M. Ortega was also supported in part by CONACYT Grant 89061. This paper was recommended by Associate Editor S. Panchanathan. Publisher Item Identifier S 1051-8215(98)06318-6.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, he is pursuing his graduate studies at the University of Illinois at Urbana Champaign. His research interests include multimedia databases, database optimization for uncertainty support, and content-based multimedia information retrieval.</p><p>Mr. Ortega received a Fulbright/CONACYT/ García Robles scholarship to pursue graduate studies as well as the Mavis Award at the University of Illinois. He is a member of Phi Kappa Phi, the IEEE Computer Society, and the ACM. Sharad Mehrotra received the M.S. and Ph.D. degrees from the University of Texas at Austin in 1990 and 1993, respectively, both in computer science.</p><p>Subsequently, he worked at MITL, Princeton, as a Scientist from 1993 to 1994. He has been an Assistant Professor in the Computer Science Department at the University of Illinois at Urbana-Champaign since 1994. He specializes in the areas of database management, distributed systems, and information retrieval. His current research projects are on multimedia analysis, content-based retrieval of multimedia objects, multidimensional indexing, uncertainty management in databases, and concurrency and transaction management. He is the author of over 50 research publications in these areas.</p><p>Dr. Mehrotra is the recipient of the NSF Career Award and the Bill Gear Outstanding Junior Faculty Award in 1997.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MPEG-7 applications document</title>
		<idno>IEC JTC1/SC29/WG11 N1922</idno>
	</analytic>
	<monogr>
		<title level="j">MPEG</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MPEG-7: Context and objectives</title>
		<idno>ISO/IEC JTC1/SC29/WG11 N1920</idno>
	</analytic>
	<monogr>
		<title level="j">MPEG</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Third draft of MPEG-7 requirements</title>
		<idno>ISO/IEC JTC1/SC29/WG11 N1921</idno>
	</analytic>
	<monogr>
		<title level="j">MPEG</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Virage image search engine: An open framework for image management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimization of relevance feedback weights</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR&apos;95</title>
		<meeting>SIGIR&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The inquery retrieval system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf</title>
		<meeting>3rd Int. Conf</meeting>
		<imprint>
			<date type="published" when="1992-09">Sept. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture analysis and classification with tree-structured wavelet transform</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="411" />
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Approaches to image retrieval based on compressed data for multimedia database systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>New York, Buffalo</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive outlining: An improved approach using active contours</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daneels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campenhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fierens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Content-based retrieval in multimedia imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrieving images from a database using texture-algorithms from the QBIC system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci., IBM Res. Rep</title>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. RJ 9805</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating user preferences in multimedia queries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Wimmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Database Theory</title>
		<meeting>Int. Conf. Database Theory</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient and effective querying by image content</title>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep., IBM Res. Rep</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Query by image and video content: The QBIC system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hafine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Gudivada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer (Special Issue on Content-Based Image Retrieval Systems)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="1995-09">Sept. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Texture features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimedia analysis and retrieval system (MARS) project</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Annu</title>
		<meeting>33rd Annu</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image retrieval: Past, present, and future</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Multimedia Inform. Processing</title>
		<meeting>Int. Symp. Multimedia Inform. essing</meeting>
		<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Workshop report: NSF workshop on visual information management systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NSF-ARPA Workshop Visual Inform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Syst</title>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Periodicity, directionality, and randomness: Wold features for image modeling and retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Texture features and learning similarity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A toolbox for navigating large image databases</title>
		<author>
			<persName><surname>Netra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image indexing using moments and wavelets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aboulmaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consumer Electron</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="557" to="565" />
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image indexing using a texture dictionary</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIE Conf. Image Storage and Archiving Syst</title>
		<meeting>SIE Conf. Image Storage and Archiving Syst</meeting>
		<imprint>
			<biblScope unit="volume">2606</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image data</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell. (Special Issue on Digital Libraries)</title>
		<imprint>
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Texture classification and segmentation using multiresolution simultaneous autoregressive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="188" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimedia analysis and retrieval system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int</title>
		<meeting>3rd Int</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive learning using a &apos;society of models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="447" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Narasimhalu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst. (Special Section on Content-Based Retrieval)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The QBIC project: Querying images by content using color, texture and shape</title>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint>
			<date type="published" when="1994-02">Feb. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance evaluation for four classes of texture features</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Ohanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supporting similarity queries in MARS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Multimedia</title>
		<meeting>ACM Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell. (Special Issue on Digital Libraries)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Photobook: Content-based manipulation of image databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vision texture for annotation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Content-based image retrieval with relevance feedback in MARS</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mars and its applications to MPEG-7</title>
		<ptr target="ISO/IECJTC1/SC29/WG11M2290" />
	</analytic>
	<monogr>
		<title level="j">MPEG</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring video structures beyond the shots</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Multimedia Computing and Syst</title>
		<meeting>IEEE Conf. Multimedia Computing and Syst</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relevance feedback techniques in interactive content-based image retrieval</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T SPIE Storage and Retrieval of Images/Video Databases VI, EI&apos;98</title>
		<meeting>IS&amp;T SPIE Storage and Retrieval of Images/Video Databases VI, EI&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic matching tool selection using relevance feedback in MARS</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Visual Inform. Syst</title>
		<meeting>2nd Int. Conf. Visual Inform. Syst</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A relevance feedback architecture in content-based multimedia information retrieval systems</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Content-Based Access of Image and Video Libraries (in conjunciton with IEEE CVPR &apos;97)</title>
		<meeting>IEEE Workshop Content-Based Access of Image and Video Libraries (in conjunciton with IEEE CVPR &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Introduction to Modern Information Retrieval</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building large-scale digital libraries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagerover: A contentbased image browser for the World Wide Web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Taycher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Content-Based Access of Image and Video Libraries</title>
		<meeting>IEEE Workshop Content-Based Access of Image and Video Libraries</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Term-relevance computations and perfect retrieval performance</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Processing Management</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visually searching the Web for content</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="459" to="496" />
			<date type="published" when="1997">1997</date>
			<pubPlace>Summer</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Columbia Univ. CU/CTR Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transform features for texture classification and discrimination in large image databases</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An image and video search engine for the World-Wide Web</title>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Storage and Retrieval for Image and Video Databases</title>
		<meeting>SPIE Storage and Retrieval for Image and Video Databases</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Probability, Random Processes, and Estimation Theory for Engineers</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Texture features corresponding to visual perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
