<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmented Surfaces: A Spatially Continuous Work Space for Hybrid Computing Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1999">MAY 1999</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
							<email>rekimoto@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Computer Science Laboratories</orgName>
								<address>
									<addrLine>Inc. 3-14-13 Higashigotanda, Shinagawa-ku</addrLine>
									<postCode>14 l-0022</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masanori</forename><surname>Saitoh</surname></persName>
							<email>saitoh@aa.cs.keio.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Keio University</orgName>
								<address>
									<addrLine>3-14-1 Hiyoshi, Kohoku-ku</addrLine>
									<postCode>223</postCode>
									<settlement>Yokohama</settlement>
									<region>Kanagawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmented Surfaces: A Spatially Continuous Work Space for Hybrid Computing Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="1999">MAY 1999</date>
						</imprint>
					</monogr>
					<idno type="MD5">6901978D0FC535DF284B23A9E9BAFD05</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multiple device user interfaces</term>
					<term>table-sized displays</term>
					<term>wall-sized displays</term>
					<term>portable computers</term>
					<term>ubiquitous computing</term>
					<term>architectural media</term>
					<term>physical space</term>
					<term>augmented reality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our design and implementation of a computer augmented environment that allows users to smoothly interchange digital information among their portable computers, table and wall displays, and other physical objects. Supported by a camera-based object recognition system, users can easily integrate their portable computers with the pre-installed ones in the environment. Users can use displays projected on tables and walls as a spatially continuous extension of their portable computers. Using an interaction technique called hyperdragging, users can transfer information from one computer to another, by only knowing the physical relationship between them. We also provide a mechanism for attaching digital data to physical objects, such as a videotape or a document folder, to link physical and digital spaces.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>These days people can take small yet powerful computers anywhere at anytime. Modem notebook-sized portable computers have of several gigabytes of disk storage, processing power almost equal to desktop computers, and an integrated set of interface devices (LCD screen, keyboard, and pointing device). Therefore, it is not impossible to store and carry almost all one's personal data (documents, presentation slides, or images) in such a small computer.</p><p>In parallel with this tendency, our working environments, such as meeting rooms, are going to be equipped with many computing facilities such as data projectors and digital Pcmlission to make digital or hard topics of all or part of this work tix personal or classroom use is granted without fee provided thar copies are not made or distributed fbr prolit or commercial advantage: and that topics hear this notice and the full citation OK the lirst page. To copy othcrwisc, to republish, tO post on scrvcrs or to rcdistrihutc to lists. requires prior specific permission and:or a fee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cl-II '!N Pittsburgh PA USA</head><p>Copyright ACM 1999 0-201-48559-1/99/05...$5.00 whiteboards. It is becoming quite common during a meeting to make a presentation using a video projector to show slide data stored in the presenter's portable computer. It is also very common for meeting attendees to bring their own computers to take notes. In the near future, we also expect that meeting room tables and walls will act as computer displays. Eventually, virtually all the surfaces of the architectural space will function as computer displays <ref type="bibr" target="#b0">[8]</ref>. As <ref type="bibr">Lange et al. [5]</ref> pointed out, large and multiple display surfaces ar,e essential for supporting collaborative, or even individual, activities. We can simultaneously spread several data items out on these surfaces without hiding each other.</p><p>Considering these two trends, the natural consequence would be to support smooth integration between portabNle/personal and pre-installed/public computers. However, in today's computerized meeting rooms, we are often frustrated by poor supports for information exchange among personal and pre-installed computers. In our physical lives, it is quite easy to circulate physical documents among meeting participants and spread paper diagrams on the table, or hang them on the wall. During a meeting, participants around the table can quickly re-arrange these diagrams. When they are displayed on computer screens, information (exchanges between computers often require tedious network. settings or re-connection of computers. It is not easy to add annotations to an image on the projector screen while another participant is presenting his data on that screen. When you want to transfer data from your computer to others', you might need to know the network address of the target computer, even if you can physically identify that computer.</p><p>In this paper we describe our design and implementation of a computer augmented environment that allows a user to smoothly interchange digital information between their portable computers and a computerized table and wall. Using the combination of camera-based marker recognition and interaction techniques called hyperdragging and anchored cursors, users can easily add their own portable computers to that environment. This intuitive, easy-to-use system is just like dragging icons from on screens to another in a single Papers ( <ref type="formula">4</ref>04 The system also provides a mechanism for attaching digital data to physical objects, such as a videotape or a document folder, to make tight connections between physical and digital spaces.</p><p>A SPATIALLY CONTINUOUS WORKSPACE While many research systems on augmented physical spaces use pre-installed computers for interaction, we are more interested in how we can smoothly integrate our existing portable computers with the pre-installed ones.</p><p>The key features of our system design can be summarized as follows:</p><p>Environmental computers as extensions of individual computers</p><p>In our design, users can bring their own portable (notebook or palmtop) computers into the environment and put them on the table. Then, the table becomes an extended desktop for the portable computers (Figure <ref type="figure" target="#fig_0">1</ref>). That is, the user can transfer digital objects or application windows to the displays on table/wall surfaces. They can use a virtually bigger workspace around the portable computer.</p><p>The user manipulates digital objects on the table (or on the wall) using the input devices (such as a track-ball or a keyboard) belonging to the portable computer. Instead of introducing other interaction techniques such as handgesture recognition, we prefer to use portable computers because notebook computes already have an integrated set of interaction devices that are enough for most applications. With these interaction devices, users do not have to change user-interface style while dealing with the table or wall. In addition, many recent sub-notebook computers have audio I/O devices, so they can also be used to create voice notes during the task.</p><p>If two or more users sit at the same table, the table also becomes a shared workspace among them; the participants can freely interchange information among the participating portable computers by placing information items on the table/wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support for links between digital information and physical objects</head><p>In addition to providing support for portable computers, the system allows users to put non-electronic objects such as VCR tapes or printed documents on the table. By reading an attached visual marker on the object, the system recognizes it and displays digital data that is linked to that object. The user can also add other digital information by simply dragging-and-dropping it onto the object.</p><p>Although other systems also support links between physical anddigital objects (such asInfoBinder[l5], mediaBlocks[l8], and Passage[7]), these objects are only for carrying digital data and there are no particular roles in a real world. On the other hand, we are more interested in making a link between digital contents and things that also have specific roles in the real world. For example, we can attach editorial instructions When a user sits at the table and puts his/her portable computer on the table, a video camera mounted above the table finds its attached visual marker and identifies, the owner of the computer. At the same time, the location of the computer is also recognized.</p><p>When the user wishes to show his/her own data to other participants, he/she can use an interaction technique called hyperdragging (Figure <ref type="figure">4</ref>). That is, the user presses the mouse cursor on a displayed item and drags it toward the edge of the computer screen. When the cursor reaches the edge of the display, it migrates from the portable computer to the table to a VCR tape, as a digital voice note. We can also bind physical documents and digital data in a single document folder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatially Continuous Operations</head><p>During these operations, we pay special attention to how the physical layout of objects (computers and other real objects) can match the digital manipulations. In other words, the user can use the integrated spatial metaphor for manipulating information in the notebooks, on the table or wall surfaces, and other physical objects placed on the table (Figure <ref type="figure" target="#fig_1">2</ref>). For example, when the user wants to transfer data from a notebook computer to the table, he/she can simply drag it from the notebook screen to the table surface across the boundary of the notebooks. At the edge of the notebook screen, the cursor automatically moves from notebook to the table. The user can also attach digital data to the physical object by simply dragging and dropping it onto the physical object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INFOTABLE and INFOWALL: A PROTOTYPE HYBRID EN-VIRONMENT</head><p>To explore the proposed workspace model, we developed a computer-augmented environment consisting of a table (called InfoTable) and a wall (called InfoWall) that can display digital data through LCD projectors. Figure <ref type="figure">3</ref> shows the system configuration of our environment. In this environment, users can dynamicaIly connect their portable computers to perform collaborative and individual tasks. This section summarizes the user-interface features of the system.</p><p>We make some assumptions about the portable computers that can be integrated into the environment. To enable the portable computers to be identified by the pre-installed environmental computers, we attach a small visual marker (printed 2D barcode) to each portable computers and other physical object. Portable computers are also equipped with a wireless network for communicating with other computers. surface (Figure <ref type="figure">4</ref>, middle). If the cursor is grabbing an object, the dragged object also migrates from the portable computer to the table surface. By manipulating the cursor, the user can place the object at any location on the table. Furthermore, the user can move the item toward the edge of the table, to cause a hyperdrag between the InfoTable and the nearby InfoWall display (Figure <ref type="figure">4</ref>, bottom panel). This hyperdragging technique supports the metaphor of the table being a spatially continuous extended workspace for portable computers. Users can place data items such as text or graphics around the notebook computer, as if they had a virtually bigger computer desktop.</p><p>The combination of two different displays --a high-resolution small display on the portable computer and a low-resolution large display on the table --represents the user's focal and peripheral information space. While keeping the focal objects on the notebook screen, the user can spread a number of items around the computer. When the user needs one of them, he/she can immediately hyperdrag it back to the notebook screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchored cursor</head><p>While a user is manipulating his/her cursor outside the notebook computer, a line is projected from the portable computer to the cursor position. This visual feedback is called the anchored cursor. When multiple users are simultaneously manipulating objects, there are multiple cursors on the table/wall. This visual feedback makes it easy for all participants to distinguish the owner of the cursors. When two or more participants manipulating objects on the table or on the wall, anchored cursors indicate the owner of the cursor in a visual and spatial way.</p><p>The anchored cursor is also used to indicate the semantic relationships between different display surfaces. For example, while the user navigates through a large map projected on the table, a notebook computer continuously displays detailed information related to the current cursor position (Figure <ref type="figure">5</ref>). The anchored cursor shows the visual connection between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table and wall as shared information surfaces</head><p>The InfoTable/InfoWall surfaces can also act as an integrated shared information space among participants. When two or more users sit at the InfoTable, they can freely place data objects on the table from their notebook computers.</p><p>Unlike desktop computer's screens, or augmented desk systems <ref type="bibr">[22]</ref>, there is no absolute notion of the "top" or "bottom" of the screen for table-type computers. Thus the multi-user capability of the InfoTable causes interesting user-interface design issues for determining the above sides. InfoTable uses the recognized spatial position of notebook computers to determine which is the "near" side for each user. For example, when a user brings a diagram from the far side to the near side of the user, the system automatically rotates it so that the user can read it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object aura</head><p>The system also supports the binding of physical objects and digital data. When an object (such as a VCR tape) with a printed visual marker is placed on the InfoTable, the system recognizes it and an oval-shaped area is displayed at the location of that object. This area, called the "object aura", representing the object's information field (Figure <ref type="figure">6</ref>). This visual feedback also indicates that the physical object has been correctly recognized by the system.</p><p>The object aura represents a data space for the corresponding object. The user can freely attach digital data, by hyperdragging an object from the table surface and dropping it on the object aura. For example, if the user wants to attach a voice memo to the VCR tape, he/she first creates a voice note on his/her notebook computer (using its built-in microphone), and then hyperdrags it from the notebook screen to the VCR tape's aura. When the user releases the mouse button, the voice note is linked to the VCR tape. When someone physically removes the object from the table, the attached data is saved in the network server. This data is re-displayed when the object is placed on any InfoTable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYSTEM ARCHITECTURE</head><p>To enable the interactions described in the previous section, we installed a computer projector and a set of CCD cameras (about 160 cm) above the table. Beside the table, we also installed the combination of a whiteboard and another computer projector as a wall-sized display. Figure <ref type="figure">7</ref> shows the device configuration of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Desksat</head><p>For the video camera used as an object recognition sensor, there is a tradeoff between camera resolution and the field of view. The camera resolution must be high enough to identify fairly small visual markers that are attached on objects. Highresolution images should also be useful for making a record of the table. However, currently-available video cameras do not cover the entire table surface with the required high resolution. DigitalDesk <ref type="bibr">[22]</ref> attempted to solve this problem by adding a second video camera, which is used to capture a fixed sub-part of the desk with higher resolution than the first one. A user is guided to place a document on that focal area.</p><p>Our solution is to use a combination of two cameras (Figure <ref type="figure">8</ref>). The first one is a motor-controlled video camera (Sony EVI-D30) that changes its panning, tilting, and zooming parameters according to commands from the computer. This camera can capture the entire table surface as well as a part of the area with higher resolution (up to 120 dpi) when the camera is zoomed in. Normally, this pan/tilt camera is scanning over the surface of the table by periodically changing the direction and orientation of the camera head. We divided the table surface into a 6-by-6 mesh and the pan/tilt camera is controlled to regularly visit all 36 areas. We called this scheme "Desksat", by analogy to Landsat (land-satellite).</p><p>In our current setup, it takes about 30 seconds to visit all the areas, including camera control and image processing (marker recognition) times.</p><p>The second camera is a fixed camera that is always looking at the entire table surface. This camera analyzes changes on the table from the difference between video images. Then it determines which sub-area has been changed and sends an "area changed" event to the pan/tilt camera. Using this event information, the pan/tilt camera can quickly re-visit the changed area. We choose a threshold value for difference detection so that the fixed camera is not affected by the projected image.</p><p>We use a small amount of heuristics to determine the order of visiting these changed areas. Since people normally use the table from the outside, changes in the inner areas are more likely to be object changes. Thus we assign higher priorities to inner areas than to outer areas; when the fixed camera finds several changes simultaneously, the pan/tilt camera checks these areas from inside to outside.</p><p>Using these techniques, when a user puts, moves (or removes) objects on the table, this effect will be recognized Papers by the system within a few seconds. Although this response time might not be satisfactory for applications that require continuous/realtime object tracking, such as the one in [20], this scheme suits our circumstances quite well where changes occur only intermittently.</p><p>The printed visual markers (2D matrix code) attached to objects (including portable computers and other non-electronic objects) on the table can identify 224 different objects using the combination of printed matrix patterns (we use a slightly different version of the matrix code system described in [10]). Using the Desksat architecture described above, 2D markers as small as 2cm x 2cm can be recognized from the pan/tilt camera above the table.</p><p>In addition to its ID being recognized, the marker's position and orientation are also identified (Figure <ref type="figure">9</ref>). This information is used to calculate object positions in related to the marker position. For example, the position of the cursor on the table while the user is doing a hyperdrag, is calculated based on the current position/orientation of the marker attached on the portable computer. The marker recognition algorithm is summarized in Figure <ref type="figure" target="#fig_0">10</ref>.</p><p>Since 2D codes cost virtually nothing and can be printed, there are some uses that could not be achieved by other ID systems. For example, we can use small Post-it notes with a 2D code. This (physical) Post-it can convey digital data such as voice notes or photographs with an attached ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperdragging</head><p>To enable hyperdragging (when the user moves the cursor of the notebook computer from notebook to the table), the system designates mouse-sensitive areas along all four edges of the notebook screen. When the cursor enters this area, the system remaps the cursor position to the screen, and calculates the offset of this remapping to maintain the cursor position on the table. While the real (original) cursor stays near the edge of the notebook screen, the user can control the virtual cursor position on the table by continuing to press the pointing device.</p><p>To correctly calculate the cursor position on the table, the system also has to know the notebook's position and orientation on the table . The system gets this information from an attached visual marker on the notebook PC. Figure <ref type="figure">9</ref> shows how the system finds the PC position/orientation based on the attached marker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object migration</head><p>As a result of hyperdragging, the system needs to transfer data between two computers (e.g., from a notebook computer to the computer running the table display). All application programs for our environment are written in Java and the system employs Java's object serialization mechanism and the remote method invocation (RMI) method to transfer objects. Currently we support text, sound (voice notes), URLs, file short-cuts, and image data as migratable object classes.</p><p>group meeting.</p><p>The concept of hyperdragging was instantly understood by the users and well accepted. Many users were surprised that they could freely move objects between different computers and other physical objects, with a simple drag-and-drop operation. People also appreciated being able to attach data onto the wall surface while sitting at the table. Many wished that they could also move physical objects with the cursor! Anchored cursors were also helpful when two or more users were performing operation simultaneously, especially when the users manipulated object far from their positions. Some users suggested (and we are considering implementing) putting small peripheral devices, such as printers or scanners, on the table and supporting hyperdragging to them. For example, the user could drop an image objet onto the printer for making a hardcopy of it.</p><p>Some users felt that moving an object across a larger distance was tiresome. We might be able to incorporate techniques other than dragging, such as described in <ref type="bibr">[2]</ref>. We also felt that the mapping scale between pointer movement and the pointing device greatly affects usability. Since the projector resolution on the table (about 20 dpi) is much coarser than the notebook computer's (100-110 dpi), mapping without scaling causes a discontinuous change in cursor speed at the boundary between the notebook and the table.</p><p>We also observed that there were interesting differences between hyperdragging and our previous multi-device interaction technique called "pick-anddrop"[9, 111. Pick-anddrop uses a digitizer stylus to pick up a displayed object from one screen and drop it on another screen. Pick-and-drop is a more direct and physical metaphor than hyperdragging, because its operation is quite similar to picking up a real object. Hyperdragging allows a user to manipulate objects that are out of the user's physical reach, while pick-and-drop does not. Pick-and-drop requires a stylus-sensitive surface for operation, but hyperdragging works on any display and projected surfaces.</p><p>There is also the question of suitability between pointing devices and interaction styles. Apparently pick-and-drop is best suited for a pen, while hyperdragging does not work well with a pen because it forces indirect mapping between the pen position and the cursor position. On the other hand, hyperdragging is more suitable for a track-ball or a trackpoint, and these are common for notebook-sized computers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Research on augmenting face-to-face interactions often assumes pre-installed computer facilities so the configuration of computers is fixed. For example, Colab[ 171 provides a projector screen and table-mounted computers for participants. There was no support for incorporating other computers that the participants might bring to that environment. However, considering recent trends in mobile computing, it would be more practical to support dynamic connections between mobile and pre-installed computers.</p><p>There are several systems that project digital mformation onto the surface of a physical desk. It also displays information which is carried by a physical block called "Passage". While these systems mainly focus on interaction between non-electronic objects and projected digital information, our system also supports i:nformation interchange among portable computers, table/wall surfaces, and physical objects.</p><p>The Desksat architecture was partially inspired by lthe whiteboard scanning system called ZombieBoard[ 141. Zombieboard controls a pan/tile camera to capture the mosaic of partial whiteboard images. By joining these ilmages together, a higher resolution image of the entire whiteboard can be produced. The <ref type="bibr">Brightboard [16]</ref> is another example of a camera augmented whiteboard system; it recognizes hand-drawn commands made by a marking pen.</p><p>As for multi-computer interactions, the Hybrid 1Jser Interfaces [l] is an application for a see-through head-mounted display that produces a virtually bigger screen around the screen of the desktop computers. The PDA-ITV system[l2] uses a palmtop computer (Apple Newton) as a commander for an interactive TV system. These systems assume a fixed-devices configuration, and are mainly designed for single-user applications.</p><p>Ariel [6] and transBOARD <ref type="bibr">[3]</ref> support connections between barcode-printed documents or cards and digital contents. Insight Lab[S] is a computer supported meeting room that extensively uses barcoded tags as physical/digital links and commands. These systems normally require a manual "scan" of each printed barcode. This may become a burden :for users, especially when they have to deal with a number of barcodes. These systems do not recognize the location of each object, so they require other mechanism to achieve spatially continuous operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS AND FUTURE WORK</head><p>We have described our design and implementation ofa hybrid work space, where people can freely display, move, or attach digital data among their computers, tables, and walls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Papers</head><p>There are a number of features that must be improved. Currently, we only support Java-based applications and users cannot directly interchange information between other applications that are not written in Java (such as PowerPoint) or native desktop environments (such as the Windows desktop).</p><p>We are also interested in implementing a smaller version of InfoTable for individual users. In this environment, user can hyperdrag items from their computer to the wall (typically a cubicle partition) in front of them, in the same way that they usually attach a post-it note to it. When the user wants to attach a To-Do item on the schedule, he/she can simply hyperdrag it to the physical calendar on the wall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Evolution of spatially continuous workspaces: (a) A user can perform individual tasks with a portable computer. (b) The table becomes an extension of the portable computer. (c) Pre-installed computer displays (table and wall) also serve as shared workspaces for collaborative tasks,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hyperdragging: A spatially continuous interaction technique for moving information between computers. (a) A user can start moving an object on a computer in the normal manner by dragging it with the pointing device. (b) When the cursor reaches the edge of the screen, it "jumps" to the table surface. (c) The user can continue to drag it to another surface, such as a wall. (d) The user can also drop an item on a physical object, such as a VCR tape, to make a link between real and virtual objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 7: System configuration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,52.80,39.36,504.48,228.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,55.20,311.00,228.48,399.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>VIDEODESiK[4]  consists of a light table and a video camera. The user can interact with the other participant's silhouette projected onto the table. DigitalDesk[21,22]  allows interactions betwe:en printed documents and digital information projected on a desk. A recent version of the DigitalDesk series also added a document identification capability based on OCR[ 131.. Luminous Room[l9] (and its underlying "I/O bulb" concept) uses a video projector mounted on a computer-controlle'd gimbal to change the projection area. Its application called Illuminating Lights[ 191 helps a holography designer to rapidly layout physical optics devices on the desk. Streitz et al. developed a set of computer augmented elements including a wall, chairs, and atable[7]. Among them, the InteracTable is a tablesized computer supporting discussion by people around it.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Takahashi Totsuka for helpful discussions and we are also indebted to Mario Tokoro for their continuing support of our research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIENCE AND DISCUSSIONS</head><p>Up to the time this paper was written, no formal evaluation had been conducted. However, with this environment, the authors and their colleagues in the laboratory have experimentally tried several collaborative activities including a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hybrid user interfaces: Breeding virtually bigger interfaces for physically smaller computers</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ZJIST&apos;91, ACM Symposium on User Interface Software and Technology</title>
		<meeting>ZJIST&apos;91, ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="1991-11">November 1991</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shuffle, throw or take it! working efficiently with an interactive wall</title>
		<author>
			<persName><forename type="first">Jorg</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;98 summary</title>
		<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tangible Bits: Towards seamless interfaces between people, bits and atoms</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brygg</forename><surname>Ullmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;97 Proceedings</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="234" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Artificial Reality II</title>
		<author>
			<persName><forename type="first">Myron</forename><forename type="middle">W</forename><surname>Krueger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Insight Lab: An immersive team environment linking paper, displays, and data</title>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;98 Proceedings</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ariel: augmenting paper engineering drawings</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Inwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Launiainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brenta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pouzol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;95 Conference Companion</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="420" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Roomware for cooperative buildings: Integrated design of architectural spaces and information spaces</title>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Holmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><forename type="middle">A</forename><surname>Streitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorg</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cooperative Buildings -Integrating Information, Organization, and Architecture</title>
		<editor>
			<persName><forename type="first">Norbert</forename><forename type="middle">A</forename><surname>Streitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shin'ichi</forename><surname>Konomi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The office of the future: A unified approach to image-based modeling and spatially immersive displays</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Cutts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Stesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;98 Proceedings</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jun Rekimoto. Matrix: A realtime object identification and registration method for augmented reality</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><surname>Pick-And-Drop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Asia Pacific Computer Human Interaction (APCHI &apos;98)</title>
		<meeting>of Asia Pacific Computer Human Interaction (APCHI &apos;98)</meeting>
		<imprint>
			<date type="published" when="1997-10">October 1997. July 1998</date>
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
	<note>Proceedings of UIST&apos;97</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multiple-device approach for supporting whiteboard-based interactions</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI&apos;98</title>
		<meeting>CHI&apos;98</meeting>
		<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual device user interface design: PDAs and interactive television</title>
		<author>
			<persName><forename type="first">Cathleen</forename><surname>Stott Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marita</forename><surname>Ashworth</surname></persName>
		</author>
		<author>
			<persName><surname>Franzke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;96 Proceedings</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Animated Paper Documents</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lay</surname></persName>
		</author>
		<ptr target="http://www.parc.xerox.comlspl/members/saund/zombieboard-public.html" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Human-Computer Interaction, HCI&apos;97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Eric Saund. ZombieBoard project description</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">InfoBinder: a pointing device for a virtual desktop system</title>
		<author>
			<persName><forename type="first">Itiro</forename><surname>Siio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Human-Computer Interaction (HCI International &apos;951</title>
		<imprint>
			<date type="published" when="1995-07">July 1995</date>
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Brightboard: A video-augmented environment</title>
		<author>
			<persName><forename type="first">Questin</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;96 proceedings</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond the chalkboard: computer support for collaboration and problem solving in meetings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stefik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Suchman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="47" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">media-Blocks: Physical containers, transports, and controls for online media</title>
		<author>
			<persName><forename type="first">Brygg</forename><surname>Ullmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Glas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;98 Proceedings</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A view from the Luminous Room</title>
		<author>
			<persName><forename type="first">John</forename><surname>Underkoffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personal Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pierre Wellner. The DigitalDesk calculator: Tangible manipulation on a desk top display</title>
		<author>
			<persName><forename type="first">John</forename><surname>Underkoffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UIST&apos;91, ACM Symposium on User Interface Software and Technology</title>
		<meeting>UIST&apos;91, ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="1991">1998. November 1991</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
	<note>CHI&apos;98 Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interacting with paper on the Digi-talDesk</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="87" to="96" />
			<date type="published" when="1993-08">August 1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
