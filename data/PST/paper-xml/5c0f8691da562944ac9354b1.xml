<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Generative Adversarial Neural Networks for Compressive Sensing (GANCS) MRI †</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
							<email>morteza@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Enhao</forename><surname>Gong</surname></persName>
							<email>enhaog@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
							<email>jycheng@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shreyas</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
							<email>vasanawala@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Zaharchuk</surname></persName>
							<email>gregz@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
							<email>pauly@stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>725 Welch Rd Ste 1866 MC 5913</addrLine>
									<postCode>94304</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering Departments</orgName>
								<orgName type="institution">Medical Physics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>875 Blake Wilbur Drive Stanford</addrLine>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>350 Serra Mall</addrLine>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Generative Adversarial Neural Networks for Compressive Sensing (GANCS) MRI †</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A164EB7686F2BF25BAF78228E1682960</idno>
					<idno type="DOI">10.1109/TMI.2018.2858752</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2018.2858752, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Generative Adversarial Networks (GAN)</term>
					<term>Convolutional Neural Networks (CNN)</term>
					<term>Rapid Reconstruction</term>
					<term>Diagnostic Quality</term>
					<term>Compressed Sensing (CS)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Undersampled magnetic resonance image (MRI) reconstruction is typically an ill-posed linear inverse task. The time and resource intensive computations require trade offs between accuracy and speed. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image diagnostic quality. To address these challenges, we propose a novel CS framework that uses generative adversarial networks (GAN) to model the (low-dimensional) manifold of high-quality MR images. Leveraging a mixture of least-squares (LS) GANs and pixel-wise 1/ 2 cost, a deep residual network with skip connections is trained as the generator that learns to remove the aliasing artifacts by projecting onto the image manifold. The LSGAN learns the texture details, while the 1/ 2 cost suppresses high-frequency noise. A discriminator network, which is a multilayer convolutional neural network (CNN), plays the role of a perceptual cost that is then jointly trained based on high quality MR images to score the quality of retrieved images. In the operational phase, an initial aliased estimate (e.g., simply obtained by zero-filling) is propagated into the trained generator to output the desired reconstruction. This demands very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. Images rated by expert radiologists corroborate that GANCS retrieves higher quality images with improved fine texture details compared with conventional Wavelet-based and dictionary-learning based CS schemes as well as with deeplearning based schemes using pixel-wise training. In addition, it offers reconstruction times of under a few milliseconds, which is two orders of magnitude faster than current state-of-the-art CS-MRI schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Magnetic resonance imaging (MRI) is a major imaging modality in clinical practice, due to its superb soft tissue contrast. Unfortunately, MRI is also a relatively slow imaging modality. This is particularly true for high-resolution, volumetric image, and time series images. One possible solution is to decrease the scan duration through significant undersampling. However, such undersampling leads to an illposed linear inverse reconstruction problem, which requires long reconstruction times. This is an issue in clinical MRI, where images are not available for immediate review to guide subsequent scans, and is a critical concern for interventional or real-time MRI, where immediate image feedback is an overiding concern.</p><p>To render the MRI reconstruction well-posed, conventional compressed sensing (CS) incorporates prior information about the inherent low dimensionality of images by means of sparsity regularization in a proper transform domain such as Wavelet (WV), or, finite differences (or Total Variation, TV) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>. This however typically demands running iterative algorithms, for solving non-smooth optimization programs, that are time and resource intensive. These are not practical for applications such as real-time MRI. Moreover, the sparsity assumption is oblivious to the inherent latent structures that are specific to each dataset. In essence, the sparsity prior spans a large manifold of images. Adapting and specifying the image manifold to the historical patient datasets enables further undersampling, and creating higher diagnostic quality images.</p><p>A few attempts have been recently carried out to speed up medical image reconstruction by leveraging historical patient data; see e.g., <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>, to name a few. The major consensus is that they train a network that learns the relation map between an initial aliased image and the gold-standard image. For MR image recovery, for instance, <ref type="bibr" target="#b17">[18]</ref> trains a fully-connected (FC) denoising autoencoder, while the AutoMap scheme in <ref type="bibr" target="#b34">[35]</ref> adopts a FC network followed by a convolutional neural network (CNN) to directly map the k-space data to the image domain for robust reconstruction. The multi-coil superCNN scheme in <ref type="bibr" target="#b14">[15]</ref> also trains a CNN from the undersampled multichannel images to the gold-standard images. A deep cascade of CNNs is also used in <ref type="bibr" target="#b25">[26]</ref> for Cartesian undersampling of 2D cardiac MR images that outperforms dictionary learning MRI in terms of quality and speed. In computer tomography (CT) reconstruction, <ref type="bibr" target="#b4">[5]</ref> applied the U-net <ref type="bibr" target="#b23">[24]</ref> based CNNs for residual learning. While the reconstruction speeds up, these approaches suffer from blurring and aliasing artifacts. This is mainly because they adopt a pixel-wise 1 / 2 cost for training, that is oblivious to structured artifacts and high-frequency texture details. Revealing these details is crucial for accurately making diagnostic decisions. In addition, these schemes lack any mechanism to ensure that the retrieved images are faithful to the measurements.</p><p>In another approach, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref> use deep neural networks to learn image priors, or, sparsifying transforms from historical data. These are then used to solve a nonlinear system using iterative optimization algorithms as in the conventional CS methods. <ref type="bibr" target="#b28">[29]</ref> unrolls the ADMM optimization for solving the inverse problem for CS-MRI, and improves the modeling by representing projections with neural networks. Similarly, <ref type="bibr" target="#b11">[12]</ref> uses a deep U-net to learn priors for converting sparse-view CT images to the full-view images. The work in <ref type="bibr" target="#b9">[10]</ref> also uses a variational autoencoder for learning the priors, which is then evaluated on knee images. The work in <ref type="bibr" target="#b3">[4]</ref> also performs iterative reconstruction and shows better recovery when the sparsity model uses a pre-trained multi-layer generative model. While improving the reconstruction performance, these methods incur high computational cost for finding the optimal reconstruction with iteration.</p><p>Generative adversarial networks (GANs) have been lately proven very successful <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[22]</ref> in modeling distributions (low-dimensional manifolds) and generating natural images (high-dimensional data), that are perceptually appealing <ref type="bibr" target="#b35">[36]</ref>. In particular, GANs achieve state-of-the-art perceptual quality for image super-resolution tasks for up to 4× upscaling for natural images e.g., from ImageNet <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b26">[27]</ref>. GANs have also been recently deployed for image inpainting <ref type="bibr" target="#b31">[32]</ref>, style transfer <ref type="bibr" target="#b12">[13]</ref>, and visual manipulation <ref type="bibr" target="#b35">[36]</ref>, with excellent performance relative to the existing alternatives. Despite the success of GANs for local image restoration such as superresolution and inpainting, to date, they have not been studied for correcting the aliasing artifacts in MRI reconstruction tasks. There is a very recent study on using GANs in <ref type="bibr" target="#b30">[31]</ref> for denoising CT images where the generator is a deep CNN and the evaluations are performed mostly with phantom models. Aliasing artifacts in MRI emanate from data undersampling in a different domain (e.g., Fourier) which globally impact the entire image, which is harder to recover and reconstruct. Contributions. Inspired by the sharp, high texture-quality images retrieved by GANs, and the high contrast of MR images, we employ GANs for modeling the low-dimensional manifold of high-quality MR images. The images lying on the manifold are not however necessarily consistent with the observed undersampled data. As a result, the reconstruction models the intersection of the image manifold and the subspace of dataconsistent images. Such a space is simply an affine subspace for linear measurements. To this end, we proposed GANCS in which we adopt a tandem network of a generator (G), an affine projection operator, and a discriminator (D). The generator aims to create gold-standard images from the complex-valued aliased inputs using a deep residual network (ResNet) with skip connections which are proven to retain high resolution information. The data-consistency projection builds upon the known signal model and performs an affine projection onto the space of data consistent images. The D network is a multilayer convolutional neural network (CNN) that is presented with both the fake images created by G, and the corresponding gold-standard images, and aims to correctly distinguish fake from real. We adopt least-squares GANs (LSGANs) due to their stability properties <ref type="bibr" target="#b18">[19]</ref>. To control the high-frequency texture details returned by LSGANs, and to further improve the training stability, we partially use the pixel-wise 1 and 2 costs for training the generator.</p><p>We perform extensive evaluations on a large dataset of pediatric patients with contrast-enhanced abdominal images. The reconstructed images using the proposed GANCS method and other state-of-the-art CS-MRI schemes are rated by expert radiologists for diagnostic quality. Our observations indicate that the GANCS results have almost similar quality to the goldstandard (fully-sampled) images, and are superior in terms of diagnostic quality relative to the existing alternatives including conventional iterative CS (for both TV and WV) as well as deep learning based methods that solely adopt the pixel-wise 2 -, and 1 -based criteria. Moreover, the reconstruction only takes around 30 msec, which is two orders of magnitude faster than state-of-the-art conventional CS toolboxes. Finally, the scope of our novel GANCS framework goes beyond MR image reconstruction, and applies to other image restoration tasks for aliasing artifacts.</p><p>The rest of this paper is organized as follows. Section II introduces the preliminaries and states the problem. Manifold learning using LSGANs is discussed in Sections III and IV. Section V reports the data evaluations, while the conclusions are drawn in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES AND PROBLEM STATEMENT Consider a generic MRI acquisition model that forms an image</head><formula xml:id="formula_0">x ∈ R N from k-space projections y ∈ R M y = A(x) + v<label>(1)</label></formula><p>where the (possibly) nonlinear map A : C N → C M encompasses the effects of sampling, coil sensitivities, and the Discrete Fourier Transform (DFT). The error term v ∈ R M also captures the noise and unmodeled dynamics. It is natural to assume the unknown complex-valued image x lies in a lowdimensional manifold, say M. No information is known about the manifold besides the training samples X := {x k } K k=1 drawn from it with the corresponding noisy observations Y := {y k } K k=1 . The data {X , Y} can be obtained for instance from the K past patients in the dataset that have been already scanned for a sufficient time, with their high-quality reconstructions. Given the training data {X , Y}, our goal is to quickly recover the image x after collecting the undersampled measurements y.</p><p>Instead of relying on a simple sparsity assumption of X , we aim to automate the image recovery by learning the nonlinear inversion map x = f (y) from the historical training data {X , Y}. We begin with an initial estimate x that is a linear transform (approximate zero-filling solution) from undersampled measurement and possibly contains aliasing artifacts. Reconstruction can then be envisioned as artifact suppression, that we propose to model as a mapping on the manifold of high quality images. Learning the corresponding manifold is the subject of next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MANIFOLD LEARNING VIA GENERATIVE ADVERSARIAL NETWORKS</head><p>The inverse imaging problem is to find a solution at the intersection of a subspace defined by the acquisition model, and the image manifold. In order to effectively learn the image manifold from the available limited number of training samples, we first need to address the following important questions:</p><p>• How to ensure the trained manifold contains plausible MR images? • How to ensure the points on the manifold are data consistent, namely y ≈ A(x), ∀x ∈ M?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Alternating mapping with GANs for plausible reconstruction</head><p>To address the first question to ensure plausibility of the reconstruction, we adopt GANs that have recently proven very successful in estimating the prior distributions for images. GANs provide sharp images that are visually plausible <ref type="bibr" target="#b7">[8]</ref>. In contrast, variational auto-encoders <ref type="bibr" target="#b13">[14]</ref>, an important class of generative models, use a pixel-wise mean-squared-error (MSE) cost that results in high peak signal-to-noise (PSNR) ratios, but often produces overly-smooth images and patterned artifacts <ref type="bibr" target="#b33">[34]</ref> that have poor perceptual quality. A standard GAN consists of a tandem network of G and D networks. Consider an initial estimate of the image x := A † (y) as the input to the G network. The estimate x could be simply obtained via zero-filling the missing k-space components, which is the least squares solution for data-consistency, and then running a single iteration of conjugate gradient optimization. The G network then projects x onto the low-dimensional manifold M containing the high-quality images X . Let x denote the output of G. It then passes through the discriminator network D, that tries to output one if x ∈ X , and zero otherwise. As will be clear later, eventually the G net learns to project to the lowdimensional manifold and achieves a realistic reconstruction, such that the D net cannot always perfectly assign the right labels to the real fully-sampled and fake recovered images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Affine projection and soft penalty for data-consistency</head><p>The output of G, x, however may not be consistent with the data. To tackle this issue, G is followed by another layer that projects onto the set of data-consistent images, C := {x : y ≈ A(x)} (see Fig. <ref type="figure" target="#fig_0">1</ref>). For a Cartesian grid with the linear acquisition model y = Ax, the projection is easily expressible as x = A † y + (I -A † A)x. Alternatively, one can impose data consistency to the output of G through a soft least-squares (LS) penalty when training the G network, as will be seen below in (P1).</p><p>To further ensure that x falls in the intersection of the manifold M and the set of data-consistent images C, we can perform multiple back-and-forth mappings. The network structure in Fig. <ref type="figure" target="#fig_0">1</ref> can then be extended by repeating the G network and P C (•) serially for a few times. In this paper we only adopt a single back-and-forth mapping for simplicity of exposition. However, our observations for using multiple mappings show significant performance improvement, and is reported in our companion work <ref type="bibr" target="#b20">[21]</ref>. The overall network architecture is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, where P N := (I -A † A) resembles projection onto the nullspace of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Least-squares GANs for stable training</head><p>Training the network in Fig. <ref type="figure" target="#fig_0">1</ref> amounts to playing a game with conflicting objectives between the generator G and the discriminator D. The D network aims to score one for the real gold-standard images, and zero for the fake/reconstructed images by G. The G network also aims to map the input aliased image x to a fake image x that looks so realistic and plausible that can fool D. Various strategies have been devised to reach the equilibrium. They mostly differ in terms of the cost function adopted for G and D networks; see e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The standard GAN uses a sigmoid crossentropy loss that leads to vanishing gradients which renders the training unstable, and as a result it suffers from severe degrees of mode collapse. In addition, for the generated images classified as real with high confidence (i.e., with large decision variable), no penalty is incurred. Hence, the standard GAN tends to pull samples away from the decision boundary, which can introduce non-realistic images <ref type="bibr" target="#b18">[19]</ref>. Such images can be hallucinated, and are thus not reliable for diagnostic decisions.</p><p>Motivated by the improved stability of LSGANs we adopt an LS cost for the discriminator decision. In essence, the LS cost penalizes the decision variables without any nonlinear transformation, and as a result it tends to pull the generated samples toward the decision boundary <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mixed costs to avoid high-frequency noise</head><p>One issue with GANs is that they may overemphasize the high-frequency texture, and thus ignore the image content. To discard the high-frequency noise and avoid hallucination, we use a supervised 1 / 2 cost as well for training the G net. Such mixtures with pixel-wise costs have been shown to be promising for natural image restoration tasks since they can properly penalize noise and stabilize the training <ref type="bibr" target="#b33">[34]</ref>. In particular, the smooth 2 -cost better preserves the main structure and leads to a stable training at the expense of introducing blurring artifacts. The non-smooth 1 -cost however may not be as stable as 2 in training, but it can better discard the low-intensity noise and achieve better solutions. All in all, to reveal fine texture details while discarding noise, we are motivated to adopt a mixture of LSGAN and 1 / 2 cost to train the generator. The overall procedure aims to jointly minimize the expected discriminator cost (P1.1) min and the expected generator cost</p><formula xml:id="formula_1">Θ d E x 1 -D(x; Θ d ) 2 + E y D(G(x; Θ g ); Θ d )</formula><formula xml:id="formula_2">(P1.2) min Θg E y y -AG(x; Θ g ) 2 + ηE x,y x -G(x; Θ g ) 1,2 + λE y 1 -D G(x; Θ g ); Θ d 2</formula><p>where E[•] refers to the expectation operator, and • 1,2 denotes a convex combination of the element-wise 1 -and 2norm with non-negative weights η 1 /(η 1 +η 2 ) and η 2 /(η 1 +η 2 ) respectively such that η 1 + η 2 = η. The LS data fidelity term in (P1.2) is a soft alternative to the hard affine projection step after the generator, and is kept in (P1.2) only for the sake of generality. Tuning parameters λ and η also control the balance between manifold mapping, noise suppression and data consistency. Looking carefully at (P1.2) shows there is a similarity with regularized-LS schemes used in conventional CS. Taking the initial estimation x as input, the generator reconstructs the image x = G(x; Θ g ) from k-space measurement y using the expected regularized-LS estimator, where the regularization is not based on sparsity but learned from training data via LSGAN and 1 -net. This is different from the conventional CS scheme, which involves an iterative optimization algorithm to solve for the 1 / 2 -regularized LS cost. The optimization only happens in training and the optimized weights in the network can generalize to any future samples. The learned generator can be immediately applied to new test data to retrieve the image "on the fly". As argued in <ref type="bibr" target="#b18">[19]</ref>, it can be shown that LSGAN game yields minimizing the Pearson-χ 2 divergence. For (P1) following the same arguments as for the standard GANS in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b18">[19]</ref> it can be readily shown that even in the presence of LS data consistency and 1 / 2 penalty, the distribution achieved by the G network coincides with the true data distribution. The overall procedure ensures the reconstruction is both data consistent and MRI realistic. Remark 1. [Controlling image hallucination] One possible concern is that adversarial training may cause image hallucina-tion that could mislead diagnosis. It is important to emphasize that the proposed GANCS scheme controls/avoids hallucination by modifying the conventional GAN in the following ways: First, the data consistency step after mapping onto the image manifold ensures that the resulting reconstructed image conforms to the k-space measurements. Second, the mixture cost used for the generator training, namely (P1.2), includes a pixel-wise cost that assures the image pixels are closely aligned with the ground-truth pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NETWORK ARCHITECTURE FOR GANCS</head><p>The detailed architecture of G and D nets are described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Residual networks for the generator</head><p>The input and output are complex-valued images of the same size, where the real and imaginary components are considered as two separate channels. As mentioned earlier, the input image x is simply an initial estimate obtained via zerofilling, which produces aliasing artifacts. After convolving the input channels with different kernels, they are added up in the next layer. All network kernels are assumed real-valued. Inspired by super-resolution networks in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> we adopt a deep residual network (ResNet) for the generator that contains 5 residual blocks. As depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, each block consists of two convolutional layers with small 3 × 3 kernels and 128 feature maps that are followed by batch normalization (BN) and rectified linear unit (ReLU) activation. It is then followed by three convolutional layers with map size 1 × 1, where the first two layers undergo ReLU activation, while the last layer uses no activation to return two output channels corresponding the real and imaginary image components. The network architecture is depicted in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The G network learns the mapping onto the manifold of realistic MR images. The manifold dimension is controlled by the number of residual blocks (RB), feature maps, stride size, and the size of discriminator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional neural networks for discriminator</head><p>The D network takes the magnitude of the complex-valued output of the G net and data consistency projection as an input. It is composed of 8 convolutional layers. In all layers except the last one, convolution operation is followed by batch normalization, and subsequently by ReLU activation. No pooling is used. For the first four layers, the number of feature maps is doubled from 8 to 64, while at the same time convolution with stride 2 is used to reduce the image resolution. Kernel size 3 × 3 is adopted for the first 5 layers, while the two to the last layers use kernel size 1 × 1. For the sixth and seventh layer we use 64 and one feature maps, respectively. The last layer simply averages out the seventh layer features to form the decision variable for binary classification. No soft-max operation is used. The network structure is concisely depicted in Fig. <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATIONS</head><p>The effectiveness of the GANCS scheme is assessed in this section for a single-coil MR acquisition model with Cartesian sampling. For n-th patient the acquired k-space data is y</p><formula xml:id="formula_3">(n) i,j = [F(X n )] i,j + v (n) i,j , (i, j) ∈ Ω.</formula><p>We adopt the single coil model for demonstration purposes, but the extension to multi-coil MRI acquisitions is straightforward by simply updating the signal model. Sampling set Ω indexes the sampled Fourier coefficients. As it is conventionally performed with CS MRI, we select Ω based on a variable density sampling with radial view ordering <ref type="bibr" target="#b6">[7]</ref> that tends to sample more low frequency components from the center of k-space. Different undersampling rates (5 and 10) are chosen throughout the experiment. The input zero-filled (ZF) image x is simply generated using an inverse 2D FT of the sampled k-space data, which produces images that are severely contaminated by aliasing artifacts. Input images are normalized to have the maximum magnitude of unity per image.</p><p>Adam optimizer is used with the momentum parameter β = 0.9, mini-batch size 4, and initial learning rate 10 -6 that is halved every 10, 000 iterations. Training is performed with the TensorFlow interface on an NVIDIA Titan X Pascal GPU, 12GB RAM. We allow 20 epochs that takes around 10 hours for training.</p><p>Abdominal dataset. T1-weighted abdominal image volumes were acquired for 350 pediatric patients with gadoliniumbased contrast enhancement. For each scan an RF-spoiled gradient recalled-echo sequence is used with a TR and TE of approximately 3.3 msec and 1.3 msec, respectively. An intermittent fat saturation pulse with an inversion time 9 msec is also used for fat suppression. The field of view for x and y axis is approximately 280 mm and 224 mm, respectively, with the slice thickness of 2.4 mm, and a 15-degree flip angle. Each volumetric Cartesian dataset was acquired using a variabledensity sampling with radial view-ordering where the center k-space values in a 20 × 20 region were repeatedly sampled over 50 times. The raw data consisted of 18 temporal frames where each temporal frame was undersampled 6-fold along the axial direction (k y -k z undersampling). The total scan was 2 minutes to capture the contrast and motion dynamics. The raw data is then combined over all time frames to create fully-sampled 3D images that are used as the gold-standard. Additionally, respiratory motion was measured intrinsically using a self-navigated approach. This motion information was used to retrospectively soft-gate the data based on the respiratory motion. With the use of light anesthesia, the volunteer subjects had regular period respiratory motion that was simple to address in the image reconstruction to avoid image artifacts. Knee dataset. This dataset includes 19 subjects scanned with a 3T GE MR750 whole body MR scanner at the Stanfords Lucile Packard Childrens Hospital. The full description is available at <ref type="bibr" target="#b24">[25]</ref>, that is briefly summarized as follows for completeness. Fully sampled sagittal images are acquired with a 3D FSE CUBE sequence with proton density weighting including fat saturation. Other parameters include FOV=160mm, TR=1550 (sagittal) and 2, 000 (axial), TE=25 (sagittal) and 35 (axial), slice thickness 0.6mm (sagittal) and 2.5mm (axial). Each image is a complex valued 3D volume of size 320×320×256. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image evaluation and comparison: Abdominal data</head><p>Retrieved images by various methods are depicted in Fig. <ref type="figure" target="#fig_5">5</ref> with 5-fold undersampling of k-space. For a random test patient, representative coronal slice is shown, where the columns from left to right depict, respectively, the gold-standard image, and the images reconstructed by GANCS with 1 -cost (η = 1,λ = 0), GANCS with 1 -cost (η = 0.975,λ = 0.025), and CS-WV. For 5-fold undersampling, the ZF reconstruction is quite blurred and not shown in Fig. <ref type="figure" target="#fig_5">5</ref>. CS reconstruction is performed using the Berkeley Advanced Reconstruction Toolbox (BART) <ref type="bibr" target="#b10">[11]</ref>, where the tunning parameters are optimized for the best SNR performance.</p><p>As apparent from the magnified regions, GANCS with 1cost (η = 0.75,λ = 0.25) returns the sharpest images with highest contrast and texture details that can reveal the small anatomical details. Images retrieved by GANCS with 1 -cost alone results in smooth textures as the 1 -cost encourages finding pixel-wise averages of all plausible solutions. We also tested GANCS with 2 -cost for training, where we observed smoother texture relative to 1 -cost. Thus, we only illustrate the 1 cost for subjective comparison. Note, in compromise the smooth 2 -cost leads to a more stable training; see e.g., <ref type="bibr" target="#b33">[34]</ref> for more details about the trade-offs between 1 and 2 costs. The reconstructed images however are not as sharp as the GANCS ones (η = 0.975,λ = 0.025) which leverages both 1 -net and GANs.</p><p>Zooming on the liver (top) and the kidney (bottom) areas in Fig. <ref type="figure" target="#fig_5">5</ref>, as pinpointed by the arrows, the liver texture is more realistic and the vessels are shaper to delineate. Focus on the thin liver vessel pinpointed by the arrow. It is almost lost in CS-WV, and blurry in GANCS with 1 -cost alone, but it is delineated by GANCS that relies on a 25% GAN cost. The kidney vessels in the bottom images are also seen sharper and more clear with the GANCS using 25% GAN cost. We have observed that using GAN alone, namely η = 0, λ = 1, the retrieved images are quite sharp with a high-frequency noise present over the image that can distort the image structure. It turns out that including the 1 cost during training behaves as a low-pass filter to discard the high-frequency noises, while still achieving reasonably sharp images. It is also evident that CS-WV introduces blurring artifacts. We also tested CS-TV, but CS-WV is observed to consistently outperform CS-TV, and thus we choose CS-WV as the representative for CS-MRI.</p><p>Quantitative metrics including SNR (dB), SSIM, and the reconstruction time (sec) are reported in Table <ref type="table" target="#tab_0">I</ref>. These metrics are averaged out over axial slices of all ten test patients (1, 920 slices). The reconstruction time for CS schemes refers to the elapsed time for running 300 iterations of conjugate gradient descent using the optimized BART toolbox. It takes around 10 seconds, that is 300 times longer than the neural-network based schemes as reported in Table <ref type="table" target="#tab_0">I</ref>. Reconstructing 30 slices per second makes GANCS a suitable choice for real-time imaging. In terms of SNR and SSIM, GANCS with 1 -cost alone achieves the best performance. GANCS with proper 1cost mixing can achieve good performance with a marginally decrease from GANCS with 1 -cost alone. However, GAN based application is not designed and trained to achieve the highest SNR which does not capture the visual perception <ref type="bibr" target="#b13">[14]</ref>. GANCS is however trained to achieve a high perceptual quality for diagnostic purposes, which is also shown Fig. <ref type="figure" target="#fig_5">5</ref>. This motivates us to ask expert opinions from radiologists regarding the diagnostic value as discussed in the next section.</p><p>Before moving on, it is worth mentioning that we also managed to compare GANCS reconstruction with the combination of 1 and SSIM costs, recently proposed in <ref type="bibr" target="#b33">[34]</ref>, to capture sharpness for natural image restoration tasks. Our observations for MRI reconstruction however indicate no improvement relative to 1 -net. The results are not included due to space limitations. It is also worth noting that the trained discriminator offers a novel score that can be a better metric for perceptual assessment of the images. Remark 2 [Impact of iteration count]. In essence, the network architecture in Fig. <ref type="figure" target="#fig_0">1</ref> resembles only a single backand-forth iteration between the image manifold and the affine subspace of data consistent solutions. One can however adopt a recurrent network architecture to model multiple iterations by simply repeating the network architecture in Fig. <ref type="figure" target="#fig_0">1</ref> for a few times. This is extensively studied in our companion work <ref type="bibr" target="#b20">[21]</ref>, which indicates that using more iterations can significantly improve the reconstruction quality by about 2 -3 dB in terms of SNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image evaluation and comparison: Knee data</head><p>For the adopted high-resolution Knee dataset described in Section V-F, the reconstructed images under different schemes are depicted in Fig. <ref type="figure">6</ref>. In particular, the data-driven CS scheme in <ref type="bibr" target="#b22">[23]</ref> based on dictionary learning is adopted as a baseline for comparison. <ref type="bibr" target="#b22">[23]</ref> models the images patches as sparsely represented over an overcomplete dictionary, and uses the undersampled data to jointly learn the dictionary and impute the missing k-space entries. We adopt the publicly available code provided by the authors, and choose the patch size 8 × 8 with overlap stride 1, 64 atoms, and 200 * 64 training samples for training the dictionary. We also use 30 iterations with 20 iterations for the operation of KSVD algorithm <ref type="bibr" target="#b1">[2]</ref>. The optimization formulation includes a data fidelity term regularized with the 1 penalty on the dictionary coefficients as well as a penalty term to impose dictionary constraint on the image patches. The regularization parameter is optimized for the best performance. Note, CS-DL is known as one of stateof-the-art data-driven CS scheme that consistently outperforms the conventional CS techniques relying on fixed sparsifying transforms such as Wavelet and TV.</p><p>For a test patient, two representative axial slices are depicted in Fig. <ref type="figure">6</ref> with their corresponding quantitative metric including the SNR, SSIM, and inference time listed under the images. It is apparent that CS-DL retrieves overly smooth images. It also takes about an hour to reconstruct the image. GANCS (η = 0.99, λ = 0.01) images however seem to better retain the high-frequency details, and reconstructs more realistic texture with sharper edges. Note, for GANCS pixel-wise 2 loss is used. CS-DL achieves the highest SNR, but as discussed in Remark 2 by using more iterations for the denoiser, GANCS can outperform CS-DL even in terms of SNR.</p><p>In order to study the breaking point of GANCS, experiments are run under various undersampling rates, namely 3, 5, 7, 10. k-space data is sampled based on a variable density with radial view ordering mask. The network is trained and tested with the same sampling mask, and the weights are set to λ = 0.1 and η = 0.9 that uses 10% of the GAN loss for all undersampling rates. It is apparent that for 5and 7-fold undersampling the images miss some high-frequency texture details, but they are still diagnostically valuable. For 10-fold however due to the large space of data consistent images, the texture seems to carry some high-frequency noise, that can hallucinate images. This can be alleviated by lowering the GAN loss weight at the expense of over smoothing the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Diagnostic quality assessment</head><p>To quantitatively assess the image quality of GANCS we performed an experiment based on the expert opinion of two pediatric radiologists at the Stanford Medical Center. Five patients with 3D images are considered where two of those suffer from subtle liver lesion and inflated adrenal gland. The radiologists were blind to the history records and diagnosis results of the patients. They independently rated the overall quality of images as well as the quality of delineation of several anatomical structures including hepatic vein and portal vein. For completeness, the criteria for the radiologists opinion score (ROS) is listed under Table <ref type="table" target="#tab_2">II</ref>; see <ref type="bibr" target="#b32">[33]</ref> for more details. ROS is ranged from one to five, where 1 refers to a "nondiagnostic" image, while 5 indicates "excellent" diagnostic quality. The readers are first asked to individually assess the reconstruction, and then asked for their preference of side-byside comparisons.</p><p>Rating is provided for the gold-standard as well as the GANCS (η = 0.75, λ = 0.25), and CS-WV schemes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheme</head><p>GANCS ( 1 ) η = 0.975, λ = 0.025  Radiologists are blind to the schemes. The mean scores with their standard deviation are reported in Table <ref type="table" target="#tab_2">III</ref>. Similar score is reported for the overall quality and the hepatic and portal vein diagnostic quality. The pairwise difference between different schemes is statistically significant. Radiologists confirm that GANCS provides sharper images with better texture than CS-WV which can better delineate the structures. They also specifically examined the GANCS images for possible hallucinations, but no sign of hallucination was observed. It is also worth noting that the merits of GANCS become more pronounced when dealing with high-resolution images with sharp edges for the structures. Our observations with simulated phantoms with sharp edges indicate the sharp delineation of structures using GNACS relative to CS-WV and pixel-wise costs. Our current research focuses on using a high-resolution dataset with sharper edges to evaluate the diagnostic quality of images.</p><formula xml:id="formula_4">GANCS ( 1 ) η = 1, λ = 0 GANCS ( 2 ) η = 1, λ = 0 GANCS ( 1 ) η = 0.75, λ = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance for pathological cases</head><p>When operating the trained GANCS on new patients with novel and abnormal cases, one concern is that GANCS may create hallucinated images, which are not reliable for diagnosis. This could be either due to lack of generalization of GANCS, or, unstable training that can lead to local minima. To validate this safety concern we pick a pathological patient that suffers from a hypertrophic adrenal gland as pinpointed by the arrow on top of the kidney in Fig. <ref type="figure" target="#fig_6">8</ref>. The adrenal gland has a similar texture as the kidney and thus not very obvious. We make sure the training data does not include patients with hypertrophic adrenal glands. Normal adrenal gland is typically a small structure compared with the kidney size. A representative coronal slice of the full-view gold-standard image and the zoomed area are shown in the first and second columns, respectively. Likewise, GANCS reconstruction with η = 0.75, λ = 0.25 is also shown in the third and fourth column for 5-fold undersampling. It appears from the enlarged regions of interest that GANCS faithfully delineates the inflated adrenal gland, and does not introduce/miss any structures that can lead to misdiagnosis.</p><p>In general, quantifying the hallucination risk is a very challenging task that is yet an open problem in medical imaging. The challenge emanates from lack of a clear definition, and Fig. <ref type="figure">6</ref>. Representative axial knee images for a test patient with k-space data acquired under 5-fold variable density sampling with radial view ordering. For GANCS we choose η = 0.99, λ = 0.01. The quantitative metrics are also listed under the images. SSIM is calculated on a window of size 100 × 50 chosen from the image center. GANCS images look sharpest and look more similar to the gold-standard.</p><p>the need for an extensive subjective assessment. It is a subject of our current research that needs an extensive separate study beyond the scope of this work. Nonetheless, it should be emphasized that this is a generic problem with any reconstruction scheme (compressed sensing as well) from undersampled data that deals with a huge space of image ambiguity. In order to shed more light on the hallucination risk of GANCS, we perform a simple test to check the generalization accuracy for novel cases that are not seen during the training phase. In essence, the generative model learns a manifold of structured MR images. To simulate novel cases, we take a test MR image and perturb its entries independently with Gaussian noise N (0, σ 2 ) to fall off the MR image manifold. For a normalized image x with x F = 1, different noise levels ranging from σ = 0.0001 to σ = 1 are simulated and subsampled to feed into the G network (trained with real Knee images) as a test example. The Gaussian white noise impacts all frequency components due to its uniform and unlimited spectrum, and thus considered as an extreme adversary for the image reconstruction task that heavily relies on the structures. The reported SSIM and NMSE indicate that the network is robust against perturbations with intensities even larger than σ = 0.1. As argued in Remark 1 this robustness is fairly attributed to the data consistency and pixel-wise loss in the training of GANs. For very large noise intensities (around σ = 1) the performance can significantly drop as the input image is quite off the manifold, and it thus inherits no structures. Of course, developing robust regularization techniques against such adversarial examples is an important part of our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Number of patients for prediction</head><p>Prediction (generalization) performance of the deep learning model heavily depends on the amount of training data. This becomes more important when dealing with scarce medical data that are typically not accessible in large scales due to privacy concerns and institutional regulations. To address this question we examined an evaluation scenario to assess the reconstruction performance for a fixed test dataset, described in the beginning of this section, for variable number of patients used for training. Fig. <ref type="figure" target="#fig_8">10</ref> plots the test SNR versus the number of training patients for the GANCS scheme with = 0.975, λ = 0.025. For all cases we use the same number Fig. <ref type="figure">7</ref>. Representative axial knee image slices for GANCS reconstruction with undersampling rates 3, 5, 7, 10 compared with the fully sampled gold standard one. For GANCS we choose η = 0.9, λ = 0.1. SSIM is calculated on a window of size 100 × 50 chosen from the image center. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Discriminator interpretation</head><p>As suggested by the training strategy, the discriminator plays a role like a radiologist thats scores the quality of images created by the generator. During adversarial training D learns to correctly discern the real fully-sampled images from the fake ones, where the fake ones becomes quite realistic over training. It is thus insightful to understand image features that drive the quality score. To this end, we visualize the feature maps of D net at the sixth hidden convolutional layer. Recall that the D net has eight layers. In essence, the sixth layer is   the last convolutional layer before the fully-connected layers that form the decision variable for classification. It thus carries the most abstract representation of regional features declaring whether an image is real, or, fake (with artifacts). Fig. <ref type="figure" target="#fig_10">11</ref> indicates that after learning from tens of thousands of generated MRI images by the G network together with the corresponding gold-standard ones, where different organs are present, the D network learns to focus on certain regions of interest that are more susceptible to artifacts. The score is determined based on feature maps of the last several layers that pick up regions with rich texture details, and particularly focus on the periphery of important organs such as kidneys and liver. Note that there is no labels to instruct the network, but the network learns from the reconstruction how to correctly pick up the important organs as shown on the top rows in Fig. <ref type="figure" target="#fig_10">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>This paper caters a novel CS framework that leverages the historical data for rapid and high diagnostic-quality image reconstruction from highly undersampled MR measurements. A low-dimensional manifold is learned where the reconstructed images have not only superior sharpness and diagnostic quality, but also consistent with both the real MRI data and the acquisition model. To this end, a neural network scheme based on LSGANs and 1 / 2 costs is trained, where a generator is used to map a readily obtainable undersampled image to a realistic-looking one consistent with the measurements, while a discriminator network is trained jointly to score the quality of the resulting image. The overall training acts a a game between generator and discriminator that makes them more intelligent at reconstruction and quality evaluation.</p><p>Extensive experiments based on a large cohort of abdominal MR data, with the evaluations performed by expert radiologists, confirm that the GANCS retrieves images with noticeable diagnostic quality improvement "on the fly" (30 msec, 300× faster than state-of-the-art CS-MRI toolboxes). Last but not least, the scoring model offered by the trained discriminator can highlights the organ of interests automatically and can be used as a metric for perceptual assessment of future patients. There are still intriguing directions that can further improve GANCS, that go beyond the scope of this paper but are important subjects of our future studies. Such directions pertain to improving the robustness of the models against patients with abnormalities, variations in the sampling trajectory, residual motion artifacts in training data, and using 3D spatial correlations for improved quality of volumetric images. In this direction, quantifying possible hallucination risks for GANCS, and devising regularization approaches to robustify the reconstruction is an important step of our current research. Using the perceptual metric learned by the discriminator network as a image quality assurance metric is also another intriguing direction to pursue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. GANCS structure for image reconstruction (x) starts with an initial estimate x. The dashed module specifies affine projection onto the set of data-consistent images.</figDesc><graphic coords="4,115.56,53.14,380.88,183.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Generator ResNet architecture with residual blocks (RB), n and k refer to number of feature maps and filter size, respectively.</figDesc><graphic coords="5,67.07,53.14,477.85,154.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>After normalizing the 3D images, we adopt a consistent 256 × 128 resolution for the axial slices that are used as the input for training the G network. Each 3D image consists of 192 axial slices. 336 patients (64, 512 2D slices) are used for training, and 10 patients (1, 920 2D slices) for test. All in vivo scans were acquired at the Stanfords Lucile Packard Childrens Hospital on a 3T MRI scanner (GE MR750).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Discriminator multilayer CNN architecture with the input magnitude image, where n, k, and s refer to number of feature maps, filter size, and stride size, respectively.</figDesc><graphic coords="6,71.51,53.14,468.98,86.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Evolution of different cost components for G and D nets when η = 0.025 and λ = 0.975.</figDesc><graphic coords="6,311.97,188.58,252.00,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Representative coronal images for a representative test patient with 5-fold undersampling. The first column (1th) shows the full view gold standard image, and the rest of columns from left to right indicate the enlarged areas of liver (top row) and kidney (bottom row) for (2th) the gold-standard, and reconstruction under (3th) GANCS with 1 -cost (η = 1, λ = 0), (4th) GANCS with 1 -cost (η = 0.75, λ = 0.25), and (5th) CS-WV.</figDesc><graphic coords="8,63.17,204.48,485.64,232.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Representative coronal images with inflated adrenal gland. From left to right first column (1st) shows the gold-standard image with the zoomed kidney and adrenal gland area in the 2nd column. Likewise, 3rd and 4th columns are GANCS reconstruction for 5-fold undersampling when η = 0.75, λ = 0.25.</figDesc><graphic coords="11,68.40,213.84,475.20,138.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Histogram of SSIM and NMSE for GANCS reconstruction with noise perturbed MR images. Various noise levels with standard deviations σ = 10 -4 , 10 -3 , 10 -2 , 10 -1 , 0.5, 1 are considered. 50 random examples from each noise level are drawn, and we set λ = 0.1, η = 0.9.</figDesc><graphic coords="11,34.81,401.38,287.88,191.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Average SNR for a subset of test dataset under different number of training patients when η = 0.975 and λ = 0.025.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 11 depicts the heat-maps of feature maps, and when overlaid on the original images. Three representative image slices are shown from left to right, and different (eight) feature maps are shown from top to bottom. Each layer includes 128 feature maps, and thus for conciseness we depict only the the first 8 dominant principal images (found via principal component analysis) for the sixth layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Heat-map of discriminator feature maps at the sixth hidden layers for three MRI images with different anatomies. Each row demonstrates the eight dominant principal components of the feature maps. Each image corresponds to two consecutive columns; the fist column highlights the activation corresponding at the sixth hidden layer, while the second column visualizes the feature maps overlaid on top of corresponding MR image.</figDesc><graphic coords="12,39.60,53.13,514.06,408.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,48.96,53.14,533.16,188.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,48.96,253.55,532.66,170.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,48.96,53.14,521.47,245.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I AVERAGE</head><label>I</label><figDesc>SNR (DB), SSIM, AND RECONSTRUCTION TIME (SEC) COMPARISON OF DIFFERENT SCHEMES UNDER 5-FOLD (TOP) AND 10-FOLD (BOTTOM)</figDesc><table /><note><p>ACCELERATION.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II SCORING</head><label>II</label><figDesc>CRITERIA FOR IMAGE DIAGNOSTIC QUALITY, WHERE WE ABBREVIATE HEPATIC VEINS AS HV, AND PORTAL VEINS AS PV.</figDesc><table><row><cell>Score</cell><cell>overall quality</cell><cell></cell><cell cols="2">portal veins</cell><cell>hepatic veins</cell></row><row><cell>nondiagnostic (1)</cell><cell cols="2">no structures assessed</cell><cell cols="2">PV blurred</cell><cell>RHV blurred</cell></row><row><cell>limited (2)</cell><cell cols="2">limited assessment of</cell><cell cols="2">1st-order branches of PV</cell><cell>1st-order branches of RHV</cell></row><row><cell></cell><cell>several structures</cell><cell></cell><cell>blurred</cell><cell>blurred</cell></row><row><cell>diagnostic (3)</cell><cell cols="2">all but 1-2 structures</cell><cell cols="2">sharp first order branches of</cell><cell>sharp first order branches of</cell></row><row><cell></cell><cell>assessed</cell><cell></cell><cell>PV</cell><cell>RHV</cell></row><row><cell>good (4)</cell><cell cols="2">all structures assessed</cell><cell cols="2">sharp 2nd-order branches of</cell><cell>sharp 2nd-order branches of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PV</cell><cell>RHV</cell></row><row><cell>excellent (5)</cell><cell cols="2">sharp delineation of</cell><cell cols="2">braches seen to within 1cm of</cell><cell>braches seen to within 1cm of</cell></row><row><cell></cell><cell>all structures</cell><cell></cell><cell cols="2">periphery</cell><cell>periphery</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell></row><row><cell cols="5">MEAN AND STANDARD DEVIATION OF THE DIAGNOSTIC QUALITY SCORES GIVEN BY RADIOLOGISTS FOR THE GOLD STANDARD, GANCS</cell></row><row><cell></cell><cell cols="4">(η = 0.75, λ = 0.25), AND CS-WV.</cell></row><row><cell></cell><cell>score/scheme</cell><cell cols="2">overall quality</cell><cell>portal vein</cell><cell>hepatic vein</cell></row><row><cell></cell><cell>gold-standard</cell><cell cols="2">4.1 ± 0.22</cell><cell>4.1 ± 0.22</cell><cell>4.1 ± 0.22</cell></row><row><cell></cell><cell>GANCS</cell><cell cols="2">3.6 ± 0.37</cell><cell>3.6 ± 0.37</cell><cell>3.6 ± 0.37</cell></row><row><cell></cell><cell>CS-WV</cell><cell cols="2">3.1 ± 0.05</cell><cell>3.1 ± 0.05</cell><cell>3.1 ± 0.05</cell></row><row><cell cols="4">12 training epochs. Apparently, when the number of patients</cell><cell>schemes and different network models is an important question</cell></row><row><cell cols="4">increases from 1 to 130, a noticeable SNR gain is observed</cell><cell>that is a focus of our current research.</cell></row><row><cell cols="4">as more patients used for training. The performance gain</cell></row><row><cell cols="4">then gradually saturates as the number of patients is over</cell></row><row><cell cols="4">150. It thus seems with 150 or more patients we can take</cell></row><row><cell cols="4">full advantage of both learning from historical data and the</cell></row><row><cell cols="4">complexity of the networks. Recall that a fixed sampling mask</cell></row><row><cell cols="4">is used for training and testing. GANCS however captures</cell></row><row><cell cols="4">the signal model, and therefore it can easily accommodate</cell></row><row><cell cols="4">different sampling trajectories. Also note that, if more datasets</cell></row><row><cell cols="4">are available for training, we can further improve the model</cell></row><row><cell cols="4">performance by increasing model complexity. Further study</cell></row><row><cell cols="4">of the number of patients needed for other random sampling</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>cost (η = 1, λ = 0) at the beginning and then gradually switch to the mixture loss intended.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON MEDICAL IMAGING (TO APPEAR)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGMENTS</head><p>We would like to acknowledge Dr. Marcus Alley from the Radiology Department at Stanford University for setting up the infrastructure to automatically collect the dataset used in this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>† Work in this paper was supported by the NIH Grant T32121940 award and Grant NIH R01EB009690. Morteza Mardani, Enhao Gong, and Joseph Y. Cheng are with the Electrical Engineering and Radiology Departments, Stanford University, 350 Serra Mall, Stanford, CA, 94305</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Axial slices of size 320 × 256 are considered as the input for training and test. 16 patients are used for training (5, 120) and 3 patients for test <ref type="bibr">(960)</ref>.</p><p>Under this setting, the ensuing parts address the following intriguing questions:</p><p>Q1. Whether the manifold learning via training GAN is stably converged?</p><p>Q2. How much speed up and quality improvement can be achieved using GANCS relative to the conventional CS?</p><p>Q3. How is the perceptual quality of GANCS images rated by expert radiologists?</p><p>Q4. Is GANCS reliable in reconstructing the regions of interest for patients with and without abnormalities?</p><p>Q5. How many samples/patients are needed to achieve a reasonable image quality?</p><p>Q6. What MR image features drive the network to learn the manifold and remove the aliasing artifacts?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training convergence</head><p>To confirm the stable convergence of GANCS, evolution of different components of G and D costs for training are depicted in Fig. <ref type="figure">4</ref> over batches (size 4), with η = 0.025 and λ = 0.975 as an example to emphasize the GAN loss in training. According to (P1.2), the G cost mainly pertains to the last term which shows how well the G net can fool the D net. The D cost also includes two components based on (P1.1) associated with the classification performance for both real and fake images. It is apparent that all cost components decrease, and after about 5, 000 batches it reaches the equilibrium cost 0.25. This implies that upon convergence the G-net images become so realistic that the D-net will start flipping coin, i.e., D(x) = 1/2. Notice also that in this setting with a hard affine projection layer no data-consistency cost is incurred.</p><p>It is also worth mentioning that to improve the convergence stability of GANCS, and to ensure the initial distributions of fake and real images are overlapping, we trained with pure</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learned primal-dual reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Öktem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06474</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">rmk-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wasserstein GANs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compressed sensing using generative models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03208</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Low-dose CT with a residual encoder-decoder convolutional neural network (RED-CNN)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00288</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learned experts&apos; assessment-based reconstruction network (&quot; learn&quot;) for sparse-data ct</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09636</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Free-breathing pediatric MRI with nonrigid motion correction and acceleration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ruangwattanapaisarn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">P-LORAKS: Low-rank modeling of local kspace neighborhoods with parallel imaging data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1499" to="1514" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning a variational network for reconstruction of accelerated mri data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klatzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00447</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized magnetic resonance image reconstruction using the Berkeley Advanced Reconstruction Toolbox</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMRM Workshop on Data Sampling and Image Reconstruction</title>
		<meeting><address><addrLine>Sedona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03679</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compressed sensing and parallel MRI using deep residual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25st Annual Meeting of ISMRM</title>
		<meeting>the 25st Annual Meeting of ISMRM<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accelerated dynamic MRI exploiting sparsity and low-rank structure: k-t SLR</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Lingala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dibella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1042" to="1054" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse MRI: The application of compressed sensing for rapid MR imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Real-time dynamic MRI reconstruction using stacked denoising autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06383[cs.CV]</idno>
		<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<idno>ArXiv:1611.04076</idno>
		<imprint>
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tracking tensor subspaces with informative random sampling for real-time mr imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ugurbil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04104</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent generative adversarial networks for proximal learning and automated compressive image recovery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Monajemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10046</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mr image reconstruction from highly undersampled k-space data by dictionary learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1028" to="1041" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Creation of fully sampled mr data repository for compressed sensing of the knee</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Uecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Virtue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healthcare</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep cascade of convolutional neural networks for MR image reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25st Annual Meeting of ISMRM</title>
		<meeting>the 25st Annual Meeting of ISMRM<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Amortised MAP inference for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A practical acceleration algorithm for real-time imaging</title>
		<author>
			<persName><forename type="first">U</forename><surname>Sumbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2042" to="2051" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep ADMM-net for compressive sensing MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">1D partial fourier parallel MR imaging with deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25st Annual Meeting of ISMRM</title>
		<meeting>the 25st Annual Meeting of ISMRM<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for noise reduction in low-dose ct</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Isgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semantic image inpainting with perceptual and contextual losses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast pediatric 3d free-breathing abdominal dynamic contrast enhanced MRI with high spatiotemporal resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Potnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vasanawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural network MR image reconstruction with AUTOMAP: Automated transform by manifold approximation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25st Annual Meeting of ISMRM</title>
		<meeting>the 25st Annual Meeting of ISMRM<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
