<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
							<email>rmaddox@ur.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
							<email>zhiyao.duan@rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
							<email>chenliang.xu@rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers sequence-level information along with frame-level information. Thoughtful experiments on several datasets and realworld samples demonstrate significantly better results obtained by our method than the state-of-the-art methods in both quantitative and qualitative comparisons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modeling the dynamics of a moving human face/body conditioned on another modality is a fundamental problem in computer vision, where applications are ranging from audio-to-video generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref> to text-to-video generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref> and to skeleton-to-image/video generation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref>. This paper considers such a task: given a target face image and an arbitrary speech audio recording, generating a photo-realistic talking face of the target subject saying that speech with natural lip synchronization while maintaining a smooth transition of facial images over time (see Fig. <ref type="figure" target="#fig_0">1</ref>). Note that the model should have a robust general- The synthesized frames (last row) consist of synthesized attention (first row) and motion (second row), which demonstrate where and how the dynamics are synthesizing. For example, the face in the green box looks similar to the example face so that the attention map is almost dark; the face in the red box differs much from the example image, and hence the attention highlights the mouth region and the motion part hints white pixels for teeth.</p><p>ization capability to different types of faces (e.g., cartoon faces, animal faces) and to noisy speech conditions (see Fig. <ref type="figure">7</ref>). Solving this task is crucial to enabling many applications, e.g., lip-reading from over-the-phone audio for hearing-impaired people, generating virtual characters with synchronized facial movements to speech audio for movies and games.</p><p>The main difference between still image generation and video generation is temporal-dependency modeling. There are two main reasons why it imposes additional challenges: people are sensitive to any pixel jittering (e.g., temporal discontinuities and subtle artifacts) in a video; they are also sensitive to slight misalignment between facial movements and speech audio. However, recent researchers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref> tended to formulate video generation as a temporally independent image generation problem. For example, Chung et al. <ref type="bibr" target="#b2">[3]</ref> proposed an encoder-decoder structure to generate one image from 0.35-second audio at each time. Song et al. <ref type="bibr" target="#b26">[27]</ref> adopted a recurrent network to consider temporal dependency. They applied RNN in the feature extraction part, however, each frame was generated inde-pendently in the generation stage. In this paper, we propose a novel temporal GAN structure, which consists of a multi-modal convolutional-RNN-based (MMCRNN) generator and a novel regression-based discriminator structure. By modeling temporal dependencies, our MMCRNNbased generator yields smoother transactions between adjacent frames. Our regression-based discriminator structure combines sequence-level (temporal) information and frame-level (pixel variations) information to evaluate the generated video.</p><p>Another challenge of the talking face generation is to handle various visual dynamics (e.g., camera angles, head movements) that are not relevant to and hence cannot be inferred from speech audio. Those complicated dynamics, if modeled in the pixel space <ref type="bibr" target="#b29">[30]</ref>, will result in low-quality videos. For example, in web videos <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> (e.g., LRW and VoxCeleb datasets), speakers move significantly when they are talking. Nonetheless, all the recent photo-realistic talking face generation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref> failed to consider this problem. In this paper, we propose a hierarchical structure that utilizes a high-level facial landmarks representation to bridge the audio signal with the pixel image. Concretely, our algorithm first estimates facial landmarks from the input audio signal and then generates pixel variations in image space conditioned on generated landmarks. Besides leveraging intermediate landmarks for avoiding directly correlating speech audio with irrelevant visual dynamics, we also propose a novel dynamically adjustable loss along with an attention mechanism to enforce the network to focus on audiovisual-correlated regions. It is worth to mention that in a recent audio-driven facial landmarks generation work <ref type="bibr" target="#b7">[8]</ref>, such irrelevant visual dynamics are removed in the training process by normalizing and identityremoving the facial landmarks. This has been shown to result in more natural synchronization between generated mouth shapes and speech audio.</p><p>Combining the above features, which are designed to overcome limitations of existing methods, our final model can capture informative audiovisual cues such as the lip movements and cheek movements while generating robust talking faces under significant head movements and noisy audio conditions. We evaluate our model along with state-of-the-art methods on several popular datasets (e.g., GRID <ref type="bibr" target="#b5">[6]</ref>, LRW <ref type="bibr" target="#b4">[5]</ref>, VoxCeleb <ref type="bibr" target="#b23">[24]</ref> and TCD <ref type="bibr" target="#b12">[13]</ref>). Experimental results show that our model outperforms all compared methods and all the proposed features contribute effectively to our final model. Furthermore, we also show additional novel examples of synthesized facial movements of the human/cartoon characters who are not in any dataset to demonstrate the robustness of our approach.</p><p>The contributions of our work can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a novel cascade network structure to reduce the effects of the sound-irrelevant visual dynamics in the image space. Our model explicitly constructs high-level representation from the audio signal and guides video generation using the inferred representation.</p><p>(2) We exploit a dynamically adjustable pixel-wise loss along with an attention mechanism which can alleviate temporal discontinuities and subtle artifacts in video generation. <ref type="bibr" target="#b2">(3)</ref> We propose a novel regression-based discriminator to improve the audio-visual synchronization and to smooth the facial movement transition while generating realistic looking images. The code has been released at https://github.com/lelechen63/ATVGnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we first briefly survey related work on the talking face generation task. Then we discuss the related work of each technique used in our model. Talking Face Synthesizing The success of traditional approaches has been mainly limited to synthesizing a talking face from speech audio of a specific person <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Suwajanakorn et al. <ref type="bibr" target="#b28">[29]</ref> synthesized a taking face of President Obama with accurate lip synchronization, given his speech audio. The mechanism is to first retrieve the best-matched lip region image from a database through audiovisual feature correlation and then compose the retrieved lip region with the original face. However, this method requires a large amount of video footage of the target person. More recently, by combining the GAN/encoder-decoder structure and the data-driven training strategy, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref> can generate arbitrary faces from arbitrary input audio. High-Level Representations In recent years, high-level representations of images <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15]</ref> have been exploited in video generation tasks by using an encoderdecoder structure as the main approach. Given a condition, we can transfer it to high-level representations and feed them to a generative network to output a distribution over locations that a pixel is predicted to move. By adopting human body landmarks, Villegas et al. <ref type="bibr" target="#b30">[31]</ref> proposed an encoder-decoder network which achieves long-term future prediction. Suwajanakorn et al. <ref type="bibr" target="#b27">[28]</ref> transferred the audio signal to lip shapes and then synthesized the mouth texture based on the transferred lip shapes. These works have inspired us to use the facial landmarks to bridge audio with row pixel generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism</head><p>Attention mechanism is an emerging topic in natural language tasks <ref type="bibr" target="#b19">[20]</ref> and image/video generation task <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. Pumarola et al. <ref type="bibr" target="#b25">[26]</ref> generated facial expression conditioned on action units annotations. Instead of using a basic GAN structure, they exploited a generator that regresses an attention mask and a RGB color transformation over the entire image. The attention mask defines a per-pixel intensity specifying to what extend each pixel of the original image will contribute to the final rendered image. We adopt this attention mechanism to make our network robust to visual variations and noisy audio conditions. Feng et al. <ref type="bibr" target="#b9">[10]</ref> observed that integrating a weighted mask into the loss function during training can improve the performance of the reconstruction network. Based on this observation, rather than using a fixed loss weights, we propose a dynamically adjustable loss by leveraging the attention mechanism to emphasize the audiovisual regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture</head><p>This section describes the architecture of the proposed model. Fig. <ref type="figure" target="#fig_1">2</ref> shows the overall diagram, which is decoupled into two parts: audio transformation network (AT-net) and visual generation network (VG-net). First, we explain the overall architecture and the training strategy in Sec. 3.1. Then, we introduce two novel components: attention-based dynamic pixel-wise loss in Sec. 3.2 and a regression-based discriminator structure in Sec. 3.3 used in our VG-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview Cascade Structure and Training Strategy</head><p>We tackle the task of talking face video generation in a cascade perspective. Given the input audio sequence a 1:T , one example frame i p and its landmarks p p , our model generates facial landmarks sequence p1:T and subsequently generates frames v1:T . To solve this problem, we come up with a novel cascade network structure:</p><formula xml:id="formula_0">p1:T = Ψ(a 1:T , p p ) ,<label>(1) v1</label></formula><formula xml:id="formula_1">:T = Φ(p 1:T , i p , p p ) ,<label>(2)</label></formula><p>where the AT-net Ψ (see Fig. Audio Transformation Network (AT-net) Specifically, the AT-net (Ψ) is formulated as:</p><formula xml:id="formula_2">[h t , c t ] = ϕ lmark (LSTM(f audio (a t ), f lmark (h p ), c t−1 )),<label>(3)</label></formula><formula xml:id="formula_3">pt = PCA R (h t ) = h t ⊙ ω * U T + M .<label>(4)</label></formula><p>Here, the AT-net observes the audio MFCC a t and landmarks PCA components h p of the target identity and outputs PCA components h t that are paired with the input audio MFCC. The f audio , f lmark and ϕ lmark indicate audio encoder, landmarks encoder and landmarks decoder. The c t−1 and c t are outputs from cell units. PCA R is PCA reconstruction and ω is a boost matrix to enhance the PCA feature. The U corresponds to the largest eigenvalues and M is the mean shape of landmarks in the training set. In our empirical study, we observe that PCA can decrease the effect of none-audio-correlated factors (e.g., head movements) for training the AT-net.</p><p>Visual Generation Network (VG-net) Intuitively, similar to <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31]</ref>, we assume that the distance between current landmarks p t and example landmarks p p in feature space can represent the distance between current image frame and example image in image feature space. Based on this assumption (see Eq. 5), we can obtain current frame feature v ′′ t (size of 128 × 8 × 8). Different from their methods, we replace element-wise addition with channel-wise concatenation in Eq. 5, which better preserves original frame information in our empirical study. At the meanwhile, we can also compute an attention map (att pt ) based on the dif- ference between p t and p p (see Eq. 6). By feeding the computed v ′′ t and att pt along with example image feature i ′ p (size of 128 × 32 × 32) into the MMCRNN part, we obtain the current image feature v ′ t (see Eq. 7). The resultant image feature v ′ t will be used to generate video frames as detailed in the next section. Specifically, the VG-net is performed by:</p><formula xml:id="formula_4">v ′′ t = f img(ip) ⊕ (f lmark (p t ) − f lmark (p p )) ,<label>(5)</label></formula><formula xml:id="formula_5">att pt = σ(f lmark (p t ) ⊕ f lmark (p p )) ,<label>(6)</label></formula><formula xml:id="formula_6">v ′ t = (CRNN(v ′′ t )) ⊙ att pt + i ′ p ⊙ (1 − att pt ) ,<label>(7)</label></formula><p>where ⊕ and ⊙ are concatenation operation and elementwise multiplication, respectively. The CRNN part consists of Conv-RNN, residual block and deconvolution layers. i ′ p is the middle layer output of f img (i p ), and σ is Sigmoid activation function. We omit some convolution operations in equations for better understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention-Based Dynamic Pixel-wise Loss</head><p>Recent works on video generation adopt either GANbased methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref> or Encoder-Decoder-based methods <ref type="bibr" target="#b2">[3]</ref>. However, one common problem is the pixel jittering between adjacent frames (see Fig. <ref type="figure" target="#fig_3">3</ref>). Pixel jittering is not obvious in single image generation but is a severe problem for video generation as humans are sensitive to any pixel jittering, e.g., temporal discontinuities and subtle artifacts in a video. The reason is that GAN loss or L1/L2 loss can barely generate perfect frames that all pixels are consistently changing in temporal domain, especially for audiovisual-non-correlated regions, e.g., background and head movements. In order to solve the pixel jittering problem, we propose a novel dynamic pixel-wise loss to enforce the generator to generate consistent pixels along temporal axis.</p><p>As mentioned in Sec. 2, Pumarola et al. <ref type="bibr" target="#b25">[26]</ref> exploited a generator that regresses an attention mask and a RGB color transformation over the entire image. We adapt this attention mechanism in our VG-net to disentangle the motion part from audiovisual-non-correlated regions. Therefore, our final frame output is governed by the combination:</p><formula xml:id="formula_7">vt = α t ⊙ m t + (1 − α t ) ⊙ i p ,<label>(8)</label></formula><p>where attention α t is obtained by applying convolution and Sigmoid activation operations on v ′ t , motion m t is obtained by applying another convolution and hyperbolic tangent activation operations on v ′ t . This step enforces the network to generate stable pixels in audiovisual-non-correlated regions while generating movements in audiovisual-correlated regions.</p><p>From Fig. <ref type="figure" target="#fig_6">5</ref>, we can conclude that the pixels in audiovisual-non-correlated regions (e.g., hair, background etc.) usually attract less attention and are irrelevant to given condition (audio). In contrast, the network is mainly focusing on correlated regions (e.g., mouth, jaw, and cheek). Intuitively, 0 ≤ α t ≤ 1 can be viewed as a spatial mask that indicates which pixels of given face image i p need to move at time step t. We can also regard α t as a reference to represent to what extend each pixel contributes to the loss. The audiovisual-non-correlated regions should contribute less to the loss compared with the correlated regions. Thus, we propose a novel dynamic adjustable pixel-wise loss by leveraging the power of α t , which is defined as:</p><formula xml:id="formula_8">L pix = T t=1 (v t − vt ) ⊙ (α t + β) 1 ) ,<label>(9)</label></formula><p>where α t is the same as α t but without gradient. It represents the weight of each pixel dynamically that eases the generation. We remove the gradient of α t when backpropagating the loss to the network to prevent trivial solutions (lower loss but no discriminative ability). We also give base weights β to all pixels to make sure all pixels will be optimized. Here, we manually tune the hyper-parameter β and set β = 0.5 in all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regression-Based Discriminator</head><p>Recently, people find that perceptual loss <ref type="bibr" target="#b15">[16]</ref> is helpful for generating sharp images in GAN/VAE <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1]</ref>. Perceptual loss utilizes high-level features to compare generated images and ground-truth images resulting in better sharpness of the synthesized images. The key idea is that the weights of the perceptual network part are fixed, and the loss will only contribute to the generator/decoder part. Based on this intuition, we propose a novel discriminator structure (see Fig. <ref type="figure" target="#fig_4">4</ref>). The discriminator observes example landmarks p p and either ground truth video frames v 1:T or synthesized video frames v1:T , then regresses landmarks shapes p1:T paired with the input frames, and additionally, gives a discriminative score s for the entire sequence. Specifically, we formulate discriminator into frame-wise part D p (blue arrows in Fig. <ref type="figure" target="#fig_4">4</ref>) and sequence-level part D s (red arrows in Fig. <ref type="figure" target="#fig_4">4</ref>). The D p observes example landmarks and video frames, then regresses the landmarks sequence based on observed information. By yielding the facial landmark, it can evaluate the input image based on high-level representation in a frame-wise fashion. Specifically, the pt is calculated by: </p><formula xml:id="formula_9">pt = D p (p p , v t ) = p p + LSTM(f lmark (p p ) ⊕ f img (v t )) ,<label>(10)</label></formula><formula xml:id="formula_10">s = D s (p p , v 1:T ) = σ( 1 T T t=1 (LSTM(f lmark (p p ) ⊕ f img (v t )))) . (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>The D p part is optimized to minimize the L2 loss between the predicted landmarks and the ground truth landmarks. Thus our GAN loss can be expressed as:</p><formula xml:id="formula_12">L gan =E pp,v 1:T [log D s (p p , v 1:T )]+ E pp,p 1:T ,ip [log(1 − D s (p p , G(p p , p 1:T , i p ))]+ (D p (p p , G(p p , p 1:T , i p )) − p 1:T ) ⊙ M p 2 2 + (D p (p p , v 1:T ) − p 1:T ) ⊙ M p 2 2 , (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where M p is a pre-defined weight mask hyper-parameter which can penalize more on lip regions. By updating the parameters based on the regression loss when training the discriminator, the D p can learn to extract low-dimensional representations from raw image data. When we train the generator, we will fix the weights of discriminator including D s and D p so that D p will not compromise to generator. The loss back-propagated from D p will enforce generator to generate accurate face shapes (e.g., cheek shape, lip shape etc.) and the loss back-propagated from D s will enforce the network to generate high-quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>By linearly combining all partial losses introduced in Sec. 3.2 and Sec. 3.3, the full loss function L can be expressed as:</p><formula xml:id="formula_14">L = L gan + λ * L pix , (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where λ is a hyper-parameter that controls the relative importance of different loss terms. We set λ = 10.0 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct thoughtful experiments to demonstrate the efficiency and effectiveness of the proposed architecture for video generation. Sec. 4.1 explains datasets and implementation in detail. Sec. 4.2 shows our results along with other state-of-the-art methods. We show user studies and ablation study in Sec.4.3 and Sec. 4.4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset We quantitatively and qualitatively evaluate our ATVGnet on LRW dataset <ref type="bibr" target="#b3">[4]</ref> and GRID dataset <ref type="bibr" target="#b5">[6]</ref>. The LRW dataset consists of 500 different words spoken by hundreds of different speakers in the wild. We follow the same train-test split as in <ref type="bibr" target="#b3">[4]</ref>. In GRID dataset, there are 1000 short videos, each spoken by 33 different speakers in the experimental condition. For the image stream, all the talking faces in the videos are aligned based on key-points (eyes and nose) of the extracted landmarks using <ref type="bibr" target="#b17">[18]</ref> at the sampling rate of 25FPS, and then resize to 128 × 128. As for audio data, each audio segment corresponds to 280ms audio. We extract MFCC at the window size of 10ms and use center image frame as the paired image data. Similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>, we remove the first coefficient from the original MFCC vector, and eventually yield a 28×12 MFCC feature for each audio chunk. Implementation Details Our network is implemented using Pytorch 0.4 library. We adopt Adam optimizer during training with the fixed learning rate of 2 × 10 -4 . We initialize all network layers using random normalization with mean=0.0, std=0.2. All models are trained and tested on a single NVIDIA GTX 1080Ti. During the training, the ATnet converges after 3 hours and the VG-net is stable after </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Real time ATVGnet(our) Chung et al. <ref type="bibr" target="#b2">[3]</ref>  The inference time of difference models. We use frame rate (FPS) to measure the time.</p><p>24 hours. Table <ref type="table" target="#tab_0">1</ref> shows generation time during inference stage. We can find that our inference time can achieve around 34.5 frames per second (FPS), which is much faster than <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref> and slightly faster than real time (30 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Image results are illustrated in Fig. <ref type="figure" target="#fig_6">5</ref> and Fig. <ref type="figure">7</ref>. To evaluate the quality of the synthesized video frames, we compute PSNR and SSIM <ref type="bibr" target="#b32">[33]</ref>. To evaluate whether the synthesized video contains accurate lip movements that correspond to the input audio, we adopt the evaluation matrix Landmarks Distance (LMD) proposed in <ref type="bibr" target="#b0">[1]</ref>. We compare our model with other three state-of-the-art meth-ods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35]</ref>. All of them are trained on LRW dataset while Chung et al. <ref type="bibr" target="#b2">[3]</ref> require extra VGG-M network pretrained on VGG Face dataset <ref type="bibr" target="#b24">[25]</ref> and Wilels et al. <ref type="bibr" target="#b34">[35]</ref> need extra MFCC feature extractor pretrained by <ref type="bibr" target="#b4">[5]</ref>. The quantitative results are illustrated in Table <ref type="table" target="#tab_1">2</ref>. The Baseline model is a straightforward model without any features (e.g., DMA, MMCRNN, DAL and RD explained in Sec. 4.4) as mentioned in Sec. 3. The model ATVG-ND has the same network structure as ATVGnet. But it is trained end-to-end without the decoupled training strategy (see Sec. 3.1). We can find that our ATVGnet achieves the best results both in image quality (SSIM, PSNR) and the correctness of audiovisual synchronization (LMD). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>LRW GRID LMD SSIM PSNR LMD SSIM PSNR Chen <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">User Studies</head><p>Our goal is to generate realistic videos based on the audio information. The evaluation in 4.2 can only evaluate the quality in a single frame style. To evaluate the performance in a video level, we conduct thoughtful user studies in this section. Human subjects evaluation (see Fig. <ref type="figure" target="#fig_7">6</ref>) is conducted to investigate the visual qualities of our generated results compared with Chung et al. <ref type="bibr" target="#b2">[3]</ref> and Zhou et al. <ref type="bibr" target="#b11">[12]</ref>. The ground truth videos are selected from different sources: we randomly select samples from the testing set of LRW <ref type="bibr" target="#b4">[5]</ref>, VoxCeleb <ref type="bibr" target="#b23">[24]</ref>, TCD <ref type="bibr" target="#b12">[13]</ref>, GRID <ref type="bibr" target="#b5">[6]</ref> and realworld samples from YouTube (in total 38 videos). Three methods are evaluated w.r.t. two different criteria: whether participants could regard the generated talking faces as realistic and whether the generated talking faces temporally sync with the corresponding audio. We shuffle all the sample videos and the participants are not aware of the mapping between videos to methods. ages on a scale of 0 (worst) to 10 (best). There are overall 10 participants involved, and the results are summed over persons and video time steps. According to the ratings from Fig. <ref type="figure" target="#fig_7">6</ref>, we can find that our method outperforms other two methods in terms of the extent of synchronization and authenticity. More specifically, our model achieves the best results on all datasets in terms of lip synchronization with audio input. As for image authenticity, our model achieves the highest score the on most of the datasets but slightly lower than Chung et al. <ref type="bibr" target="#b2">[3]</ref> on the VoxCeleb testing set. We attribute this to the audio noise (e.g. background music) in the test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We conduct ablation experiments to study the contributions of the four components introduced in Sec. 3: Dynamic Motion &amp; Attention (DMA), Multi-Modal-crnn (MMCRNN), Dynamically Adjustable Loss (DAL) and Recreational Discriminator (RD). The ablation studies are conducted on both LRW dataset and GRID dataset. Results are shown in Table <ref type="table" target="#tab_2">3</ref>. Here we follow the protocols mentioned in Sec. 4.1. We test each model using ground truth landmarks rather than fake landmarks generated by AT-net, so that we can eliminate the errors caused by uncorrelated noise and focus on each component.</p><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, each component contributes to the full model. We can find that MMCRNN and DMA are critical to our full model. We attribute this to the better ability of generating smooth transactions between adjacent frames. The ATVG-P model has the same structure as ATVGnet but conditioned on the last fake frame vt−1 rather than the example frame i p in Eq. 8 in Sec. 3.2. We suppose it could yield better performance. However, the error amplifies quickly through time until it overwhelms the visual information from example frame, which leads to a trivial solution that α t = 0 n×n and decreases the performance.</p><p>We investigate the model performance w.r.t. the gen-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In this paper, we present a cascade talking face video generation approach utilizing facial landmarks as intermediate high-level representations to bridge the gap between two different modalities. We propose a novel Multi-Modal Convolutional-RNN structure, which considers the correlation between adjacent frames in the generation stage. Meanwhile, we propose two novel components: dynamically adjustable loss and regression-based discriminator. In our perspective, these two techniques are general that could be adopted in other tasks (e.g., human body generation and facial expression generation) in the future. Our final model ATVGnet achieves the best performance on several popular datasets in both qualitative and quantitative comparisons. For future work, applying other techniques to enable our network to generate unconscious head movements/expressions could be an interesting topic, which has been bypassed in our current approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Problem description. The model takes an arbitrary audio speech and one face image, and synthesizes a talking face saying the speech. The synthesized frames (last row) consist of synthesized attention (first row) and motion (second row), which demonstrate where and how the dynamics are synthesizing. For example, the face in the green box looks similar to the example face so that the attention map is almost dark; the face in the red box differs much from the example image, and hence the attention highlights the mouth region and the motion part hints white pixels for teeth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our network architecture. The blue part illustrates the AT-net, which transfers audio signal to lowdimensional landmarks representation and the green part illustrates the VG-net, which generates video frames conditioned on the landmark. During training, the input to VG-net are ground truth landmarks (p 1:T ). During inference, the input to VG-net are fake landmarks (p 1:T ) generated by AT-net. The AT-net and VG-net are trained separately to avoid error accumulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2 blue part) is a conditional LSTM encoder-decoder and the VG-net Φ (see Fig.2green part) is a multi-modal convolutional recurrent network. During inference, the AT-net Ψ (see Eq. 1) observes audio sequence a 1:T and example landmarks p p and then predicts low-dimensional facial landmarks p1:T . By passing p1:T into VG-net Φ (see Eq. 2) along with example image i p and p p , we subsequently get synthesized video frames v1:T . Ψ and Φ are trained in a decoupled way so that Φ can be trained with teacher forcing strategy. To avoid the error accumulation caused by p1:T , Φ is conditioned on ground truth landmarks p 1:T during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The results of our baseline method. The synthesized frames with pixel jittering problem. The discontinuous problem and subtle artifacts will be amplified after composing into a video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The overview of the regression-based discriminator. The ⊕ means concatenation. The + means elementwise addition. The blue arrow and red arrow represent D p and D s , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>which observes ground truth image during discriminator training stage and observes synthesized image during generator training stage. Besides D p , the LSTM cell unit yields another branch D s , which obtains vectors from each LSTM cell unit and aggregates them by average pooling. By passing through a Sigmoid activation function, D s yields final discriminative score s for the overall input sequence. The score s can obtained by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The outputs of ATVGnet. The inputs are one real-world audio sequence and different example identity images range from real-world people to cartoon characters. The first row is ground truth images paired with the given audio sequence. We mark the different sources of the identity image on the left side. From this figure, we can find that the lip movements of our synthesized frames (e.g., the green box in the last row) are well-synchronized with the ground truth (red box in first row). Meanwhile, the attention (middle row of the green box) accurately indicates where need to move and the motion (last row of the green box) indicates what the dynamics look like (e.g. white pixels for teeth and red pixels for lips).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Statistics of user studies. The y-axis is the percentage of votes and the x-axis is different data sources (e.g., total means all the video samples, Other means sampled videos from YouTube.) The left histogram is the rating on authenticity. The right histogram is the rating on synchronization between facial movements and audio.</figDesc><graphic url="image-343.png" coords="7,59.00,73.57,104.54,118.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Qualitative results produced by ATVGnet, Chung et al.<ref type="bibr" target="#b2">[3]</ref> and Zhou et al.<ref type="bibr" target="#b11">[12]</ref> on samples from LRW and VoxCeleb dataset. We can observe from it that our mouth opening is closer to ground truth compared with the other two methods. It is worthwhile to mention that the second sample is recorded outside with loud background noise.</figDesc><graphic url="image-480.png" coords="8,53.03,379.68,231.72,75.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Zhou et al.[12] Wiles et al.[35]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results of different methods on LRW dataset and GRID dataset. Our models mentioned in this table are trained from scratch. We bold each leading score.</figDesc><table><row><cell>1]</cell><cell>1.73 0.73 29.65 1.59 0.76 29.33</cell></row><row><cell>Wiles [35]</cell><cell>1.60 0.75 29.82 1.48 0.80 29.39</cell></row><row><cell>Chung [3]</cell><cell>1.63 0.77 29.91 1.44 0.79 29.87</cell></row><row><cell>Baseline</cell><cell>1.71 0.72 28.95 1.82 0.77 28.78</cell></row><row><cell cols="2">ATVG-ND 1.35 0.78 30.27 1.34 0.79 30.51</cell></row><row><cell>ATVGnet</cell><cell>1.37 0.81 30.91 1.29 0.83 32.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>They are asked to score the im-Ablation studies on the LRW dataset and the GRID dataset. We remove each feature at a time. We bold the highest scores.</figDesc><table><row><cell>Method</cell><cell></cell><cell>LRW</cell><cell>GRID</cell></row><row><cell></cell><cell></cell><cell cols="2">LMD SSIM PSNR LMD SSIM PSNR</cell></row><row><cell cols="2">ATVGnet</cell><cell cols="2">0.80 0.86 33.45 0.70 0.89 33.84</cell></row><row><cell cols="2">w/o DMA</cell><cell cols="2">0.98 0.83 30.22 1.10 0.84 29.90</cell></row><row><cell>w/o</cell><cell>MM-</cell><cell cols="2">1.03 0.80 30.61 0.81 0.86 32.68</cell></row><row><cell>CRNN</cell><cell></cell><cell></cell></row><row><cell cols="2">w/o DAL</cell><cell cols="2">0.86 0.86 31.35 0.76 0.87 33.11</cell></row><row><cell>w/o RD</cell><cell></cell><cell cols="2">0.82 0.84 32.84 0.73 0.88 33.25</cell></row><row><cell cols="2">Baseline</cell><cell cols="2">1.27 0.81 29.55 1.17 0.80 29.45</cell></row><row><cell cols="2">ATVG-P</cell><cell cols="2">0.90 0.84 30.45 0.75 0.87 31.78</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported in part by NSF IIS 1741472, IIS 1813709, and the University of Rochester AR/VR Pilot Award. This article solely reflects the opinions and conclusions of its authors and not the funding agents.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="538" to="553" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep crossmodal audio-visual generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
				<meeting>the on Thematic Workshops of ACM Multimedia<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-23">2017. October 23 -27, 2017. 2017</date>
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
				<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-04">2017. 2017. September 4-7, 2017, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2016 -13th Asian Conference on Computer Vision</title>
				<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 20-24, 2016. 2016</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part II</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Out of time: Automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2016 Workshops -ACCV 2016 International Workshops</title>
				<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">November 20-24, 2016. 2016</date>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part II</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gp-gan: Gender preserving gan for synthesizing faces from landmarks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<date type="published" when="2018-08">Aug 2018</date>
			<biblScope unit="page" from="1079" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating talking face landmarks from speech</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Eskimez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Variable Analysis and Signal Separation -14th International Conference</title>
				<meeting><address><addrLine>Guildford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 2-5, 2018. 2018</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
	<note>LVA/ICA 2018. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Photo-real talking head with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015</title>
				<meeting><address><addrLine>South Brisbane, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">April 19-24, 2015. 2015</date>
			<biblScope unit="page" from="4884" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="557" to="574" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L P L X W</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TCD-TIMIT: an audio-visual corpus of continuous speech</title>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic image manipulation through structured representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2708" to="2718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="7065" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal</meeting>
		<imprint>
			<date type="published" when="2015">September 17-21, 2015. 2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="405" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Da-gan: Instancelevel image translation by deep attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentive semantic video generation using captions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2017</title>
				<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="1435" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference 2015, BMVC 2015</title>
				<meeting>the British Machine Vision Conference 2015, BMVC 2015<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 7-10, 2015. 2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="41" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ganimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
	<note>Proceedings, Part X</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno>CoRR, abs/1804.04786</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno>95:1-95:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno>95:1-95:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017. 2017</date>
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden</meeting>
		<imprint>
			<date type="published" when="2018">July 10-15. 2018. 2018</date>
			<biblScope unit="page" from="6033" to="6041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno>CoRR, abs/1805.08318</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
