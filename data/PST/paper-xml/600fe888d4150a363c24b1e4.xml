<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FACE-KEG: FAct Checking Explained using KnowledgE Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nikhita</forename><surname>Vedula</surname></persName>
							<email>vedula.5@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
							<email>parthasarathy.2@osu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FACE-KEG: FAct Checking Explained using KnowledgE Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3437963.344182</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, a plethora of fact checking and fact verification techniques have been developed to detect the veracity or factuality of online information text for various applications. However, limited efforts have been undertaken to understand the interpretability of such veracity detection, i.e. explaining why a particular piece of text is factually correct or incorrect. In this work, we seek to bridge this gap by proposing a technique, FACE-KEG, to automatically perform explainable fact checking. Given an input fact or claim, our proposed model constructs a relevant knowledge graph for it from a large-scale structured knowledge base. This graph is encoded via a novel graph transforming encoder. Our model also simultaneously retrieves and encodes relevant textual context about the input text from the knowledge base. FACE-KEG then jointly exploits both the concept-relationship structure of the knowledge graph as well as semantic contextual cues in order to (i) detect the veracity of an input fact, and (ii) generate a human-comprehensible natural language explanation justifying the fact's veracity. We conduct extensive experiments on three large-scale datasets, and demonstrate the effectiveness of FACE-KEG while performing fact checking. Automatic and human evaluations further show that FACE-KEG significantly outperforms competitive baselines in learning concise, coherent and informative explanations for the input facts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Huge volumes of data are continuously being generated and retrieved as a result of the rapid development in information extraction techniques. Detecting if a given piece of information is true or factually correct plays a vital role in various natural language applications such as language understanding <ref type="bibr" target="#b51">[41,</ref><ref type="bibr" target="#b70">60]</ref>, knowledge graph completion <ref type="bibr" target="#b27">[17]</ref> and open domain question answering <ref type="bibr" target="#b21">[11,</ref><ref type="bibr" target="#b55">45]</ref>. A plethora of techniques have been developed to tackle the problem of automated fact checking <ref type="bibr" target="#b15">[5,</ref><ref type="bibr" target="#b26">16,</ref><ref type="bibr" target="#b27">17,</ref><ref type="bibr" target="#b43">33,</ref><ref type="bibr" target="#b46">36,</ref><ref type="bibr" target="#b55">[45]</ref><ref type="bibr" target="#b56">[46]</ref><ref type="bibr" target="#b57">[47]</ref><ref type="bibr" target="#b66">56]</ref>. However, most approaches merely focus on detecting if a claim is true. They cannot adequately explain why a claim was detected as true or false. Such explanations are desirable because they can (i) help non-experts comprehend the veracity of niche or domain-specific claims; and (ii) provide new and useful insights that can improve the general performance of fact checking. As an example, rather than just detecting that the claim "Due Date is a horror film" (from <ref type="bibr" target="#b67">[57]</ref>) is false, it is useful to have an accompanying explanation such as "Due Date is a 2010 American comedy film...". Prior work has been done to judge claim reliability based on the trustworthiness of the authors of the claims <ref type="bibr" target="#b29">[19,</ref><ref type="bibr" target="#b58">48,</ref><ref type="bibr" target="#b73">63]</ref>. Fact checking websites such as www.politifact.com and www.snopes.com explain the veracity of input claims by providing supporting or refuting evidence. However, they involve expensive and time-consuming checks and assessments by human reviewers. In this work, we seek to automate the process of explaining fact checking decisions.</p><p>A step towards explainable fact checking is solving the task of fact verification <ref type="bibr" target="#b23">[13,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b43">33,</ref><ref type="bibr" target="#b55">45,</ref><ref type="bibr" target="#b66">56,</ref><ref type="bibr" target="#b67">57,</ref><ref type="bibr" target="#b81">71]</ref>. It involves verifying if an input claim is supported or refuted by a given piece of text. Unlike our work that seeks to generate concise explanatory evidence, the fact verification task takes the evidence text as input. Extractive approaches that retrieve evidence sentences to reason about a claim have achieved promising results <ref type="bibr" target="#b15">[5,</ref><ref type="bibr" target="#b27">17,</ref><ref type="bibr" target="#b43">33,</ref><ref type="bibr" target="#b57">47,</ref><ref type="bibr" target="#b81">71]</ref>. Though such evidence can serve as an explanation, it is often longwinded and contains irrelevant information unnecessary to justify the claim. The retrieved evidence also often contains text that is semantically related to the input claim, but cannot explain its factuality. Efforts have also been undertaken to explain fact veracity via (i) semantic traces and patterns derived from large, generalpurpose knowledge ontologies (e.g. DBPedia <ref type="bibr" target="#b13">[3]</ref>); (ii) formal logic rules; and (iii) attributed relations from semi-structured tables or databases <ref type="bibr" target="#b23">[13,</ref><ref type="bibr" target="#b26">16,</ref><ref type="bibr" target="#b27">17,</ref><ref type="bibr" target="#b56">46]</ref>. However, explanations formulated in this fashion are often incoherent, longer than necessary with extraneous information, and are not as easily readable and interpretable as plain text. We desire to address these limitations in our work by jointly detecting the veracity of input claims, and explaining their veracity in a concise, coherent, easily comprehensible manner.</p><p>To this end, we propose a framework FACE-KEG. It first builds a knowledge graph, and retrieves unstructured textual context pertaining to each input claim. A graph transformer network and a bidirectional recurrent neural network are employed to encode the knowledge graph and textual context respectively. This is followed by jointly training a classifier that predicts if the claim is true or not, and a decoder that learns to generate a natural language explanation clarifying the veracity of the claim. We propose to learn explanations for input claims from the interrelated perspectives of the claim content, suitable background context and structured conceptual knowledge relevant to the claim. Apart from gaining a semantic understanding, analyzing pertinent context can help unearth indirect cues such as the stance or sentiment of the input fact or claim, that can be useful for fact checking <ref type="bibr" target="#b15">[5,</ref><ref type="bibr" target="#b49">39]</ref>. We also introduce open-domain, open-topic auxiliary knowledge <ref type="bibr" target="#b71">[61,</ref><ref type="bibr" target="#b72">62]</ref> in FACE-KEG via general-purpose knowledge bases, that (i) is not restricted to small-scale domain-specific information <ref type="bibr" target="#b19">[9,</ref><ref type="bibr" target="#b35">25]</ref>; and (ii) leverages the rich graphical structure of linked knowledge entities, instead of only considering linear connections [1, <ref type="bibr" target="#b26">16,</ref><ref type="bibr" target="#b27">17,</ref><ref type="bibr" target="#b36">26,</ref><ref type="bibr" target="#b56">46]</ref>. To the best of our knowledge, this is the first attempt in the literature to explain fact checking by directly generating textual explanations clarifying the veracity of input facts in an abstractive manner. To summarize, the main contributions of our work are:</p><p>• FACE-KEG provides a fresh perspective to explainable fact checking by using structured and unstructured contextual knowledge, to generate human-understandable explanations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Fact Checking, Fact Verification and Explainability: Automated fact checking <ref type="bibr" target="#b66">[56]</ref> has been studied from several perspectives over the last few years, e.g. as natural language inference (NLI) <ref type="bibr" target="#b43">[33,</ref><ref type="bibr" target="#b45">35,</ref><ref type="bibr" target="#b75">65]</ref>, multimodally using images <ref type="bibr" target="#b62">[52]</ref>, via reasoning from relational tables <ref type="bibr" target="#b23">[13]</ref>, textual entailment <ref type="bibr" target="#b75">[65]</ref>, and as the closely related problem of fake news or misinformation detection <ref type="bibr" target="#b55">[45]</ref>. Unlike our proposed model FACE-KEG, most techniques only detect if an input claim is true or false <ref type="bibr" target="#b15">[5,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b43">33,</ref><ref type="bibr" target="#b46">36,</ref><ref type="bibr" target="#b55">45,</ref><ref type="bibr" target="#b66">56,</ref><ref type="bibr" target="#b81">71]</ref> without justifying their decision. Fact verification aims to verify if certain input 'evidence' can justify the veracity of a claim <ref type="bibr" target="#b32">[22,</ref><ref type="bibr" target="#b67">57,</ref><ref type="bibr" target="#b81">71]</ref>. Many techniques <ref type="bibr" target="#b20">[10,</ref><ref type="bibr" target="#b32">22,</ref><ref type="bibr" target="#b38">28,</ref><ref type="bibr" target="#b40">30,</ref><ref type="bibr" target="#b43">33,</ref><ref type="bibr" target="#b78">68,</ref><ref type="bibr" target="#b81">71]</ref> have been proposed to solve the popular FEVER shared task for fact verification <ref type="bibr" target="#b67">[57,</ref><ref type="bibr" target="#b68">58]</ref>. However unlike these methods, FACE-KEG does not take as input any explanatory or evidence text associated with the input claims. Logic rules as well as patterns derived from external knowledge bases like Wikipedia have been used to detect and explain fact veracity <ref type="bibr" target="#b26">[16,</ref><ref type="bibr" target="#b27">17,</ref><ref type="bibr" target="#b56">46]</ref>. Extractive explainable fact checking approaches <ref type="bibr" target="#b12">[2,</ref><ref type="bibr" target="#b57">47,</ref><ref type="bibr" target="#b76">66]</ref>. also take additional 'evidence' text as input (e.g. online user comments, fact checking web pages, explanatory comments from experts), and output a relevant subset of this text as an explanation. On the other hand, FACE-KEG actually generates concise, abstractive explanations without taking any of the actual explanatory text as input. This is in contrast to lengthier explanations with extraneous details (e.g. in FEVER task or fact checking website pages); or human-unreadable explanations (e.g. knowledge entity patterns, logic rule sequences). Since it is not always possible or feasible to obtain such additional, human-curated evidence information per claim, we do not compare FACE-KEG against the above extractive approaches. Thus, through this work we make a preliminary attempt to automate the generation of explanatory justifications provided by humans for fact checking.</p><p>Knowledge Enhanced Text Generation: Past work has used contextual cues from external knowledge bases in both structured and unstructured forms to enhance natural language generation. Contrary to FACE-KEG which employs a large scale knowledge base with generic entities, concepts and relationships; some studies <ref type="bibr" target="#b35">[25,</ref><ref type="bibr" target="#b80">70]</ref> utilize small, domain-specific knowledge bases with a limited number of relation types (e.g baseline KBLLH'19 in Section 4). This renders them difficult to adapt for open-domain, opentopic text generation. Prior work has modeled external knowledge entities both individually and as linear sequences of paths; failing to take their rich connectivity structure into account [1, <ref type="bibr" target="#b26">16,</ref><ref type="bibr" target="#b27">17,</ref><ref type="bibr" target="#b36">26,</ref><ref type="bibr" target="#b56">46]</ref> (e.g baseline FACE-KEG-linear enc. in Section 4). We find that modeling the external structured knowledge as a graph improves text generation. Graph convolutional networks <ref type="bibr" target="#b31">[21,</ref><ref type="bibr" target="#b41">31]</ref>, graph attention networks (GATs) <ref type="bibr" target="#b74">[64,</ref><ref type="bibr" target="#b80">70]</ref>, graph neural networks (GNNs) <ref type="bibr" target="#b31">[21]</ref>, gated GNNs <ref type="bibr" target="#b17">[7,</ref><ref type="bibr" target="#b59">49]</ref> and more recently, graph transformer networks <ref type="bibr" target="#b19">[9,</ref><ref type="bibr" target="#b35">25,</ref><ref type="bibr" target="#b79">69]</ref> have been proposed to directly encode graph inputs. In this work, we propose a graph transformer network comparable to the graph-to-sequence model proposed by Cai et al <ref type="bibr" target="#b19">[9]</ref> (baseline CL'20 in Section 4). However, our work differs in the following ways: (i) Cai et al assume the presence of largely connected, dense knowledge graphs with root vertices, unlike the sparse, disconnected knowledge graphs that FACE-KEG works with. (ii) Unlike FACE-KEG, they do not employ additional textual context that can potentially enhance our understanding of the input claim, while performing text generation. (iii) Cai et al require the edge label (relation) information from their graphs to be explicitly available and encoded in their model, unlike FACE-KEG, since such information (if any) is much sparser in our knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE FACE-KEG FRAMEWORK</head><p>Formally, given a textual claim or fact and an external knowledge base; our goal is to investigate the veracity of the fact (i.e. identify if it is true or not) and generate a human-comprehensible explanation clarifying the truthfulness of the fact. Figure <ref type="figure">1</ref> displays the overall architecture of FACE-KEG. We construct a knowledge graph associated with the input claim (Section 3.2). We then extract textual context relevant to the claim from the available knowledge base, as described subsequently. A bidirectional RNN (Section 3.1) and a graph transformer network (Section 3.2) are employed to encode the associated textual context and knowledge graph respectively. This is followed by jointly training a classifier that predicts if the input fact is true or false, and a decoder that learns to generate a natural language explanation about the veracity of the fact (Section 3.3). We employ both the structured knowledge graph and the unstructured textual context in FACE-KEG so that the two complement each other to perform explainable fact checking. This ensures that even if sufficient background information needed to assess the veracity of a fact is not present in the constructed knowledge graph, the extracted context will compensate for it, and vice versa.</p><p>We assume that we have available a large, structured knowledge base to help understand background information about the given facts. The implicit assumption here is that the chosen knowledge base contains sufficient supporting information to help predict and justify the veracity of the facts under consideration. We use the DBPedia ontology <ref type="bibr" target="#b13">[3]</ref> (and its associated Wikipedia text corpus) as the knowledge base in this work. For each claim or fact, we first perform entity linking using a constituency parser <ref type="bibr" target="#b28">[18]</ref> to extract all named entities E belonging to DBPedia from the fact text. We also extract relevant background context from the knowledge base regarding the claim, to enhance our understanding of the claim <ref type="bibr" target="#b32">[22]</ref>. For each input claim, we search for Wikipedia documents relevant the neighborhood N . We scale the dot products by their dimensionality 1</p><p>√ to prevent small gradient values due to large dot product magnitudes <ref type="bibr" target="#b69">[59]</ref>. denotes the concatenation of independent attention heads before a residual connection is applied, represents attention scores, W ∈ R is a weight matrix to be learned, and x and x denote initial representations of nodes in .</p><p>This multi-headed attention module in the graph transformer is then followed by a fully connected two-layer feed-forward network FFN( ), with a non-linear transformation (W 1 + 1 )W 2 + 2 , between layers. Both the above two blocks use a residual connection followed by a normalization layer at the output for node :</p><formula xml:id="formula_0">x = Norm(FFN(Norm(x ) + Norm(x ))<label>(3)</label></formula><p>The attention and FFN blocks are stacked times in the transformer to promote information propagation through the graph. The output x</p><p>, of the previous transformer layer is used as the input x +1 of the next layer +1 for node . The encoding of all nodes in the knowledge graph is thus represented by the representation matrix [x ] output by the graph transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Explainable Veracity Prediction</head><p>Once suitable encoded representations have been generated for the input fact, its associated knowledge graph and relevant context, we utilize these to (i) determine whether the input fact is true or not (veracity prediction); and (ii) generate a textual explanation clarifying the veracity or falsity of the fact. This is a step ahead of most prior efforts in the literature, that only identify the veracity of facts without explaining the reason behind their decisions. We jointly train FACE-KEG for both these tasks. The respective encodings T and X learnt by the text encoder (Section 3.1) and the knowledge graph transformer encoder (Section 3.2) are attended over independently by the attention function () defined in Equation 1:</p><formula xml:id="formula_1">, = (h , x L ) ; o = h + =1 ∈ , W x L<label>(4)</label></formula><p>Here h denotes the last hidden state of the knowledge graph transformer encoder, , represents attention weights and W ∈ R indicates a weight matrix to be learned. The representation vector o from the claim and context sequences is computed similarly by attending over the encoding T learnt by the text encoder. To perform veracity prediction, the two representations o and o are passed through attention layers <ref type="bibr" target="#b39">[29]</ref> to get a final aggregated representation o, which is then passed to a classifier. It consists of dense ReLU layers and a softmax layer to predict veracity (Figure <ref type="figure">1</ref>). We next outline how to simultaneously generate a natural language explanation justifying the veracity of the input fact. For this, we train a sequential LSTM decoder with attention <ref type="bibr" target="#b39">[29]</ref>, and a copy mechanism <ref type="bibr" target="#b53">[43]</ref> for copying inputs from the knowledge graph and the text sequence. It shares the same graph transformer encoder and text encoder as the veracity prediction classifier. The representation of the global node is used to initialize the hidden states of the decoder at each time step, analogous to the final encoder hidden state in a traditional sequence-to-sequence encoder-decoder framework. At each decoding time step , we use the decoder's hidden state h and multi-headed attention to compute context vectors o and o for the graph and text sequence respectively. This computation and the learning of the attention scores and weight matrices is done in the same way as while performing veracity prediction (see Equation 4). The only difference is that h now represents the decoder's hidden state. We next construct the final context vector o of the decoder at time step by concatenating the two vectors o and o . Both the hidden state h and decoder context o are passed to the next decoder time step. Decoding with a copying mechanism addresses the data sparsity issue in word token prediction, and facilitates the generation of specific terms or named entities relevant to the input fact. ( ) gives the probability distribution of generating the next token , by generating a word from the vocabulary or copying a word from the model input with probability <ref type="bibr" target="#b53">[43]</ref>.</p><formula xml:id="formula_2">= (W [h ⊕ o ] + )<label>(5)</label></formula><formula xml:id="formula_3">( ) = * + (1 − ) * (6) , = ([h ⊕ o ], y )<label>(7)</label></formula><p>where denotes the sigmoid function, is the probability distribution over the named entities and input tokens for y ∈ X ⊕ T, and () is the attention function from Equation 1. The distribution is computed by scaling the concatenation [h ⊕ o ] to the vocabulary size and taking a softmax over it. The output sequence of words thus generated by the decoder over each time step forms the natural language explanation for the input claim.</p><p>Implementation Details: Both the veracity detection classifier and the explanation generation decoder share the same knowledge graph transformer encoder and text encoder. The decoder loss function L minimizes the negative joint log likelihood of the target text vocabulary and the copied entity indices. The classifier loss function L minimizes the binary cross entropy across both classes. The final objective for training both components is given by L + L , where is a hyperparameter controlling the trade-off between the two loss functions. The values of sampling parameters , 1 and 2 while constructing the knowledge graph are set to 25%, 25% and 10% respectively. The number of hidden states of the LSTMs and the dimensionality of the attention layers are set to 500. In the graph transformer encoder, the FFN block has a PReLU <ref type="bibr" target="#b33">[23]</ref> activation function with an intermediate size of 2000 units. The attention and FFN blocks are stacked = 6 times. We use dropout <ref type="bibr" target="#b60">[50]</ref> with a probability of 0.3 in the self attention layers, and 4 attention heads. We use beam search <ref type="bibr" target="#b64">[54]</ref> in the decoder with a beam size of 4. While training to optimize the loss function, we use SGD with momentum <ref type="bibr" target="#b50">[40]</ref> and warm restarts, with an initial learning rate of 0.25. We train FACE-KEG for 20 epochs with early stopping based on the validation loss. All parameters have been tuned based on performance on a validation set. To offset the randomness injected in our model due to certain hyperparameters, all results have been reported after averaging over 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION 4.1 Experimental Setup</head><p>Datasets: We evaluate the performance of FACE-KEG on three large, challenging and human-annotated fact checking datasets described in Table 1: (i) FEVER shared task data <ref type="bibr" target="#b67">[57,</ref><ref type="bibr" target="#b68">58]</ref> domains <ref type="bibr" target="#b14">[4]</ref>; and (iii) FakeCOVID dataset containing cross domain facts on the COVID-19 pandemic <ref type="bibr" target="#b54">[44]</ref>. The total number of DBPedia entities with respect to the facts present in these three datasets is 966K, with 735K links among these entities. Claims for which sufficient evidence is not available to justify their veracity are eliminated from consideration. For each dataset, we split the claims such that 70% are used for model training, 15% for validation and 15% for testing. The FEVER dataset contains human-annotated ground truth explanations that support the veracity labels for each claim. However, the MultiFC and FakeCOVID datasets do not provide direct explanations for the claims. They provide links to fact checking web pages whose content can justify the claims' veracity labels. Therefore, to acquire ground truth explanations for these two datasets, we filter out the metadata, extract the web page content, and utilize a tree-based convolutional neural network with heuristic matching <ref type="bibr" target="#b14">[4]</ref>. It chooses the top four ranked evidence sentences from the associated web pages as the true explanations for the claims. We use a random sample of claim-explanation pairs to manually verify that sensible ground truth explanations are obtained for these two datasets. We assume that the true explanation accompanying each claim is authentic and reliable. We also eliminate all sentences in our extracted input textual context (from Section 3) that overlap with the ground truth explanation (if any). Finally, for explainable fact checking, FACE-KEG requires sufficient context relevant to the input claim, as well as good coverage of named entities (associated with the input claim) in the external knowledge base being used. DBPedia and Wikipedia work well for the FEVER and MultiFC data.</p><p>For the FakeCOVID data, we employ an additional news corpus 2 containing 1.2M news articles, as a source of useful context for many claims on COVID-19 which are underrepresented in Wikipedia.</p><p>Baselines: As per our knowledge, our work is the first to jointly detect veracity of input claims (Task I henceforth), and generate natural language explanations for them (Task II). We thus evaluate both tasks separately. For Task I, we compare FACE-KEG with:</p><p>(1) The FEVER shared task baseline <ref type="bibr" target="#b67">[57,</ref><ref type="bibr" target="#b68">58]</ref>;</p><p>(2) FEVER leader board methods: Athene <ref type="bibr" target="#b32">[22]</ref>, UCL MR <ref type="bibr" target="#b43">[33]</ref>, UNC <ref type="bibr" target="#b78">[68]</ref>, Papelo <ref type="bibr" target="#b40">[30]</ref>, DOMLIN <ref type="bibr" target="#b61">[51]</ref>, GEAR <ref type="bibr" target="#b81">[71]</ref>, KGAT <ref type="bibr" target="#b38">[28]</ref>. For Task II, we compare FACE-KEG's explanation learning module with state-of-art text generation methods, using the same knowledge graphs, text corpora and context (in Section 3) for all methods.</p><p>(1) BHC'18 <ref type="bibr" target="#b17">[7]</ref>: encodes the input knowledge graph via a GNN.</p><p>(2) KBLLH'19 <ref type="bibr" target="#b35">[25]</ref>: their graph transformer encoder and knowledge graph construction process are different from FACE-KEG. Unlike FACE-KEG, their graphs need explicit relation labels between all entity pairs, which are then added as nodes into the graph. This 2 https://blog.aylien.com/free-coronavirus-news-dataset/ increases the graph size fast for even a moderate number of entityrelation types, hurting performance. Also, their graphs only contain entity-relation edges, and no direct connections between entities.</p><p>(3) CL'20 <ref type="bibr" target="#b19">[9]</ref> and (4) YWW'20 <ref type="bibr" target="#b77">[67]</ref>: both use graph transformer encoders with a different architecture than FACE-KEG, and transformer decoders unlike the RNN decoder of FACE-KEG. They formulate their models assuming much denser, rooted, connected graphs with dense edge relation label information; unlike the sparse, disconnected knowledge graphs in our fact checking problem scenario.</p><p>(5) FACE-KEG-only our KG enc. and ( <ref type="formula">6</ref>) FACE-KEG-only our text enc.: variants of FACE-KEG using only our proposed graph transformer encoder and only our text encoder, respectively;</p><p>(7) FACE-KEG-no context: variant of FACE-KEG where the text encoder only encodes the claim, and no supporting context;</p><p>(8) FACE-KEG-single att.: varying FACE-KEG by passing our proposed graph and text encoders through a single attention block, in lieu of two sets of attention layers (Figure <ref type="figure">1</ref>) before prediction;</p><p>(9) FACE-KEG-linear enc.: variant of FACE-KEG that ignores graph structure. It uses a bi-LSTM to sample and encode linear entity paths to represent the graph [1], instead of our graph encoder;</p><p>(10) FACE-KEG-GAT enc.: variant of FACE-KEG using a GAT <ref type="bibr" target="#b74">[64]</ref> with PReLU activations stacked between self-attention layers to encode the input knowledge graph, in lieu of our graph encoder.</p><p>(11) FACE-KEG-GTN enc.: replaces our proposed graph encoder with a different graph transformer network (GTN) <ref type="bibr" target="#b79">[69]</ref>, that learns multiple meta-path graphs from the input graph, and performs convolutions on them to get an aggregated graph encoding.</p><p>Performance Metrics: We evaluate Task I using classification accuracy and F1-score. For Task II, we use BLEU <ref type="bibr" target="#b44">[34]</ref>, METEOR <ref type="bibr" target="#b16">[6]</ref>, ROUGE <ref type="bibr" target="#b37">[27]</ref>, Entity Overlap (compares the overlap of entity phrases between the generated and true explanations), and a user study (Table <ref type="table" target="#tab_2">5</ref>). We test the difference between the best method for each experiment and all other methods via an approximate randomization test <ref type="bibr" target="#b52">[42]</ref> with the Bonferroni correction for multiple comparisons, at a confidence level of 0.05 ( &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>Task I Evaluation: Table <ref type="table">2</ref> evaluates predicting the veracity of the test input claims for the three datasets. We observe an improvement for FACE-KEG of at least 2% in terms of F1-score and 3% in label accuracy, compared to different systems from the FEVER shared task leader board (first eight rows of Table <ref type="table">2</ref>). The proposed FACE-KEG (last row) and the recently proposed GEAR <ref type="bibr" target="#b81">[71]</ref> and KGAT <ref type="bibr" target="#b38">[28]</ref> are competitive on the FEVER dataset, with FACE-KEG significantly outperforming them on the other two datasets. This is a remarkable result since unlike the baselines, FACE-KEG does not take any evidence or explanation text as input and more importantly, independently explains its results in Task II below. The ninth to sixteenth rows of Table <ref type="table">2</ref> show that FACE-KEG as proposed outperforms its variant baselines described earlier by at least 4%, in terms of label accuracy and F1 score. The decrease in both these metrics on using only the graph encoder or only the text encoder (ninth and tenth rows) shows that both the structured and unstructured knowledge inputs complement each other, and contribute to veracity prediction.</p><p>Task II Evaluation: Tables 3(a), 3(b) and 3(c) evaluate the quality of explanations generated by various approaches to justify the Table <ref type="table">3</ref>: Assessing quality of generated explanations (Task II). Best results are shown in bold. † shows no significant difference from the best result. 'BL', 'MT', 'RG' and 'EO' stand for the BLEU, METEOR, ROUGE and Entity Overlap metrics respectively.</p><p>(a) FEVER data.</p><p>path lengths are likely to have more complex interactions between relatively distant nodes, and would require an effective modeling of both the local and global graph topology. We observe FACE-KEG to be better than both baselines ("KBLLH'19", "FACE-KEG-GAT enc. ") across graphs with varying mean path lengths, for explainable fact checking. All these observations highlight that FACE-KEG is well suited to modeling long distance dependencies and more global interactions, that arise as a result of including indirect entity connections (two hop neighbors) within the knowledge graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Context:</head><p>We observe a clear drop in the values of all evaluation metrics and all datasets for the baseline "FACE-KEGno context", compared to the proposed FACE-KEG that encodes relevant background context (Section 3.1). This shows that utilizing the input claim text as well as additional context pertinent to it enhances our understanding of the claim, and the coverage of the knowledge entities relevant to it. This in turn leads to the creation of a globally representative knowledge graph with higher connectivity, improving the efficacy of explainable fact checking by FACE-KEG. Also, in Tables <ref type="table">2 and 3</ref>(c) for the FakeCOVID data, the performance gap between the baselines "FACE-KEG-only our text enc." and "FACE-KEG-only our KG enc." is not as much as the other two datasets. On drilling down, we find that this is because sufficient background information is not present in the constructed knowledge graph for certain claims, which is compensated by the extracted context from the input text corpora. This further shows the importance of the unstructured textual context from the input text corpora, and how it complements the structured knowledge graph for FACE-KEG.</p><p>Case Study: Table <ref type="table">4</ref> presents samples of input claims from the three datasets, their predicted veracity label, and the explanations generated to justify their veracity by FACE-KEG and the top two baselines that can jointly perform both Tasks I and II ("FACE-KEG-GAT enc." and "KBLLH'19"). The value N/A for the predicted veracity label for "KBLLH'19" denotes that this baseline does not predict veracity. We observe that for the first, second and last rows, "FACE-KEG-GAT enc. " produces an incorrect veracity label and an inaccurate explanation. For the third row, "FACE-KEG-GAT enc." learns a correct veracity label but an incorrect explanation. All approaches perform incorrectly in the second row. The explanations generated by "KBLLH'19" are fluent but noisier than FACE-KEG, with multiple extraneous entities and unnecessary topics (e.g. first row of Table <ref type="table">4</ref>). Explanations learned by both "KBLLH'19" and FACE-KEG are generally more coherent and sensible than those of "FACE-KEG-GAT enc. ", with FACE-KEG producing more relevant and informative explanations. FACE-KEG's explanations have less word repetition, and the named entities in them are more relevant to the input claim, compared to the baselines. We also notice that it is more difficult to generate good quality explanations for the MultiFC and FakeCOVID facts, compared to FEVER (also seen in Tables 3(a), 3(b) and 3(c)). This is possibly due to the presence of relatively lesser relevant supporting context and knowledge base entities for the claims in these two datasets. The true explanations for claims in these two datasets are also longer compared to FEVER.</p><p>We summarize the common errors of our model below. Insufficient background knowledge (either missing in the external knowledge base or unable to be retrieved by FACE-KEG) for some claims leads to erroneous results. For example, in the second row of Table <ref type="table">4</ref>, FACE-KEG is unable to link a crucial named entity 'psychological trauma' to any of the other relevant knowledge base entities identified (e.g. mental-and-behavioral-disorders, psychiatric-diagnosis), and thus cannot correctly determine the veracity of the claim and explain it. Another common error made by FACE-KEG is selecting related but incorrect knowledge entities to be copied into the generated explanation. For instance, the claim "Noah Cyrus is the youngest daughter of Billy Ray Cyrus, born on November 3rd, 2004" is false, and the true explanation for its falsity is "Noah Lindsey Cyrus born January 8, 2000 is an American ...". But our copy mechanism selects an incorrect entity Miley Cyrus (possibly due to its higher degree), linked to the correct entity Noah Cyrus. This generates the incorrect explanation "Miley Cyrus born 2000 is an American ... ".</p><p>Human Evaluation: Since the automated metrics largely compare the true and predicted explanations at the word level, we also perform a manual evaluation of the overall explanation quality. We obtain the topics associated with each fact based on the highlevel DBPedia categories linked with the facts. We then collect human judgments from crowd workers on Amazon Mechanical Turk, for 15K claims randomly sampled from diverse topics. We follow recommended practices <ref type="bibr" target="#b18">[8]</ref> to ensure good quality crowd sourced evaluation. For each input fact, participants were presented with the veracity label (true or false) for the fact, and explanations generated by the top three performing approaches: (i) FACE-KEG-GAT enc., (ii) KBLLH'19, and (iii) FACE-KEG (as we propose). They were asked to rely only on the provided information to rank the Table <ref type="table">4</ref>: Outputs of three methods (predicted VERACITY LABEL: generated explanation) for input facts from the three datasets.</p><p>Fact 1 (FEVER data) Henry III of France was succeeded by Henry IV, who became the first French king of the House of Bourbon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of entities in KG</head><p>Henry-III-of-France, Henry-IV-of-France, House-of-Bourbon, Kings-of-France, 16th-century-peers-of-France, Assassinated-French-people ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACE-KEG-GAT enc.</head><p>[64] FALSE: jacques clément succeeded the throne catholic and become king of house of bourbon. KBLLH'19 <ref type="bibr" target="#b35">[25]</ref> N/A: in 1692, jacques clément, a catholic tribe historian, assassinated henry iii and henry iv joined the throne of france a er suffering. he was the first monarch of france known as good king henry who was a counts of foix and with burials at the basilica of st denis.</p><p>FACE-KEG (as we propose) TRUE: a catholic murdered henry iii, who was succeeded by henry iv and become the first french king of bourbon.</p><p>Ground truth label: expl. TRUE: In 1589, Jacques Clément, a catholic fanatic, murdered Henry III, who was succeeded by the King of Navarre who, as Henry IV, would assume the throne of France a er converting to catholicism, and become the first French king of the House of bourbon.</p><p>Fact 2 (FEVER data) Dissociative identity disorder, or DID, may result from disruptions in memory caused by psychological trauma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of entities in KG</head><p>Dissociative-disorders, Identity-disorders, Memory-disorders, Psychiatric-diagnosis, Memory, Mental-and-behavioral-disorders, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACE-KEG-GAT enc.</head><p>[64] FALSE: humans by psychological males humans disorder, including dyssomnias, have been described to treat in memory caused by discrimination or other forms of phonation. KBLLH'19 <ref type="bibr" target="#b35">[25]</ref> N/A:dissociative identity disorder did suffer from mental and behavioral disorders and are characterized by abnormal psychology, forgetfulness. FACE-KEG (as we propose) FALSE: dissociative identity disorder did is an identity disorder caused by forgetfulness or childhood abuse with alternate personalities, childhood stress abuse or memory psychiatric stress.</p><p>Ground truth label: expl. TRUE: Psychological disorders, including DID, have been a ributed to disruptions in memory caused by trauma or other forms of stress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact 3 (MultiFC data)</head><p>On June 8, Labor released its policy "10 Year Plan for Australia's Economy", the most comprehensive plan in living memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of entities in KG</head><p>Australian-Labor-Party, Budget, Economic-forecasting, Chris-Bowen, Economy-of-Australia, Economic-History-of-Australia, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACE-KEG-GAT enc.</head><p>[64] FALSE: The 10 year economic plan by Labor Chris Bowen is the most comprehensive plan by an opposition leader. KBLLH'19 <ref type="bibr" target="#b35">[25]</ref> N/A: Economic history of Australia shows that 15 pages budget by Chris Bowen continues decreasing levels of detail in pre-election policies. FACE-KEG (as we propose) FALSE: The 15 pages economic content plan by Australian Labor Party provides lesser details by historical standards. Ground truth label: expl. FALSE: Labor's plan is not in the same league as Mr Hewson's notoriously detailed Fightback! plan from the 1993 election, which spanned around 650 pages. Its "10 Year Plan on the Economy" is not the most comprehensive economic plan by historical standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact 4 (FakeCOVID data)</head><p>The largest hole in the ozone layer over the Arctic region has healed due to the COVID-19 lockdown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset of entities in KG</head><p>Ozone-layer, Ozone-hole, Ozone-depletion-and-climate-change, Arctic, COVID-19-pandemic-lockdowns, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACE-KEG-GAT enc. [64] TRUE:</head><p>The ozone layer shields the earth from the sun's ultraviolet radiation, healing positive effect of lockdown. KBLLH'19 <ref type="bibr" target="#b35">[25]</ref> N/A: The Arctic ozone depletion has nothing to do with the global lockdown and climate change in the Arctic and global warming. FACE-KEG (as we propose) FALSE: Scientists clarified that the healing Arctic ozone hole was closed by a polar vortex and not due to COVID-19 pandemic lockdowns. Ground truth label: expl. FALSE: Scientists have confirmed that the largest hole in the ozone layer over the Arctic region has closed in. The plugging of the ozone layer hole has nothing to do with the Covid-19 lockdown. According to scientists, it has been driven by an unusually strong and long-lived polar vortex, the high-altitude currents that bring cold air to the polar regions. three given explanations from best to worst in terms of grammar and fluency in English, relevance of the explanation to the fact, the explanation's adequacy in justifying the fact's veracity, and length (verbosity) of the explanation. Each fact was assigned to three different participants, leading to an inter-annotator agreement (Cohen's <ref type="bibr" target="#b24">[14]</ref>) of 71%. Table <ref type="table" target="#tab_2">5</ref> shows that FACE-KEG generates the best explanation for 48% of the facts, the best or second-best explanation for more than 83% facts, and the worst explanation for only 16% of the facts for the FEVER dataset. The second best performing baseline, KBLLH'19, generates the best explanation for 30% of the facts. Manual evaluation for the MultiFC [and FakeCOVID] datasets shows that FACE-KEG again outperforms the two baselines by generating the best explanation for 42% [and 46%] of the facts, the best or second-best explanation for more than 75% [and 79%] facts, and the worst explanation for about 23% [and 20%] of the facts. Among the facts for which FACE-KEG obtained the lowest ranked explanation, we observe similar kinds of errors as those reported previously. These trends echo the automatic evaluation results, and demonstrate that the explanations learnt by FACE-KEG are more coherent, concise and grammatical compared to competitive baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUDING REMARKS</head><p>We propose FACE-KEG, a novel deep learning model that simultaneously predicts fact veracity, and generates a coherent explanation supporting its decision. FACE-KEG outperforms state-of-the-art baselines on three large-scale datasets. Since FACE-KEG's efficacy is dependent on the accuracy of our constructed knowledge graphs and retrieved context for the facts, in future, we will work towards improving the quality of these two elements. We also seek to decouple our explanation generation mechanism from veracity prediction, enabling its use to explain any veracity prediction model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• FACE-KEG advances over explanations from past work based on (i) extractive, lengthy evidence text with extraneous details (e.g. FEVER task, fact checking websites); (ii) knowledge base entity pattern sequences; or (iii) logic rule sequences.• FACE-KEG significantly outperforms state-of-the-art baselines, as per both automated and human evaluation measures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>; (ii) MultiFC dataset containing real-world fact checking claims from multiple We show the number of claim instances; average length of claims, contexts and explanation texts; and average knowledge graph size per claim (#vertices, #edges).</figDesc><table><row><cell>Session 13: Knowledge</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation of learnt explanations (FEVER).</figDesc><table><row><cell>Approach</cell><cell cols="2">Rank1(best) Rank2 Rank3(worst)</cell></row><row><cell>FACE-KEG-GAT enc. [64]</cell><cell>21.2%</cell><cell>35.1% 43.7%</cell></row><row><cell>KBLLH'19 [25]</cell><cell>30.4%</cell><cell>29.8% 39.8%</cell></row><row><cell cols="2">FACE-KEG (as we propose) 48.3 %</cell><cell>35.3% 16.4%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Science Foundation grant EAR-1520870 and an Ohio State University Presidential Fellowship for the first author. All content represents the opinion of the authors, and is not necessarily endorsed by their sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">/ 63.6 † FACE-KEG-only our text enc</title>
		<idno>27.6 / 32.3 / 30.1 / 53.1 FACE-KEG-no context 30.1 / 38.0 / 35.9 / 56.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Face-Keg-Gat</forename><surname>Enc</surname></persName>
		</author>
		<idno>32.6 / 39.0 / 35.8 / 53.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Face-Keg-Gtn</forename><surname>Enc</surname></persName>
		</author>
		<idno>32.2 / 38.8 / 35.3 / 54.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">(b) MultiFC data</title>
		<author>
			<persName><surname>Face-Keg</surname></persName>
		</author>
		<idno>37.0 / 44.8 / 41.2 / 63.7</idno>
		<ptr target="ApproachBL/MT/RG/EOBHC&apos;18[7]19.6/24.5/22.8/39.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Face-Keg-Gat</forename><surname>Enc</surname></persName>
		</author>
		<idno>27.3 / 35.2 / 32.8 / 54.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Face-Keg-Gtn</forename><surname>Enc</surname></persName>
		</author>
		<idno>26.3 / 34.7 / 31.01 / 56.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">(c) FakeCOVID data</title>
		<author>
			<persName><surname>Face-Keg</surname></persName>
		</author>
		<idno>BL / MT / RG / EO BHC&apos;18 [7] 16.6 / 22.7 / 20.01 / 37.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">† FACE-KEG-only our text enc</title>
		<idno>FACE-KEG-only our KG enc. 25.6 / 30.1 / 27.8 / 56.3</idno>
		<imprint/>
	</monogr>
	<note>22.9 / 27.7 / 24.9 / 47.2 FACE-KEG-no context 24.1 / 29.5 / 25.3 / 51</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Face-Keg-Gat</forename><surname>Enc</surname></persName>
		</author>
		<idno>24.5 / 29.2 / 26.8 / 50.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Face-Keg-Gtn</forename><surname>Enc</surname></persName>
		</author>
		<idno>23.6 / 28.6 / 26.2 / 51.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Face-Keg</surname></persName>
		</author>
		<idno>28.8 / 33.5 / 30.2 / 56.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<idno>1808.01400</idno>
		<title level="m">Generating sequences from structured representations of code</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05773</idno>
		<title level="m">Generating Fact Checking Explanations</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims</title>
		<author>
			<persName><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03242</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Integrating stance detection and fact checking in a unified corpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno>1804.08012</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno>1806.09835</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Amazon&apos;s Mechanical Turk: A new source of inexpensive, yet high-quality data?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buhrmester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gosling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Transformer for Graph-to-Sequence Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust Document Retrieval and Individual Evidence Modeling for Fact Extraction and Verification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alhindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP FEVER Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer questions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno>1704.00051</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<idno>1609.06038</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv preprint 1909.02164</idno>
		<title level="m">TabFact: A Large-scale Dataset for Table-based Fact Verification</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and psychological measurement</title>
		<imprint>
			<date type="published" when="1960">1960. 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fact checking via evidence patterns</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fionda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pirrò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exfakt: A framework for explaining facts over knowledge graphs and text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Urbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop for NLP Open Source Software</title>
				<imprint>
			<publisher>NLP-OSS</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Social context-aware trust prediction: methods for identifying fake news</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ghafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yakhchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beheshti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orgun</surname></persName>
		</author>
		<editor>WISE</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In IEEE ICASSP</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>arXiv preprint 1809.01479</idno>
		<title level="m">UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>1704.08381</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Manual and automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Automatic Summarization</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fine-grained Fact Verification with Kernel Graph Attention Network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno>1508.04025</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Malon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02534</idno>
		<title level="m">Team papelo: Transformer networks at fever</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<idno>1810.09995</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>1606.01933</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automatic detection of fake news</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno>1708.07104</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno>arXiv preprint 1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Where the truth lies: Explaining the credibility of emerging claims on the web and social media</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In World Wide Web Companion</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A stack-propagation framework with token-level intent detection for spoken language understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02188</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On some pitfalls in automatic evaluation and significance testing for MT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno>1704.04368</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">FakeCovid-A Multilingual Cross-domain Fact Check News Dataset for COVID-19</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nandini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11343</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Combating fake news: A survey on identification &amp; mitigation techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>ACM TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Finding streams in knowledge graphs to support fact checking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciampaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">defend: Explainable fake news detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Understanding user profiles on social media for fake news detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MIPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">N-ary relation extraction using graph state lstm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<idno>1808.09101</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Team DOMLIN: Exploiting evidence enhancement for the fever shared task</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stammbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP FEVER Workshop</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Breaking cycles in noisy hierarchies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ajwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Web Science</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Depth-first search and linear graph algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on computing</title>
		<imprint>
			<date type="published" when="1972">1972. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Automated fact checking: Task formulations, methods and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<idno>1806.07687</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The fact extraction and verification shared task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<idno>1811.10971</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The FEVER 2.0 shared task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP FEVER Workshop</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Open Intent Extraction from Natural Language Interactions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maneriker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">BOLT-K: Bootstrapping Ontology Learning via Transfer of Knowledge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maneriker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Enriching Taxonomies With Functional Domain Knowledge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ajwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Predicting trust relations within a social network: A case study on emergency response</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Shalin</surname></persName>
		</author>
		<editor>ACM WebSci</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">XFake: explainable fake news detector with visualizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Pentyala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Transformer for Graph-to-Sequence Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Ucl machine reading group: Four factor framework for fact finding (hexaf)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In FEVER workshop</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Commonsense Knowledge Aware Conversation Generation with Graph Attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">GEAR: Graphbased evidence aggregating and reasoning for fact verification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
