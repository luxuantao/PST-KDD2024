<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Object-to-Class Kernels for Scene Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>zhanglei@hrbeu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Communication Engi-neering</orgName>
								<orgName type="institution">Harbin Engineering University</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
							<email>xzhen7@uwo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Communication Engi-neering</orgName>
								<orgName type="institution">Harbin Engineering University</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Communication Engi-neering</orgName>
								<orgName type="institution">Harbin Engineering University</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Medical Biophysics</orgName>
								<orgName type="institution">University of Western Ontario</orgName>
								<address>
									<postCode>N6A 3K7</postCode>
									<settlement>London</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic and Electrical Engi-neering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<postCode>S1 3JD</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Medical Biophysics</orgName>
								<orgName type="institution">University of Western Ontario</orgName>
								<address>
									<postCode>N6A 3K7</postCode>
									<settlement>London</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Object-to-Class Kernels for Scene Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">530C6259F42F26043BD1541DA139E9AA</idno>
					<idno type="DOI">10.1109/TIP.2014.2328894</idno>
					<note type="submission">received December 17, 2013; revised April 4, 2014 and May 27, 2014; accepted May 30, 2014. Date of publication June 4, 2014; date of current version June 23, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object bank</term>
					<term>scene classification</term>
					<term>object-to-class distances</term>
					<term>object filters</term>
					<term>kernels</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-level image representations have drawn increasing attention in visual recognition, e.g., scene classification, since the invention of the object bank. The object bank represents an image as a response map of a large number of pretrained object detectors and has achieved superior performance for visual recognition. In this paper, based on the object bank representation, we propose the object-to-class (O2C) distances to model scene images. In particular, four variants of O2C distances are presented, and with the O2C distances, we can represent the images using the object bank by lower-dimensional but more discriminative spaces, called distance spaces, which are spanned by the O2C distances. Due to the explicit computation of O2C distances based on the object bank, the obtained representations can possess more semantic meanings. To combine the discriminant ability of the O2C distances to all scene classes, we further propose to kernalize the distance representation for the final classification. We have conducted extensive experiments on four benchmark data sets, UIUC-Sports, Scene-15, MIT Indoor, and Caltech-101, which demonstrate that the proposed approaches can significantly improve the original object bank approach and achieve the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a rich amount of semantically meaningful information and even can describe the content of the image by sentences or tell a story behind the picture. It was proved in <ref type="bibr" target="#b0">[1]</ref> that, even for video sequences in real-world circumstances, only brief glimpses may be sufficient to confirm or deny it as the expected scene.</p><p>Image understanding, scene classification and object detection are closely related topics. Fig. <ref type="figure" target="#fig_0">1</ref> gives a rough sketch of the relationship among them. Image understanding is the highest level compared to retrieval or classification. The content understanding may go beyond the image itself. Similarly to speech and text understanding, image understanding is far from just recognizing the objects and their locations in the image. The aim of image understanding lies in digging the information hidden in the image by analyzing the complex relations among different parts of the image. Yao et al. <ref type="bibr" target="#b1">[2]</ref> provide a holistic scene understanding approach that simultaneously reasons about regions, locations, classes and spatial extent of objects, as well as scene types. Object detection, however, plays a basic role for both image classification and understanding. No matter for rigid or non-rigid objects, the detection aims at high accuracy under different conditions including luminance changes and view angle variations. Being the middle level, on the one hand, scene classification can well utilize the achievements from object detection and dig the semantic knowledge as scene class labels by analyzing the relations between objects. On the other hand, it can further provide additional knowledge which can be expanded by similar scenes for image understanding.</p><p>In this paper, we focus on scene classification based on object detection. Scene classification has long been regarded as a challenging task due to the difficulties caused by huge intra-class variations and inter-class ambiguities.</p><p>Take the scene of a birthday party for instance. The most representative object is the birthday cake, together with the balloons and cheering people. In addition to the different shapes of birthday cakes or other objects, a long-distance shot is quite different from a close-distance shot. The same disparity can be observed in Fig. <ref type="figure" target="#fig_1">2</ref> (a) for a polo scene. The left sub-figure is from the close-distance shot while the right one is from the long-distance shot, which looks significantly different to the other one.</p><p>In addition, similar objects in similar locations could also be shared by scenes from different categories, which is illustrated in Fig. <ref type="figure" target="#fig_6">2 (b</ref>). The television in the living room is extremely similar to the computer in the office while the cabinet in the kitchen looks very alike to the wardrobe in the bed room.</p><p>Over the past years, many methods, from feature extraction to refined classifier models, have been proposed for scene classification. With respect to feature extraction, global features such as texture statistics and color histograms are extracted from the whole image. These features are regarded as low-level representations, since they are based on pixels, which cannot extract the semantic differences among different scenes. Typically, low-level features are adopted for simple scene classification such as indoor/outdoor <ref type="bibr" target="#b2">[3]</ref> scenes. For example, in <ref type="bibr" target="#b3">[4]</ref>, the simple line drawings are proposed to grasp the key information of scenes. As for local features, most of the methods are built on the bag-of-words (BoW) model and the sparse coding (SC) algorithm. Since these representations can characterize the statistical relations among local descriptors, they are considered as the mid-level features <ref type="bibr" target="#b4">[5]</ref>.</p><p>Although the mid-level features can, to some extent, extract distributional information of local feature descriptors from images, there is still a gap between the mid-level representation and the semantic meaning. In fact, each scene is composed of several objects organized in an unpredictable layout, and objects would also play different roles in different scenes. For example, a bed is more significant than a window for a bedroom scene, while a table is crucial to an office scene. From this point of view, the Object Bank <ref type="bibr" target="#b5">[6]</ref> approach which decomposes each scene into a high-dimensional space in which each dimension indicates an object response is reasonable. It can fill the semantic gap by imitating the understanding process of human beings at the first glance of an image. Actually, the Object Bank approach represents an image by the responses of pre-trained object filters, which could be regarded as a high-level representation of the image. Due to the explicit detection of objects in images, the Object Bank provides an effective avenue to understand scene images.</p><p>However, in the original Object Bank approach, the object filters have no prior knowledge of the distributions of objects in images and the responses of object filters are treated equally in the final image representation. This tends to be less discriminative for classification leading to suboptimal representations. Inspired by the image-to-class (I2C) distance in NBNN, we, in this paper, propose the Object-to-Class (O2C) distances based on the Object Bank for high-level scene representation. In practice, we provide four different versions of the O2C distances. Furthermore, we propose to kernelize the representations based on the O2C distances, which shows impressive effectiveness. Our method enjoys advantages of the Object Bank representation while significantly improves its performance. The benefits of using the O2C distances are two-fold: 1) The O2C distances, different from I2C, are employed to build the subspace representation based on each object, which is more discriminative; 2) O2C provides an intuitive and effective venue to link the distance space to the classification, which improves the performance of classification.</p><p>The contributions of this work can be summarized in the following aspects:</p><p>1) We propose the Object-to-Class (O2C) distances for scene classification. Specially, four variants of the O2C distances are provided;</p><p>2) Based on O2C distances, a kernelization framework is built to map the Object Bank representation into a new distance space leading to more discriminative ability;</p><p>3) Our method combines the benefits of the high-level representation of the Object Bank with the kernel methods.</p><p>The remainder of this paper is organized as follows. We briefly review the related work in Section II. The Object Bank representation is summarized in Section III. The objectto-class (O2C) distances are introduced in Section IV. The basic O2C distance and its variants are presented in Section V. The kernelization of the O2C distance representation is given in Section VI. Finally, we show experiments and results in Section VII and conclude our work in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Representations based on local features have dominated the field of image/scene classification. Interest points are firstly extracted from the images by detectors <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and then described by local descriptors such as SIFT <ref type="bibr" target="#b8">[9]</ref> and DAISY <ref type="bibr" target="#b9">[10]</ref>. The key issue is then to measure the similarity between two images on sets of local descriptors, which, however, is non-trivial due to the different cardinality and orderlessness of local features from different images. Over the past decades, approaches based on the bag-of-words (BoW) model and the sparse coding algorithm have achieved impressive results in many challenging tasks. The representations from both BoW and SC can be regarded as mid-level features <ref type="bibr" target="#b4">[5]</ref>. The main deficits of the BoW model and SC are the quantization errors and the loss of structural information of local features. Many efforts have been made to alleviate the quantization errors caused by the hard assignment method and to compensate for the loss of the location information of local descriptors.</p><p>To deal with the quantization errors, the soft assignment was proposed to encode local descriptors by multiple visual words <ref type="bibr" target="#b10">[11]</ref>. They apply techniques from kernel density estimation to allow a degree of ambiguity in assigning local feature descriptors to codewords. By using kernel density estimation, the uncertainty between codewords and image features is lifted beyond the vocabulary and becomes part of the codebook model. The sparse coding algorithm circumvents this problem by relaxing the restrictive cardinality constraint in vector quantization (VQ) <ref type="bibr" target="#b11">[12]</ref>. Furthermore, the locality constraint has been further imposed on the encoding <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, which has proven to be crucial to classification.</p><p>The pyramid match kernel (PMK) <ref type="bibr" target="#b14">[15]</ref> approximates the optimal partial matching by computing a weighted intersection over multi-resolution histograms, which implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Bo et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> proved that the BoW representation can be formulated in terms of match kernels over image patches. This novel view combines BoW and kernelization to some extent, and is helpful for designing a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes into compact patch-level features.</p><p>However, all the above methods suffer from the inability to encode sufficient structure of local descriptors, which, however, is important for modeling scene images. To cope with the loss of structure, spatial pyramid matching (SPM) <ref type="bibr" target="#b17">[18]</ref>, a special case of pyramid match kernel (PMK) <ref type="bibr" target="#b14">[15]</ref>, is proposed to incorporate location information into the BoW model for image representation. An image is partitioned into increasingly fine sub-regions and a histogram of local features is formed inside each sub-region. Apart from being utilized in spatial pyramid matching, spatial layout also plays an important role in real-world scene understanding. In <ref type="bibr" target="#b18">[19]</ref>, spatial envelope of an environment is made by a composite set of boundaries, such as walls, sections and ground, to build relations between the outlines of the surfaces and the properties including the inner textured pattern.</p><p>In addition, many topic models have also be explored for image modeling and classification, ranging from the probabilistic latent semantic analysis (pLSA) <ref type="bibr" target="#b19">[20]</ref> and latent Dirichlet analysis (LDA) <ref type="bibr" target="#b20">[21]</ref> to the hybrid generative and discriminative approach <ref type="bibr" target="#b21">[22]</ref>. Bosch et al. <ref type="bibr" target="#b19">[20]</ref> learned categories and their distributions in unlabelled training images by pLSA. Fei-Fei and Perona <ref type="bibr" target="#b20">[21]</ref> represented each region as part of a theme while the theme distributions as well as the codeword distributions over the themes were learned without supervision by modifying the LDA approach. Although both LDA and pLSA were originally proposed for text/document analysis, they have also demonstrated remarkable performance in scene classification. The advantage of these two models lies in that they add the hidden variables to enhance the model representation abilities. As for the hybrid generative and discriminative approach in <ref type="bibr" target="#b21">[22]</ref>, at the generative learning stage, the latent topic is firstly discovered by pLSA and then at the discriminative learning stage, a multi-class classifier using SVM are trained based on the topic distribution vectors.</p><p>With the development of object recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, a promising direction is to analyze images with high-level image features. A well-known algorithm is the Object Bank representation <ref type="bibr" target="#b5">[6]</ref>, in which images are decomposed into different components associated with object filters. In <ref type="bibr" target="#b5">[6]</ref>, it is shown that the low-level features can not handle ambiguities among different scenes. Instead, the Object Bank, which represents the image on a collection of object sensing filters, can deal with this kind of ambiguity elegantly. Even if the object detectors could not exactly determine all objects, the responses to different detectors can still capture much of the variation of the corresponding objects. This can encode abundant prior information of the visual space. Additionally, the Object Bank treats the object labels as attributes to help the final scene classification.</p><p>Recently, attempts have been made to improve the performance of the Object Bank <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. In <ref type="bibr" target="#b24">[25]</ref>, an optimal object bank (OOB) by imposing weights on the detectors according to their discriminative abilities has been proposed, while in <ref type="bibr" target="#b25">[26]</ref>, Zhang et al. proposed to project the high-level features from the Object Bank representation into discriminative subspaces, obtained by clustering the features in a supervised way, to attain a more compact and discriminative representation.</p><p>Another interesting direction in scene recognition is to challenge the most difficult task, i.e., the overlap problem. In this scenario, the class labels are not mutually exclusive, meaning that there exist some images belonging to multiple classes. To address this task, Boutell et al. <ref type="bibr" target="#b26">[27]</ref> proposed a cross-training algorithm, which has later been extended to multi-instance multi-label learning <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OBJECT BANK REPRESENTATION</head><p>The core idea in the Object Bank representation is to decompose an image according to a pre-defined object filter bank. Specifically, when an object filter traverses all pixels in one image, only the maximum of the responses is meaningful to represent how likely the corresponding object occurs in this image. By concatenating the max response of each filter, we can generate the representation with each dimension corresponding to one object filter with a certain configure (scale, location and profile). From a semantic point of view, the obtained representation can give the details about the content of the image on behalf of how likely each object in the object list bank appears in this image. We revisit the Object Bank approach from two aspects: the object filter generation and the Object Bank representation, in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Filters</head><p>The essence of the Object Bank representation could be considered as a mapping procedure, which projects images into a new space spanned by objects. In the new space, dimensions correspond to the responses of object filters (Note that an object has multiple filters with different scales and profiles), and an image is just one point whose distance to each dimension shows the likelihood of the image containing this object. In this representation, how to select the object list or object bank and how to build them are important.</p><p>Theoretically, the number of objects during decomposition could be infinite, while only a small proportion of the objects appear in images with high frequencies. This is analogous to the Zipf's law <ref type="bibr" target="#b28">[29]</ref> known in the natural language processing area, which implies that only a small proportion of words account for the majority of documents. In <ref type="bibr" target="#b5">[6]</ref>, according to the frequency of occurrences of objects in different datasets, 177 of the most frequent objects are selected. Table <ref type="table" target="#tab_0">I</ref> shows some of the objects used in the 177 object filters.</p><p>With regards to object filters, different kinds of objects can be generated by distinctive approaches. For instance, the latent SVM object detectors <ref type="bibr" target="#b29">[30]</ref> are suitable for most of the blobby objects (tables, cars, humans, etc.) while a texture classifier <ref type="bibr" target="#b30">[31]</ref> is better for texture and material based objects (sky, road, sand, etc.). Fortunately, the availability of largescale image datasets, e.g., LabelMe <ref type="bibr" target="#b31">[32]</ref> and ImageNet <ref type="bibr" target="#b32">[33]</ref>, makes it possible to obtain object detectors for a wide range of visual concepts. Fig. <ref type="figure" target="#fig_2">3</ref> shows some examples of object filters. It can be seen that for some objects with simple contours such as wheel and fork, the corresponding filters can give the more distinctive details. But for some complex objects like monkey or dog, the difference between them is not significant enough to distinguish them only by the filter templates. The ambiguities will produce additional difficulty during classification.</p><p>In principle, by selecting a proper object list, the main content of an image can be sufficiently represented by the responses of the object filters with semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Bank</head><p>This subsection describes how to reflect the probability of each object occurring in an image and how to deal with the probabilities from all object filters.</p><p>To answer the first question, the normalized max response of the filter can be treated as the corresponding probability. Given an image G and a filter F in the object bank, the response of the filter at the point (x, y) in the image is the sum of the products of the filter coefficients and the corresponding neighborhood points in the area spanned by the filter mask, which can be formulated as:</p><formula xml:id="formula_0">(x ,y ) ∈neighborhood of (x,y) F x , y • G x + x , y + y (1)</formula><p>Moving the center point (x, y) to go through all the pixels in the image, we can obtain the responses from the filters for all the pixels. Since each filter can reflect the outline of the object to some extent, the sum operation in Eq. ( <ref type="formula">1</ref>) essentially calculates the similarity between the object in the filter and a patch around the pixel in the image. If normalized, the maximum value can be viewed as the probability of the object occurring in the image.</p><p>As in <ref type="bibr" target="#b5">[6]</ref>, if each object is described by filters with two profiles (front and side), 6 scales and a 3-level spatial pyramid (with 1 + 4 + 16 sub-regions), there will be 177(obj ect) × 2( pr o f ile) × 6(scale) × 21(sub-region number) = 44604 maximum responses for each image. For the values of maximum responses from different objects, the largest ones indicate the objects that are most probably contained in the image. Some of the objects used in the 177 object filters are listed in Table <ref type="table" target="#tab_0">I</ref>.</p><p>Instead of only using the most proper object response <ref type="bibr" target="#b33">[34]</ref>, we concatenate all the maximum responses from object filters to encode richer semantic information of the content in the image. Then one image is just a point in the high-dimensional space spanning by the object filters, which is similar to the idea of the bag-of-words model where the space is spanned by visual words in the vocabulary. Instead of using the word frequency in one document as the feature vector in the vector space, here, the max response of each object is adopted in the final Object Bank representation. This can be explained as how likely this object happening in the corresponding image, with the similar meaning of the word frequency in document analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OBJECT-TO-CLASS DISTANCES</head><p>Inspired by the success of Image-to-Class (I2C) distance in NBNN <ref type="bibr" target="#b34">[35]</ref> as well as its extensions such as the NBNN kernel <ref type="bibr" target="#b35">[36]</ref> and local NBNN <ref type="bibr" target="#b36">[37]</ref>, in this paper, we propose the Object-to-Class (O2C) distances for high-level object representation, which generalizes the Object Bank representation.</p><p>Before introducing our object-to-class distances, we revisit the image-to-class distance. Additionally, comparison between the image-to-image distance and the image-to-class distance is also described. With regards to the Object Bank representation, we provide a different view from subspace decomposition, based on which the object-to-class distance is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image-to-Class Distance</head><p>Models based on local features have achieved state-of-theart results in many visual object recognition tasks. An image can be described by a collection of local feature descriptors, e.g., SIFT, extracted from patches around salient interest points or regular grids. How to measure the similarity between two images represented as sets of local features becomes a basic problem in both image classification and matching. The problem is non-trivial due to that the cardinality of the set varies with different images and the elements are unordered <ref type="bibr" target="#b15">[16]</ref>.</p><p>The bag-of-words model can be deemed as the most widely used algorithm for image representation based on local features. Local feature descriptors for an image are projected into a new representation space spanned by visual words in a vocabulary, which yields a histogram as a fixed-length vector to represent this image. In this case, the image-toimage (I2I) distance is typically computed to generate kernels to feed the support vector machine (SVM) for classification. Since the histogram actually expresses the distribution of local descriptors over the dictionary, the I2I distance in fact measures the distance between distributions.</p><p>However, if we measure the distance between distributions of local features from one image and from a set of images in one class, then the distance becomes the image-toclass (I2C) <ref type="bibr" target="#b34">[35]</ref> distance in the naive Bayes nearest neighbor (NBNN) classifier. Due to the use of the I2C distances, which actually deal with the large intra-class variations, the NBNN classifier obviates the quantization errors in the BoW model and proves to be successful in image classification.</p><p>The I2C distance from an image to a candidate class is formulated as the sum of all the Euclidean distances from local feature descriptors in this image to their corresponding nearest neighbor descriptors searched from the descriptor set in the candidate class. This distance similarity measure directly deals with each image represented by a set of local descriptors. This resembles having a huge ensemble of very weak classifiers associated with local feature descriptors, which can exploit the discriminative power of both high and low informative descriptors <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subspace Perspective of Object Bank</head><p>In the original Object Bank approach, all maximum responses from different object filters, scales, profiles and levels of pyramid are concatenated as a huge vector to represent the content of the image, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. If we use I to represent this vector, as analyzed above, the representation belongs to the space of R 44604 . One of the disadvantages of this representation is being less discriminative due to that all the object filters are equally treated. As shown in Fig. <ref type="figure" target="#fig_6">2(b)</ref>, the bed class is irreplaceably crucial for the bedroom scene. Since different objects play distinct roles in the same scene class, and even for the same object, the effects on different scene classes could also be different, it is sensible to consider these objects separately and distinctively.</p><p>In fact, it is assumed in the Object Bank approach that objects are independent to each other. Therefore, for the representations, responses from all the object filters can be treated separately. The space with 44604 dimensions is then divided into 177 subspaces, with each one corresponding to one object with 2 × 6 × 21 = 252 dimensions.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows the difference between our subspace decomposition and the traditional Object Bank approach. Once we decompose the Object Bank representation into the object subspaces, we can treat different objects separately. Also, we can divide the image into parts that correspond to object subspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object-to-Class (O2C) Distances</head><p>In this subsection, we will introduce our newly proposed object-to-class (O2C) distances, which are inspired by the success of image-to-class (I2C) distance in the NBNN classifier.</p><p>Given an image Q represented as a set of subspace vectors, I 1 , . . . , I n , . . . , I N , where I n ∈ R D is the subspace associated with the n-th object and D = 252 is the dimensionality of each subspace. We can find the class of the image Q by the Maximum-A-Posterior (MAP) classifier. Similarly to the NBNN classifier, with the assumption that the class (</p><formula xml:id="formula_1">)<label>2</label></formula><p>Under the Naive-Bayes assumption that I 1 , . . . , I n , . . . , I N are i.i.d. given its class c, we have:</p><formula xml:id="formula_2">p(Q|c) = p(I 1 , . . . , I N |c) = N n=1 p(I n |c),<label>(3)</label></formula><p>where p(I n |c) can be approximated using the non-parametric Parzen density estimation.</p><p>Taking the log probability of the ML decision rule, we rewrite Eq. (3) as:</p><formula xml:id="formula_3">ĉ = arg max c log p(Q|c) = arg max c N n=1 log p(I n |c) (4)</formula><p>For the estimation of p(I n |c), denoted as p(I n |c), by properly selecting the kernel function in the Parzen likelihood estimation, we can turn the probability into a distance representation as follows:</p><formula xml:id="formula_4">p(I n |c) = 1 L L j =1 K (I n -I c n ( j )), (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where L is the total number of n-th object's responses from class c and I c n ( j ) corresponds to the j th response of the object n in the class c. K (•) is a kernel function, which determines the range of covered samples.</p><p>Theoretically, as L goes close to infinite, the estimate distribution p(I n |c) converges to the true one p(I n |c). Considering the long-tail characteristic of the object response distribution as shown in Fig. <ref type="figure" target="#fig_4">5</ref>, most of terms in the summation of Eq. ( <ref type="formula" target="#formula_4">5</ref>) can be negligible by only keeping the r nearest neighbors of object n. Then Eq. ( <ref type="formula" target="#formula_4">5</ref>) becomes:</p><formula xml:id="formula_6">pN N (I n |c) = 1 L r j =1 K (I n -N N j c (I n )),<label>(6)</label></formula><p>where N N j c (I n ) means the j th nearest neighbor of object n of the query image in the class c. For an extremely simple case, we set r = 1 as in NBNN, where N N j c (I n ) can be expressed as N N c (I n ).</p><p>K (•) in Eq. ( <ref type="formula" target="#formula_4">5</ref>) and ( <ref type="formula" target="#formula_6">6</ref>) denotes the kernel function which is typically chosen as a Gaussian kernel in Eq. <ref type="bibr" target="#b6">(7)</ref>. In practice, the Gaussian assumption is reasonable. We show two examples in Fig. <ref type="figure" target="#fig_4">5</ref>, from which we can see that the maximum responses from different object filters obey Gaussian distributions.</p><formula xml:id="formula_7">K (I n -N N c (I n )) = exp - 1 2σ I n -N N c (I n ) 2 (7)</formula><p>Furthermore, assuming that the kernel bandwidths in the Parzen function are the same for all the classes, then the log function and the exp function can be merged as:</p><formula xml:id="formula_8">ĉ = arg max c N n=1 log p(I n |c) = arg max c N n=1 - 1 2σ I n -N N c (I n ) 2 = arg min c N n=1 ( I n -N N c (I n ) 2 ) (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>We define</p><formula xml:id="formula_10">d N N (I n , c) = I n -N N c (I n ) 2 (9)</formula><p>as the Object-to-Class (O2C) distance from the object I n to the class c. It is associated with subspace n, which means this distance may be different for different objects.</p><p>The definition of the O2C distance is inspired by the I2C distance while they are essentially different. We summarize the differences between O2C and I2C in the following three aspects:</p><p>1) The I2C distance is defined as the distance from an image to a class which is the sum of the distances from all the local features, e.g., SIFT, of the image to their corresponding nearest local features in a class. The basic O2C distance is defined as the distance from an object (which is represented as responses of a series of object filters and may/may not appear in an image) to its nearest object in a class. In a nutshell, the I2C distance is the sum of many distances between local features while the O2C distance is the distance between two objects (responses of two object filters). In addition, we have also provided several variants of the O2C distance based on the basic definition which makes O2C further more different from the I2C distance.</p><p>2) In contrast to the I2C distance, the O2C distance has explicit semantic meaning because the response of an object filter to an image indicates the probability of this object appearing in the image, while the local features such as SIFT are just low-level features without semantic meaning.</p><p>3) An important advantage of the O2C distance over the I2C distance is that O2C is defined on a bank of ordered object filters (while local features in I2C distances are orderless). This makes the O2C distance more flexible in the construction of kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BASIC O2C DISTANCES AND ITS VARIATIONS</head><p>In this subsection, several variants of the O2C distance which reflect the different effects of an object on scene classes are proposed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Basic O2C Distance</head><p>As defined in Eq. ( <ref type="formula" target="#formula_8">8</ref>), we just adopt the distance from I n to its nearest neighbor in class c as the O2C distance d N N (I n , c), which we take as a baseline of the O2C distance. Suppose Q n is the training set in the subspace of the object n and Q c n represents the samples from the class c. The algorithm to compute the O2C distance d N N (I n , c) is given in Algorithm 1.</p><p>In the basic O2C distance, the candidate competitor is just selected as the nearest neighbor. It is suitable for the scenarios where, for any object I n of a test sample, its nearest neighbor belongs the right class. However, when there are noisy samples, this assumption would be violated leading to inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. O2C Distance With Anchors</head><p>As is known in NBNN, one of the most important advantages is the avoidance of the vector quantization, which, however, leads to two shortcomings. On the one hand, due to the nearest neighbor search, the computational cost would be extremely high, especially when there are a huge number of training samples with high dimensions. On the other hand, as mentioned above, in practice, there always exist noisy samples, which make the basic O2C distance less discriminative and unreliable. Inspired by the previous work in <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b25">[26]</ref>, we propose the O2C distance d A (I n , c) with anchor points which can be cluster centers obtained by the k-means clustering algorithm.</p><p>The difference between O2C with anchors d A (I n , c) and the basic d N N (I n , c) is shown in Fig. <ref type="figure">6</ref> from which it can be seen that the anchor points possess better generalization properties than the nearest neighbor points. The red star represents the test sample, and the length of the dashed line means the NN distance while the length of a real line denotes the distance to an anchor point. For the points in the region of intersection </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. O2C Distance With Locality</head><p>The idea of locality has been extensively exploited in many machine learning algorithms including manifold learning <ref type="bibr" target="#b37">[38]</ref>, sparse coding <ref type="bibr" target="#b12">[13]</ref> and NBNN as well <ref type="bibr" target="#b36">[37]</ref>. Inspired by its success in local NBNN, we incorporate the locality into our O2C distance.</p><p>Supposing that different classes for object I n are not equally important, here, instead of finding the nearest neighbor from each class, we only focus on those classes in the neighborhood of I n .</p><p>We search k-nearest neighbors over the whole training set containing all classes and only the classes that have samples in the local neighborhood of the object are considered to contribute significantly. The other classes can be treated as the background classes, since they are less relevant to I n . In order to assign a value for those background classes, during the k-nearest neighbors searching procedure, we extend the neighborhood by including an additional sample, which is the (k + 1)-th nearest neighbor of I n .</p><p>More specifically, among all classes, only those with at least one sample falling in this local neighborhood are important for discriminative representation, while for the rest of classes, the distances are fixed to be the (k + 1)-th nearest neighbor. It is analogous to draw a hypersphere around I n , and only k samples are allowed to be included according to the distances to I n . Moreover, for those classes which have samples falling in the hypersphere, we give them a special mark for particular focus, while for the rest without samples in the hypersphere, we treat them equally. The calculation is summarized in Algorithm 3.</p><p>In this algorithm, only the classes with samples falling into the k-nearest neighbors have candidate competitors, which are the nearest points to the test sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. O2C Distance With Class Locality</head><p>We would like to look into the neighborhood of the locality incorporated in d L . In Fig. <ref type="figure" target="#fig_8">7</ref>, we plot the statistics on different For the same object subspace (n = 1, k = 5 is in blue line and n = 1, k = 45 is in green line), with the increase of the scale of the neighborhood, the number of classes included in it is also increased. But even when k = 45, for some samples, the neighborhood still can not cover all classes. For the comparison between the brown line and the green line, it is manifested that different object subspaces possess different neighborhoods covering different classes.</p><p>Table <ref type="table" target="#tab_0">II</ref> shows the maximum distances between some samples and their k-nearest neighbors for object subspace 1 and the k +1 neighbor. Obviously, for the second sample, even the distance out of k-nearest neighbors is still smaller than the distance within k-nearest neighbors of other samples. These findings motivate us to consider the locality by incorporating the class label information.</p><p>To deal with this problem, we search for candidate competitors of scene classes with weak class number information. That is we fix the class number in the neighborhood instead of the sample number in the neighborhood. It can compensate for the shortcoming that we have no idea about how many classes will participate into computing the O2C distance in Algorithm 3. Additionally, in order to verify whether the background classes are helpful for classification or not, we propose a new O2C distance d C L , namely O2C distance with class locality. It enlarges the neighbor set to ensure a specific number of classes will happen in the neighborhood. That means the size of neighborhood varies to keep a fixed number N C of classes involved in computation of the O2C distance. The calculation of d C L is described in Algorithm 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. O2C KERNELS</head><p>Having the O2C distances defined above, we can predict a test sample by choosing the class with the smallest sum of the O2C distances over the object subspaces. In this scenario, the only concern is whether the distance of the test sample to the class it belongs to is the shortest one or not, while the In fact, the distances to all scene categories can contain much discriminative information for classification. In order to effectively utilize such information, we propose to kernelize the distances on O2C, namely, the discriminative kernels on the O2C distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Match Kernels</head><p>Given two sets of features as X = {I  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.I (y)</head><p>N }, which could be the Object Bank representation, the normalized sum match kernel <ref type="bibr" target="#b38">[39]</ref> is selected as it satisfies the mercer condition in Eq. ( <ref type="formula" target="#formula_11">10</ref>):</p><formula xml:id="formula_11">K (X, Y ) = c∈C K c (X, Y ) = 1 |X||Y | c∈C m n k c (I (x) m , I (y) n ) (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where C = {1, ..., c, ..., C} is the set of all classes. Furthermore, in order to combine the O2C distance into kernel function K (X, Y ), local kernel k c (I (x)  n , I</p><p>n ) is defined as:</p><formula xml:id="formula_14">k c (I (x) n I (y) n ) = φ c (I (x) n ) T φ c (I (y) n ) = f c (d(I (x) n , 1), ..., d(I (x) n , C)) T × f c (d(I (y) n , 1), ..., d(I (y) n , C))<label>(11)</label></formula><p>where d(I</p><formula xml:id="formula_15">(x)</formula><p>n , c) denotes the O2C distance from the object I (x) n to the class c. Note that the distance could be any one of the above O2C distances. To be convenient, we denote the kernels with the different distances as K N N , K A , K L and K C L corresponding to d N N , d A , d L and d C L , respectively.</p><p>Through the kernel function, I </p><formula xml:id="formula_17">f c (d(I (x) n , 1), ..., d(I (x) n , C)) = d(I (x) n , c)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head><p>We conduct comprehensive experiments on the UIUC-Sports, Scene-15, MIT Indoor and Caltech101 datasets which range from generic natural scene images (Scene-15) to complex event and activity images (UIUC-Sports). <ref type="bibr" target="#b39">[40]</ref> was firstly introduced in <ref type="bibr" target="#b39">[40]</ref>, which consists of 8 sports event categories. The number of images in each class ranges from 137 to 250. We follow the experimental settings in <ref type="bibr" target="#b5">[6]</ref> by randomly selecting 70 images as the training set and 60 images as the test set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UIUC-Sports</head><p>Scene-15 <ref type="bibr" target="#b17">[18]</ref> is a dataset of 15 natural scene classes which contains scenes from our daily life such as bedroom, kitchen to some outdoor natural scenes such as mountain, coast, and so on. We use 100 images in each class for training and the rest for testing.</p><p>MIT Indoor <ref type="bibr" target="#b40">[41]</ref> contains 15620 images over 67 indoor scenes assembled by <ref type="bibr" target="#b40">[41]</ref>. We follow the experimental setting in <ref type="bibr" target="#b40">[41]</ref> by using 80 images from each class for training and 20 for testing.</p><p>Caltech-101 <ref type="bibr" target="#b41">[42]</ref> was collected using Google Image Search, and the images contain significant clutter, occlusions, and intra-class appearance variance. Each class has at least 31 images and the total class number is 102 including a background class. We use 30 images from each class for training and 20 images for testing.</p><p>Among these approaches, the basic distance between two object subspace representations I n (i ) and I n ( j ) can be chosen as either one of the following:</p><formula xml:id="formula_18">d M (I n (i ), I n ( j )) = D d=1 |I n (i ) d -I n ( j ) d |<label>(13)</label></formula><p>and</p><formula xml:id="formula_19">d E (I n (i ), I n ( j )) = D d=1 (I n (i ) d -I n ( j ) d ) 2 , (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where d M and d E denote the Manhattan distance and the Euclidean distance respectively. A linear SVM classifier <ref type="bibr" target="#b42">[43]</ref> is employed for the final scene classification. The 'one-against-the rest' is employed for multi-class classification. The parameters including γ and C is obtained by cross validation in our experiments.</p><p>In terms of computational complexity, we compare with the work in <ref type="bibr" target="#b15">[16]</ref> which is representative work using kernel methods. In our method, the computational complexity of kernel matrix is O(n 2 d), where n is the number of images in the training set and d is dimensionality. Furthermore, after space mapping in our method, the dimensionality is reduced to N C × 177, where N C is the number of image classes. In <ref type="bibr" target="#b15">[16]</ref>, the complexity of kernel matrix is O(n 2 m 2 d), where m is the average number of points in one image which is always around 2000. Therefore, our methods are more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Scene-15</head><p>The performance of the kernels associated with different O2C distances on the Scene-15 dataset is reported in Table <ref type="table" target="#tab_1">III</ref>. The kernel K C L based on the proposed O2C distance d C L (I n , c) achieves the best results and significantly outperforms the other kernels.</p><p>Among the first three rows, K N N yields significant performance with all results over 84.6% for both distances d M and d E , which indicates that it is reasonable to adopt the nearest neighbor to represent the corresponding class. For K A , where the cluster anchor number is set 10, the original idea in this distance is to exploit the generalization capabilities of vector quantization. From the results in Table <ref type="table" target="#tab_1">III</ref>, we can observe that the generalization does take effect as expected on the Scene-15 dataset. The improvement is limited which could be due to the sparsity of training samples, and it cannot guarantee that all anchors in a scene class could well represent the samples from this class. While for K L , just counts the contribution to the classes found in the local neighborhood with k nearest neighbors. The performance here is compromised by not taking into consideration of the label information of samples in the local neighborhood, which tends to be less discriminative. But the computational complexity is much lower than K N N , especially for datasets with large-scale scene categories and a huge number of training samples.</p><p>For the last row in Table <ref type="table" target="#tab_1">III</ref>, K C L on the O2C distances d C L (I n , c) with class locality yields the best performance. In computation of this distance, the number of classed in the local neighborhood is fixed, and is kept the same for all samples. When computing the distance to a certain class, it selects the nearest neighbor among all the neighbors including N C classes. Compared with K L , the number of the neighbors varies to ensure the same number of classed in the locality, which makes the computed I2C distances more discriminative.</p><p>We have also experimented to investigate effects of the parameters in each algorithm. For the O2C distance with locality d L (I n , c), the only parameter is the number k of nearest neighbors. We have tested k ranging from 3 to 45, and the results are shown in Fig. <ref type="figure" target="#fig_13">8 (a)</ref>. We can see that the performance keeps going up with the increase of k, and reaches its peak with k = 40 for both distances of d M and as d E . After k = 40, the performance decreases due to the limited number of training samples. If the neighborhood region determined by k covers almost two many samples in the training set, the locality constraint trends to be less effective.</p><p>With regards to the O2C distance with class locality, d C L (I n , c), the number of N C classes in the neighborhood is the most important parameter. The experimental results are shown in Fig. <ref type="figure" target="#fig_13">8 (b)</ref>. N C indicates how many classes should be included in the neighborhood for each sample. Note that in this case the sample number in the neighborhood is uncertain. For the Scene-15 dataset, the largest class number is 15, so we test N C ranging from 3 to 15. For this case, the trend of d M and d E is not consistent with that in Fig. <ref type="figure" target="#fig_13">8 (b)</ref>. Besides the rapid dropping for d E with N C = 15, in Fig. <ref type="figure" target="#fig_12">8</ref> (a), the performance of Manhattan distance d M is better than that of Euclidean distance d E , while the situation is the opposite in Fig. <ref type="figure" target="#fig_13">8 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on UIUC Sports</head><p>In contrast to the Scene-15 dataset, the UIUC Sports database is more complicated with larger interclass ambiguity as shown in Fig. <ref type="figure" target="#fig_6">2 (a)</ref>. The same procedure as that conducted on Table <ref type="table" target="#tab_1">III</ref> also summarizes the results on the UIUC-Sports dataset for the proposed methods. Among these four O2C distances, K C L consistently yields the best performance which is similar as on the Scene-15 database. For the first three rows in Table <ref type="table" target="#tab_1">III</ref>, the performance stays almost on the same level. The reason is the same as the above analysis for the scene-15 categories database. While for K C L , the improvement is obviously more significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on MIT Indoor</head><p>Compared to the UIUC Sports and Scene-15 datasets, the MIT Indoor dataset contains more image categories.  <ref type="table" target="#tab_2">IV</ref>, in which the best result 39.90% is achieved by K A with d M . Different form the results on UIUC Sports and Scene-15, K L and K C L produce relatively low results, which would result from the larger number of image categories in this dataset. This trend can also be found in the following Caltech-101 dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Caltech-101</head><p>The Caltech-101 dataset contains even more class categories than the UIUC Sports, Scene-15 and MIT Indoor datasets, which poses more challenges for classification due to the larger variations of backgrounds. Similarly, the effects of the number k of nearest neighbors on the performance of The results of different O2C distances with d M and d E are also reported in Table <ref type="table" target="#tab_2">IV</ref>. On this dataset, K A can also achieve the best result, which is consistent with that on the MIT Indoor dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparisons With Other Methods</head><p>To show the contributions of the proposed methods for scene classification, we have also conducted a comparison with the state-of-the-art algorithms on the four datasets. The results on these four datasets, Scene-15, UIUC-Sports, MIT Indoor and Caltech-101, are reported in Table <ref type="table" target="#tab_3">V</ref>. The proposed methods can significantly outperform the original Object Bank approach for all the four datasets, which demonstrates the effectiveness of our work.</p><p>It is worth to mention that for the original Object Bank with a linear SVM classifier, the accuracy on the Scene-15 and UIUC-Sports datasets are 82.03% and 77.50% respectively, which are lower than the methods proposed in this work. On the MIT Indoor and Caltech-101 datasets with larger class numbers, the side effects of background class become significant, which causes degradation of system performance. However, the proposed K N N and K A approaches can still improve the original Object Bank approach with large margins.</p><p>Compared with state-of-the-art approaches, to the best of our knowledge, K C L outperforms most of published results on the Scene-15 and UIUC-Sports datasets. For MIT Indoor, the best performance of four proposed algorithms is 39.85%, which is also comparable with the performance of the state-ofthe-art methods. The Caltech-101 dataset contains 101 image categories, which is even comparable with the number (177) of object filters listed in Table <ref type="table" target="#tab_0">I</ref>. Relatively to the large number of image categories, the number of object filters is small and therefore the representation ability will become weak, which is the reason why the Object Bank approach as well as the proposed methods based on the Object Bank cannot beat the state-of-the-art methods. However, creation of more object filters will definitely improve the performance of methods based on the Object Bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, based on the Object Bank representation, we have proposed the Object-to-Class distance for scene classification. Four variants of the O2C distances, namely the O2C distances with the nearest neighbor, anchors, locality and class locality, are considered with different scenarios. With the O2C distances, the representations obtained from the Object Bank are mapped into a more compact but discriminative space, called distance space. The obtained distance representations carry sufficient semantic meaning due to the use of the Object Bank. To classify the scenes, we then further kernelize the distance representations to obtain the O2C kernels, which not only take the advantages of the high-level representation of the Object Bank, but also generalize the distance representation.</p><p>To validate the proposed O2C kernels, we have performed comprehensive experiments on four widely used image datasets including Scene-15, UIUC-Sports MIT Indoor and Caltech-101. We have also provided extensive investigation of the parameters in our methods. The results on the four datasets have consistently shown the proposed methods can significantly improve the original Object Bank approach and achieve comparable performance with state-of-the-art methods for image classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Relations among object detection, scene classification and image understanding.</figDesc><graphic coords="1,329.31,159.24,215.77,114.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of inter-class ambiguity (a) and intra-class variability (b). (a) Two images from the same scene class 'polo'. (b) Four similar images in appearance from different scene classes of living room, kitchen, office and bedroom from top left to bottom right.</figDesc><graphic coords="2,86.51,171.05,175.82,124.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The profiles of object filters. (a) Fork (left) and Wheel (right). (b) Monkey (left) and Dog (right).</figDesc><graphic coords="4,349.55,321.53,175.82,70.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Difference between Object Bank and our subspace treatment.</figDesc><graphic coords="6,125.99,58.37,359.90,256.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of probability density of responses from the object filters: (a) Carpet and (b) Truck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 Fig. 6 .</head><label>16</label><figDesc>Fig. 6. Sketch map for the effect of anchors.</figDesc><graphic coords="7,308.39,167.81,250.10,121.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2 O2C</head><label>2</label><figDesc>Distance With Anchors d A (I n , c) between two classes, anchors are helpful for correcting the mistake for distance computing. So in d A (I n , c) distance of O2C in Algorithm 2, the objects in each class are firstly clustered by the k-means algorithm, then the minimum distance between I n and the cluster center can be viewed as the final distance of object n to the class c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 3 O2C</head><label>3</label><figDesc>Distance With Locality d L (I n , c) object subspaces and different sizes of neighborhood of each sample for the UIUC-Sports training dataset with 8 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Statistics of the numbers of classes in the neighborhood in Algorithm 3, where k is the number of the nearest neighbors and n indexes the object subspace.</figDesc><graphic coords="9,79.91,56.81,462.98,161.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>n</head><label></label><figDesc>are not compared directly. Instead, the corresponding distances to classes are contrasted with each other. Even if the two features I (x) n and I (y) n are far apart in the original feature space, they are considered to be close if they have similar distances to each class. In practice, the function f c (d(I (x) n , 1), ..., d(I (x) n , C)) is rewritten as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The effects of the nearest neighbor number k in d L (I n , c) and the class number N C in d CL (I n , c) on Scene-15 (a and b), UIUC Sports (c and d), MIT Indoor (e and f), Caltech-101 (g and h).</figDesc><graphic coords="11,94.13,57.26,423.22,218.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 (</head><label>8</label><figDesc>e) shows the performance with different nearest neighbors k used in the computation of d L (I n , c). The performance of d M and d E is consistent with the best results under 60 nearest neighbors. Fig. 8(f) illustrates the performance of d C L (I n , c) under different class numbers N C . The results of different O2C distances with d M and d E are summarized in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>d L (I n , c) and the class number N C on the performance of d C L (I n , c) for this dataset are shown in Fig. 8 (g) and (h), respectively. The best results with different k and N C are 64.26% and 62.79% for d L (I n , c) and d C L (I n , c), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SOME</head><label>I</label><figDesc>OBJECTS USED IN 177 OBJECT FILTERS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON ON THE SCENE-15 DATASET AND UIUC-SPORTS WITH FOUR PROPOSED O2C DISTANCES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON ON MIT INDOOR AND CALTECH-101 WITH THE FOUR PROPOSED O2C DISTANCES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON OF THE PROPOSED APPROACHES WITH STATE OF THE ARTS</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Support Plan of Young Teachers of Heilongjiang Province and in part by Harbin Engineering University, Harbin, China, under Grant 1155G17. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Vladimir Stankovic.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meaning in visual search</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="issue">4180</biblScope>
			<biblScope unit="page" from="965" to="966" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Indoor-outdoor image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Content-Based Access Image Video Database</title>
		<meeting>IEEE Int. Workshop Content-Based Access Image Video Database</meeting>
		<imprint>
			<date type="published" when="1998-01">Jan. 1998</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple line drawings suffice for functional MRI decoding of natural scene categories</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caddigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="9661" to="9666" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification and semantic feature sparsification</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inform. Process. Syst.</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1378" to="1386" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DAISY: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="815" to="830" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual word ambiguity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In defense of soft-assignment coding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis., ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis., ICCV</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="2486" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis., ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis., ICCV</meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient match kernel between sets of features for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene classification via pLSA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis., ECCV</title>
		<meeting>Eur. Conf. Comput. Vis., ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="517" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene classification using a hybrid generative/discriminative approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature learning for image classification via multiobjective genetic programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1371" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards optimal object bank for scene classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="1967" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative high-level representations for scene classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2013-09">Sep. 2013</date>
			<biblScope unit="page" from="4345" to="4348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning with application to scene classification</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv</title>
		<meeting>Adv</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1609" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lexical frequency profiles and Zipf&apos;s law</title>
		<author>
			<persName><forename type="first">R</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Learn</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LabelMe: A database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic modeling of natural scenes for content-based image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="157" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">In defense of nearest-neighbor based image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The NBNN kernel</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis., ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis., ICCV</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Local naive Bayes nearest neighbor for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="3650" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mercer kernels for object recognition with local features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What, where and who? Classifying events by scene and object recognition</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis., ICCV</title>
		<meeting>IEEE Int. Conf. Comput. Vis., ICCV</meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Locality-constrained and spatially regularized coding for scene categorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leborgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context aware topic model for scene recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2743" to="2750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernel sparse representation for image classification and face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis., ECCV</title>
		<meeting>Eur. Conf. Comput. Vis., ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adapted Gaussian models for image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="937" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Local features are not lonely-Laplacian sparse coding for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3555" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervised kernel descriptors for visual recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit., CVPR</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="2858" to="2865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CENTRIST: A visual descriptor for scene categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1489" to="1501" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">His research interests include computer vision, image/video processing, pattern recognition, and machine learning</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang ; Sheffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ling Shao (M&apos;09-SM&apos;10) received the B.Eng. degree in electronic and information engineering from the University of Science and Technology of China, Hefei, China, the M.Sc. degree in medical image analysis and the Ph.D. degree in computer vision from the Robotics Research Group</title>
		<meeting><address><addrLine>Harbin, China; Lanzhou, China; London, ON, Canada; Oxford, U.K; Eindhoven, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
		<respStmt>
			<orgName>Professor of Computer Science with the College of Information and Communication Engineering, Harbin Engineering University ; University of Sheffield ; University of Oxford ; Electronic and Electrical Engineering, University of Sheffield, Sheffield, U.K. Before joining the University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note>Her research interests include signal/image processing, computer vision, and machine learning</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">He is an Associate Editor of the IEEE TRANSACTIONS ON CYBERNETICS, Information Sciences, Neurocomputing, and several other journals, and has edited several special issues for the journals of the IEEE</title>
		<imprint>
			<publisher>British Computer Society</publisher>
		</imprint>
	</monogr>
	<note>He is a fellow of the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
