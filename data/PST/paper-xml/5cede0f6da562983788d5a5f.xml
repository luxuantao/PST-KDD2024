<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Relational Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate Relational Graph Attention Networks, a class of models that extends non-relational graph attention mechanisms to incorporate relational information, opening up these methods to a wider variety of problems. A thorough evaluation of these models is performed, and comparisons are made against established benchmarks. To provide a meaningful comparison, we retrain Relational Graph Convolutional Networks, the spectral counterpart of Relational Graph Attention Networks, and evaluate them under the same conditions. We find that Relational Graph Attention Networks perform worse than anticipated, although some configurations are marginally beneficial for modelling molecular properties. We provide insights as to why this may be, and suggest both modifications to evaluation strategies, as well as directions to investigate for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) successfully solve a variety of tasks in Euclidean grid-like domains, such as image captioning <ref type="bibr" target="#b8">(Donahue et al., 2017)</ref> and classifying videos <ref type="bibr" target="#b15">(Karpathy et al., 2014)</ref>. CNNs are successful because they assume the data is locally stationary and compositional <ref type="bibr" target="#b7">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b14">Henaff et al., 2015;</ref><ref type="bibr" target="#b5">Bruna et al., 2013)</ref>.</p><p>However, data often occurs in the form of graphs or manifolds, which are classic examples of non-Euclidean domains. Specific instances include knowledge bases, molecules, and point clouds captured by 3D data acquisition devices <ref type="bibr" target="#b34">(Wang et al., 2018)</ref>. The generalisation of Neural Networks (NNs) to non-Euclidean domains is termed Geometric Deep Learning (GDL), and may be roughly divided into spectral, spatial and hybrid approaches <ref type="bibr" target="#b4">(Bronstein et al., 2017)</ref>. Spectral approaches <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>, most notably Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b18">(Kipf and Welling, 2016)</ref>, are limited by their basis-dependence. A filter that is learned with respect to a basis on one domain is not guaranteed to behave similarly when applied to another basis and domain. Spatial approaches are limited by an absence of shift invariance and lack of coordinate system <ref type="bibr" target="#b9">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b2">Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b21">Monti et al., 2017)</ref>. Hybrid approaches combine spectral and spatial approaches, trading their advantages and deficiencies against each-other <ref type="bibr" target="#b4">(Bronstein et al., 2017;</ref><ref type="bibr" target="#b26">Rustamov and Guibas, 2013;</ref><ref type="bibr" target="#b31">Szlam et al., 2005;</ref><ref type="bibr" target="#b11">Gavish et al., 2010)</ref>.</p><p>A recent approach that began with Graph Attention Networks (GATs), applied attention mechanisms to graphs, and does not share these limitations <ref type="bibr" target="#b33">(Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b12">Gong and Cheng, 2018;</ref><ref type="bibr" target="#b38">Zhang et al., 2018;</ref><ref type="bibr" target="#b22">Monti et al., 2018;</ref><ref type="bibr" target="#b19">Lee et al., 2018</ref>).</p><p>An alternative direction has been to generalise Recurrent Neural Networks (RNNs) from sequential message passing on one-dimensional signals, to message passing on graphs <ref type="bibr" target="#b30">(Sperduti and Starita, 1997;</ref><ref type="bibr" target="#b10">Frasconi et al., 1997;</ref><ref type="bibr" target="#b13">Gori et al., 2005)</ref>. Incorporating gating mechanisms led to the development of Gated Graph Neural Networks (GGNNs) <ref type="bibr" target="#b27">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b0">Allamanis et al., 2017)</ref> <ref type="foot" target="#foot_0">1</ref> .</p><p>Relational Graph Convolutional Networks (RGCNs) have been proposed as an extension of GCNs to the domain of relational graphs <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>. This model has achieved impressive performance on node classification and link prediction tasks, however, its mechanisms still resides within spectral methods and shares their deficiencies. The focus of this work investigate generalisations of RGCN away from its spectral origins.</p><p>We take RGCN as a starting point, and investigate a class of models we term Relational Graph Attention Networks (RGATs), extending attention mechanisms to the relational graph domain. We consider two variants, Within-Relation Graph Attention (WIRGAT) and Across-Relation Graph Attention (ARGAT), each with either additive or multiplicative attention. We perform an extensive hyperparameter search, and evaluate these models on challenging transductive node classification and inductive graph classification tasks. These models are compared against established benchmarks, as well as a re-tuned RGCN model.</p><p>We show that RGAT performs worse than expected, although some configurations produce marginal benefits on inductive graph classification tasks. In order to aid further investigation in this direction, we present the full Cumulative Distribution Functions (CDFs) for the hyperparameter searches in Appendix D, and statistical hypothesis tests in Appendix E. We also provide a vectorised, sparse, batched implementation of RGAT and RGCN in TensorFlow which is compatible with eager execution mode to open up research into these models to a wider audience<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RGAT architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relational graph attention layer</head><p>We follow the construction of the GAT layer in <ref type="bibr" target="#b33">Veli?kovi? et al. (2017)</ref>, extending to the relational setting, using ideas from <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer input and output</head><p>The input to the layer is a graph with R = |R| relation types and N nodes. The i th node is represented by a feature vector h i ? R F , and the features of all nodes are summarised in the feature matrix H = [h 1 h 2 . . . h N ] ? R N ?F . The output of the layer is the transformed feature matrix H = [h 1 h 2 . . . h N ] ? R N ?F , where h i ? R F is the transformed feature vector of the i th node.</p><p>Intermediate representations Different relations convey distinct pieces of information. The update rule of <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> made this manifest by assigning each node i a distinct intermediate representation g</p><formula xml:id="formula_0">(r) i ? R F under relation r G (r) = H W (r) ? R N ?F ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">G (r) = g (r) 1 g (r) 2 . . . g (r) N</formula><p>is the intermediate representation feature matrix under relation r, and W (r) ? R F ?F are the learnable parameters of a shared linear transformation.</p><p>Logits Following <ref type="bibr" target="#b33">Veli?kovi? et al. (2017)</ref>; <ref type="bibr" target="#b38">Zhang et al. (2018)</ref>, we assume the attention coefficient between two nodes is based only on the features of those nodes up to a neighborhoodlevel normalisation. To keep computational complexity linear in R, we assume that, given linear transformations W (r) , the logits</p><formula xml:id="formula_2">E (r) i,j of each relation r are independent E (r) i,j = a g (r) i , g (r) j ,<label>(2)</label></formula><p>and indicate the importance of node j's intermediate representation to that of node i under relation r. The attention is masked so that, for node i, coefficients ? i,j . These attention coefficients construct a weighted sum over the nodes in the neighborhood for each relation (black rectangle). These are then aggregated and passed through a nonlinearity to produce the updated representation for node i (right red rectangle). node representations h i <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref>. A separate query kernel Q (r) ? R F ?D and key kernel K (r) ? R F ?D project the intermediate representations g </p><formula xml:id="formula_3">(r) i = g (r) i Q (r) ? R D , k (r) i = g (r) i K (r) ? R D .<label>(3)</label></formula><p>For convenience, the query and key kernels are combined to form the attention kernels A (r) = Q (r) ? K (r) ? R 2F ?D . These query and key representations are the building blocks of the two specific realisations of a in Equation (2) that we now consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additive attention logits</head><p>The first realisation of a we consider is the relational modification of the logit mechanism of Veli?kovi? et al. ( <ref type="formula">2017</ref>)</p><formula xml:id="formula_4">E (r) i,j = LeakyReLu q (r) i + k (r) j ,<label>(4)</label></formula><p>where the query and key dimensionality are both D = 1, and q (r) i and k</p><p>(r) i are scalar flattenings of their one-dimensional vector counterparts q (r) i , k (r) i ? R 1 . We refer to any instance of RGAT using logits of the form in Equation (4) as additive RGAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiplicative attention logits</head><p>The second realisation we consider is the multiplicative mechanism of <ref type="bibr" target="#b32">Vaswani et al. (2017)</ref>; <ref type="bibr" target="#b38">Zhang et al. (2018)</ref> </p><formula xml:id="formula_5">3 E (r) i,j = q (r) i ? k (r) j ,<label>(5)</label></formula><p>where the query and key dimensionality D can be any positive integer. We refer to any instance of RGAT using logits of the form in Equation (4) as multiplicative RGAT.</p><p>It should be noted that there are many types of attention mechanisms beyond vanilla additive and multiplicative. These include mechanisms leveraging the structure of the dual graph <ref type="bibr" target="#b22">Monti et al. (2018)</ref> as well as learned edge features <ref type="bibr" target="#b12">Gong and Cheng (2018)</ref>.</p><p>The attention coefficients should be comparable across nodes. This can be achieved by applying softmax appropriately to any logits E (r) i,j . We investigate two candidates, each encoding a different prior belief about how the importance of different relations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i,j</head><p>Figure 2: ARGAT. The logits are produced identically to those in Figure <ref type="figure" target="#fig_0">1</ref>. A softmax is taken across all logits independent of relation type to form the attention coefficients ? (r) i,j . The remaining weighting and aggregation steps are the same as those in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>WIRGAT The simplest way to take the softmax over the logits E (r) i,j of Equation ( <ref type="formula" target="#formula_4">4</ref>) or Equation ( <ref type="formula" target="#formula_5">5</ref>) is to do so independently for each relation r ?</p><formula xml:id="formula_6">(r) i,j = softmax j E (r) i,j = exp E (r) i,j k?N (r) i exp E (r) i,k , ? i, r : j?N (r) i ? (r) i,j = 1.<label>(6)</label></formula><p>We call the attention in Equation ( <ref type="formula" target="#formula_6">6</ref>) Within-Relation Graph Attention (WIRGAT), and it is shown in Figure <ref type="figure" target="#fig_0">1</ref>. This mechanism encodes the prior that relation importance is a purely global property of the graph by implementing an independent probability distribution over nodes in the neighborhood of i for each relation r. Explicitly, for any node i and relation r,</p><formula xml:id="formula_7">nodes j, k ? N (r) i yield competing attention coefficients ? (r)</formula><p>i,j and ? (r)</p><p>i,k with sizes depending on their corresponding representations g (r) j and g</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(r)</head><p>k . There is no competition between any attention coefficients ? (r) i,j and ? (r ) i,k for all nodes i and nodes j ? N (r) i , j ? N (r ) where r = r irrespective of node representations.</p><p>ARGAT An alternative way to take the softmax over the logits E (r) i,j of Equation (4) or Equation ( <ref type="formula" target="#formula_5">5</ref>) is across node neighborhoods irrespective of relation r ?</p><formula xml:id="formula_8">(r) i,j = softmax j,r E (r) i,j = exp E (r) i,j r ?R k?N (r ) i exp E (r ) i,k , ? i : r?R j?N (r) i ? (r) i,j = 1.<label>(7)</label></formula><p>We call the attention in Equation ( <ref type="formula" target="#formula_8">7</ref>) Equation ( <ref type="formula" target="#formula_6">6</ref>) Across-Relation Graph Attention (AR-GAT), and it is shown in Figure <ref type="figure">2</ref>. This mechanism encodes the prior that relation importance is a local property of the graph by implementing a single probability distribution over the different representations g (r) j for nodes j in the neighborhood of node i. Explicitly, for any node i and all r, r ? R, all nodes j ? N  Propagation rule Combining the attention mechanism of either Equation ( <ref type="formula" target="#formula_6">6</ref>) or Equation ( <ref type="formula" target="#formula_8">7</ref>) with the neighborhood aggregation step of <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> gives</p><formula xml:id="formula_9">h i = ? ? ? ? r?R j?N (r) i ? (r) i,j g (r) j ? ? ? ? R N ?F ,<label>(8)</label></formula><p>where ? represents an optional nonlinearity. Similar to <ref type="bibr" target="#b32">Vaswani et al. (2017)</ref>; <ref type="bibr" target="#b33">Veli?kovi? et al. (2017)</ref>, we also find that using multiple heads in the attention mechanism can enhance performance</p><formula xml:id="formula_10">h i = K k=1 ? ? ? ? r?R j?N (r) i ? (r,k) i,j g (r,k) j ? ? ? ? R N ?K F ,<label>(9)</label></formula><p>where ? denotes vector concatenation, ? It might be interesting to consider cases where there are a different number of heads for different relationship types, as well as when a mixture of ARGAT and WIRGAT produce the attention coefficients, however, we leave that subject for future investigation and will not consider it further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basis decomposition</head><p>The number of parameters in the RGAT layer increases linearly with the number of relations R and heads K, and can lead quickly to overparameterisation. In RGCNs it was found that decomposing the kernels was beneficial for generalisation, although it comes at the cost of increased model bias <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>. We follow this approach, decomposing both the kernels W (r,k) as well as the kernels of attention mechanism</p><formula xml:id="formula_11">A (r,k) into B V basis matrices V (b) ? R F ?F and B X basis vectors X (b) ? R 2F ?D W (r,k) = B W b=1 c (r,k) b V (b) , A (r,k) = B X b=1 d (r,k) b X (b) ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">c (r,k) b , d (r,k) b</formula><p>? R are basis coefficients. We consider models using full and decomposed W and A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Node classification</head><p>For the transductive task of semi-supervised node classification, we employ a two-layer RGAT architecture shown in Figure <ref type="figure" target="#fig_4">3a</ref>. We use a Rectified Linear Unit (ReLU) activation after the RGAT concat layer, and a node-wise softmax on the final layer to produce an estimate for the probability that the i th label is in the class ?</p><formula xml:id="formula_13">P (class i = ?) ? ?i,? = softmax(h (2) i ) ? .</formula><p>(11) We then employ a masked cross-entropy loss L to constrain the network updates to the subset of nodes Y whose labels are known</p><formula xml:id="formula_14">L = - i?Y n classes ?=1 y i,? ln (? i,? ) ,<label>(12)</label></formula><p>where y i is the one-hot representation of the true label for node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph classification</head><p>For inductive graph classification, we employ a two-layer RGAT followed by a graph gather and dense network architecture shown in Figure <ref type="figure" target="#fig_4">3b</ref>. We use ReLU activations after each RGAT layer and the first dense layer. We use a tanh activation after the GraphGather : R N ?F ? R 2F , which is a vector concatenation of the mean of the node representations with the feature-wise max of the node representations</p><formula xml:id="formula_15">H = GraphGather(H) = 1 N N i=1 h i ? ? ? F f =1 max i h i,f ? ? . (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>The final dense layer then produces logits of the size n classes ? n tasks , and we apply a task-wise softmax to its output to produce an estimate ?t,? for the probability that the graph is in class ? for a given task t, analogous to Equation (11). Weighted cross-entropy loss L is then used to form the learning objective</p><formula xml:id="formula_17">L(w, y, ?) = - n tasks t=1 n classes ?=1 w t,? y t,? ln (? t,? ) ,<label>(14)</label></formula><p>where w t,? and y t,? are the weights and one-hot true labels for task t and class ? respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate the models on transductive and inductive tasks. Following the experimental setup of <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> for the transductive tasks, we evaluate our model on the Resource Description Framework (RDF) datasets AIFB and MUTAG. We also evaluate our model for an inductive task on the molecular dataset, Tox21. Details of these data sets are given in Table <ref type="table" target="#tab_0">1</ref>. For further details on the transductive and inductive datasets, please see <ref type="bibr" target="#b25">Ristoski and Paulheim (2016)</ref> and <ref type="bibr" target="#b35">Wu et al. (2018)</ref> respectively.</p><p>Transductive baselines We consider as a baseline the recent state-of-the-art results from <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref> obtained with a two-layer RGCN model with 16 hidden units and basis function decomposition. We also include the same challenging baselines of FEAT <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b6">de Vries and de Rooij, 2015)</ref> and RDF2Vec <ref type="bibr" target="#b25">(Ristoski and Paulheim, 2016)</ref>. In-depth details of these baselines are given by <ref type="bibr" target="#b25">Ristoski and Paulheim (2016)</ref>.</p><p>Inductive baselines As baselines for Tox21, we compare against the most competitive methods on Tox21 reported in <ref type="bibr" target="#b35">Wu et al. (2018)</ref>. Specifically, we compare against deep multitask networks <ref type="bibr" target="#b24">Ramsundar et al. (2015)</ref>, deep bypass multitask networks <ref type="bibr" target="#b35">Wu et al. (2018)</ref>, Weave <ref type="bibr" target="#b16">Kearnes et al. (2016)</ref>, and a RGCN model whose relational structure is determined by the degree of the node to be updated <ref type="bibr" target="#b1">Altae-Tran et al. (2016)</ref>. Specifically, up to and including some maximum degree D max ,</p><formula xml:id="formula_18">h i = ? ? ? (W deg(i) ) T h i + j?Ni (U deg(i) ) T h j + b deg(i) ? ? ,<label>(15)</label></formula><p>where W deg(i) ? R F ?F is a degree-specific linear transformation for self-connections, U deg(i) ? R F ?F is a degree-specific linear transformation for neighbours into their intermediate representations g i ? R F , and b deg(i) is a degree-specific bias. Any update for any degree d(i) &gt; D max gets assigned to the update for the maximum degree D max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental setup</head><p>Transductive learning For the transductive learning tasks, the architecture discussed in Section 2.2 was applied. Its hyperparameters were optimised for both AIFB and MUTAG on their respective training/validation sets defined in Ristoski and Paulheim (2016), using 5-fold cross validation. Using the found hyperparameters, we retrain on the full training set and report results on the test set across 200 seeds. We employ early stopping on the validation set during cross-validation to determine the number of epochs we will run on the final training set. Hyperparameter optimisation details are given in Table <ref type="table">4</ref> of Appendix B.</p><p>Inductive learning For the inductive learning tasks, the architecture discussed in Section 2.3 was applied. In order to optimise hyperparameters once, ensure no data leakage, but also provide comparable benchmarks to those presented in <ref type="bibr" target="#b35">Wu et al. (2018)</ref>, three benchmark splits were taken from the MolNet benchmarks 4 , and graphs belonging to any of the test sets were isolated. Using the remaining graphs we performed a hyperparameter search using 2 folds of 10-fold cross validation. Using the found hyperparameters, we then retrained on the three benchmark splits provided with 2 seeds each, giving an unbiased estimate of model performance. We employ early stopping during both the cross-validation and final run (the validation set of the inductive task is available for the final benchmark, in contrast to the transductive tasks) to determine the number of training epochs. Hyperparameter optimisation details are given in Table <ref type="table">5</ref> of Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constant attention</head><p>In all experiments, we train with the attention mechanism turned on. At evaluation time, however, we report results with and without the attention mechanism to provide insight into whether the attention mechanism helps. ARGAT (WIRGAT) without the attention is called C-ARGAT (C-WIRGAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Benchmarks and additional analyses</head><p>Model means and standard deviations are presented in Table <ref type="table">2</ref>. To provide a picture of characteristic model behaviour, the CDFs for the hyperparameter sweep are presented in Figure <ref type="figure">5</ref> of Appendix D. To draw meaningful conclusions, we compare against our own implementation of RGCN rather than the results reported in <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref>; <ref type="bibr" target="#b35">Wu et al. (2018)</ref>.</p><p>We will occasionally employ a one-sided hypothesis test in order to make concrete statements about model performance. The details and complete results of this test are presented in Appendix E. When we refer to significant results this corresponds to a test statistic supporting our hypothesis with a p-value p ? 0.05.</p><p>4 Retrieved from http://deepchem.io.s3-website-us-west-1.amazonaws.com/trained_ models/Hyperparameter_MoleculeNetv3.tar.gz.</p><p>Table 2: (a) Entity classification results accuracy (mean and standard deviation over 10 seeds) for FEAT <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b6">de Vries and de Rooij, 2015)</ref>, RDF2Vec <ref type="bibr" target="#b25">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation over 200 seeds) for our implementation of RGCN, as well as additive and multiplicative attention for (C-)WIRGAT and (C-)WIRGAT (this work). Test performance is reported on the splits provided in <ref type="bibr" target="#b25">Ristoski and Paulheim (2016)</ref>. (b) Graph classification mean Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) across all 12 tasks (mean and standard deviation over 3 splits) for Multitask <ref type="bibr" target="#b24">(Ramsundar et al., 2015)</ref>, Bypass <ref type="bibr" target="#b35">(Wu et al., 2018)</ref>, Weave <ref type="bibr" target="#b16">(Kearnes et al., 2016)</ref>, RGCN <ref type="bibr" target="#b1">(Altae-Tran et al., 2016)</ref>, and (mean and standard deviation over 3 splits, 2 seeds per split) our implementation of RGCN, additive and multiplicative attention for (C-)WIRGAT and (C-)ARGAT (this work). Test performance is reported on the splits provided in <ref type="bibr" target="#b35">Wu et al. (2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Transductive learning</head><p>In Table <ref type="table">2a</ref> we evaluate RGAT on MUTAG and AIFB. With additive attention, WIRGAT outperforms ARGAT, consistent with <ref type="bibr" target="#b28">Schlichtkrull et al. (2018)</ref>. Interestingly, when employing multiplicative attention, the converse appears true. For node classification tasks on RDF data, this indicates that the importance of a particular relation type does not vary much (if at all) across the graph unless one employs a multiplicative comparison<ref type="foot" target="#foot_3">5</ref> between node representations.</p><p>AIFB On AIFB, the best to worst performing models are: 1) additive WIRGAT 2) multiplicative ARGAT 3) RGCN 4) additive ARGAT, and 5) multiplicative WIRGAT, with each comparison being significant.</p><p>When comparing against their constant attention counterparts, the significant differences observed were for additive and multiplicative ARGAT, where attention gives a relative mean performance improvements of 1.03% and 0.31% respectively, and multiplicative WIRGAT, where attention gives a relative mean performance drop of 0.84%.</p><p>Although we present state-of-the art result on AIFB with additive WIRGAT, since its performance with and without attention are not significantly different, it is unlikely that this is due to the attention mechanism itself, at least at inference time. Over the hyperparameter space, additive WIRGAT and RGCN are comparable in performance (see Figure <ref type="figure">5a</ref> in Appendix D), leading us to conclude that the result is more likely attributable to finding a better hyperparameter point for additive WIRGAT during the search.</p><p>MUTAG On MUTAG, the best to worst performing models are: 1) RGCN 2) multiplicative ARGAT 3) additive WIRGAT tied with multiplicative WIRGAT, and 4) additive ARGAT, with each comparison being significant.</p><p>When comparing against their constant attention counterparts, the significant differences observed were for additive WIRGAT and ARGAT, where attention gives relative mean performance improvements of 0.66% and 2.90% respectively, and multiplicative ARGAT, where attention gives a relative mean performance drop of 1.63%.</p><p>We note that RGCN consistently outperforms RGAT on MUTAG, contrary to what might be expected <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>. The result is surprising given that RGCN lies within the parameter space of RGAT (where the attention kernel is zero), a configuration we check through evaluating C-WIRGAT. In our experiments we have observed that both RGCN and RGAT can memorise the MUTAG training set with 100% accuracy without difficulty (this is not the case for AIFB). The performance gap between RGCN and RGAT could then be explained by the following:</p><p>-During training, the RGAT layer uses its attention mechanism to solve the learning objective. Once the objective is solved, the model is not encouraged by the loss function to seek a point in the parameter space that would also behave well when attention is set to a normalising constant within neighbourhoods (i.e. the parameter space point that would be found by RGCN).</p><p>-The RDF tasks are transductive, meaning that a basis-dependent spectral approach is sufficient to solve them. As RGCN already memorises the MUTAG training set, a model more complex<ref type="foot" target="#foot_4">6</ref> than RGCN, for example RGAT, that can also memorise the training set is unlikely to generalise as well, although this is a hotly debated topic -see e.g. <ref type="bibr" target="#b36">Zhang et al. (2016)</ref>.</p><p>We employed a suite of regularisation techniques to get RGAT to generalise on MUTAG, including L2-norm penalties, dropout in multiple places, batch normalisation, parameter reduction and early stopping, however, no evaluated harshly regularised points for RGAT generalise well on MUTAG.</p><p>Our final observation is that the attention mechanism presented in Section 2.1 relies on node features. The node features for the above tasks are learned from scratch (the input feature matrix is a one-hot node index) as part of the task. It is possible that in this semi-supervised setup, there is insufficient signal in the data to learn both the input node embeddings as well as a meaningful attention mechanism to act upon them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Inductive learning</head><p>In Table <ref type="table">2b</ref> we evaluate RGAT on Tox21. The number of samples is lower for these evaluations than for the transductive tasks, and so fewer model comparisons will be accompanied with a reasonable significance, although there are still some conclusions we can draw.</p><p>Through a thorough hyperparameter search, and incorporating various regularisation techniques, we obtained the relative mean performance of 0.72% for RGCN compared to the result reported in <ref type="bibr" target="#b35">Wu et al. (2018)</ref>, providing a much stronger baseline.</p><p>Both additive attention models match the performance of RGCN, whereas multiplicative WIRGAT and ARGAT marginally outperform RGCN, although this is not significant (p = 0.24 and p = 0.41 respectively).</p><p>When comparing against their constant attention counterparts, significant differences observed were for multiplicative WIRGAT and ARGAT, where attention gives a relative mean performance improvements of 3.33% and 4.36% respectively. We do not observe any significant gains coming from additive attention when compared to their constant counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have investigated a class of models we call Relational Graph Attention Networks (RGATs). These models act upon graph structures, inducing a masked self-attention that takes account of local relational structure as well as node features. This allows both nodes and their properties under specific relations to be dynamically assigned an importance for different nodes in the graph, and opens up graph attention mechanisms to a wider variety of problems.</p><p>We evaluted two specific attention mechanisms, Within-Relation Graph Attention (WIRGAT) and Across-Relation Graph Attention (ARGAT), under both an additive and multiplicative logit construction, and compared them to their equivalently evaluated spectral counterpart Relational Graph Convolutional Networks (RGCNs).</p><p>We find RGATs perform competitively or poorly on established baselines. This behavior appears strongly task-dependent. Specifically, relational inductive tasks such as graph classification benefit from multiplicative ARGAT, whereas transductive relational tasks, such as knowledge base completion, at least in the absence of node features, are better tackled using spectral methods like RGCNs or other graph feature extraction methods like Weisfeiler-Lehman (WL) graph kernels.</p><p>In general we have found that WIRGAT should be paired with an additive logit mechanism, and fares marginally better than ARGAT on transductive tasks, whereas ARGAT should be paired with a multiplicative logit mechanism, and fares marginally better on inductive tasks.</p><p>We have found no cases where choosing any variation of RGAT is guaranteed to significantly outperform RGCN, although we have found that in cases where RGCN can memorise the training set, we are confident that RGAT will not perform as well as RGCN. Consequently, we suggest that before attempting to train RGAT, a good first test is to inspect the training set performance of RGCN.</p><p>Through our thorough evaluation and presentation of the behaviours and limitations of these models, insights can be derived that will enable the discovery of more powerful model architectures that act upon relational structures. Observing that model variance on all of the tasks presented here is high, any future work developing and expanding these methods must choose larger, more challenging datasets. In addition, a comparison between the generalisation of spectral methods, like those presented here, and generalisations of Recurrent Neural Networks (RNNs), like Gated Graph Sequence Networks, is a necessary ingredient for determining the most promising future direction for these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Tox21 Results</head><p>For completeness, we present the training, validation and test set performance of our models in addition to those in <ref type="bibr" target="#b35">Wu et al. (2018)</ref> in Table <ref type="table">3</ref>.</p><p>Table <ref type="table">3</ref>: Graph classification mean Area Under the Curve (AUC) across all 12 tasks (mean and standard deviation over 3 splits) for Multitask <ref type="bibr" target="#b24">(Ramsundar et al., 2015)</ref>, Bypass <ref type="bibr" target="#b35">(Wu et al., 2018)</ref>, Weave <ref type="bibr" target="#b16">(Kearnes et al., 2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Cumulative distribution functions</head><p>To aid further insight into our results, we present the Cumulative Distribution Functions (CDFs) for each model on each task in Figure <ref type="figure">4</ref>. In this context, we treat the performance metric of interest during the hyperparameter search as the empirical distribution of some random variable X. We then define its CDF F X (x) in the standard way</p><formula xml:id="formula_19">F X (x) = P (X ? x),<label>(16)</label></formula><p>where P (X ? x) is the probability that X takes on a value less than or equal to x. The CDF allows one to gauge whether any given architecture typically performs better than another across the whole space, rather than comparison of the tuned hyperparameters, which in some cases may be outliers in terms of generic behavior for that architecture. Figure <ref type="figure">5</ref>: CDFs for all models on a) AIFB, b) MUTAG and c) TOX21. Green lines correspond to our implementation of RGCN, blue lines correspond to ARGAT, and orange lines correspond to WIRGAT. Solid lines correspond to additive attention (and whereas dashed lines correspond to multiplicative attention. A lower CDF value is better in the sense that a greater proportion of models of achieve a higher value of that metric.</p><p>AIFB Additive and multiplicative ARGAT perform poorly for most areas of the hyperparameter space, whereas RGCN and multiplicative WIRGAT perform comparably across the entire hyperparameter space.</p><p>MUTAG Interestingly, the models that have a greater amount of hyperparameter space covering poor performance (i.e. RGCN, multiplicative and additive WIRGAT) are also the models which also have a greater amount of hyperparameter space covering good performance. In other words, on the MUTAG, the ARGAT prior resulted in a model whose test set performance was relatively insensitive to hyperparameter choice when compared against the other candidates. Given that the ARGAT model was the most flexible of the models evaluated, and that it was able to memorise the training set, this suggests that the task contained insufficient information for the model to learn its attention mechanism. Given that WIRGAT was able to at least partially learn to its attention mechanism suggests that WIRGAT is less data hungry than ARGAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tox21</head><p>The multiplicative attention models fare poorly on the majority of the hyperparameter space compared to the other models. There is a slice of the hyperparameter space where the multiplicative attention models outperform the other models, however, indicating that although they are difficult to train, it may be worth spending time hyperoptimising them if you need the best performing model on a relational inductive task. The additive attention models and RGCN perform comparably across the entirety of the hyperparameter space and generally perform better than the multiplicative methods except for the very small region of hyperparameter space mentioned above.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: WIRGAT. The intermediate representations for node i (left red rectangle) are combined with the intermediate representations for nodes in its neighborhood (blue rectangles) under each relation r, to form each logit E (r) i,j . A softmax is taken over each logit matrix for each relation type to form the attention coefficients ? (r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Comparison to RGCN For comparison, the coefficients of RGCN are given by ? (r) i,j = |N (r) i | -1 . This encodes the prior that the intermediate representations of nodes j ? N (r) i to node i under relation r are equally important.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) The network architecture used for node classification on AIFB and MUTAG. This architecture is the same as in Schlichtkrull et al. (2018) except with RGCNs replaced with RGATs. (b) The network architecture used for multi-task graph classification on Tox21. This architecture is the same as the GCNs architecture in Altae-Tran et al. (2016) except with RGCNs replaces with GATs and we do not use graph pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are the normalised attention coefficients under relation r computed by either WIRGAT or ARGAT, and g (r,k) i = h i W (r,k) T is the head specific intermediate representation of node i under relation r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A summary of the datasets used in our experiments and how they are partitioned.</figDesc><table><row><cell>Datasets</cell><cell>AIFB</cell><cell>MUTAG</cell><cell>Tox21</cell></row><row><cell>Task</cell><cell>Transductive</cell><cell>Transductive</cell><cell>Inductive</cell></row><row><cell>Nodes</cell><cell cols="3">8,285 (1 graph) 23,644 (1 graph) 145,459 (8014 graphs)</cell></row><row><cell>Edges</cell><cell>29,043</cell><cell>74,227</cell><cell>151,095</cell></row><row><cell>Relations</cell><cell>45</cell><cell>23</cell><cell>4</cell></row><row><cell>Labelled</cell><cell>176</cell><cell>340</cell><cell>96,168 (12 per graph)</cell></row><row><cell>Classes</cell><cell>4</cell><cell>2</cell><cell>12 (multi-label)</cell></row><row><cell>Train nodes</cell><cell>112</cell><cell>218</cell><cell>(6411 graphs)</cell></row><row><cell cols="2">Validation nodes 28</cell><cell>54</cell><cell>(801 graphs)</cell></row><row><cell>Test nodes</cell><cell>28</cell><cell>54</cell><cell>(802 graphs)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Best performance in class in boldened, and best performance overall is underlined. For completeness, we present the training and validation mean ROC-AUCs alongside the test ROC-AUCs in Appendix A. For a graphical representation of these results, see Figure4in Appendix C.</figDesc><table><row><cell>Model</cell><cell>AIFB</cell><cell>MUTAG</cell><cell>Model</cell><cell>Tox21</cell></row><row><cell>Feat</cell><cell cols="2">55.55 ? 0.00 77.94 ? 0.00</cell><cell>Multitask</cell><cell>0.803 ? 0.012</cell></row><row><cell>WL</cell><cell cols="2">80.55 ? 0.00 80.88 ? 0.00</cell><cell>Bypass</cell><cell>0.810 ? 0.013</cell></row><row><cell>RDF2Vec</cell><cell cols="2">88.88 ? 0.00 67.20 ? 1.24</cell><cell>Weave</cell><cell>0.820 ? 0.010</cell></row><row><cell>RGCN</cell><cell cols="2">95.83 ? 0.62 73.23 ? 0.48</cell><cell>RGCN</cell><cell>0.829 ? 0.006</cell></row><row><cell cols="3">RGCN (ours) 94.64 ? 2.75 74.15 ? 2.40</cell><cell cols="2">RGCN (ours) 0.835 ? 0.008</cell></row><row><cell cols="2">Additive attention</cell><cell></cell><cell cols="2">Additive attention</cell></row><row><cell cols="3">C-WIRGAT 96.86 ? 0.94 69.37 ? 2.75</cell><cell>C-WIRGAT</cell><cell>0.832 ? 0.009</cell></row><row><cell>WIRGAT</cell><cell cols="2">96.83 ? 1.01 69.83 ? 2.74</cell><cell>WIRGAT</cell><cell>0.835 ? 0.006</cell></row><row><cell>C-ARGAT</cell><cell cols="2">93.05 ? 3.05 63.69 ? 8.41</cell><cell>C-ARGAT</cell><cell>0.829 ? 0.010</cell></row><row><cell>ARGAT</cell><cell cols="2">94.01 ? 2.76 65.54 ? 6.25</cell><cell>ARGAT</cell><cell>0.835 ? 0.006</cell></row><row><cell cols="2">Multiplicative attention</cell><cell></cell><cell cols="2">Multiplicative attention</cell></row><row><cell>C-WIRGAT</cell><cell cols="2">93.71 ? 3.33 69.57 ? 3.70</cell><cell>C-WIRGAT</cell><cell>0.811 ? 0.008</cell></row><row><cell>WIRGAT</cell><cell cols="2">92.92 ? 3.75 69.60 ? 3.75</cell><cell>WIRGAT</cell><cell>0.838 ? 0.007</cell></row><row><cell>C-ARGAT</cell><cell cols="2">95.89 ? 1.93 74.38 ? 3.78</cell><cell>C-ARGAT</cell><cell>0.802 ? 0.007</cell></row><row><cell>ARGAT</cell><cell cols="2">96.19 ? 1.70 73.17 ? 3.41</cell><cell>ARGAT</cell><cell>0.837 ? 0.007</cell></row><row><cell></cell><cell>(a) Transductive</cell><cell></cell><cell cols="2">(b) Inductive</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, RGCN<ref type="bibr" target="#b1">(Altae-Tran et al., 2016)</ref>, our implementation of RGCN, additive and multiplicative attention versions of WIRGAT and ARGAT (this work). Training, validation and test performance is reported on the splits provided in<ref type="bibr" target="#b35">Wu et al. (2018)</ref>. Best performance in class in boldened, and best performance overall is underlined.</figDesc><table><row><cell>Model</cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Multitask</cell><cell cols="2">0.884 ? 0.001 0.795 ? 0.017</cell><cell>0.803 ? 0.012</cell></row><row><cell>Bypass</cell><cell cols="2">0.938 ? 0.001 0.800 ? 0.008</cell><cell>0.810 ? 0.013</cell></row><row><cell>Weave</cell><cell cols="2">0.875 ? 0.004 0.828 ? 0.008</cell><cell>0.820 ? 0.010</cell></row><row><cell>RGCN</cell><cell cols="2">0.905 ? 0.004 0.825 ? 0.013</cell><cell>0.829 ? 0.006</cell></row><row><cell cols="4">RGCN (ours) 0.883 ? 0.010 0.845 ? 0.003 0.835 ? 0.008</cell></row><row><cell cols="2">Additive attention</cell><cell></cell><cell></cell></row><row><cell>C-WIRGAT</cell><cell cols="2">0.897 ? 0.022 0.842 ? 0.004</cell><cell>0.832 ? 0.009</cell></row><row><cell>WIRGAT</cell><cell cols="3">0.902 ? 0.024 0.845 ? 0.005 0.835 ? 0.006</cell></row><row><cell>C-ARGAT</cell><cell cols="2">0.884 ? 0.012 0.848 ? 0.003</cell><cell>0.829 ? 0.010</cell></row><row><cell>ARGAT</cell><cell cols="3">0.896 ? 0.016 0.851 ? 0.004 0.835 ? 0.006</cell></row><row><cell cols="2">Multiplicative attention</cell><cell></cell><cell></cell></row><row><cell>C-WIRGAT</cell><cell cols="2">0.859 ? 0.016 0.830 ? 0.007</cell><cell>0.811 ? 0.008</cell></row><row><cell>WIRGAT</cell><cell cols="3">0.904 ? 0.022 0.852 ? 0.002 0.838 ? 0.007</cell></row><row><cell>C-ARGAT</cell><cell cols="2">0.838 ? 0.007 0.816 ? 0.007</cell><cell>0.802 ? 0.007</cell></row><row><cell>ARGAT</cell><cell cols="2">0.802 ? 0.007 0.846 ? 0.003</cell><cell>0.837 ? 0.007</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We note that GGNNs support relation types. Evaluating these models on the tasks presented here is necessary to acquire a better understanding neural models of relational data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/anonymous/rgat.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The form of our mechanism is not precisely that of<ref type="bibr" target="#b38">Zhang et al. (2018)</ref> as they also consider residual concatenation and gating mechanism applied across the heads of the attention mechanism.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Or potentially other comparisons beyond additive or constant, i.e. RGCN.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Measured in terms of Minimum Description Length (MDL), for example.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>We perform hyperparameter optimisation using hyperopt <ref type="bibr" target="#b3">Bergstra et al. (2013)</ref> with priors for the transductive tasks specified in Table <ref type="table">4</ref> and priors for the inductive tasks specified in Table <ref type="table">5</ref>. In all experiments we use the Adam optimiser <ref type="bibr" target="#b17">(Kingma and Ba, 2014)</ref>.</p><p>Table <ref type="table">4</ref>: Priors on the hyperparameter search space for the transductive tasks. When multihead attention is used, the number of units per head is appropriately reduced in order to keep the total number of output units of an RGAT layer independent of the number of heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Prior</head><p>Graph kernel units MultiplesOfFour(4, 20)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heads</head><p>OneOf(1, 2, 4) Feature dropout rate Uniform(0.0, 0.8) Edge dropout Uniform(0.0, 0.8) W basis size OneOf <ref type="bibr">(Full,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">20,</ref><ref type="bibr">30</ref>) Graph layer 1 W L2 coef LogUniform(10 -6 , 10 -1 ) Graph layer 2 W L2 coef LogUniform(10 -6 , 10 -1 ) A basis size OneOf <ref type="bibr">(Full,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">20,</ref><ref type="bibr">30</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Charts</head><p>To aid interpretability of the results presented in Table <ref type="table">2</ref> we present a chart representation in Figure <ref type="figure">4</ref>.  <ref type="bibr" target="#b23">(Paulheim and F?mkranz, 2012)</ref>, WL <ref type="bibr" target="#b29">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b6">de Vries and de Rooij, 2015)</ref>, RDF2Vec <ref type="bibr" target="#b25">(Ristoski and Paulheim, 2016)</ref> and RGCN <ref type="bibr" target="#b28">(Schlichtkrull et al., 2018)</ref>, and (mean and standard deviation over 200 runs) for our implementation of RGCN. Yellow Entity classification accuracy (mean and standard deviation over 200 seeds) for additive attention (this work). Red Entity classification accuracy (mean and standard deviation over 200 seeds) for multiplicative attention (this work). Test performance is reported on the splits provided in <ref type="bibr" target="#b25">Ristoski and Paulheim (2016)</ref>. (c): Blue Baseline graph classification mean Receiver Operating Characteristic (ROC) AUC across all 12 tasks (mean and standard deviation over 3 splits) for Multitask <ref type="bibr" target="#b24">(Ramsundar et al., 2015)</ref>, Bypass <ref type="bibr" target="#b35">(Wu et al., 2018)</ref>, Weave <ref type="bibr" target="#b16">(Kearnes et al., 2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Significance testing</head><p>In order to determine if any of our model comparisons are significant, we employ the one-sided Mann-Whitney U test <ref type="bibr" target="#b20">Mann and Whitney (1947)</ref> as we are interested in the direction of movement (i.e. performance) and do not want to make any parametric assumptions about model response. For two populations X and Y :</p><p>? The null hypothesis H 0 is that the two populations are equal, and</p><p>? The alternative hypothesis H 1 is that the probability of an observation from population X exceeding an observation from population Y is larger than the probability of an observation from Y exceeding an observation from X; i.e., H 1 : P (X &gt; Y ) &gt; P (Y &gt; X).</p><p>We treat the empirical distributions of Model A as samples from population X and the empirical distributions of Model B as samples from population Y . This allows us a window into whether, given a task, whether which is the better model out of a pair of models. Results on AIFB, MUTAG and TOX21 are given in Figure <ref type="figure">6</ref>, Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref> respectively. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<idno>CoRR, abs/1711.00740</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Low Data Drug Discovery with One-shot Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
		<title level="m">Diffusion-Convolutional Neural Networks. (Nips)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mcallester</surname></persName>
		</editor>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Substructure counting graph kernels for machine learning from rdf data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K D</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semant</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="84" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<title level="m">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the efficient classification of data structures by neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D S</forename><surname>Marta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiscale Wavelets on Trees, Graphs and High Dimensional Data: Theory and Applications to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gavish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Icml</title>
		<imprint>
			<biblScope unit="page" from="367" to="374" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive Edge Features Guided Graph Attention Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exact and approximate graph matching using random walks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1100" to="1111" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Convolutional Networks on Graph-Structured Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Aided. Mol. Des</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Attention Models in Graphs: A Survey. 0(1</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On a test of whether one of two random variables is stochastically larger than the other</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-01">2017. 2017. January</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dual-Primal Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised generation of data mining features from linked open data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>F?mkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Web Intell. Min. Semant. -WIMS &apos;12, P. 1</title>
		<meeting>2nd Int. Conf. Web Intell. Min. Semant. -WIMS &apos;12, P. 1<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Massively Multitask Networks for Drug Discovery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Konerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Icml</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RDF2Vec: RDF graph embeddings for data mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">9981</biblScope>
			<biblScope unit="page" from="498" to="514" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Rustamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">Wavelets on graphs via deep learning. Nips</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Neural Networks Council</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bremerjr</surname></persName>
		</author>
		<title level="m">Diffusion-driven multiscale analysis on manifolds and graphs: top-down and bottomup constructions</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>P. 59141D</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Graph Attention Networks</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MoleculeNet: A benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno>abs/1611.03530</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
