<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Study of Face Recognition as People Age</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>haibin.ling@siemens.com</email>
							<affiliation key="aff0">
								<orgName type="department">Integrated Data Systems Department</orgName>
								<orgName type="institution">Siemens Corporate Research</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soatto@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Narayanan</forename><surname>Ramanathan</surname></persName>
							<email>ramanath@umiacs.umd.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
							<email>djacobs@umiacs.umd.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">University of Mary-land College Park</orgName>
								<orgName type="institution" key="instit2">University of California Los Angeles</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Study of Face Recognition as People Age</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">85EE14DD2763EC03DBD04845C2375FB6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study face recognition across ages within a real passport photo verification task. First, we propose using the gradient orientation pyramid for this task. Discarding the gradient magnitude and utilizing hierarchical techniques, we found that the new descriptor yields a robust and discriminative representation. With the proposed descriptor, we model face verification as a two-class problem and use a support vector machine as a classifier. The approach is applied to two passport data sets containing more than 1,800 image pairs from each person with large age differences. Although simple, our approach outperforms previously tested Bayesian technique and other descriptors, including the intensity difference and gradient with magnitude. In addition, it works as well as two commercial systems. Second, for the first time, we empirically study how age differences affect recognition performance. Our experiments show that, although the aging process adds difficulty to the recognition task, it does not surpass illumination or expression as a confounding factor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition across ages is an important problem and has many applications, such as passport photo verification, image retrieval, surveillance, etc. This is a challenging task because human faces can vary a lot over time in many aspects, including facial texture (e.g. wrinkles), shape (e.g. weight gain), facial hair, presence of glasses, etc. In addition, the image acquisition conditions and environment often undergo large changes, which can cause non-uniform illumination and scale changes. Fig. <ref type="figure" target="#fig_0">1</ref> shows several typical images with different age gaps. Despite its importance, recognition across ages has been relatively less studied, which is mainly due to the lack of suitable data sets.</p><p>In this paper we study the problem of face recognition across ages and how age gaps affect face recognition tasks. The study is applied to passport photo verification tasks that involve more than 1,800 image pairs, where each pair is the same person taken at different years.</p><p>First, we are interested in finding robust face descriptions especially to lighting and aging. Gradient orientation has been shown to be insensitive to lighting change under a Lambertian assumption and has been applied to face recognition <ref type="bibr" target="#b4">[5]</ref>. Based on previous study of craniofacial growth <ref type="bibr" target="#b17">[18]</ref> and skin color models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, we conjecture that the gradient direction can be used to build descriptors that are insensitive to the aging process as well. Furthermore, a pyramid technique is used for hierarchical modeling. With this idea, we propose using the gradient orientation pyramid (GOP) for robust face representation. Then, given a face image pair, we use the cosines between gradient orientations at all scales to build the "difference" between the pair. Finally, the "difference" is combined with an SVM for face verification tasks in a way similar to <ref type="bibr" target="#b16">[17]</ref>. We applied the proposed approach for passport verification tasks and tested it on two passport image datasets with large age differences. Promising results are observed in comparison with several other approaches, including three different rep-1 978-1-4244-1631-8/07/$25.00 ©2007 IEEE resentations with the same SVM-based framework (intensity difference <ref type="bibr" target="#b16">[17]</ref>, gradient with magnitude, gradient orientation), the Bayesian face <ref type="bibr" target="#b18">[19]</ref>, and two commercial face recognition products. It is also worth noting that our approach is rather simple in comparison to its competitors.</p><p>Second, we study how recognition performance varies with increasing time lapses between images. This is the first such study, to our knowledge, and it uses the largest dataset reported in the literature. Surprisingly, we found that the added difficulty of recognition produced by age gaps becomes saturated after the gap is larger than four years, for gaps of up to ten years. This is observed with all different image representations that have been tested.</p><p>The rest of the paper is organized as follow. Sec. 2 discusses related work. After that, the framework for face verification using a support vector machine is described in Sec. 3. Then, we introduce the gradient orientation pyramid in Sec. 4. Sec. 5 describes our experiments on two passport image datasets involving large age separations. Sec. 6 presents our empirical study of how age gaps affect recognition algorithms. Sec. 7 gives a preliminary study on the relation between aging and gradient orientation. Finally, Sec. 8 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Face recognition and detection has been widely studied for several decades. A lot of work has been done to handle the problem under different conditions, including lighting, pose, expression, etc. A thorough survey can be found in <ref type="bibr" target="#b25">[26]</ref>. The aging process and its effect on face analysis, which we are interested in, has recently attracted research effort. Most work has focused on modelling the aging process <ref type="bibr" target="#b19">[20]</ref>, age estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>, and simulation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. In comparison, face verification across ages is far less studied <ref type="bibr" target="#b18">[19]</ref>.</p><p>Ramanathan and Chellappa <ref type="bibr" target="#b18">[19]</ref> adapted the probabilistic eigenspace framework <ref type="bibr" target="#b15">[16]</ref> for face identification across age progression, which is most closely related to our work. Instead of using a whole face, only a half face (called a PointFive face) is used to alleviate the non-uniform illumination problem. Then, eigenspace techniques and a Bayesian model are combined to capture the intra-personal and extra-personal image differences. Targeting the same task, our work differs from their work in both the representation (we use gradient orientation pyramids) and the classification frameworks (we use SVM).</p><p>Modelling face verification as a two-class classification problem is not new. Moghaddam et al. <ref type="bibr" target="#b14">[15]</ref> used a Bayesian framework for the intra-personal and extra-personal face classification. Phillips <ref type="bibr" target="#b16">[17]</ref> used SVM for face recognition problems and observed good results on the FERET dataset compared to component based approaches. Jonsson et al. <ref type="bibr" target="#b8">[9]</ref> used SVM for face authentication problems. Our work is different in that we use the gradient orientation pyramid instead of intensity differences <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> or the intensity itself <ref type="bibr" target="#b8">[9]</ref> as a face description. Furthermore, we are more interested in passport photos with age differences.</p><p>Image gradients are widely used for feature building and image representation, e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref>. In most of these works, the gradient magnitude information is included (e.g. to build a weighted histogram). The direction of image gradient has been proposed for lighting insensitive recognition (e.g. <ref type="bibr" target="#b2">[3]</ref>) and was shown to be insensitive to changes in lighting direction under a Lambertian assumption <ref type="bibr" target="#b4">[5]</ref>. Recently, Hammond and Simoncelli <ref type="bibr" target="#b6">[7]</ref> proposed a nonlinear image representation using only the orientation information. Inspired by these works, we propose using the gradient orientation for robust face representation with age variation. To the best of our knowledge, it is the first time the gradient direction is combined with SVM for face verification problems. Our experiments show that by discarding magnitude information, the gradient orientation achieves significantly better recognition performance. We also propose using hierarchical structure to further improve discriminability.</p><p>Our work also relates to work on skin appearance <ref type="bibr" target="#b7">[8]</ref>. It is known that melanin and hemoglobin are the two most important factors that determine human skin color. Tsumura et al. <ref type="bibr" target="#b21">[22]</ref> used independent component analysis (ICA) to separate the effect of melanin and hemoglobin as two independent components from skin color. They showed that the logarithm of skin color can be decomposed as linear combinations of two components corresponding to melanin and hemoglobin respectively. We show that this fact is closely related to the insensitivity of gradient orientation across the aging process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task Description</head><p>Passport photo verification is important in the process of passport renewal and related face authentication applications. For example, when a person submits a new photo for renewal, the ideal system can automatically tell whether he is an imposter by comparing the new photo to previous photos that were usually taken years before.</p><p>A common way to evaluate verification uses two criteria: the rate of correct rejection on imposter images (correct reject rate) and the rate of correct acceptance (correct accept rate) on true images. These two rates conflict although we want both rates to be as high as possible. In practice, for the passport verification task, the correct reject rate is most important because rejected images will be examined by a human. For example, we can fix such a rate at a high level (e.g. 99%) while making the correct acceptance rate as high as possible, which measures the human labor that is saved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification Framework</head><p>We model face verification as a two-class classification problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>. Given an input image pair I 1 and I 2 , the task is to assign the pair as either intra-personal (i.e. I 1 and I 2 from the same people) or extra-personal (i.e. I 1 and I 2 from the different individuals). In this section we briefly describe the framework of using a support vector machine for this task. Details about support vector machines can be found in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Given any image pair (I 1 , I 2 ), it is first mapped it onto the feature space. Formally, we have x = F(I 1 , I 2 ) ∈ R d , where x is the feature vector extracted from the image pair (I 1 , I 2 ) through the feature extraction function (functional) F(., .), and R d is the d-dimensional feature space.</p><p>Then the support vector machine is used to divide the feature space into two classes, one for intra-personal pairs and the other for extra-personal pairs. Using the same terminology as in <ref type="bibr" target="#b16">[17]</ref>, we denote the separating boundary with the following equation</p><formula xml:id="formula_0">Ns i=1 α i y i K(s i , x) + b = ∆ (1)</formula><p>where N s is the number of support vectors and s i is the ith support vector. ∆ is used to trade off the correct reject rate and correct accept rate as described in Sec. 5. K(., .) is the kernel function that provides SVM with non-linear abilities. The RBF kernel is chosen for our task due to its effectiveness and efficiency. The RBF kernel is defined as</p><formula xml:id="formula_1">K(x 1 , x 2 ) = exp(-γ * |x 1 -x 2 | 2 )<label>(2)</label></formula><p>where γ is a parameter determining the size of RBF kernels (γ = 1 d is used in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Gradient Orientation Pyramid</head><p>There is one question left open in the previous section: what is F(., .)? A natural choice is to use the intensity difference between I 1 and I 2 , which is called difference space in <ref type="bibr" target="#b14">[15]</ref> and also has been used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>. With an appropriate normalization scheme, the intensity difference can be made robust to affine lighting changes. However, the affine lighting model is not always sufficient for face images, especially for images taken at times separated by years. Instead, to alleviate this problem, we propose using the cosine of gradient orientations as features of image pairs, because gradient orientation is known to be insensitive to lighting change <ref type="bibr" target="#b4">[5]</ref>. In addition, we organize the gradient orientation in a hierarchical fashion, which further improves its discriminability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Gradient Orientation Pyramid</head><p>Our proposed features are partly motivated by recent work using gradient information for object representations ( <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6]</ref>). In these works, the gradient directions were weighted by gradient magnitudes. In contrast, inspired by <ref type="bibr" target="#b4">[5]</ref>, we discard magnitude information and use only orientations, which demonstrates significant improvement in our experiments (Sec. 5). Furthermore, the gradient directions at different scales are combined to make a hierarchical representation.</p><p>Given an image I(p), where p = (x, y) indicates pixel locations, we first define the pyramid of I as</p><formula xml:id="formula_2">P(I) = {I(p; σ)} s σ=0 as I(p; 0) = I(p) I(p; σ) = [I(p; σ -1) * Φ(p)] ↓ 2 σ = 1, ..., s<label>(3)</label></formula><p>where Φ(p) is the Gaussian kernel (0.5 is used as the standard deviation in our experiments), ↓ 2 denotes half size downsampling, and s is the number of pyramid layers. Note that in (3) the notation I is used both for the original image and the images at different scales for convenience. Then, the gradient orientation at each scale σ is defined by its normalized gradient vectors at each pixel.</p><formula xml:id="formula_3">g(I(p; σ)) = ∇(I(p, σ)) / |∇(I(p, σ))| (4)</formula><p>Naturally, the gradient orientation pyramid (GOP) of I, is defined as G(I) = {g(I(p, σ))} s σ=0 . Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the computation of a GOP from an input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Differences Between GOPs</head><p>Given an image pair (I 1 , I 2 ) and corresponding GOPs (G(I 1 ), G(I 2 )), the feature vector x = F(I 1 , I 2 ) is computed as the concatenation of the cosines of the difference between gradient orientations at each pixel and all scales. The computation can be efficiently achieved through the inner product of the corresponding entries of GOP, i.e., for pixel p at scale σ, it is computed as</p><formula xml:id="formula_4">f (I 1 (p; σ), I 2 (p; σ)) = g(I 1 (p; σ)) • g(I 2 (p; σ)) (5)</formula><p>The cosine values are organized into a feature vector x as</p><formula xml:id="formula_5">x = F(I 1 , I 2 ) = (. . . , f(I 1 (p; σ), I 2 (p; σ)) , . . .) (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where p is organized in lexicographic order and σ in increasing order. We summarize the advantages of using GOPs for face verification tasks as follow.</p><p>• GOP is insensitive to illumination changes <ref type="bibr" target="#b4">[5]</ref>. As a result, no normalization is needed on the input images.</p><p>• The pyramid technique provides a natural way to perform face comparison at different scales.</p><p>• The inner product between normalized gradients lies in a finite range ([-1, 1]) that automatically limits the effect of outliers.</p><p>In the following, we use SVM+GOP to indicate the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Verification Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Passport Datasets</head><p>We tested the proposed approach on two real passport image datasets, which we will refer to as Passport I and Passport II respectively. Passport I is the dataset used in <ref type="bibr" target="#b18">[19]</ref>. It contains 452 intra-personal image pairs (several duplicate pairs were removed) and 2251 randomly generated extra-personal image pairs. Passport II contains 1824 intrapersonal image pairs and 9492 randomly generated extrapersonal image pairs. Images in both datasets are scanned passport images. They are in general frontal images with small pose variations. The lighting condition varies, and can be non-uniform and saturated. The age differences between image pairs are summarized in Table <ref type="table" target="#tab_0">1</ref>. It shows that both datasets have significant age gaps for intra-personal images. Fig. <ref type="figure" target="#fig_2">3</ref> further shows the distribution of age differences in the datasets. Intuitively, Passport II is more challenging than Passport I for verification tasks because of the relatively larger age differences. Furthermore, we observed that the image resolution change in Passport II is also larger than that in Passport I.</p><p>In our experiments (SVM-based approaches), the images are preprocessed using the same scheme as in <ref type="bibr" target="#b18">[19]</ref>. This includes manual eye location labelling, alignment by eyes and cropping with an elliptic region. Image sizes are reduced to 96 × 84 for Passport I and 72 × 63 for Passport II. Example images at these resolution are shown in Fig. <ref type="figure" target="#fig_6">8</ref>, where we can also see the "disappearance" of wrinkles. While relatively larger resolutions could be used to make use of more information that is still insensitive to wrinkles (e.g. 160 × 140 as in Fig. <ref type="figure" target="#fig_6">8 (c</ref>)), we run out of memory with them in the current Matlab-based implementation.</p><p>To alleviate the alignment problem, when comparing two GOPs, we tried different alignments with small shiftings (two pixels). In our experiments it helped to improve the performance by around 0.5% (equal error rate). A similar technique is used by <ref type="bibr" target="#b13">[14]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Methods</head><p>We compared the following approaches. 1) SVM+GOP: the approach proposed in this paper. 2) SVM+GO: this is similar to SVM+GOP, except that only the gradient orientation (GO) at the finest scale is used without a hierarchical representation. 3) SVM+G: this one is similar to SVM+GO, except that the gradient (G) itself is used instead of gradient orientation. It can also be viewed as weighting gradient orientations with gradient magnitudes. 4) SVM+diff <ref type="bibr" target="#b16">[17]</ref>. As in <ref type="bibr" target="#b16">[17]</ref>, we use the differences of normalized images as input features combined with SVM. 5) GO: this is the method using gradient orientation proposed in <ref type="bibr" target="#b4">[5]</ref>. 6) Bayesian+PFF <ref type="bibr" target="#b18">[19]</ref>. This is the approach combining Bayesian framework <ref type="bibr" target="#b15">[16]</ref> and PointFive Face (PFF) <ref type="bibr" target="#b18">[19]</ref>. In addition, two commercial systems are tested on the datasets, which we will name Vendor A and Vendor B <ref type="foot" target="#foot_0">1</ref> .</p><p>The first approaches use exactly the same configurations and the same SVM framework, but different representations. The purpose is to study the superiority of the proposed GOP representations. The other four approaches are different from our method in both representations and classification frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Evaluation</head><p>For verification tasks, the correct reject rate (CRR) and the correct acceptance rate (CAR) are two critical criteria, which are defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRR =</head><p># correct rejected extra-personal pairs # total extra-personal pairs CAR = # correct accepted intra-personal pairs # total intra-personal pairs <ref type="bibr" target="#b6">(7)</ref> The performance of algorithms is evaluated using the CRR-CAR curves that are usually created by varying some classifier parameters. As mentioned in Sec. 3.1, although both rates are desired to be as high as possible, the more important is the performance when CRR is high.</p><p>We used three-fold cross validation in our experiments. For each experiment, the CRR-CAR curve is created by adjusting parameter ∆ in <ref type="bibr" target="#b0">(1)</ref>. The total performance is evaluated as the average of the output CRR-CAR curves of three folds. For Vendor A and B, all original color images are input to their systems. To compare with Bayesian+PFF, we also test SVM+GOP in the experimental setup according to <ref type="bibr" target="#b18">[19]</ref>, i.e., we use 200 positive and 200 negative pairs as a training set.</p><p>Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref> show the CRR-CAR curves for the experiments. In addition, Table <ref type="table" target="#tab_1">2</ref> lists the equal error rates (i.e. when CRR=CAR). There are several observations from the experimental results.</p><p>First, among the SVM-based approaches, GOP works the best. The gradient direction obviously plays a main role in GOP's excellent performance, since both SVM+GOP and SVM+GO largely outperform SVM+G, which includes the gradient magnitude information. In comparison, the use of a hierarchical structure in GOP further improves upon GO. Later we will show that the improvement is more obvious when larger image resolution is used.</p><p>Second, SVM+GO largely outperforms GO. Note that, for face recognition, SVM+diff is previously used in <ref type="bibr" target="#b16">[17]</ref> and GO is previously used in <ref type="bibr" target="#b4">[5]</ref>. This shows that our method, as a combination of these two, greatly improves both of them.</p><p>Third, SVM+GOP outperforms the Bayesian approach <ref type="bibr" target="#b18">[19]</ref> on both datasets. In addition, from Fig. <ref type="figure">5</ref> it is obvious that SVM+GOP is more suitable for passport verification tasks because it performs much better at a high correct reject rate, which is desired as mentioned in Sec. 3.1. Furthermore, given an image pair, our approach does not require the information of which one is older, which is used in the Bayesian approach as a prior.</p><p>Fourth, on Passport I, SVM+GOP performs similarly to Vendor A while much better than Vendor B. While on Passport II, SVM+GOP outperforms Vendor A but performs worse than Vendor B (interestingly, the ranks of Vendor A and Vendor B alternate). This observation shows that, though very simple, our approach performs close to commercial systems, which combine many additional heuristic techniques and are well tuned. Furthermore, only low resolution gray images are used in our approach, while the original color images are used in both commercial systems.</p><p>In addition to the verification experiments mentioned above, another two experiments have been done to further understand the proposed approach. One of them studies the hierarchical information captured by GOP. The other tries to find irrelevant features on the face that are related to wrinkles to some degree. Study on Hierarchical Information. As mentioned above, the improvement from adding hierarchical information is not significant due to the limited image sizes. To further study the effect of the pyramid, we conducted experiments comparing SVM+GOP and SVM+GO on larger resolution images. We used randomly chosen subsets of Passport I and Passport II due to memory limitations. Specifically, for Passport I, one third of the image pairs are used with the size 207×180. Note that the wrinkle effect at this resolution may still exist (e.g. Fig. <ref type="figure" target="#fig_6">8 (b)</ref>), but we focus on the study of hierarchical information instead. For Passport II, one fourth of the image pairs are used with the size 160 × 140 (wrinkles almost disappear at this resolution, e.g. Fig. <ref type="figure" target="#fig_6">8 (c)</ref>). The improvement of GOP from GO for different resolutions are summarized in Table <ref type="table" target="#tab_2">3</ref>. The table shows that the improve- 12.5% ment of using pyramid is larger when higher resolution images are used. This validates including hierarchical information in the GOP representation. Study of Irrelevant Wrinkle Related Features. This experiment provides a preliminary study of how wrinkle related regions affect the recognition system. For this task, we use two feature masks (Fig. <ref type="figure" target="#fig_4">6</ref>). One is the usual ellipse mask. The second one removed the forehead and cheeks, which are known to carry much wrinkle information <ref type="bibr" target="#b3">[4]</ref> but little shape information. We tested SVM+GO and SVM+diff with both masks. The performance curves shown in Fig. <ref type="figure" target="#fig_4">6</ref> show that there is no performance drop by removing the wrinkle regions. This suggests that SVM is already learning to ignore these regions. Or it might mean that the noise added by wrinkles balances whatever positive information is there. Right: performance curves, where "+FS" means with feature selection masks (the bottom one on the left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Effects of Age Gaps on Recognition Performance</head><p>We are interested in how age differences affect the performance of machine recognition algorithms. Taking advantage of the large number of image pairs in Passport II, an empirical study of this problem is conducted.</p><p>First, intra-personal image pairs are grouped into four classes according to their age gaps. Specifically, these are groups with ages gaps from 0 to 2 years, 3 to 5 years, 6 to 8 years, and 9 to 11 years. The goal is to test the recognition performance for different groups. Specifically, we use the average equal error rates as a criterion. For each group, 80 intra pairs and 80 extra pairs are randomly selected as training set. Testing set are created similarly but with 15 intra pairs and 15 extra pairs. There is no overlap between training and testing sets. After that, four SVM-based approaches are tested on the data sets and equal error rates are recorded. To reduce the variance caused by the lack of training samples, 20 different training/testing sets are generated and the average equal error rates are recorded. The above experiments have been run 50 times with randomly chosen training/testing sets (i.e., 50 × 20 training/testing sets). Finally, the mean and standard deviation of equal error rates are summarized to evaluate the performance. Fig. <ref type="figure" target="#fig_5">7</ref> shows the performance of the experiments on all four groups. From the plots, we found that faces separated by more than a year are more difficult than those within one year. What surprised us is that the difficulty becomes saturated after the age gap is larger than four years. This phenomenon is observed on all four different representations tested in the experiments. It shows that, when the age gap is larger than four years (up to ten years), the difficulty added by aging is not a dominant factor compared to other nuisance factors such as illumination, expression, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Gradient Orientation Under Aging Progress</head><p>As shown in Sec. 5, gradient orientation yields a very robust representation for faces undergoing aging. Though we have not developed rigorous theoretic support for this, we give some intuitive explanations of why the gradient orien- tation is robust to aging for face recognition tasks.</p><p>The change of skin appearance over time is very complicated <ref type="bibr" target="#b7">[8]</ref>. Discriminative invariance to aging is very difficult to achieve (if it exists). Focusing on aging problem and based on the observation from our datasets, we made following assumptions. 1) We assume that all faces are frontal and there is no alignment problem. With this assumption, we can ignore viewpoint issues in image formation. 2) Reflectance can be modelled as Lambertian, which has been widely and successfully done for face recognition tasks. 3) We are interested in faces with age ranging from 20 to 70. All these assumptions are roughly satisfied by the passport photos used in our experiments.</p><p>With the above assumptions, we model a human face as a surface ξ(x, t) = {n(x, t), ρ(x, t) : x ∈ Ω} described by surface normals n(x, t) and albedos ρ(x, t) at age or time t ≥ 0. For recognition tasks, Ω ⊂ R<ref type="foot" target="#foot_1">2</ref> is the shared support for both the face surface and the corresponding image plane. With the Lambertian model, the image I(x, t) of a face ξ(x, t) is generated by the following formula</p><formula xml:id="formula_7">I(x, t) = ρ(x, t) n(x, t), l(t) ∀x ∈ Ω, t ≥ 0<label>(8)</label></formula><p>where l(t) . = i l i (t) ∈ R<ref type="foot" target="#foot_2">3</ref> , and l i (t) are different lighting sources. Next, we will discuss how n and ρ change over time with respect to recognition tasks.</p><p>For the task of face recognition, we can roughly treat the surface normal n(x, t) as consistent over age gap ∆t (e.g. ∆t = 10 in our experiments). This is based on the following arguments. First, previous study of craniofacial growth <ref type="bibr" target="#b17">[18]</ref> has shown that face profiles (e.g. boundary shapes, eye locations) are relatively stable for people older than 18 year. This motivates assumption 3. Second, it has been shown that wrinkles are one of the most important factors for the visual perception of age <ref type="bibr" target="#b0">[1]</ref>. In fact, the width and depth of wrinkles grow roughly linearly with age <ref type="bibr" target="#b0">[1]</ref>. While wrinkles do affect the surface normals, the effects are largely reduced due to following facts.</p><p>• Wrinkles are hardly perceptible for images with low resolution <ref type="bibr" target="#b9">[10]</ref>, which are used for most recognition tasks. For example, the wrinkles on the face in Fig. <ref type="figure" target="#fig_6">8</ref> "disappear" when the resolutions decreases.</p><p>• The distribution of wrinkles on human faces is known to be nonhomogeneous. For example, Boissieux et al. <ref type="bibr" target="#b3">[4]</ref> summarized eight generic wrinkle maps from the qualitative data from L'Oreal. From the studies in cosmetics, we know that wrinkles appear frequently on forehead and cheeks, which are not important for face recognition. This means that most regions with heavy wrinkles can be ignored by face recognition algorithms, either through manually adjusted masks or by automatic feature selection (e.g. through SVM).</p><p>One support comes from the experiment in Sec. 5. It compares face recognition with forehead and cheeks to recognition without them, and observes no significant difference in recognition performance.</p><p>As a result, we can roughly write n(x, t) ≈ n(x) and simplify (8) as 2</p><formula xml:id="formula_8">I(x, t) ≈ ρ(x, t) n(x), l(t) ∀x ∈ Ω, t ≥ 0 . (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>The gradient orientation is known to be robust under the Lambertian lighting model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>, which is true if we assume ρ is independent of t. Alternatively, if ρ(x, t) is a homogeneous scaling function across t, then the gradient of I(x, t) is invariant across t. Specifically, we want ρ to have the following decomposition</p><formula xml:id="formula_10">ρ(x, t) ≈ a(t)ρ 0 (x)<label>(10)</label></formula><p>where a(t) is the positive scaling function and ρ 0 (x) is a "reference" albedo (equivalently, <ref type="bibr" target="#b9">(10)</ref> can also be written as ρ(x, t) ≈ a(tt 0 )ρ(x, t 0 )). Next we will show the support for <ref type="bibr" target="#b9">(10)</ref> from studies of skin optics. Studies in skin optics <ref type="bibr" target="#b7">[8]</ref> show that skin color is mainly determined by two kind of chromophores, melanin and hemoglobin. Tsumura et al. <ref type="bibr" target="#b21">[22]</ref> used ICA to separate the effect of melanin and hemoglobin as two independent components of the logarithm of skin color. By adjusting the amount of the two components, their approach is successfully applied to skin color synthesis tasks <ref type="bibr" target="#b22">[23]</ref>. With their model, the skin color c(x) ∈ R 3 can be derived as 3 c(x, t) = c p(x,t) 1 c q(x,t) 2 c 3 <ref type="bibr" target="#b10">(11)</ref> where c 1 , c 2 ∈ R 3 correspond to melanin and hemoglobin respectively. p and q correspond to the amount of associated pigments at time t and are mutually independent. c 3 denotes a constant shift. As a result, the change of skin color over time is roughly determined by the change of p and q.</p><p>In tasks where age gaps are limited (e.g. t ≤ 10 in our passport verification task, Sec. 5.1), the change of q is small. This is because hemoglobin reflects blood color that is relative stable. As a result, we approximate q with, q(x, t) ≈ q(x). Under the same assumption, and considering the fact that the change of melanin is mainly due to exposure to sunlight (for healthy skin), we approximately treat the change of p as additive, p(x, t) ≈ p 0 (x) + p 1 (t). Therefore, <ref type="bibr" target="#b10">(11)</ref> can be approximated as c(x, t) ≈ c </p><p>where a(t) . = c p1(t) 1 and c 0 (x) . = c 1 (x) p0(x) c q(x) 2 c 3 . Equation ( <ref type="formula" target="#formula_11">12</ref>) is close to <ref type="bibr" target="#b9">(10)</ref>. We expect future works to reveal deeper connections between this decomposition and the aging insensitivity of gradient orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and Discussion</head><p>In this paper we studied the problem of face recognition with age variation. First, we proposed a robust face descriptor, the gradient orientation pyramid, for face verification tasks across ages. Compared to previously used descriptors such as image intensity, the new descriptor is more robust and performs well on face images with large age differences. In addition, the pyramid technique enables the descriptor to capture hierarchical facial information. In our experiments with comparison to several techniques, the new approach demonstrated very promising results on two challenging passport databases. Second, the effect of the aging process on recognition algorithms are studied empirically. In the experiments we observed that the difficulty of face recognition algorithms saturated after the age gap is larger than four years (up to ten years).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Typical images with age differences. Source images are from the FG-NET Aging Database, http://sting.cycollege.ac.cy/˜alanitis/fgnetaging/index.htm.</figDesc><graphic coords="1,315.66,207.23,221.38,91.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Computation of a GOP from an input image. Note: 1) In the right figure, the gradient orientations at "flat" regions are excluded. 2) The right figure is made brighter for better illustration.</figDesc><graphic coords="3,315.64,23.52,220.99,56.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Distribution of age differences in the passport image databases. Left: Passport I. Right: Passport II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. CRR-CAR curves for three-fold cross validation experiments. Top: on Passport I. Bottom: on Passport II. This figure is better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Experiment on irrelevant wrinkle region selection. Left: two masks with resolution 72×63 (enlarged for better illustration). Right: performance curves, where "+FS" means with feature selection masks (the bottom one on the left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Effect of aging on recognition performance. The curves are shifted a bit along the x axis for better illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Perception of wrinkles at different resolution (in pixels) of a face image at age 50. The resolutions in (b-e) will be used in our experiments (see Sec. 5). (a) Original image, height=347. (b) Height=207. (c) Height=160. (d) Height=96. (e) Height=72.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Passport datasets for identification tasks. "Std." is short for standard deviation.</figDesc><table><row><cell cols="7">Dataset # intra mean age std. age mean</cell><cell>std.</cell></row><row><cell></cell><cell></cell><cell>pairs</cell><cell></cell><cell></cell><cell></cell><cell>age diff. age diff.</cell></row><row><cell cols="3">Passport I 452</cell><cell>39</cell><cell></cell><cell>10</cell><cell>4.27</cell><cell>2.9</cell></row><row><cell cols="3">Passport II 1824</cell><cell>48</cell><cell></cell><cell>14.7</cell><cell>7.45</cell><cell>3.2</cell></row><row><cell></cell><cell></cell><cell>Passport I</cell><cell></cell><cell></cell><cell></cell><cell>Passport II</cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell>800</cell><cell></cell></row><row><cell>number of pairs</cell><cell>50 100 150</cell><cell></cell><cell>number of pairs</cell><cell>200 400 600</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>[0,2] age difference (years) (2,4] (4,7] (7,9]</cell><cell></cell><cell>0</cell><cell>[0,2]</cell><cell>(2,4] age difference (years) (4,7] (7,9]</cell><cell>&gt;9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Equal error rates. Left table: experiments of three-fold cross validation. Right table: experiments using 200 intra-and 200 extra-pairs as training, as in [19].</figDesc><table><row><cell cols="7">GO [5] SVM+diff [17] SVM+G SVM+GO SVM+GOP Vendor A Vendor B</cell><cell cols="2">SVM+GOP Bayesian [19]</cell></row><row><cell>Pass. I 17.6%</cell><cell>16.5%</cell><cell>17.8%</cell><cell>9.5%</cell><cell>8.9%</cell><cell>9.5%</cell><cell>11.5%</cell><cell>5.1%</cell><cell>8.5%</cell></row><row><cell>Pass. II 20.7%</cell><cell>18.8%</cell><cell>17.4%</cell><cell>12.0%</cell><cell>11.2%</cell><cell>13.5%</cell><cell>8.0%</cell><cell>10.8%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The improvement of GOP from GO. The content inside each cell is the difference of equal error rates from GO to GOP.</figDesc><table><row><cell cols="2">Reso. 96 × 84 207 × 180</cell><cell cols="3">Reso. 160 × 140 72 × 63</cell></row><row><cell>Pass. I 0.6%</cell><cell>1.9%</cell><cell>Pass. II</cell><cell>0.8%</cell><cell>1.6%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Anonymous due to agreements with the companies.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Strictly speaking, we should use Ω 0 ⊂ Ω in (9) for feature selection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In<ref type="bibr" target="#b10">(11)</ref> and<ref type="bibr" target="#b11">(12)</ref> the product between vectors are element wise.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Rama Chellappa for helpful comments and Sameer Shirdhonkar for help on experiments. Ling, Ramanathan, and Jacobs were supported in part by a fellowship from Apptis Inc.. Ling and Soatto were supported in part by ONR 67F-1080868 and NSF ECS-0622245.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Age-related changes in skin wrinkles assessed by a novel three-dimensional morphometric analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Akazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kazama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Osanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Imokawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British J. of Dermatology</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="689" to="695" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Axioms and fundamental equations of image processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guichard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Lions</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer Berlin / Heidelberg</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Strategies of Robust Object Recognition for the Automatic Identification of Human Faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bichsel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>ETH Zurich</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simulation of skin aging and wrinkles with cosmetics insight</title>
		<author>
			<persName><forename type="first">L</forename><surname>Boissieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Wksp Animation &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In search of Illumination Invariants</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, I</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nonlinear Image Representation via Local Multiscale Orientation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno>, TR2005-875</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Courant Institute Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Appearance of Human Skin</title>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<idno>Uni- versity CUCS-024-05</idno>
		<imprint>
			<date type="published" when="2005-06">Jun, 2005</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Columbia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Support vector machines for face authentication</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Age classification from facial images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitoria</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward Automatic Simulation of Aging Effects on Face Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparing Different Classifiers for Automatic Age Estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Draganova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christodoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. SMC B</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="621" to="628" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing imprecisely localized, partially occluded and expression variant faces from a single sample per class</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="748" to="763" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond Eigenfaces: Probabilistic Matching for Face Recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic Visual Learning for Object Representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Support vector machines applied to face recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="803" to="809" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aging faces as viscal-elastic events: Implications for a theory of nonrigid shape perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pittenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Exp. Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="374" to="382" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face Verification across Age Progression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3349" to="3361" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling Age Progression in Young Faces</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="387" to="394" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Multi-Resolution Dynamic Model for Face Aging Simulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Independent component analysis of skin color image</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tsumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haneishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2169" to="2176" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-based skin color and texture analysis/synthesis by extracting hemoglobin and melanin information in the skin</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tsumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="770" to="779" />
			<date type="published" when="2003">2003. 2003</date>
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Event-Based Analysis of Video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comp. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image Based Regression Using Boosting Method</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="541" to="548" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
