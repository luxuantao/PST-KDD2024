<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-13">13 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
							<email>lvshangwen@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Yingqi Qu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<email>liujing46@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">‡</forename><surname>Qiaoqiao She</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wu_hua@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-13">13 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.06027v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques <ref type="bibr" target="#b6">(Guu et al., 2020;</ref><ref type="bibr" target="#b12">Karpukhin et al., 2020)</ref>. As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includ- * Equal contribution.</p><p>† The work was done when Ruiyang Ren was doing internship at Baidu.</p><p>‡ Corresponding authors. 1 Our code is available at https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2021-PAIR</p><formula xml:id="formula_0">q p - p + (a) q p - p + (b)</formula><p>Figure <ref type="figure">1</ref>: An illustrative case of a query q, its positive passage p + and negative passage p − : (a) Query-centric similarity relation enforces s(q, p + ) &gt; s(q, p − ); (b) Passage-centric similarity relation further enforces s(p + , q) &gt; s(p + , p − ), where s(p + , q) = s(q, p + ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering <ref type="bibr" target="#b16">(Lee et al., 2019;</ref><ref type="bibr" target="#b30">Xiong et al., 2020b)</ref>, information retrieval <ref type="bibr" target="#b18">(Luan et al., 2021;</ref><ref type="bibr" target="#b13">Khattab and Zaharia, 2020)</ref>, dialogue <ref type="bibr" target="#b10">(Ji et al., 2014;</ref><ref type="bibr" target="#b8">Henderson et al., 2017)</ref> and entity linking <ref type="bibr" target="#b5">(Gillick et al., 2019;</ref><ref type="bibr" target="#b28">Wu et al., 2020)</ref>.</p><p>Typically, the dual-encoder architecture is used to learn the dense representations of queries and passages, and the dot-product similarity between the representations of queries and passages becomes ranking measurement for retrieval. A number of studies have been devoted to improving this architecture <ref type="bibr" target="#b6">(Guu et al., 2020;</ref><ref type="bibr" target="#b12">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b29">Xiong et al., 2020a)</ref> for dense passage retrieval. Previous studies mainly consider learning query-centric similarity relation, where it tries to increase the similarity s(q, p + ) between a query and a positive (i.e., relevant) passage meanwhile decrease the similarity s(q, p − ) between the query and a negative (i.e., irrelevant) passage. We argue that query-centric similarity relation ignores the relation between passages, and it brings difficulty to discriminate between positive and negative passages. To illustrate this, we present an example in Figure <ref type="figure">1</ref>, where a query q and two passages p + and p − are given. As we can see in Figure <ref type="figure">1</ref>(a), although query-centric similarity relation can enforce s(q, p + ) &gt; s(q, p − ) and identify the positive passages in this case, the distance (i.e., dissimilarity) between positive and negative passages is small. When a new query is issued, it is difficult to discriminate between positive passage p + and negative passage p − .</p><p>Considering this problem, we propose to further learn passage-centric similarity relation for enhancing the dual-encoder architecture. The basic idea is shown in Figure <ref type="figure">1</ref>(b), where we set an additional similarity relation constraint s(p + , q) &gt; s(p + , p − ): the similarity between query q and positive passage p + should be larger than that between positive passage p + and negative passage p − . In this way, it is able to better learn the similarity relations among query, positive passages and negative passages. Although the idea is appealing, it is not easy to implement due to three major issues. First, it is unclear how to formalize and learn both query-centric and passage-centric similarity relations. Second, it requires large-scale and highquality training data to incorporate passage-centric similarity relation. However, it is expensive to manually label data. Additionally, there might be a large number of unlabeled positives even in the existing manually labeled datasets <ref type="bibr" target="#b25">(Qu et al., 2020)</ref>, and it is likely to bring false negatives when sampling hard negatives. Finally, learning passagecentric similarity relation (an auxiliary task) is not directly related to the query-centric similarity relation (a target task). In terms of multi-task viewpoint, multi-task models often perform worse than their single-task counterparts <ref type="bibr" target="#b0">(Alonso and Plank, 2017;</ref><ref type="bibr" target="#b21">McCann et al., 2018;</ref><ref type="bibr" target="#b1">Clark et al., 2019)</ref>. Hence, it needs a more elaborate design for the training procedure.</p><p>To this end, in this paper, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. In order to address the aforementioned issues, we have made three important technical contributions. First, we design formal loss functions to characterize both query-centric and passage-centric similarity relations. Second, we propose to generate pseudolabeled data via knowledge distillation. Third, we devise a two-stage training procedure that utilizes passage-centric similarity relation during pre-training and then fine-tunes the dual-encoder according to the task goal. The improvements in the three aspects make it possible to effectively leverage both kinds of similarity relations for improving dense passage retrieval.</p><p>The contributions of this paper can be summarized as follows:</p><p>• We propose an approach that simultaneously learns query-centric and passage-centric similarity relations for dense passage retrieval. It is the first time that passage-centric similarity relation has been considered for this task.</p><p>• We make three major technical contributions by introducing formal formulations, generating high-quality pseudo-labeled data and designing an effective training procedure.</p><p>• Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, dense passage retrieval has demonstrated better performance than traditional sparse retrieval methods (e.g., TF-IDF and BM25). Different from sparse retrieval, dense passage retrieval represents queries and passages into lowdimensional vectors <ref type="bibr" target="#b6">(Guu et al., 2020;</ref><ref type="bibr" target="#b12">Karpukhin et al., 2020)</ref>, typically in a dual-encoder architecture, and uses dot product as the similarity measurement for retrieval. The existing approaches for dense passage retrieval can be divided into two categories: (1) unsupervised pre-training for retrieval (2) fine-tuning only on labeled data.</p><p>In the first category, different pre-training tasks for retrieval were proposed. <ref type="bibr" target="#b16">Lee et al. (2019)</ref> proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly finetuned the retriever and a reader on labeled data. REALM <ref type="bibr" target="#b6">(Guu et al., 2020)</ref> proposed a new pretraining approach, which jointly trained a masked language model and a neural retriever. Different from them, our proposed approach utilizes the pseudo-labeled data via knowledge distillation in the pre-training stage, and the quality of the generated data is high (see Section 4.6).</p><p>In the second category, the existing approaches fine-tuned pre-trained language models on labeled data <ref type="bibr" target="#b12">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b18">Luan et al., 2021)</ref>. Both DPR <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> and ME-BERT <ref type="bibr" target="#b18">(Luan et al., 2021)</ref> used in-batch random sampling and hard negative sampling by BM25, while ANCE <ref type="bibr" target="#b29">(Xiong et al., 2020a)</ref>, NPRINC <ref type="bibr" target="#b17">(Lu et al., 2020)</ref> and RocketQA <ref type="bibr" target="#b25">(Qu et al., 2020)</ref> explored more sophisticated hard negative sampling approach. <ref type="bibr" target="#b9">Izacard and Grave (2020)</ref> and <ref type="bibr" target="#b34">Yang et al. (2020)</ref> leveraged a reader and a crossencoder for knowledge distillation on labeled data, respectively. RocketQA found large batch size can significantly improve the retrieval performance of dual-encoders. ColBERT <ref type="bibr" target="#b13">(Khattab and Zaharia, 2020)</ref> incorporated light-weight attention-based re-ranking while increasing the space complexity.</p><p>The existing studies mainly focus on learning the similarity relation between the queries and the passages, while ignoring the relation among passages. It makes the model difficult to discriminate the positive passages and negative passages. In this paper, we propose an approach simultaneously learn query-centric and passage-centric similarity relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present an approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The task of dense passage retrieval <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> is described as follows. Given a query q, we aim to retrieve k most relevant passages {p j } k j=1 from a large collection of M passages. For this task, the dual-encoder architecture is widely adopted <ref type="bibr" target="#b12">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b25">Qu et al., 2020)</ref>, where two separate encoders E Q (•) and E P (•) are used to represent the query q and the passage p into d-dimensional vectors in different representation spaces. Then a dot product is performed to measure the similarity between q and p based on their embeddings:</p><formula xml:id="formula_1">s(q, p) = E Q (q) • E P (p).</formula><p>(1)</p><p>Previous studies mainly capture the query-centric similarity relation. As shown in Figure <ref type="figure">1</ref>, passagecentric similarity relation reflects important evidence for improving the retrieval performance. Therefore, we extend the original query-centric learning framework by leveraging the passagecentric similarity relation.</p><p>To develop our approach, we need to address the issues described in Section 1, and we consider three aspects to extend. First, we design a new loss function that considers both query-centric and passage-centric similarity relations. Second, we utilize knowledge distillation to obtain large-scale and high-quality pseudo-labeled data to capture more comprehensive similarity relations. Third, we design a two-stage training procedure to effectively learn the passage-centric similarity relation and improve the final retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Defining the Loss Functions</head><p>Our approach considers two kinds of losses, namely query-centric loss and passage-centric loss, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The two kinds of losses are characterized by the two different similarity relations, query-centric similarity relation and passage-centric similarity relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-centric Loss</head><p>The query-centric similarity relation regards the query q as the center and pushes the negative passages p − farther than the positive passages p + . That is:</p><formula xml:id="formula_2">s (Q) (q, p + ) &gt; s (Q) (q, p − ) ,<label>(2)</label></formula><p>where s (Q) (q, p + ) and s (Q) (q, p − ) represent the similarities for the relevant and irrelevant passages to query q, and they are defined the same as s(q, p) in Eq. (1). Following <ref type="bibr" target="#b12">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b25">Qu et al., 2020)</ref>, we learn the query-centric similarity relation by optimizing query-centric loss that is the negative log likelihood of the positive passage:</p><formula xml:id="formula_3">LQ = − 1 N q,p+ log e s (Q) (q,p + ) e s (Q) (q,p + ) + p − e s (Q) (q,p − ) .</formula><p>(3)</p><p>As shown in Figure <ref type="figure">1</ref>, for a given query, there might exist some negative passages similar to the positive passage, making it difficult to discriminate between positive and negative passages. Hence, we further incorporate passage-centric loss to address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passage-centric Loss</head><p>The aim of learning passage-centric similarity relation is to push negative passage p − farther from positive passage p + , and making the similarity between positive passage p + and query q larger than the similarity between positive passage p + and negative passage  p − . Formally, we introduce the following passagecentric similarity relation:</p><formula xml:id="formula_4">s (P ) (p + , q) &gt; s (P ) (p + , p − ),<label>(4)</label></formula><p>where s (P ) (p + , q) and s (P ) (p + , p − ) are defined as</p><formula xml:id="formula_5">E P (p + ) •E Q (q) and E P (p + ) •E P (p − ), respec- tively.</formula><p>Similarly, we learn the passage-centric similarity relation by optimizing the passage-centric loss function that is the negative log likelihood of the query:</p><formula xml:id="formula_6">LP = − 1 N q,p+</formula><p>log e s (P ) (p + ,q) e s (P ) (p + ,q) + p − e s (P ) (p + ,p − ) .</p><p>(5)</p><p>By comparing Eq. (3) and Eq. ( <ref type="formula">5</ref>), we can observe that the difference in two kinds of loss lies in the normalization part (underlined).</p><p>The Combined Loss We present an illustrative sketch of the above two loss functions in Figure <ref type="figure" target="#fig_0">2</ref>. Next, we propose to simultaneously learn both query-centric and passage-centric similarity relations in Eq.( <ref type="formula" target="#formula_2">2</ref>) and Eq.( <ref type="formula" target="#formula_4">4</ref>). Therefore, we combine query-centric and passage-centric loss functions defined in Eq. ( <ref type="formula">3</ref>) and ( <ref type="formula">5</ref>) to obtain the final loss function:</p><formula xml:id="formula_7">L = (1 − α) * L Q + α * L P , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where α is a hyper-parameter and is tuned in experiments. By considering passage-centric similarity relation, our approach will be more capable of discriminating between a positive passage and a highly similar yet irrelevant passage See Dual-encoder with Shared Parameters Most of the existing studies (Eq. ( <ref type="formula" target="#formula_2">2</ref>)) equip the dualencoders with two separate encoders (E Q and E P )</p><p>for queries and passages, respectively. In this case, different encoders may project queries and passages into two different spaces. However, to simultaneously model the query-centric similarity relation and the passage-centric similarity relation, the representations of queries and passages should be in the same space. Otherwise, the similarity between passages and the similarity between queries and passages are not comparable. Therefore, we propose using the encoders that share the same parameters and structures for both queries and passages, i.e., E Q (•)=E P (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating the Pseudo-labeled Training Data via Knowledge Distillation</head><p>By optimizing both query-centric loss and passage-centric loss, we can capture more comprehensive similarity relations. However, more similarity relation constraints require large-scale and high-quality training data for optimization. Additionally, there might be a large number of unlabeled positives even in the existing manually labeled datasets <ref type="bibr" target="#b25">(Qu et al., 2020)</ref>, and it is likely to bring false negatives when sampling hard negatives. Hence, we propose to generate pseudolabeled training data via knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-encoder Teacher Model</head><p>The teacher model is used to generate large-scale pseudolabeled data. Following RocketQA <ref type="bibr" target="#b25">(Qu et al., 2020)</ref>, we adopt the cross-encoder architecture to implement the teacher, which takes as input the concatenation of query and passage and models the semantic interaction between query and passage representations. Such an architecture has been demonstrated to be more effective than the dual-encoder architecture in characterizing querypassage relevance <ref type="bibr" target="#b34">(Yang et al., 2020)</ref>. We follow <ref type="bibr" target="#b25">Qu et al. (2020)</ref> to train the cross-encoder teacher with the labeled data.</p><p>Generating Pseudo Labels In this paper, we follow <ref type="bibr" target="#b25">Qu et al. (2020)</ref> to obtain positives and hard negatives<ref type="foot" target="#foot_0">2</ref> for unlabeled queries<ref type="foot" target="#foot_1">3</ref> . First, we retrieve the top-k candidate passages of unlabeled queries from the corpus by an efficient retriever DPR <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref>, and score them by the well-trained cross-encoder (i.e., teacher model). We set two values s pos and s neg (s pos &gt; s neg ) as the positive and hard negative thresholds, Dataset #q in train #q in dev #q in test #p Here, "q" and "p" are the abbreviations of queries and passages, respectively.</p><p>respectively. Then, given each query, a candidate passage with a score above s pos or below s neg will be considered as positive or negative. Note that we also apply this on labeled corpus to obtain more positives and reliable hard negatives.</p><p>Because there might be a large number of unlabeled positives even in the existing manually labeled datasets <ref type="bibr" target="#b25">(Qu et al., 2020)</ref> and it is likely to bring false negatives in hard negative sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Two-stage Training Procedure</head><p>Although passage-centric similarity relation Eq. ( <ref type="formula">5</ref>) is able to incorporate additional relevance evidence, it is not directly related to the final task goal (i.e., query-centric similarity relation).</p><p>Therefore, we design a two-stage training procedure that incorporates the passage-centric loss in the pre-training stage, and then only optimize the tasks-specific loss (i.e., query-centric loss) in the fine-tuning stage. We present an illustration for the two-stage training procedure in Figure <ref type="figure" target="#fig_2">3</ref>. Next, we present the detailed training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>In the pre-training stage, we train the dual-encoder by optimizing the loss function L in Eq. ( <ref type="formula" target="#formula_7">6</ref>) (i.e., a combination of query-centric loss and passage-centric loss). The pseudo-labeled data from unlabeled corpus is adopted as the pretraining data (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning</head><p>In the fine-tuning stage, we only fine-tune the dual-encoder (pre-trained in the first stage) according to the query-centric loss L Q in Eq. ( <ref type="formula">3</ref>). In this way, our approach focuses on learning the task-specific loss, yielding better retrieval performance. In this stage, we use both ground-truth labels and pseudo labels derived from the labeled corpus for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe the experimental settings, then report the main experimental results, ablation study and detailed analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets This paper focuses on the passage retrieval task. We conduct experiments on two public datasets: MSMARCO <ref type="bibr" target="#b22">(Nguyen et al., 2016)</ref> and Natural Questions <ref type="bibr" target="#b15">(Kwiatkowski et al., 2019)</ref>. The statistics of the datasets are listed in Table 1. MSMARCO was originally designed for multiple passage machine reading comprehension, and its queries were sampled from Bing search logs. Based on the queries and passages in MS-MARCO Question Answering, a dataset for passage retrieval and ranking was created, namely MSMARCO Passage Ranking. Natural Questions (NQ) was originally introduced as a dataset for open-domain QA. The queries were collected from Google search logs. DPR <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> selected the queries that had short answers, and processed all the Wikipedia articles as the collection of passages. In our experiments, we reuse the version of NQ created by DPR.</p><p>Evaluation Metrics Following previous work, we adopt Mean Reciprocal Rank (MRR) and Recall at top k ranks (Recall@k) to evaluate the performance of passage retrieval. MRR calculates the averaged reciprocal of the rank at which the first positive passage is retrieved. Recall@k calculates the proportion of questions to which the top k retrieved passages contain positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unlabeled Corpus To obtain the augmenta-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLM MSMARCO Dev</head><p>Natural Questions Test MRR@10 R@50 R@1000 R@5 R@20 R@100 BM25 (anserini) <ref type="bibr">(Yang et al.,</ref>  tion data, we collect about 1.8 million unlabeled queries from Yahoo! Answers 4 , OR-CAS <ref type="bibr" target="#b2">(Craswell et al., 2020)</ref>, SQuAD <ref type="bibr">(Rajpurkar et al.)</ref>, TriviaQA <ref type="bibr" target="#b11">(Joshi et al., 2017)</ref> and Hot-potQA <ref type="bibr" target="#b33">(Yang et al., 2018)</ref>. In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We conduct experiments with the deep learning framework PaddlePaddle <ref type="bibr" target="#b19">(Ma et al., 2019)</ref> on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained LMs</head><p>The dual-encoder is initialized with the parameters of ERNIE-2.0 base <ref type="bibr" target="#b27">(Sun et al., 2020)</ref>. ERNIE-2.0 has the same networks as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross-encoder in RocketQA <ref type="bibr" target="#b25">(Qu et al., 2020)</ref> Hyper-parameters (a) batch size: Our dualencoder is trained with a batch size of 512 × 1 in fine-tuning stage on NQ and 512 × 8 in other settings. We use the in-batch negative setting <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> on NQ and crossbatch negative setting <ref type="bibr" target="#b25">(Qu et al., 2020)</ref>  tives and hard negatives: The ratio of the positive to the hard negative is set to 1:4 on dual-encoder.</p><p>Optimizers We use LAMB optimizer <ref type="bibr" target="#b34">(You et al., 2020)</ref> to train the dual-encoder on MS-MARCO, which is more suitable in cross-batch negative setting. In other settings, we always use ADAM optimizer <ref type="bibr" target="#b14">(Kingma and Ba, 2015)</ref>.</p><p>The choice of alpha α is a hyper-parameter to balance the query-centric loss and passage-centric loss (Eq. ( <ref type="formula" target="#formula_7">6</ref>)). We searched for α from 0 to 1 by setting an equal interval to 0.1, and the model achieves the best performance when α is set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Experimental Results</head><p>We consider both sparse and dense passage retrievers for baselines. The sparse retrievers include the traditional retriever BM25 <ref type="bibr" target="#b31">(Yang et al., 2017)</ref>, and four traditional retrievers enhanced by neural networks, including doc2query <ref type="bibr" target="#b24">(Nogueira et al., 2019b)</ref>, DeepCT <ref type="bibr" target="#b3">(Dai and Callan, 2019)</ref>, docTTTTTquery <ref type="bibr" target="#b23">(Nogueira et al., 2019a)</ref> and GAR <ref type="bibr" target="#b20">(Mao et al., 2020)</ref>. Both doc2query and docTTTTTquery employ neural query generation to expand documents. In contrast, GAR employs neural generation models to expand queries. Different from them, DeepCT utilizes BERT to learn the term weight. The dense passage retrievers include DPR <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref>, DPR-E, ANCE <ref type="bibr" target="#b29">(Xiong et al., 2020a)</ref>, ME-BERT <ref type="bibr" target="#b18">(Luan et al., 2021)</ref>, NPRINC <ref type="bibr" target="#b17">(Lu et al., 2020)</ref>, Col-BERT <ref type="bibr" target="#b13">(Khattab and Zaharia, 2020)</ref> and Rock-etQA <ref type="bibr" target="#b25">(Qu et al., 2020)</ref>. DPR-E is our implementation of DPR using ERNIE <ref type="bibr" target="#b27">(Sun et al., 2020)</ref> Methods R@5 R@20 R@100 instead of BERT, to examine the effects of pretrained LMs.</p><p>Table <ref type="table" target="#tab_2">2</ref> presents the main experimental results.</p><p>(1) We can see that PAIR significantly outperforms all the baselines on both MSMARCO and NQ datasets. The major difference between our approach and baselines lies in that we incorporate both query-centric and passage-centric similarity relations, which can capture more comprehensive semantic relations. Meanwhile, we incorporate the augmented data via knowledge distillation.</p><p>(2) We notice that baseline methods use different pre-trained LMs, as shown in the second column of Table <ref type="table" target="#tab_2">2</ref>. In PAIR, we use the ERNIE-base. To examine the effects of ERNIE-base, we implement DPR-E by replacing BERT-base used in DPR as ERNIE-base. From Table <ref type="table" target="#tab_2">2</ref>, we can observe that PAIR significantly outperforms DPR-E, although they employ the same pre-trained LM.</p><p>(3) Another observation is that the dense retrievers are overall better than the sparse retrievers. Such a finding has also been reported in prior studies <ref type="bibr" target="#b12">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b29">Xiong et al., 2020a;</ref><ref type="bibr" target="#b18">Luan et al., 2021)</ref>, which indicates the effectiveness of the dense retrieval approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we conduct ablation study to examine the effectiveness of each strategy in our proposed approach. We only report the results on the NQ, while the results on the MSMARCO are similar and omitted here due to limited space.</p><p>Here, we consider five variants based on our approach for comparison: Table <ref type="table" target="#tab_3">3</ref> presents the results for the ablation study. We can observe the following findings:</p><p>• The performance drops in w/o PSR, demonstrating the effectiveness of learning passagecentric similarity relation;</p><p>• The performance drops in w/o KD, demonstrating the necessity and effectiveness of the knowledge distillation for obtaining large-scale and high-quality pseudo-labeled data, since the passage-centric loss tries to distinguish highly similar but semantically different passages;</p><p>• The performance slightly drops in w/ PSR FT, because passage-centric loss is not directly related to the target task (i.e., query-based retrieval), which suggests that passage-centric loss should be only used in the pre-training stage;</p><p>• The performance drops in w/o SP, demonstrating the effectiveness of dual-encoders with shared parameters;</p><p>• The performance significantly drops in w/o PT, demonstrating the importance of our pretraining procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis on Passage-centric Similarity Relation</head><p>The previous results demonstrate the effectiveness of our proposed approach PAIR. Here, we further analyze the effect of passage-centric loss (Eq. ( <ref type="formula">5</ref>)) in a more intuitive way. To examine this, we prepare two variants of our approach, in pigs (swine influenza) and in birds (avian influenza) . . . H5N1 is a subtype virus which can cause illness in humans and many other animal species. A bird-adapted strain of H5N1, called HPAIA (H5N1) for . . .</p><p>Where is gall bladder situated in human body?</p><p>The gall bladder is a small hollow organ where bile is stored . . . In humans, the pear-shaped gall bladder lies :::::::::</p><p>beneath the liver, although the structure and position . . .</p><p>The urinary bladder is a hollow muscular organ in humans and some other animals that collects and stores urine from the kidneys before disposal by urination . . . Table <ref type="table">4</ref>: The comparison of the top-1 passages retrieved by PAIR and PAIR ¬PSR , respectively. The bold words represent the main topics in queries and passages. The :::::::::::::::::::::::::: italic words with wavy underline are the right answers. The words with straight underline among passages have many words in common and may mislead the model PAIR ¬PSR to select the wrong passage. namely the complete PAIR and the variant removing the passage-centric loss (Eq. ( <ref type="formula">5</ref>)) denoted by PAIR ¬PSR .</p><p>We first analyze how the passage-centric similarity relation (PSR) influences the similarity relations among query, positive passage and negative passage. Figure <ref type="figure" target="#fig_4">4</ref> shows the comparison of PAIR and PAIR ¬PSR for computing the similarities of s(p + , p − ) and s(p + , q). We obtain s(p + , p − ) and s(p + , q) by the averaging the similarity of top 100 retrieved passages for each query in the testing data of Natural Questions. We can see that before incorporating passage-centric similarity relation (PSR), s(p + , p − ) is higher than s(p + , q). As a result, the negatives are close to the positives. After incorporating PSR, s(p + , p − ) becomes lower than s(p + , q). It indicates that passage-centric loss pulls positive passages closer to queries and push them farther away from negative passages in the representation space. The comparison result is consistent with passage-level similarity relation in Eq. ( <ref type="formula" target="#formula_4">4</ref>).</p><p>Next, we further present two examples in Table 4 to understand the performance difference between PAIR and PAIR ¬PSR . In the first example, the top-1 passage retrieved by PAIR has the same topic "H1N1" as the query. In contrast, the top-1 passage retrieved by PAIR ¬PSR has an incorrect but highly relevant topic "H5N1". Actually, the sentences among the positive passage (retrieved by PAIR) and the negative passage (retrieved by PAIR ¬PSR ) share many common words. Such a negative passage is likely to mislead the retriever to yield incorrect rankings. Hence, these two passages should be far away from each other in the representation space. This problem cannot be well solved by only considering the query-passage similarity in existing studies. Similar observations can be find from the second example. The top-1 passage retrieved by PAIR has the same topic "gall  bladder" as the query, while the top-1 passage retrieved by PAIR ¬PSR is about "urinary bladder". These results show that passage-centric similarity relations are particularly useful to discriminate between positive and hard negative passages (highly similar to positive passages).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis on Knowledge Distillation</head><p>In this section, we examine the influence of the thresholds on pseudo-labeled data via knowledge distillation, including the data quality and the retrieval performance. We conduct the analyses by using different positive thresholds s pos and negative thresholds s neg (See Section 3.3).</p><p>We first manually evaluate the quality of the pseudo-labeled data via knowledge distillation w.r.t. different threshold settings (i.e., the combinations of s neg and s pos ). For each threshold setting, we randomly select 100 queries, each of which corresponding to a positive passage and a hard-negative passage. In total, we have 4 threshold settings (as shown in Table <ref type="table" target="#tab_6">5</ref>) and 800 querypassage pairs. We ask two experts to manually annotate the query-passage pairs and evaluate the quality of pseudo-labeled data, the Cohen's Kappa of experts is 0.9. As shown in the first two columns of Table <ref type="table" target="#tab_6">5</ref>, we can observe that when s pos = 0.9 and s neg = 0.1, the data quality is relatively good. Additionally, when setting a low value of s pos and a high value of s neg , the data quality becomes worse.</p><p>The last three columns of Table 5 also present the retrieval performance w.r.t. different threshold settings. When choosing a low value of s pos and a high value of s neg , the retrieval performance drops. Hence, our approach is configured with a strict threshold setting (s pos = 0.9, s neg = 0.1) in experiments to achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper presented a novel dense passage retrieval approach that leverages both query-centric and passage-centric similarity relations for capturing more comprehensive semantic relations. To implement our approach, we made three important technical contributions in the loss formulation, training data augmentation and effective training procedure. Extensive results demonstrated the effectiveness of our approach. To our knowledge, it is the first time that passage-centric similarity relation has been considered for dense passage retrieval. We believe such an idea itself is worth exploring in designing new ranking mechanism. In future work, we will design more principle ranking functions and apply current retrieval approach to downstream tasks such as question answering and passage re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ethical Impact</head><p>The technique of dense passage retrieval is effective for question answering, where the majority of questions are informational queries. Semantic crowdedness problem of passages, and term mismatch between questions and passages are typical problems, which bring barriers for the machine to accurately find the information. Our technique contributes toward the goal of asking machines to find the answer passages to natural language questions from a large collection of documents. With these advantages also come potential downsides: Wikipedia or any potential external knowledge source will probably never fully cover the breadth of user questions. The goal is still far from being achieved, and more efforts from the community is needed for us to get there.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the combination of querycentric loss and passage-centric loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1(b) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the proposed two-stage method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on MS-MARCO. (b) training epochs: The number of training epochs is set up to 10 for both pre-training and fine-tuning for dual-encoder. (c) warm-up and learning rate: The learning rate of the dualencoder is set to 3e-5 and the rate of linear scheduling warm-up is set to 0.1. (d) # of posi-4 http://answers.yahoo.com/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The comparison of PAIR and PAIR ¬PSR on s(p + , p − ) and s(p + , q) with standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The detailed statistics of MSMARCO and Natural Questions.</figDesc><table><row><cell>MSMARCO</cell><cell>502,939</cell><cell>6,980</cell><cell>6.837</cell><cell>8,841,823</cell></row><row><cell>Natural Questions</cell><cell>58,812</cell><cell>6,515</cell><cell>3,610</cell><cell>21,015,324</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on MSMARCO and Natural Questions datasets. Note that we copy the results from original papers and we leave it blank if the original paper does not report the result.</figDesc><table><row><cell>2017)</cell><cell>-</cell><cell>18.7</cell><cell>59.2</cell><cell>85.7</cell><cell>-</cell><cell>59.1</cell><cell>73.7</cell></row><row><cell>doc2query (Nogueira et al., 2019b)</cell><cell>-</cell><cell>21.5</cell><cell>64.4</cell><cell>89.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepCT (Dai and Callan, 2019)</cell><cell>-</cell><cell>24.3</cell><cell>69.0</cell><cell>91.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">docTTTTTquery (Nogueira et al., 2019a) -</cell><cell>27.7</cell><cell>75.6</cell><cell>94.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GAR (Mao et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.4</cell><cell>85.3</cell></row><row><cell>DPR (single) (Karpukhin et al., 2020)</cell><cell>BERTbase</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.4</cell><cell>85.4</cell></row><row><cell>DPR-E</cell><cell>ERNIEbase</cell><cell>32.5</cell><cell>82.2</cell><cell>97.3</cell><cell>68.4</cell><cell>80.7</cell><cell>87.3</cell></row><row><cell>ANCE (single) (Xiong et al., 2020a)</cell><cell>RoBERTabase</cell><cell>33.0</cell><cell>-</cell><cell>95.9</cell><cell>-</cell><cell>81.9</cell><cell>87.5</cell></row><row><cell>ME-BERT (Luan et al., 2021)</cell><cell>BERTlarge</cell><cell>34.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NPRINC (Lu et al., 2020)</cell><cell>BERTbase</cell><cell>31.1</cell><cell>-</cell><cell>97.7</cell><cell>73.3</cell><cell>82.8</cell><cell>88.4</cell></row><row><cell>ColBERT (Khattab and Zaharia, 2020)</cell><cell>BERTbase</cell><cell>36.0</cell><cell>82.9</cell><cell>96.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RocketQA (Qu et al., 2020)</cell><cell>ERNIEbase</cell><cell>37.0</cell><cell>85.5</cell><cell>97.9</cell><cell>74.0</cell><cell>82.7</cell><cell>88.5</cell></row><row><cell>PAIR (Ours)</cell><cell>ERNIEbase</cell><cell>37.9</cell><cell>86.4</cell><cell>98.2</cell><cell>74.9</cell><cell>83.5</cell><cell>89.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The ablation study and controlled experiments of different variants of PAIR on Natural Questions.</figDesc><table><row><cell>Complete (PAIR)</cell><cell>74.9</cell><cell>83.5</cell><cell>89.1</cell></row><row><cell>w/o PSR</cell><cell>73.6</cell><cell>83.3</cell><cell>88.8</cell></row><row><cell>w/o KD</cell><cell>70.9</cell><cell>82.7</cell><cell>88.1</cell></row><row><cell>w/ PSR FT</cell><cell>74.6</cell><cell>83.4</cell><cell>89.0</cell></row><row><cell>w/o SP</cell><cell>74.0</cell><cell>83.4</cell><cell>88.9</cell></row><row><cell>w/o PT</cell><cell>73.0</cell><cell>82.8</cell><cell>88.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The data quality and retrieval performance in different thresholds on NQ. Acc pos denotes accuracy of positives and Acc neg denotes accuracy of negatives.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><ref type="bibr" target="#b29">Xiong et al. (2020a)</ref> and<ref type="bibr" target="#b12">Karpukhin et al. (2020)</ref> demonstrate the importance of hard negatives.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">  3  We obtain easy negatives from in-batch sampling.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by the National Key Research and Development Project of China (No.2018AAA0101900), National Natural Science Foundation of China under Grant No. 61872369 and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When is multitask learning effective? semantic sequence prediction under varying data conditions</title>
		<author>
			<persName><forename type="first">Héctor</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alonso</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-03">2017. 2017. April 3-7, 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="44" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bam! born-again multi-task networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5931" to="5937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ORCAS: 20 million clicked query-document pairs for analyzing search</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodo</forename><surname>Billerbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
				<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="2983" to="2989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-21">2019. July 21-25, 2019</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>García-Olano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. 2019. November 3-4, 2019</date>
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">REALM: retrieval-augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002.08909</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient natural language response suggestion for smart reply</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">László</forename><surname>Lukács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balint</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>CoRR, abs/1705.00652</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling knowledge from reader to retriever for question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno>CoRR, abs/2012.04584</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1408.6988</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-30">2017. 2017. July 30 -August 4</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<editor>Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural passage retrieval with improved negative contrast</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hernández Ábrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/2010.12523</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparse, dense, and attentional representations for text retrieval</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="329" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Paddlepaddle: An open-source deep learning platform from industrial practice</title>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Data and Domputing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generation-augmented retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2009.08553</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1806.08730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ms marco: A human-generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">From doc2query to doctttttquery</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Epistemic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note>Online preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2010.08191</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01">2016. November 1-4, 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<idno>CoRR, abs/2007.00808</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions with multi-hop dense retrieval</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><surname>Oguz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2020b. 2009.12756</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-07">2017. August 7-11, 2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural retrieval for question answering with cross-attention supervised data augmentation</title>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2020. 2009.13815</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
