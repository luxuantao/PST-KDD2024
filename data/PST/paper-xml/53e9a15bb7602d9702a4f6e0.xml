<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Elastic-net regularization in learning theory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-01-30">30 January 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christine</forename><surname>De Mol</surname></persName>
							<email>demol@ulb.ac.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and ECARES</orgName>
								<orgName type="institution">Université Libre de Bruxelles</orgName>
								<address>
									<addrLine>Campus Plaine CP 217, Bd du Triomphe</addrLine>
									<postCode>1050</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ernesto</forename><surname>De Vito</surname></persName>
							<email>devito@dima.unige.it</email>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Scienze per l&apos;Architettura</orgName>
								<orgName type="institution" key="instit1">Università di Genova</orgName>
								<orgName type="institution" key="instit2">Stradone Sant&apos;Agostino</orgName>
								<address>
									<addrLine>37</addrLine>
									<postCode>16123</postCode>
									<settlement>Genova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">INFN</orgName>
								<orgName type="institution" key="instit2">Sezione di Genova</orgName>
								<address>
									<addrLine>Via Dodecaneso 33</addrLine>
									<postCode>16146</postCode>
									<settlement>Genova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
							<email>lrosasco@mit.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Center for Biological and Computational Learning</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>43 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Dipartimento di Informatica e Scienze dell&apos;Informazione</orgName>
								<orgName type="institution">Università di Genova</orgName>
								<address>
									<addrLine>Via Dodecaneso 35</addrLine>
									<postCode>16146</postCode>
									<settlement>Genova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Center for Biological and Computational Learning</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>43 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Elastic-net regularization in learning theory</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-01-30">30 January 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">A30E77DC2690D2D7D7B5020498FED062</idno>
					<idno type="DOI">10.1016/j.jco.2009.01.002</idno>
					<note type="submission">Received 4 August 2008 Accepted 9 January 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Learning Regularization Sparsity Elastic net</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a b s t r a c t</head><p>Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by <ref type="bibr">Zou and</ref> Hastie [H. Zou, T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B, 67 <ref type="bibr" target="#b1">(2)</ref> (2005) 301-320] for the selection of groups of correlated variables. To investigate the statistical properties of this scheme and in particular its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combinations of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular ''elastic-net representation'' of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in the above-cited work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the standard framework of supervised learning, that is non-parametric regression with random design. In this setting, there is an input-output pair (X, Y ) ∈ X × Y with unknown probability distribution P, and the goal is to find a prediction function f n : X → Y, based on a training set (X 1 , Y 1 ), . . . , (X n , Y n ) of n independent random pairs distributed as (X, Y ). A good solution f n is such that, given a new input x ∈ X, the value f n (x) is a good prediction of the true output y ∈ Y.</p><p>When choosing the square loss to measure the quality of the prediction, as we do throughout this paper, this means that the expected risk E |Y -f n (X)| 2 is small, or, in other words, that f n is a good approximation of the regression function f * (x) = E [Y | X = x] minimizing this risk.</p><p>In many learning problems, a major goal besides prediction is that of selecting the variables that are relevant to achieving good predictions. In the problem of variable selection we are given a set (ψ γ ) γ ∈Γ of functions from the input space X into the output space Y and we aim at selecting those functions which are needed to represent the regression function, where the representation is typically given by a linear combination. The set (ψ γ ) γ ∈Γ is usually called a dictionary and its elements features. We can think of the features as measurements used to represent the input data, as providing some relevant parametrization of the input space, or as a (possibly overcomplete) dictionary of functions used to represent the prediction function. In modern applications, the number p of features in the dictionary is usually very large, possibly much larger than the number n of examples in the training set. This situation is often referred to as the ''large p, small n paradigm'' <ref type="bibr" target="#b0">[1]</ref>, and a key to obtaining a meaningful solution in such a case is the requirement that the prediction function f n is a linear combination of only a few elements in the dictionary, i.e. f n admits a sparse representation.</p><p>The above setting can be illustrated by two examples of applications we are currently working on and which provide an underlying motivation for the theoretical framework developed in the present paper. The first application is a classification problem in computer vision, namely face detection <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>.</p><p>The training set contains images of faces and non-faces and each image is represented by a very large redundant set of features capturing the local geometry of faces, for example wavelet-like dictionaries or other local descriptors. The aim is to find a good predictor able to detect faces in new images.</p><p>The second application is the analysis of microarray data, where the features are the expression level measurements of the genes in a given sample or patient, and the output is either a classification label discriminating between two or more pathologies or a continuous index indicating, for example, the gravity of an illness. In this problem, besides prediction of the output for examples-to-come, another important goal is the identification of the features that are the most relevant to building the estimator and would constitute a gene signature for a certain disease <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In both applications, the number of features we have to deal with is much larger than the number of examples and assuming sparsity of the solution is a very natural requirement.</p><p>The problem of variable/feature selection has a long history in statistics and it is known that the brute-force approach (trying all possible subsets of features), though theoretically appealing, is computationally unfeasible. A first strategy to overcome this problem is provided by greedy algorithms. A second route, which we follow in this paper, makes use of sparsity-based regularization schemes (convex relaxation methods). The most well-known example of such schemes is probably the so-called Lasso regression <ref type="bibr" target="#b6">[7]</ref> -also referred to in the signal processing literature as Basis Pursuit Denoising <ref type="bibr" target="#b7">[8]</ref> -where a coefficient vector β n is estimated as the minimizer of the empirical risk penalized with the 1 -norm, namely</p><formula xml:id="formula_0">β n = argmin β=(β γ ) γ ∈Γ 1 n n i=1 |Y i -f β (X i )| 2 + λ γ ∈Γ |β γ | ,</formula><p>where f β = γ ∈Γ β γ ψ γ , λ is a suitable positive regularization parameter and (ψ γ ) γ ∈Γ a given set of features. An extension of this approach, called bridge regression, amounts to replacing the 1 -penalty by an p -penalty <ref type="bibr" target="#b8">[9]</ref>. It has been shown that this kind of penalty can still achieve sparsity when p is bigger, but very close to 1 (see <ref type="bibr" target="#b9">[10]</ref>). For this class of techniques, both consistency and computational aspects have been studied. Non-asymptotic bounds within the framework of statistical learning have been studied in several papers <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b9">10]</ref>. A common feature of these results is that they assume that the dictionary is finite (with cardinality possibly depending on the number of examples) and satisfies some assumptions about the linear independence of the relevant features -see <ref type="bibr" target="#b9">[10]</ref> for a discussion on this point -whereas Y is usually assumed to be R. Several numerical algorithms have also been proposed to solve the optimization problem underlying Lasso regression and are based e.g. on quadratic programming <ref type="bibr" target="#b7">[8]</ref>, on the so-called LARS algorithm <ref type="bibr" target="#b17">[18]</ref> or on iterative softthresholding (see <ref type="bibr" target="#b18">[19]</ref> and references therein). Despite its success in many applications, the Lasso strategy has some drawback in variable selection problems where there are highly correlated features and we need to identify all the relevant ones. This situation is of uttermost importance for e.g. microarray data analysis since, as well known, there is a lot of functional dependency between genes which are organized in small interacting networks. The identification of such groups of correlated genes involved in a specific pathology is desirable to make progress in the understanding of the underlying biological mechanisms.</p><p>Motivated by microarray data analysis, Zou and Hastie <ref type="bibr" target="#b19">[20]</ref> proposed the use of a penalty which is a weighted sum of the 1 -norm and the square of the 2 -norm of the coefficient vector β. The first term enforces the sparsity of the solution, whereas the second term ensures democracy among groups of correlated variables. In <ref type="bibr" target="#b19">[20]</ref> the corresponding method is called (naive) elastic net. The method allows selecting groups of correlated features when the groups are not known in advance (algorithms to enforce group sparsity with preassigned groups of variables have been proposed in e.g. <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> using other types of penalties).</p><p>In the present paper we study several properties of the elastic-net regularization scheme for vector-valued regression in a random design. In particular, we prove consistency under some adaptive and non-adaptive choices for the regularization parameter. As concerns variable selection, we assess the accuracy of our estimator for the vector β with respect to the 2 -norm, whereas the prediction ability of the corresponding function</p><formula xml:id="formula_1">f n = f β n is measured by the expected risk E |Y -f n (X)| 2 .</formula><p>To derive such error bounds, we characterize the solution of the variational problem underlying elasticnet regularization as the fixed point of a contractive map and, as a byproduct, we derive an explicit iterative thresholding procedure to compute the estimator. As explained below, in the presence of highly collinear features, the presence of the 2 -penalty, besides enforcing grouped selection, is crucial to ensure stability with respect to random sampling.</p><p>In the remainder of this section, we define the main ingredients for elastic-net regularization within our general framework, discuss the underlying motivations for the method and then outline the main results established in the paper.</p><p>As an extension of the setting originally proposed in <ref type="bibr" target="#b19">[20]</ref>, we allow the dictionary to have an infinite number of features. In such a case, to cope with infinite sums, we need some assumptions on the coefficients. We assume that the prediction function we have to determine is a linear combination of the features (ψ γ ) γ ∈Γ in the dictionary and that the series</p><formula xml:id="formula_2">f β (x) = γ ∈Γ β γ ψ γ (x),</formula><p>converges absolutely for all x ∈ X and for all sequences</p><formula xml:id="formula_3">β = (β γ ) γ ∈Γ satisfying γ ∈Γ u γ β 2 γ &lt; ∞,</formula><p>where u γ are given positive weights. The latter constraint can be viewed as a constraint on the regularity of the functions f β we use to approximate the regression function. For infinite-dimensional sets, as for example wavelet bases or splines, suitable choices of the weights correspond to the assumption that f β is in a Sobolev space (see Section 2 for more details about this point). Such a requirement of regularity is common when dealing with infinite-dimensional spaces of functions, as happens in approximation theory, signal analysis and inverse problems.</p><p>To ensure the convergence of the series defining f β , we assume that</p><formula xml:id="formula_4">γ ∈Γ |ψ γ (x)| 2 u γ</formula><p>is finite for all x ∈ X .</p><p>(</p><formula xml:id="formula_5">)<label>1</label></formula><p>Notice that for finite dictionaries, the series becomes a finite sum and the previous condition as well as the introduction of weights becomes superfluous.</p><p>To simplify the notation and the formulation of our results, and without any loss in generality, we will in the following rescale the features by defining ϕ γ = ψ γ / √ u γ , so that on this rescaled dictionary, f β = γ ∈Γ βγ ϕ γ will be represented by means of a vector βγ = √ u γ β γ belonging to 2 ; condition (1) then becomes γ ∈Γ |ϕ γ (x)| 2 &lt; +∞, for all x ∈ X . From now on, we will only use this rescaled representation and we drop the tilde on the vector β.</p><p>Let us now define our estimator as the minimizer of the empirical risk penalized with a (weighted) elastic-net penalty, that is, a combination of the squared 2 -norm and a weighted 1 -norm of the vector β. More precisely, we define the elastic-net penalty as follows.</p><p>Definition 1. Given a family (w γ ) γ ∈Γ of weights w γ ≥ 0 and a parameter ε ≥ 0, let p ε : 2 → [0, ∞] be defined as</p><formula xml:id="formula_6">p ε (β) = γ ∈Γ (w γ |β γ | + εβ 2 γ )<label>(2)</label></formula><p>which can also be rewritten as</p><formula xml:id="formula_7">p ε (β) = β 1,w + ε β 2 2 , where β 1,w = γ ∈Γ w γ |β γ |.</formula><p>The weights w γ allow us to enforce more or less sparsity on different groups of features. We assume that they are prescribed in a given problem, so that we do not need to explicitly indicate the dependence of p ε (β) on these weights. The elastic-net estimator is defined by the following minimization problem. Definition 2. Given λ &gt; 0, let E λ n : 2 → [0, +∞] be the empirical risk penalized by the penalty p ε (β)</p><formula xml:id="formula_8">E λ n (β) = 1 n n i=1 |Y i -f β (X i )| 2 + λp ε (β),<label>(3)</label></formula><p>and let β λ n ∈ 2 be the or a minimizer of (3) on 2</p><formula xml:id="formula_9">β λ n = argmin β∈ 2 E λ n (β).<label>(4)</label></formula><p>The positive parameter λ is a regularization parameter controlling the trade-off between the empirical error and the penalty. Clearly, β λ n also depends on the parameter ε, but we do not write explicitly this dependence since ε will always be fixed.</p><p>Setting ε = 0 in (3), we obtain as a special case an infinite-dimensional extension of the Lasso regression scheme. On the other hand, setting w γ = 0, ∀γ , the method reduces to 2 -regularized least-squares regression -also referred to as ridge regression -with a generalized linear model. The 1 -penalty has selection capabilities since it enforces sparsity of the solution, whereas the 2 -penalty induces a linear shrinkage on the coefficients leading to stable solutions. The positive parameter ε controls the trade-off between the 1 -penalty and the 2 -penalty.</p><p>We will show that, if ε &gt; 0, the minimizer β λ n always exists and is unique. In the paper we will focus on the case ε &gt; 0. Some of our results, however, still hold for ε = 0, possibly under some supplementary conditions, as will be indicated in due time.</p><p>As previously mentioned one of the main advantages of the elastic-net penalty is that it allows achieving stability with respect to random sampling. To illustrate this property more clearly, we consider a toy example where the (rescaled) dictionary has only two elements ϕ 1 and ϕ 2 with weights w 1 = w 2 = 1. The effect of random sampling is particularly dramatic in the presence of highly correlated features. To illustrate this situation, we assume that ϕ 1 and ϕ 2 exhibit a special kind of linear dependency, namely that they are linearly dependent on the input data X 1 , . . . , X n : ϕ 2 (X i ) = tan θ n ϕ 1 (X i ) for all i = 1, . . . , n, where we have parametrized the coefficient of proportionality by means of the angle θ n ∈ [0, π /2]. Notice that this angle is a random variable since it depends on the input data. Observe that the minimizers of (3) must lie at a tangency point between a level set of the empirical error and a level set of the elastic-net penalty. The level sets of the empirical error are all parallel straight lines with slopecot θ n , as depicted by a dashed line in the two panels of Fig. <ref type="figure" target="#fig_1">2</ref>, whereas the level sets of the elastic-net penalty are elastic-net balls (ε-balls) with center at the origin and corners at the intersections with the axes, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. When ε = 0, i.e. with a pure 1 -penalty (Lasso), the ε-ball is simply a square (dashed line in Fig. <ref type="figure" target="#fig_0">1</ref>) and we see that the unique tangency point will be the top corner if θ n &gt; π /4 (the point T in the two panels of Fig. <ref type="figure" target="#fig_1">2</ref>), or the right corner if θ n &lt; π /4. For θ n = π /4 (that is, when ϕ 1 and ϕ 2 coincide on the data), the minimizer of (3) is no longer unique since the level sets will touch along an edge of the square. Now, if θ n randomly tilts around π /4 (because of the random sampling of the input data), we see that the Lasso estimator is not stable since it randomly jumps between the top and the right corner. If ε → ∞, i.e. with a pure 2 -penalty (ridge regression), the ε-ball becomes a disc (dotted line in Fig. <ref type="figure" target="#fig_0">1</ref>) and the minimizer is the point of the straight line having minimal distance from the origin (the point Q in the two panels of Fig. <ref type="figure" target="#fig_1">2</ref>). The solution always exists, is stable under random perturbations, but it is never sparse (if 0 &lt; θ n &lt; π /2).</p><p>The situation changes if we consider the elastic-net estimator with ε &gt; 0 (the corresponding minimizer is the point P in the two panels of Fig. <ref type="figure" target="#fig_1">2</ref>). The presence of the 2 -term ensures a smooth and stable behavior when the Lasso estimator becomes unstable. More precisely, letcot θ + be the slope of the right tangent at the top corner of the elastic-net ball (θ + &gt; π /4), andcot θ -the slope of the upper tangent at the right corner (θ -&lt; π /4). As depicted in top panel of Fig. <ref type="figure" target="#fig_1">2</ref>, the minimizer will be the top corner if θ n &gt; θ + . It will be the right corner if θ n &lt; θ -. In both cases the elastic-net solution is sparse. On the other hand, if θ -≤ θ n ≤ θ + the minimizer has both components β 1 and β 2 different from zero -see the bottom panel of Fig. <ref type="figure" target="#fig_1">2</ref>; in particular, β 1 = β 2 if θ n = π /4. Now we observe that if θ n randomly tilts around π /4, the solution smoothly moves between the top corner and the right corner. However, the price we paid to get such stability is a decrease in sparsity, since the solution is sparse only when θ n ∈ [θ -, θ + ].</p><p>The previous elementary example could be refined in various ways to show the essential role played by the 2 -penalty to overcome the instability effects inherent to the use of the 1 -penalty for variable selection in a random-design setting.</p><p>Remark 1. Stability in the case of collinear features can also be achieved by using an p -penalty with p &gt; 1 instead of p = 1. However, since such penalty term is differentiable, the corresponding pballs in our two-dimensional example are delimited by a smooth curve without any corner and, as a consequence, sparse solutions are not obtained in the presence of collinear features. Nevertheless, when assuming that the relevant features are linearly independent, sparsity could still be enforced by means of p -penalties as shown in <ref type="bibr" target="#b9">[10]</ref>.</p><p>We now conclude this introductory section by a summary of the main results which will be derived in the core of the paper. A key result will be to show that for ε &gt; 0, β λ n is the fixed point of the following contractive map</p><formula xml:id="formula_10">β = 1 τ + ελ S λ (τ I -Φ * n Φ n )β + Φ * n Y</formula><p>where τ is a suitable relaxation constant, Φ * n Φ n and Φ * n Y are respectively the matrix and the vector with entries</p><formula xml:id="formula_11">(Φ * n Φ n ) γ ,γ = 1 n n i=1 ϕ γ (X i ), ϕ γ (X i ) and (Φ * n Y ) γ = 1 n n i=1 ϕ γ (X i ), Y i , ( •,</formula><p>• denotes the scalar product in the output space Y). Moreover, S λ (β) is the soft-thresholding operator acting componentwise as follows</p><formula xml:id="formula_12">[S λ (β)] γ =            β γ - λw γ 2 if β γ &gt; λw γ 2 0 if |β γ | ≤ λw γ 2 β γ + λw γ 2 if β γ &lt; - λw γ 2 .</formula><p>As a consequence of the Banach fixed point theorem, β λ n can be computed by means of an iterative algorithm. This procedure is completely different from the modification of the LARS algorithm used in <ref type="bibr" target="#b19">[20]</ref> and is akin instead to the algorithm developed in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Another interesting property which we will derive from the above equation is that the non-zero components of β λ n are such that w γ ≤ C λ , where C is a constant depending on the data. Hence the only active features are those for which the corresponding weight lies below the threshold C /λ. If the features are organized into finite subsets of increasing complexity (as happens for example for wavelets) and the weights tend to infinity with increasing feature complexity, then the number of active features is finite and can be determined for any given data set. Let us recall that in the case of ridge regression, the so-called representer theorem, see <ref type="bibr" target="#b23">[24]</ref>, ensures that we only have to solve in practice a finite-dimensional optimization problem, even when the dictionary is infinite-dimensional (as in kernel methods). This is no longer true, however, with an 1 -type regularization and, for practical purposes, one would need to truncate infinite dictionaries. A standard way to do this is to consider only a finite subset of m features, with m possibly depending on n -see for example <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. Notice that such a procedure implicitly assumes some order in the features and makes sense only if the retained features are the most relevant ones. For example, in <ref type="bibr" target="#b24">[25]</ref>, it is assumed that there is a natural exhaustion of the hypothesis space with nested subspaces spanned by finite-dimensional subsets of features of increasing size. In our approach we adopt a different strategy, namely the encoding of such information in the elastic-net penalty by means of suitable weights in the 1 -norm.</p><p>The main result of our paper concerns the consistency for variable selection of β λ n . We prove that, if the regularization parameter λ = λ n satisfies the conditions lim n→∞ λ n = 0 and lim n→∞ (nλ 2 n -</p><formula xml:id="formula_13">2 log n) = +∞, then lim n→∞ β λ n n -β ε 2 = 0 with</formula><p>probability one, where the vector β ε , which we call the elastic-net representation of f β , is the minimizer of min</p><formula xml:id="formula_14">β∈ 2 γ ∈Γ w γ |β γ | + ε γ ∈Γ |β γ | 2 subject to f β = f * .</formula><p>The vector β ε exists and is unique provided that ε &gt; 0 and the regression function f * admits a sparse representation on the dictionary, i.e.</p><formula xml:id="formula_15">f * = γ ∈Γ β * γ ϕ γ for at least a vector β * ∈ 2 such that γ ∈Γ w γ |β * γ | is finite.</formula><p>Notice that, when the features are linearly dependent, there is a problem of identifiability since there are many vectors β such that f * = f β . The elastic-net regularization scheme forces β λ n n to converge to β ε . This is precisely what happens for linear inverse problems where the regularized solution converges to the minimum-norm solution of the least-squares problem. As a consequence of the above convergence result, one easily deduces the consistency of the corresponding prediction function</p><formula xml:id="formula_16">f n := f β λn n , that is, lim n→∞ E |f n -f * | 2</formula><p>= 0 with probability one. When the regression function does not admit a sparse representation, we can still prove the previous consistency result for f n provided that the linear span of the features is sufficiently rich. Finally, we use a datadriven choice for the regularization parameter, based on the so-called balancing principle <ref type="bibr" target="#b25">[26]</ref>, to obtain non-asymptotic bounds which are adaptive to the unknown regularity of the regression function.</p><p>The rest of the paper is organized as follows. In Section 2, we set up the mathematical framework of the problem. In Section 3, we analyze the optimization problem underlying elastic-net regularization and the iterative thresholding procedure we propose to compute the estimator. Finally, Section 4 contains the statistical analysis with our main results concerning the estimation of the errors on our estimators as well as their consistency properties under appropriate a priori and adaptive strategies for choosing the regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mathematical setting of the problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations and assumptions</head><p>In this section we describe the general setting of the regression problem we want to solve and specify all the required assumptions.</p><p>We assume that X is a separable metric space and that Y is a (real) separable Hilbert space, with norm and scalar product denoted respectively by | • | and •, • . Typically, X is a subset of R d and Y is R. Recently, however, there has been an increasing interest in vector-valued regression problems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and multiple supervised learning tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>: in both settings Y is taken to be R m . Also infinitedimensional output spaces are of interest as e.g. in the problem of estimating glycemic response during a time interval depending on the amount and type of food; in such a case, Y is the space L 2 or some Sobolev space. Other examples of applications in an infinite-dimensional setting are given in <ref type="bibr" target="#b30">[31]</ref>.</p><p>Our first assumption concerns the set of features.</p><p>Assumption 1. The family of features (ϕ γ ) γ ∈Γ is a countable set of measurable functions ϕ γ :</p><formula xml:id="formula_17">X → Y such that ∀x ∈ X k(x) = γ ∈Γ |ϕ γ (x)| 2 ≤ κ,<label>(5)</label></formula><p>for some finite number κ.</p><p>The index set Γ is countable, but we do not assume any order. As for the convergence of series, we use the notion of summability: given a family (v γ ) γ ∈Γ of vectors in a normed vector space</p><formula xml:id="formula_18">V , v = γ ∈Γ v γ means that (v γ ) γ ∈Γ is summable 1 with sum v ∈ V .</formula><p>Assumption 1 can be seen as a condition on the class of functions that can be recovered by the elastic-net scheme. As already noted in the Introduction, we have at our disposal an arbitrary (countable) dictionary (ψ γ ) γ ∈Γ of measurable functions, and we try to approximate f * with linear combinations</p><formula xml:id="formula_19">f β (x) = γ ∈Γ β γ ψ γ (x)</formula><p>where the set of coefficients (β γ ) γ ∈Γ satisfies some decay condition equivalent to a regularity condition on the functions f β . We make this condition precise by assuming that there exists a sequence of positive weights (u γ ) γ ∈Γ such that γ ∈Γ u γ β 2 γ &lt; ∞ and, for any of such vectors β = (β γ ) γ ∈Γ , that the series defining f β converges absolutely for all x ∈ X. These two facts follow from the requirement that the set of rescaled features <ref type="formula" target="#formula_17">5</ref>) is a little bit stronger since it requires that sup x∈X γ ∈Γ |ϕ γ (x)| 2 &lt; ∞, so that we also have that the functions f β are bounded. To simplify the notation, in the rest of the paper, we only use the (rescaled) features ϕ γ and, with this choice, the regularity condition on the coefficients (β γ ) γ ∈Γ becomes γ ∈Γ β 2 γ &lt; ∞. An example of features satisfying condition ( <ref type="formula" target="#formula_17">5</ref>) is given by a family of rescaled wavelets on X = [0, 1]. Let ψ jk | j = 0, 1 . . . ; k ∈ ∆ j be a orthonormal wavelet basis in L 2 ([0, 1]) with regularity C r , r &gt; 1  2 , where for j ≥ 1 ψ jk | k ∈ ∆ j is the orthonormal wavelet basis (with suitable boundary conditions) spanning the detail space at level j. To simplify notation, it is assumed that the set {ψ 0k | k ∈ ∆ 0 } contains both the wavelets and the scaling functions at level j = 0. Fix s such that 1 2 &lt; s &lt; r and let ϕ jk = 2 -js ψ jk . Then</p><formula xml:id="formula_20">ϕ γ = ψ γ √ u γ satisfies γ ∈Γ |ϕ γ (x)| 2 &lt; ∞. Condition (</formula><formula xml:id="formula_21">∞ j=0 k∈∆ j |ϕ jk (x)| 2 = ∞ j=0 k∈∆ j 2 -2js |ψ jk (x)| 2 ≤ C ∞ j=0 2 -2js 2 j = C 1 1 -2 1-2s = κ,</formula><p>where C is a suitable constant depending on the number of wavelets that are non-zero at a point</p><p>x ∈ [0, 1] for a given level j, and on the maximum values of the scaling function and of the mother wavelet; see <ref type="bibr" target="#b31">[32]</ref> for a similar setting. Condition (5) allows defining the hypothesis space in which we search for the estimator. Let 2 be the Hilbert space of the families (β γ ) γ ∈Γ of real numbers such that γ ∈Γ β 2 γ &lt; ∞, with the usual scalar product •, • 2 and the corresponding norm • 2 . We will denote by (e γ ) γ ∈Γ the canonical basis of 2 and by supp(β) = γ ∈ Γ | β γ = 0 the support of β. The Cauchy-Schwarz inequality and condition <ref type="bibr" target="#b4">(5)</ref> ensure that, for any β = (β γ ) γ ∈Γ ∈ 2 , the series</p><formula xml:id="formula_22">γ ∈Γ β γ ϕ γ (x) = f β (x) 1 That is, for all η &gt; 0, there is a finite subset Γ 0 ⊂ Γ such that v -γ ∈Γ v γ V ≤ η for all finite subsets Γ ⊃ Γ 0 . If Γ = N,</formula><p>the notion of summability is equivalent to requiring the series to converge unconditionally (i.e. its terms can be permuted without affecting convergence). If the vector space is finite-dimensional, summability is equivalent to absolute convergence, but in the infinite-dimensional setting, there are summable series which are not absolutely convergent.</p><p>is summable in Y uniformly on X with</p><formula xml:id="formula_23">sup x∈X |f β (x)| ≤ β 2 κ 1 2 . (6)</formula><p>Later on, in Proposition 3, we will show that the hypothesis space H = f β | β ∈ 2 is then a vectorvalued reproducing kernel Hilbert space on X with a bounded kernel <ref type="bibr" target="#b32">[33]</ref>, and that (ϕ γ ) γ ∈Γ is a normalized tight frame for H. In the example of the wavelet features one can easily check that H is the Sobolev space H s on [0, 1] and β 2 is equivalent to f β H s .</p><p>The second assumption concerns the regression model.</p><formula xml:id="formula_24">Assumption 2. The random couple (X, Y ) in X × Y obeys the regression model Y = f * (X) + W where f * = f β * for some β * ∈ 2 with γ ∈Γ w γ |β * γ | &lt; +∞<label>(7)</label></formula><p>and</p><formula xml:id="formula_25">E [W | X ] = 0 (8) E exp |W | L - |W | L -1 X ≤ σ 2 2L 2<label>(9)</label></formula><p>with σ , L &gt; 0. The family (w γ ) γ ∈Γ forms the positive weights defining the elastic-net penalty p ε (β)</p><p>in <ref type="bibr" target="#b1">(2)</ref>.</p><p>Observe that f * = f β * is always a bounded function by <ref type="bibr" target="#b5">(6)</ref>. Moreover condition ( <ref type="formula" target="#formula_24">7</ref>) is a further regularity condition on the regression function and will not be needed for some of the results derived in the paper. Assumption ( <ref type="formula" target="#formula_25">9</ref>) is satisfied by bounded, Gaussian or sub-Gaussian noise. In particular, it implies</p><formula xml:id="formula_26">E |W | m |X ≤ 1 2 m! σ 2 L m-2 , ∀m ≥ 2,<label>(10)</label></formula><p>see <ref type="bibr" target="#b33">[34]</ref>, so that W has a finite second moment. It follows that Y has a finite first moment and ( <ref type="formula">8</ref>)</p><formula xml:id="formula_27">implies that f * is the regression function E [Y | X = x].</formula><p>Condition (7) controls both the sparsity and the regularity of the regression function. If inf γ ∈Γ w γ = w 0 &gt; 0, it is sufficient to require that β * 1,w is finite. Indeed, the Hölder inequality gives that</p><formula xml:id="formula_28">β 2 ≤ 1 w 0 β 1,w .<label>(11)</label></formula><p>If w 0 = 0, we also need β * 2 to be finite. In the example of the (rescaled) wavelet features a natural choice for the weights is w jk = 2 ja for some a ∈ R, so that β 1,w is equivalent to the norm</p><formula xml:id="formula_29">f β B s 1,1</formula><p>, with s = a + s + 1 2 , in the Besov space B s 1,1 on [0, 1] (for more details, see e.g. the appendix in <ref type="bibr" target="#b18">[19]</ref>).</p><p>In such a case, ( <ref type="formula" target="#formula_24">7</ref>) is equivalent to requiring that f * ∈ H s ∩ B s 1,1 . Finally, our third assumption concerns the training sample. Assumption 3. The sequence of random pairs (X n , Y n ) n≥1 are independent and identically distributed (i.i.d.) according to the distribution of (X, Y ).</p><p>In the following, we let P be the probability distribution of (X, Y ), and L 2 Y (P) be the Hilbert space of (measurable) functions f : X × Y → Y with the norm</p><formula xml:id="formula_30">f 2 P = X×Y |f (x, y)| 2 dP(x, y).</formula><p>With a slight abuse of notation, we regard the random pair (X, Y ) as a function on X × Y, that is, X (x, y) = x and Y (x, y) = y. Moreover, we denote by</p><formula xml:id="formula_31">P n = 1 n n i=1 δ X i ,Y i the empirical distribution</formula><p>and by L 2 Y (P n ) the corresponding (finite-dimensional) Hilbert space with norm</p><formula xml:id="formula_32">f 2 n = 1 n n i=1 |f (X i , Y i )| 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Operators defined by the set of features</head><p>The choice of a quadratic loss function and the Hilbert structure of the hypothesis space suggest using some tools from the theory of linear operators. In particular, the function f β depends linearly on β and can be regarded as an element of both L 2 Y (P) and of L 2 Y (P n ). Hence it defines two operators, whose properties are summarized by the next two propositions, based on the following lemma.</p><p>Lemma 1. For any fixed x ∈ X, the map Φ x : 2 → Y defined by</p><formula xml:id="formula_33">Φ x β = γ ∈Γ ϕ γ (x)β γ = f β (x) is a Hilbert-Schmidt operator, its adjoint Φ * x : Y → 2 acts as (Φ * x y) γ = y, ϕ γ (x) γ ∈ Γ , y ∈ Y.<label>(12)</label></formula><p>In particular Φ * x Φ x is a trace-class operator with</p><formula xml:id="formula_34">Tr (Φ * x Φ x ) ≤ κ. (13) Moreover, Φ * X Y is an 2 -valued random variable with Φ * X Y 2 ≤ κ 1 2 |Y |,<label>(14)</label></formula><p>and Φ * X Φ X is an L HS -valued random variable with</p><formula xml:id="formula_35">Φ * X Φ X HS ≤ κ, (<label>15</label></formula><formula xml:id="formula_36">)</formula><p>where L HS denotes the separable Hilbert space of the Hilbert-Schmidt operators on 2 , and • HS is the Hilbert-Schmidt norm.</p><p>Proof. Clearly Φ x is a linear map from 2 to Y. Since Φ x e γ = ϕ γ (x), we have <ref type="bibr" target="#b11">(12)</ref>. Finally, since X and Y are separable, the map (x,</p><formula xml:id="formula_37">γ ∈Γ |Φ x e γ | 2 = γ ∈Γ |ϕ γ (x)| 2 ≤ κ, so that Φ x is a Hilbert-Schmidt operator and Tr (Φ * x Φ x ) ≤ κ by (5). Moreover, given y ∈ Y and γ ∈ Γ (Φ * x y) γ = Φ * x y, e γ 2 = y, ϕ γ (x) which is</formula><formula xml:id="formula_38">y) → y, ϕ γ (x) is measurable, then (Φ * X Y ) γ is a real random variable and, since 2 is separable, Φ * X Y is an 2 -valued random variable with Φ * X Y 2 2 = γ ∈Γ Y , ϕ γ (X) 2 ≤ κ|Y | 2 .</formula><p>A similar proof holds for Φ * X Φ X , recalling that any trace-class operator is in L HS and Φ * X Φ X HS ≤ Tr (Φ * X Φ X ). The following proposition defines the distribution-dependent operator Φ P as a map from 2 into L 2 Y (P). </p><formula xml:id="formula_39">Φ * P Y = E Φ * X Y (16) Φ * P Φ P = E Φ * X Φ X (17) Tr (Φ * P Φ P ) = E [k(X )] ≤ κ.<label>(18)</label></formula><p>Proof. Since f β is a bounded (measurable) function,</p><formula xml:id="formula_40">f β ∈ L 2 Y (P) and γ ∈Γ Φ P e γ 2 P = γ ∈Γ E |ϕ γ (X)| 2 = E [k(X )] ≤ κ.</formula><p>Hence Φ P is a Hilbert-Schmidt operator with Tr (Φ * P Φ P ) = γ ∈Γ Φ P e γ 2 P so that (18) holds. By (9) W has a finite second moment and by ( <ref type="formula">6</ref>)</p><formula xml:id="formula_41">f * = f β * is a bounded function, hence Y = f * (X) + W is in L 2 Y (P). Now for any β ∈ 2 we have Φ * P Y , β 2 = Y , Φ P β P = E [ Y , Φ X β ] = E Φ * X Y , β 2 .</formula><p>On the other hand, by <ref type="bibr" target="#b13">(14)</ref>, Φ * X Y has finite expectation, so that ( <ref type="formula">16</ref>) follows. Finally, given β, β ∈ 2 <ref type="formula">17</ref>) is clear, since Φ * X Φ X has finite expectation as a consequence of the fact that it is a bounded L HS -valued random variable.</p><formula xml:id="formula_42">Φ * P Φ P β , β 2 = Φ P β , Φ P β P = E Φ X β , Φ X β = E Φ * X Φ X β , β 2 so that (</formula><p>Replacing P by the empirical measure we get the sample version of the operator.</p><formula xml:id="formula_43">Proposition 2. The map Φ n : 2 → L 2 Y (P n ) defined by Φ n β = f β is Hilbert-Schmidt operator and Φ * n Y = 1 n n i=1 Φ * X i Y i (19) Φ * n Φ n = 1 n n i=1 Φ * X i Φ X i (20) Tr (Φ * n Φ n ) = 1 n n i=1 k(X i ) ≤ κ.<label>(21)</label></formula><p>The proof of Proposition 2 is analogous to the proof of Proposition 1, except that P is to be replaced by P n .</p><p>By <ref type="bibr" target="#b11">(12)</ref> with y = ϕ γ (x), we have that the matrix elements of the operator Φ *</p><p>x Φ x are (Φ * x Φ x ) γ γ = ϕ γ (x), ϕ γ (x) so that Φ * n Φ n is the empirical mean of the Gram matrix of the set (ϕ γ ) γ ∈Γ , whereas Φ * P Φ P is the corresponding mean with respect to the distribution P. Notice that if the features are linearly dependent in L 2 Y (P n ), the matrix Φ * n Φ n has a non-trivial kernel and hence is not invertible. More important, if Γ is countably infinite, Φ * n Φ n is a compact operator, so that its inverse (if it exists) is not bounded. On the contrary, if Γ is finite and (ϕ γ ) γ ∈Γ are linearly independent in L 2 Y (P n ), then Φ * n Φ n is invertible. A similar reasoning holds for the matrix Φ * P Φ P . To control whether these matrices have a bounded inverse or not, we introduce a lower spectral bound κ 0 ≥ 0, such that</p><formula xml:id="formula_44">κ 0 ≤ inf β∈ 2 | β 2 =1 Φ * P Φ P β, β 2</formula><p>and, with probability 1,</p><formula xml:id="formula_45">κ 0 ≤ inf β∈ 2 | β 2 =1 Φ * n Φ n β, β 2 .</formula><p>Clearly we can have κ 0 &gt; 0 only if Γ is finite and the features (ϕ γ ) γ ∈Γ are linearly independent both in L 2 Y (P n ) and L 2 Y (P).</p><p>On the other hand, ( <ref type="formula" target="#formula_39">18</ref>) and ( <ref type="formula" target="#formula_43">21</ref>) give the crude upper spectral bounds sup</p><formula xml:id="formula_46">β∈ 2 | β 2 =1 Φ * P Φ P β, β 2 ≤ κ, sup β∈ 2 | β 2 =1 Φ * n Φ n β, β 2 ≤ κ.</formula><p>One could improve these estimates by means of a tight bound on the largest eigenvalue of Φ * P Φ P .</p><p>We end this section by showing that, under the assumptions we made, a structure of reproducing kernel Hilbert space emerges naturally. Let us denote by Y X the space of functions from X to Y.</p><p>Proposition 3. The linear operator Φ : 2 → Y X , Φβ = f β , is a partial isometry from 2 onto the vector-valued reproducing kernel Hilbert space H on X, with reproducing kernel K :</p><formula xml:id="formula_47">X × X → L(Y) K (x, t)y = (Φ x Φ * t )y = γ ∈Γ ϕ γ (x) y, ϕ γ (t) x, t ∈ X, y ∈ Y,<label>(22)</label></formula><p>the null space of Φ is</p><formula xml:id="formula_48">ker Φ = β ∈ 2 | γ ∈Γ ϕ γ (x)β γ = 0 ∀x ∈ X ,<label>(23)</label></formula><p>and the family (ϕ γ ) γ ∈Γ is a normalized tight frame in H, namely</p><formula xml:id="formula_49">γ ∈Γ f , ϕ γ H 2 = f 2 H ∀f ∈ H.</formula><p>Conversely, let H be a vector-valued reproducing kernel Hilbert space with reproducing kernel K such that K (x, x) : Y → Y is a trace-class operator for all x ∈ X, with trace bounded by κ. If (ϕ γ ) γ ∈Γ is a normalized tight frame in H, then (5) holds.</p><p>Proof. Proposition 2.4 of <ref type="bibr" target="#b32">[33]</ref> (with K = Y, H = 2 , γ (x) = Φ * x and A = Φ) gives that Φ is a partial isometry from 2 onto the reproducing kernel Hilbert space H, with reproducing kernel K (x, t). On the other hand ( <ref type="formula" target="#formula_48">23</ref>) is clear. Since Φ is a partial isometry with range H and Φe γ = ϕ γ where (e γ ) γ ∈Γ is a basis in 2 , then (ϕ γ ) γ ∈Γ is a normalized tight frame in H .</p><p>To show the converse result, given x ∈ X and y ∈ Y, we apply the definition of a normalized tight frame to the function K x y defined by (K x y)(t) = K (t, x)y. K x y belongs to H by the definition of a reproducing kernel Hilbert space and is such that the following reproducing property f , K x y H = f (x), y holds for any f ∈ H. Then</p><formula xml:id="formula_50">K (x, x)y, y = K x y 2 H = γ ∈Γ K x y, ϕ γ H 2 = γ ∈Γ y, ϕ γ (x) 2 ,</formula><p>where we used twice the reproducing property. Now, if</p><formula xml:id="formula_51">(y i ) i∈I is a basis in Y and x ∈ X γ ∈Γ |ϕ γ (x)| 2 = γ ∈Γ i∈I y i , ϕ γ (x) 2 = i∈I K (x, x)y i , y i = Tr (K (x, x)) ≤ κ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Minimization of the elastic-net functional</head><p>In this section, we study the properties of the elastic-net estimator β λ n defined by <ref type="bibr" target="#b3">(4)</ref>. First of all, we characterize the minimizer of the elastic-net functional (3) as the unique fixed point of a contractive map. Moreover, we characterize some sparsity properties of the estimator and propose a natural iterative soft-thresholding algorithm to compute it. Our algorithmic approach is totally different from the method proposed in <ref type="bibr" target="#b19">[20]</ref>, where β λ n is computed by first reducing the problem to the case of a pure 1 -penalty and then applying the LARS algorithm <ref type="bibr" target="#b17">[18]</ref>.</p><p>In the following we make use of the following vector notation. Given a sample of n i.i.d. observations</p><p>(X 1 , Y 1 ), . . . , (X n , Y n ), and using the operators defined in the previous section, we can rewrite the elastic-net functional (3) as</p><formula xml:id="formula_52">E λ n (β) = Φ n β -Y 2 n + λp ε (β),<label>(24)</label></formula><p>where the p ε (•) is the elastic-net penalty defined by (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fixed point equation</head><p>The main difficulty in minimizing <ref type="bibr" target="#b23">(24)</ref> is that the functional is not differentiable because of the presence of the 1 -term in the penalty. Nonetheless the convexity of such a term enables us to use tools from subdifferential calculus. Recall that, if F : 2 → R is a convex functional, the subgradient at a point β ∈ 2 is the set of elements η ∈ 2 such that</p><formula xml:id="formula_53">F (β + β ) ≥ F (β) + η, β 2 ∀β ∈ 2 .</formula><p>The subgradient at β is denoted by ∂F (β), see <ref type="bibr" target="#b34">[35]</ref>. We compute the subgradient of the convex functional p ε (β), using the following definition of sgn(t)</p><formula xml:id="formula_54">sgn(t) = 1 if t &gt; 0 sgn(t) ∈ [-1, 1] if t = 0 sgn(t) = -1 if t &lt; 0.<label>(25)</label></formula><p>We first state the following lemma. Proof. Define the map F :</p><formula xml:id="formula_55">Γ × R → [0, ∞] F (γ , t) = w γ |t| + εt 2 .</formula><p>Given γ ∈ Γ , F (γ , •) is a convex, continuous function and its subgradient is</p><formula xml:id="formula_56">∂F (γ , t) = τ ∈ R | τ = w γ sgn(t) + 2εt ,</formula><p>where we used the fact that the subgradient of |t| is given by sgn(t). Since</p><formula xml:id="formula_57">p ε (β) = γ ∈Γ F (γ , β γ ) = sup Γ finite γ ∈Γ F (γ , β γ )</formula><p>and β → β γ is continuous, a standard result of convex analysis <ref type="bibr" target="#b34">[35]</ref> ensures that p ε (•) is convex and lower semi-continuous.</p><p>The computation of the subgradient is standard. Given β ∈ 2 and η ∈ ∂p ε (β) ⊂ 2 , by the definition of a subgradient,</p><formula xml:id="formula_58">γ ∈Γ F (γ , β γ + β γ ) ≥ γ ∈Γ F (γ , β γ ) + γ ∈Γ η γ β γ ∀β ∈ 2 .</formula><p>Given γ ∈ Γ , choosing β = te γ with t ∈ R, it follows that η γ belongs to the subgradient of F (γ , β γ ), that is,</p><formula xml:id="formula_59">η γ = w γ sgn(β γ ) + 2εβ γ .<label>(26)</label></formula><p>Conversely, if <ref type="bibr" target="#b25">(26)</ref> holds for all γ ∈ Γ , by the definition of a subgradient</p><formula xml:id="formula_60">F (γ , β γ + β γ ) ≥ F (γ , β γ ) + η γ β γ .</formula><p>By summing over γ ∈ Γ and taking into account the fact that (η γ β γ ) γ ∈Γ ∈ 1 , then</p><formula xml:id="formula_61">p ε (β + β ) ≥ p ε (β) + η, β 2 .</formula><p>To state our main result about the characterization of the minimizer of ( <ref type="formula" target="#formula_52">24</ref>), we need to introduce the soft-thresholding function S λ : R → R, λ &gt; 0 which is defined by</p><formula xml:id="formula_62">S λ (t) =            t - λ 2 if t &gt; λ 2 0 if |t| ≤ λ 2 t + λ 2 if t &lt; - λ 2 ,<label>(27)</label></formula><p>and the corresponding nonlinear thresholding operator S λ : 2 → 2 acting componentwise as</p><formula xml:id="formula_63">[S λ (β)] γ = S λw γ β γ . (<label>28</label></formula><formula xml:id="formula_64">)</formula><p>We note that the soft-thresholding operator satisfies</p><formula xml:id="formula_65">S aλ (aβ) = aS λ (β) a &gt; 0, β ∈ 2 ,<label>(29)</label></formula><formula xml:id="formula_66">S λ (β) -S λ β 2 ≤ β -β 2 β, β ∈ 2 .<label>(30)</label></formula><p>These properties are immediate consequences of the fact that</p><formula xml:id="formula_67">S aλ (at) = aS λ (t) a &gt; 0, t ∈ R |S λ (t) -S λ t | ≤ |t -t | t, t ∈ R.</formula><p>Notice that (30) with β = 0 ensures that S λ (β) ∈ 2 for all β ∈ 2 .</p><p>We are ready to prove the following theorem.</p><p>Theorem 1. Given ε ≥ 0 and λ &gt; 0, a vector β ∈ 2 is a minimizer of the elastic-net functional (3) if and only if it solves the nonlinear equation</p><formula xml:id="formula_68">1 n n i=1 Y i -(Φ n β)(X i ), ϕ γ (X i ) -ελβ γ = λ 2 w γ sgn(β γ ) ∀γ ∈ Γ ,<label>(31)</label></formula><p>or, equivalently,</p><formula xml:id="formula_69">β = S λ (1 -ελ)β + Φ * n (Y -Φ n β) . (<label>32</label></formula><formula xml:id="formula_70">)</formula><p>If ε &gt; 0 the solution always exists and is unique. If ε = 0, κ 0 &gt; 0 and w 0 = inf γ ∈Γ w γ &gt; 0, the solution still exists and is unique.</p><p>Proof. If ε &gt; 0 the functional E λ n is strictly convex, finite at 0, and it is coercive by</p><formula xml:id="formula_71">E λ n (β) ≥ p ε (β) ≥ λε β 2 2 .</formula><p>Observing that Φ n β -Y 2 n is continuous and, by Lemma 2, the elastic-net penalty is l.s.c., then E λ n is l.s.c. and, since 2 is reflexive, there is a unique minimizer β λ n in 2 . If ε = 0, E λ n is convex, but the fact that κ 0 &gt; 0 ensures that the minimizer is unique. Its existence follows from the observation that</p><formula xml:id="formula_72">E λ n (β) ≥ p ε (β) ≥ λ β 1,w ≥ λw 0 β 2 ,</formula><p>where we used <ref type="bibr" target="#b10">(11)</ref> We now prove <ref type="bibr" target="#b31">(32)</ref>, which is equivalent to the set of equations</p><formula xml:id="formula_73">β γ = S λw γ (1 -ελ)β γ + 1 n n i=1 Y i -(Φ n β)(X i ), ϕ γ (X i ) ∀γ ∈ Γ .<label>(33)</label></formula><p>Setting β γ = Y -Φ n β, ϕ γ (X) n -ελβ γ , we have β γ = S λw γ β γ + β γ if and only if</p><formula xml:id="formula_74">β γ =              β γ + β γ - λw γ 2 if β γ + β γ &gt; λw γ 2 0 if |β γ + β γ | ≤ λw γ 2 β γ + β γ + λw γ 2 if β γ + β γ &lt; - λw γ 2 , that is,              β γ = λw γ 2 if β γ &gt; 0 |β γ | ≤ λw γ 2 if β γ = 0 β γ = - λw γ 2 if β γ &lt; 0 or else β γ = λw γ 2 sgn(β γ )</formula><p>which is equivalent to <ref type="bibr" target="#b30">(31)</ref>.</p><p>The following corollary gives some more information about the characterization of the solution as the fixed point of a contractive map. In particular, it provides an explicit expression for the Lipschitz constant of this map and it shows how it depends on the spectral properties of the empirical mean of the Gram matrix and on the regularization parameter λ. </p><formula xml:id="formula_75">β = T n β where T n β = 1 τ + ελ S λ (τ I -Φ * n Φ n )β + Φ * n Y . (<label>34</label></formula><formula xml:id="formula_76">)</formula><p>With the choice τ = κ 0 +κ 2 , the Lipschitz constant is bounded by</p><formula xml:id="formula_77">q = κ -κ 0 κ + κ 0 + 2ελ ≤ 1.</formula><p>In particular, with this choice of τ and if ε &gt; 0 or κ 0 &gt; 0, T n is a contraction. denotes the operator norm of a bounded operator on 2 . Hence, using <ref type="bibr" target="#b29">(30)</ref>, we get</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><formula xml:id="formula_78">T n β -T n β 2 ≤ 1 τ + ελ (τ I -Φ * n Φ n )(β -β ) 2 ≤ max τ -κ 0 τ + ελ , τ -κ τ + ελ β -β 2 =: q β -β 2 . The minimum of q with respect to τ is obtained for τ -κ 0 τ + ελ = κ -τ τ + ελ ,</formula><p>that is, τ = κ+κ 0 2 , and, with this choice, we get</p><formula xml:id="formula_79">q = κ -κ 0 κ + κ 0 + 2ελ</formula><p>.</p><p>By inspecting the proof, we notice that the choice τ = κ 0 +κ 2 provides the best possible Lipschitz constant under the assumption that κ 0 I ≤ Φ * n Φ n ≤ κI. If ε &gt; 0 or κ 0 &gt; 0, T n is a contraction and β λ n can be computed by means of the Banach fixed point theorem. If ε = 0 and κ 0 = 0, T n is only non-expansive, so that proving the convergence of the successive approximation scheme is not straightforward. 2   Let us now write down explicitly the iterative procedure suggested by Corollary 1 to compute β λ n . Define the iterative scheme by</p><formula xml:id="formula_80">β 0 = 0, β = 1 τ + ελ S λ (τ I -Φ * n Φ n )β -1 + Φ * n Y with τ = κ 0 +κ 2 .</formula><p>The following corollary shows that the β converges to β λ n when goes to infinity.</p><p>Corollary 2. Assume that ε &gt; 0 or κ 0 &gt; 0. For any ∈ N the following inequality holds</p><formula xml:id="formula_81">β -β λ n 2 ≤ (κ -κ 0 ) (κ + κ 0 + 2ελ) (κ 0 + ελ) Φ * n Y 2 . (<label>35</label></formula><formula xml:id="formula_82">)</formula><p>In particular, lim →∞ ββ λ n 2 = 0. Proof. Since T n is a contraction with Lipschitz constant q = κ-κ 0 κ+κ 0 +2ελ &lt; 1, the Banach fixed point theorem applies and the sequence β ∈N converges to the unique fixed point of T n , which is β λ n by</p><p>Corollary 1. Moreover we can use the Lipschitz property of T n to write</p><formula xml:id="formula_83">β -β λ n 2 ≤ β -β +1 2 + β +1 -β λ n 2 ≤ q β -1 -β 2 + q β -β λ n 2 ≤ q β 0 -β 1 2 + q β -β λ n 2 ,</formula><p>2 Interestingly, it was proved in <ref type="bibr" target="#b18">[19]</ref> using different arguments that the same iterative scheme can still be used for the case ε = 0 and κ 0 = 0. so that we immediately get</p><formula xml:id="formula_84">β -β λ n 2 ≤ q 1 -q β 1 -β 0 2 ≤ (κ -κ 0 ) (κ 0 + κ + 2ελ) (κ 0 + ελ) Φ * n Y 2 since β 0 = 0, β 1 = 1 τ +ελ S λ Φ * n Y and 1 -q = 2(κ 0 +ελ) κ 0 +κ+2ελ .</formula><p>Let us remark that bound <ref type="bibr" target="#b34">(35)</ref> provides a natural stopping rule for the number of iterations, namely to select such that ββ λ n 2 ≤ η, where η is a bound on the distance between the estimator β λ n and the true solution. For example, if Φ * n Y 2 is bounded by M and if κ 0 = 0, the stopping rule is</p><formula xml:id="formula_85">stop ≥ log M ελη log(1 + 2ελ κ ) so that β stop -β λ n 2 ≤ η.</formula><p>Note that in the case of an infinite-dimensional dictionary, the above iteration involves infinitedimensional matrices. In Section 3.2 we will show that under mild assumptions on the weights it is always possible to reduce the problem to a finite-dimensional one.</p><p>Finally we notice that all previous results also hold when considering the distribution-dependent version of the method. The following proposition summarizes the results in this latter case. Proposition 4. Let ε ≥ 0 and λ &gt; 0. Pick any arbitrary τ &gt; 0. Then a vector β ∈ 2 is a minimizer of</p><formula xml:id="formula_86">E λ (β) = E |Φ P β -Y | 2 + λp ε (β).</formula><p>if and only if it is a fixed point of the following Lipschitz map, namely</p><formula xml:id="formula_87">β = T β where T β = 1 τ + ελ S λ (τ I -Φ * P Φ P )β + Φ * P Y . (<label>36</label></formula><formula xml:id="formula_88">)</formula><p>If ε &gt; 0 or κ 0 &gt; 0, the minimizer is unique.</p><p>If it is unique, we denote it by β λ :</p><formula xml:id="formula_89">β λ = argmin β∈ 2 E |Φ P β -Y | 2 + λp ε (β) . (<label>37</label></formula><formula xml:id="formula_90">)</formula><p>We add a comment. Under Assumption 2 and the definition of β ε , the statistical model is Y = Φ P β ε + W where W has zero mean, so that β λ is also the minimizer of</p><formula xml:id="formula_91">Φ P β -Φ P β ε 2 P + λp ε (β). (<label>38</label></formula><formula xml:id="formula_92">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sparsity properties</head><p>The results of the previous section immediately yield a crude estimate of the number and localization of the non-zero coefficients of our estimator. Indeed, although the set of features could be infinite, β λ n has only a finite number of coefficients different from zero provided that the sequence of weights is bounded away from zero. Corollary 3. Assume that the family of weights satisfies inf γ ∈Γ w γ &gt; 0, then for any β ∈ 2 , the support of S λ (β) is finite. In particular, β λ n , β and β λ are all finitely supported.</p><formula xml:id="formula_93">Proof. Let w 0 = inf γ ∈Γ w γ &gt; 0. Since γ ∈Γ |β γ | 2 &lt; +∞, there is a finite subset Γ 0 ⊂ Γ such that |β γ | ≤ λ 2 w 0 ≤ λ 2 w γ for all γ ∈ Γ 0 .</formula><p>This implies that S λw γ β γ = 0 for γ ∈ Γ 0 , by the definition of soft-thresholding, so that the support of S λ (β) is contained in Γ 0 . Eqs. (32) and ( <ref type="formula" target="#formula_87">36</ref>) and the definition of β imply that β λ n , β λ and β have finite support.</p><p>However, the supports of β and β λ n are not known a priori and to compute β one would need to store the infinite matrix Φ * n Φ n . The following corollary suggests a strategy to overcome this problem.</p><p>Corollary 4. Given ε ≥ 0 and λ &gt; 0, let</p><formula xml:id="formula_94">Γ λ = γ ∈ Γ | ϕ γ n = 0 and w γ ≤ 2 Y n ( ϕ γ n + √ ελ) λ then supp(β λ n ) ⊂ Γ λ . (<label>39</label></formula><formula xml:id="formula_95">)</formula><p>Proof. If ϕ γ n = 0, clearly β γ = 0 is a solution of <ref type="bibr" target="#b30">(31)</ref>. Let M = Y n ; the definition of β λ n as the minimizer of ( <ref type="formula" target="#formula_52">24</ref>) yields the bound</p><formula xml:id="formula_96">E λ n (β λ n ) ≤ E λ n (0) = M 2 , so that Φ n β λ n -Y n ≤ M p ε (β λ n ) ≤ M 2 λ .</formula><p>Hence, for all γ ∈ Γ , the second inequality gives that ελ(β λ n ) 2 γ ≤ M 2 , and we have</p><formula xml:id="formula_97">Y -Φ n β λ n , ϕ γ (X) n -ελ(β λ n ) γ ≤ M ϕ γ n + √ ελ</formula><p>and, therefore, by <ref type="bibr" target="#b30">(31)</ref>,</p><formula xml:id="formula_98">|sgn((β λ n ) γ )| ≤ 2M( ϕ γ n + √ ελ) λw γ . Since |sgn((β λ n ) γ )| = 1 when (β λ n ) γ = 0, this implies that (β λ n ) γ = 0 if 2M( ϕ γ n + √ ελ) λw γ &lt; 1.</formula><p>Now, let Γ be the set of indices γ such that the corresponding feature ϕ γ (X i ) = 0 for some i = 1, . . . , n. If the family of corresponding weights (w γ ) γ ∈Γ goes to infinity, 3 then Γ λ is always finite.</p><p>Remark 2. This last property has immediate computational implications. Since supp(β λ n ) ⊂ Γ λ , one can replace Γ with Γ λ in the definition of Φ n so that Φ * n Φ n is a finite matrix and Φ * n Y is a finite vector. In particular the iterative procedure given by Corollary 1 can then be implemented by means of finite matrices.</p><p>Finally, by inspecting the proof above one sees that a similar result holds true for the distributiondependent minimizer β λ . Its support is always finite, as already noticed, and moreover is included in the following set</p><formula xml:id="formula_99">γ ∈ Γ | ϕ γ P = 0 and w γ ≤ 2 Y P ( ϕ γ P + √ ελ)</formula><p>λ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probabilistic error estimates</head><p>In this section we provide an error analysis for the elastic-net regularization scheme. Our primary goal is the variable selection problem, so that we need to control the error β λ n n -β 2 , where λ n is a suitable choice of the regularization parameter as a function of the data, and β is an explanatory vector encoding the features that are relevant to reconstructing the regression function f * , that is, such that f * = Φ P β. Although Assumption <ref type="bibr" target="#b6">(7)</ref> implies that the above equation has at least a solution β * with 3 The sequence (w γ ) γ ∈Γ goes to infinity, if for all M &gt; 0 there exists a finite set</p><formula xml:id="formula_100">Γ M such that |w γ | &gt; M, ∀γ ∈ Γ M . p ε (β * ) &lt; ∞, nonetheless, the operator Φ P is injective only if (ϕ γ (X)) γ ∈Γ is 2 -linearly independent in L 2</formula><p>Y (P). As usually done for inverse problems, to restore uniqueness we choose, among all the vectors β such that f * = Φ P β, the vector β ε which is the minimizer of the elastic-net penalty. The vector β ε can be regarded as the best representation of the regression function f * according to the elastic-net penalty and we call it the elastic-net representation. Clearly this representation will depend on ε.</p><p>Next we focus on the following error decomposition (for any fixed positive λ),</p><formula xml:id="formula_101">β λ n -β ε 2 ≤ β λ n -β λ 2 + β λ -β ε 2 ,<label>(40)</label></formula><p>where β λ is given by <ref type="bibr" target="#b36">(37)</ref>. The first error term in the right-hand side of the above inequality is due to finite sampling and will be referred to as the sample error, whereas the second error term is deterministic and is called the approximation error. In Section 4.2 we analyze the sample error via concentration inequalities and we consider the behavior of the approximation error as a function of the regularization parameter λ. The analysis of these error terms leads us to discuss the choice of λ and to derive statistical consistency results for elastic-net regularization. In Section 4.3 we discuss a priori and a posteriori (adaptive) parameter choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Identifiability condition and elastic-net representation</head><p>The following proposition provides a way to define a unique solution of the equation</p><formula xml:id="formula_102">f * = Φ P β. Let B = β ∈ 2 | Φ P β = f * (X) = β * + ker Φ P where β * ∈ 2 is</formula><p>given by ( <ref type="formula" target="#formula_24">7</ref>) in Assumption 2 and ker</p><formula xml:id="formula_103">Φ P = {β ∈ 2 | Φ P β = 0} = β ∈ 2 | f β (X) = 0 with probability 1 . Proposition 5. If ε &gt; 0 or κ 0 &gt; 0, there is a unique β ε ∈ 2 such that p ε (β ε ) = inf β∈B p ε (β).<label>(41)</label></formula><p>Proof. If κ 0 &gt; 0, B reduces to a single point, so that there is nothing to prove. If ε &gt; 0, B is a closed subset of a reflexive space. Moreover, by Lemma 2, the penalty p ε (•) is strictly convex, l.s.c. and, by (7) of Assumption 2, there exists at least one</p><formula xml:id="formula_104">β * ∈ B such that p ε (β * ) is finite. Since p ε (β) ≥ ε β 2 2 , p ε (•) is coercive.</formula><p>A standard result of convex analysis implies that the minimizer exists and is unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Consistency: Sample and approximation errors</head><p>The main result of this section is a probabilistic error estimate for β λ n -β λ 2 , which will provide a choice λ = λ n for the regularization parameter as well as a convergence result for β λ n n -β ε 2 . We first need to establish two lemmas. The first one shows that the sample error can be studied in terms of the following quantities</p><formula xml:id="formula_105">Φ * n Φ n -Φ * P Φ P HS and Φ * n W 2<label>(42)</label></formula><p>measuring the perturbation due to random sampling and noise (we recall that • HS denotes the Hilbert-Schmidt norm of a Hilbert-Schmidt operator on 2 ). The second lemma provides suitable probabilistic estimates for these quantities.</p><formula xml:id="formula_106">Lemma 3. Let ε ≥ 0 and λ &gt; 0. If ε &gt; 0 or κ 0 &gt; 0, then β λ n -β λ 2 ≤ 1 κ 0 + ελ (Φ * n Φ n -Φ * P Φ P )(β λ -β ε ) 2 + Φ * n W 2 .<label>(43)</label></formula><p>Proof. Let τ = κ 0 +κ 2 and recall that β λ n and β λ satisfy ( <ref type="formula" target="#formula_75">34</ref>) and <ref type="bibr" target="#b35">(36)</ref>, respectively. Taking into account <ref type="bibr" target="#b29">(30)</ref> we get</p><formula xml:id="formula_107">β λ n -β λ 2 ≤ 1 τ + ελ (τ β λ n -Φ * n Φ n β λ n + Φ * n Y ) -(τ β λ -Φ * P Φ P β λ + Φ * P Y ) 2 .<label>(44)</label></formula><p>By Assumption 2 and the definition of β ε , Y = f * (X) + W , and Φ P β ε and Φ n β ε both coincide with the function f * , regarded as an element of L 2 Y (P) or L 2 Y (P n ) respectively. Moreover by ( <ref type="formula">8</ref>)</p><formula xml:id="formula_108">Φ * P W = 0, so that Φ * n Y -Φ * P Y = (Φ * n Φ n -Φ * P Φ P )β ε + Φ * n W . Moreover (τ I -Φ * n Φ n )β λ n -(τ I -Φ * P Φ P )β λ = (τ I -Φ * n Φ n )(β λ n -β λ ) -(Φ * n Φ n -Φ * P Φ P )β λ .</formula><p>From the assumption on Φ * n Φ n and the choice τ = κ+κ 0 2 , we have τ</p><formula xml:id="formula_109">I -Φ * n Φ n 2 , 2 ≤ κ-κ 0</formula><p>2 , so that <ref type="bibr" target="#b44">(44)</ref> gives</p><formula xml:id="formula_110">(τ + ελ) β λ n -β λ 2 ≤ (Φ * n Φ n -Φ * P Φ P )(β λ -β ε ) 2 + Φ * n W 2 + κ -κ 0 2 β λ n -β λ 2 .</formula><p>Bound ( <ref type="formula" target="#formula_106">43</ref>) is established by observing that τ + ελ -(κκ 0 )/2 = κ 0 + ελ.</p><p>The probabilistic estimates for <ref type="bibr" target="#b42">(42)</ref> are straightforward consequences of the law of large numbers for vector-valued random variables. More precisely, we recall the following probabilistic inequalities based on a result of <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>; see also Th. 3.3.4 of <ref type="bibr" target="#b38">[38]</ref> as well as <ref type="bibr" target="#b39">[39]</ref> for concentration inequalities for Hilbert-space-valued random variables. Proposition 6. Let (ξ n ) n∈N be a sequence of i.i.d. zero-mean random variables taking values in a real separable Hilbert space H and satisfying</p><formula xml:id="formula_111">E[ ξ i m H ] ≤ 1 2 m!M 2 H m-2 ∀m ≥ 2, (<label>45</label></formula><formula xml:id="formula_112">)</formula><p>where M and H are two positive constants. Then, for all n ∈ N and η &gt; 0</p><formula xml:id="formula_113">P 1 n n i=1 ξ i H ≥ η ≤ 2e - nη 2 M 2 +Hη+M √ M 2 +2Hη = 2e -n M 2 H 2 g Hη M 2 (46)</formula><p>where g(t) =</p><formula xml:id="formula_114">t 2 1+t+ √ 1+2t,</formula><p>or, for all δ &gt; 0,</p><formula xml:id="formula_115">P 1 n n i=1 ξ i H ≤ Hδ n + M √ 2δ √ n ≥ 1 -2e -δ . (<label>47</label></formula><formula xml:id="formula_116">)</formula><p>Proof. Bound (46) is given in <ref type="bibr" target="#b35">[36]</ref> with a wrong factor, see <ref type="bibr" target="#b36">[37]</ref>. To show (47), observe that the inverse of the function</p><formula xml:id="formula_117">t 2 1+t+ √ 1+2t is the function t + √ 2t so that the equation 2e -n M 2 H 2 g Hη M 2 = 2e -δ has the solution η = M 2 H H 2 δ nM 2 + 2 H 2 δ nM 2 .</formula><p>Lemma 4. With probability greater than 1 -4e -δ , the following two inequalities hold, for any λ &gt; 0 and ε &gt; 0,</p><formula xml:id="formula_118">Φ * n W 2 ≤ L √ κδ n + σ √ κ √ 2δ √ n ≤ √ 2κδ(σ + L) √ n if δ≤n<label>(48)</label></formula><p>and</p><formula xml:id="formula_119">Φ * n Φ n -Φ * P Φ P HS ≤ κδ n + κ √ 2δ √ n ≤ 3κ √ δ √ n if δ≤n . (49) Proof. Consider the 2 -valued random variable Φ * X W . From (8), E Φ * X W = E E Φ * X W |X = 0 and, for any m ≥ 2, E Φ * X W m 2 = E   γ ∈Γ ϕ γ (X), W 2 m 2   ≤ κ m 2 E |W | m ≤ κ m 2 m! 2 σ 2 L m-2 ,</formula><p>due to ( <ref type="formula" target="#formula_17">5</ref>) and <ref type="bibr" target="#b9">(10)</ref>. Applying (47) with H = √ κL and M = √ κσ , and recalling definition <ref type="bibr" target="#b18">(19)</ref>, we get that</p><formula xml:id="formula_120">Φ * n W 2 ≤ √ κLδ n + √ κσ √ 2δ √ n</formula><p>with probability greater than 1 -2e -δ . Consider the random variable Φ X Φ * X taking values in the Hilbert space of Hilbert-Schmidt operators (where • HS denotes the Hilbert-Schmidt norm). One has that E Φ X Φ * X = Φ P Φ * P and, by ( <ref type="formula">13</ref>)</p><formula xml:id="formula_121">Φ X Φ * X HS ≤ Tr (Φ X Φ * X ) ≤ κ. Hence E Φ X Φ * X -Φ P Φ * P m HS ≤ E Φ X Φ * X -Φ P Φ * P 2 HS (2κ) m-2 ≤ m! 2 κ 2 κ m-2 , by m! ≥ 2 m-1 . Applying (47) with H = M = κ Φ n Φ * n -Φ P Φ * P HS ≤ κδ n + κ √ 2δ √ n ,</formula><p>with probability greater than 1 -2e -δ . The simplified bounds are clear provided that δ ≤ n.</p><p>Remark 3. In both (48) and (49), the condition δ ≤ n allows simplifying the bounds enlightening the dependence on n and the confidence level 1 -4e -δ . In the following results we always assume that δ ≤ n, but we stress the fact that this condition is only needed to simplify the form of the bounds.</p><p>Moreover, observe that, for a fixed confidence level, this requirement on n is very weak -for example, to achieve a 99% confidence level, we only need to require that n ≥ 6.</p><p>The next proposition gives a bound on the sample error. This bound is uniform in the regularization parameter λ in the sense that there exists an event independent of λ such that its probability is greater than 1 -4e -δ and (50) holds true.</p><p>Proposition 7. Assume that ε &gt; 0 or κ 0 &gt; 0. Let δ &gt; 0 and n ∈ N such that δ ≤ n, for any λ &gt; 0 the bound</p><formula xml:id="formula_122">β λ n -β λ 2 ≤ c √ δ √ n(κ 0 + ελ) 1 + β λ -β ε 2 (50)</formula><p>holds with probability greater than 1 -4e -δ , where c = max √ 2κ(σ + L), 3κ .</p><p>Proof. Plug bounds (49) and ( <ref type="formula" target="#formula_118">48</ref>) in <ref type="bibr" target="#b43">(43)</ref>, taking into account that</p><formula xml:id="formula_123">(Φ * n Φ n -Φ * P Φ P )(β λ -β ε ) 2 ≤ Φ * n Φ n -Φ * P Φ P HS β λ -β ε 2 .</formula><p>By inspecting the proof, one sees that the constant κ 0 in <ref type="bibr" target="#b43">(43)</ref> can be replaced by any constant κ λ such that</p><formula xml:id="formula_124">κ 0 ≤ κ λ ≤ inf β∈ 2 | β 2 =1 γ ∈Γ λ β γ ϕ γ 2 n with probability 1,</formula><p>where Γ λ is the set of active features given by Corollary 4. If κ 0 = 0 and κ λ &gt; 0, i.e. when Γ λ is finite and the active features are linearly independent, one can improve bound (52) below. Since we mainly focus on the case of linearly-dependent dictionaries we will not discuss this point any further.</p><p>The following proposition shows that the approximation error β λ β ε 2 tends to zero when λ tends to zero.</p><formula xml:id="formula_125">Proposition 8. If ε &gt; 0 then lim λ→0 β λ -β ε 2 = 0.</formula><p>Proof. It is enough to prove the result for an arbitrary sequence (λ j ) j∈N converging to 0. Putting</p><formula xml:id="formula_126">β j = β λ j , since Φ P β -Y 2 P = Φ P β -f * (X) 2 P + f * (X) -Y 2 P</formula><p>, by the definition of β j as the minimizer of (37) and the fact that β ε solves Φ P β = f * , we get</p><formula xml:id="formula_127">Φ P β j -f * (X) 2 P + λ j p ε (β j ) ≤ Φ P β ε -f * (X)</formula><p>2 P + λ j p ε (β ε ) = λ j p ε (β ε ). Condition <ref type="bibr" target="#b6">(7)</ref> of Assumption 1 ensures that p ε (β ε ) is finite, so that</p><formula xml:id="formula_128">Φ P β j -f * (X)</formula><p>2 P ≤ λ j p ε (β ε ) and p ε (β j ) ≤ p ε (β ε ). Since ε &gt; 0, the last inequality implies that (β j ) j∈N is a bounded sequence in 2 . Hence, possibly passing to a subsequence, (β j ) j∈N converges weakly to some β * . We claim that β * = β ε . Since β → Φ P βf * (X) 2 P is l.s.c.</p><formula xml:id="formula_129">Φ P β * -f * (X) 2 P ≤ lim inf j→∞ Φ P β j -f * (X) 2 P ≤ lim inf j→∞ λ j p ε (β ε ) = 0, that is β * ∈ B. Since p ε (•) is l.s.c., p ε (β * ) ≤ lim inf j→∞ p ε (β j ) ≤ p ε (β ε ).</formula><p>By the definition of β ε , it follows that β * = β ε and, hence,</p><formula xml:id="formula_130">lim j→∞ p ε (β j ) = p ε (β ε ).<label>(51)</label></formula><p>To prove that β j converges to β ε in 2 , it is enough to show that lim j→∞ β j 2 = β ε From (50) and the triangular inequality, we easily deduce that</p><formula xml:id="formula_131">β λ n -β ε 2 ≤ c √ δ √ n(κ 0 + ελ) 1 + β λ -β ε 2 + β λ -β ε 2 (52)</formula><p>with probability greater than 1 -4e -δ . Since the tails are exponential, the above bound and the Borel-Cantelli lemma imply the following theorem, which states that the estimator β λ n converges to the solution β ε , for a suitable choice of the regularization parameter λ.</p><p>Theorem 2. Assume that ε &gt; 0 and κ 0 = 0. Let λ n be a choice of λ as a function of n such that lim n→∞ λ n = 0 and lim n→∞ nλ 2 n -2 log n = +∞. Then</p><formula xml:id="formula_132">lim n→∞ β λ n n -β ε 2 = 0 with probability 1.</formula><p>If κ 0 &gt; 0, the above convergence result holds for any choice of λ n such that lim n→∞ λ n = 0.</p><p>Proof. The only non-trivial statement concerns the convergence with probability 1. We give the proof only for κ 0 = 0, the other one being similar. Let (λ n ) n≥1 be a sequence such that lim n→∞ λ n = 0 and lim n→∞ nλ 2 n -2 log n = +∞. Since lim n→∞ λ n = 0, Proposition 8 ensures that lim n→∞ β λ nβ ε 2 = 0. Hence, it is enough to show that lim n→∞ β λ n n -β λ n 2 = 0 with probability 1. Let D = sup n≥1 ε -1 c(1 + β λ nβ ε</p><p>2 ), which is finite since the approximation error goes to zero if λ tends to zero. Given η &gt; 0, let δ = nλ 2 n η 2 D 2 ≤ n for n large enough, so that bound (50) holds providing that</p><formula xml:id="formula_133">P β λ n n -β λ n 2 ≥ η ≤ 4e -nλ 2 n η 2 D 2 .</formula><p>The condition that lim n→∞ nλ 2 n -2 log n = +∞ implies that the series ∞ Let</p><formula xml:id="formula_134">f n = f β λn n . Since f * = f β ε and E |f n (X) -f * (X)| 2 = Φ P (β λ n n -β ε ) 2 P , the above theorem implies that lim n→∞ E |f n (X) -f * (X)| 2 = 0</formula><p>with probability 1, that is, the consistency of the elastic-net regularization scheme with respect to the square loss.</p><p>Let us remark that we are also able to prove such consistency without assuming (7) in Assumption 2. To this end, we need the following lemma, which is of interest by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5. Instead of Assumption 2, assume that the regression model is given by</head><formula xml:id="formula_135">Y = f * (X) + W ,</formula><p>where f * : X → Y is a bounded function and W satisfies <ref type="bibr" target="#b7">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref>. For fixed λ and ε &gt; 0, with probability greater than 1 -2e -δ we have</p><formula xml:id="formula_136">Φ * n (f λ -f * ) -Φ * P (f λ -f * ) 2 ≤ √ κD λ δ n + √ 2κδ f λ -f * P √ n ,<label>(53)</label></formula><p>where f λ</p><formula xml:id="formula_137">= f β λ and D λ = sup x∈X |f λ (x) -f * (x)|.</formula><p>We notice that in (53), the function f λ f * is regarded both as an element of L 2 Y (P n ) and as an element of L 2 Y (P).</p><p>Proof. Consider the 2 -valued random variable</p><formula xml:id="formula_138">Z = Φ * X (f λ (X) -f * (X)) Z γ = f λ (X) -f * (X), ϕ γ (X) .</formula><p>A simple computation shows that E</p><formula xml:id="formula_139">[Z ] = Φ * P (f λ -f * ) and Z 2 ≤ √ κ|f λ (X) -f * (X)|.</formula><p>Hence, for any m ≥ 2,</p><formula xml:id="formula_140">E Z -E [Z ] m 2 ≤ E Z -E [Z ] 2 2 2 √ κ sup x∈X |f λ (x) -f * (x)| m-2 ≤ κE |f λ (X) -f * (X)| 2 2 √ κ sup x∈X |f λ (x) -f * (x)| m-2 ≤ m! 2 ( √ κ f λ -f * P ) 2 ( √ κD λ ) m-2 .</formula><p>Applying (47) with H = √ κD λ and M = √ κ f λ f * P , we obtain bound (53).</p><p>Observe that under Assumption <ref type="bibr" target="#b6">(7)</ref> and by the definition of β ε one has that</p><formula xml:id="formula_141">D λ ≤ √ κ β λ -β ε 2 , so that (53) becomes (Φ * n Φ n -Φ * P Φ P )(β λ -β ε ) 2 ≤ κδ β λ -β ε 2 n + √ 2κδ Φ P (β λ -β ε ) P √ n .</formula><p>Since Φ P is a compact operator this bound is tighter than the one deduced from (49). However, the price we pay is that the bound does not hold uniformly in λ. We are now able to state the universal strong consistency of the elastic-net regularization scheme. Theorem 3. Assume that (X, Y ) satisfy (8) and <ref type="bibr" target="#b8">(9)</ref> and that the regression function f * is bounded. If the linear span of features (ϕ γ ) γ ∈Γ is dense in L 2 Y (P) and ε &gt; 0, then</p><formula xml:id="formula_142">lim n→∞ E |f n (X) -f * (X)| 2 = 0 with probability 1,</formula><p>provided that lim n→∞ λ n = 0 and lim n→∞ nλ 2 n -2 log n = +∞.</p><p>Proof. As above we bound separately the approximation error and the sample error. As for the first term, let f λ = f β λ . We claim that E |f λ (X) -f * (X)| 2 goes to zero when λ goes to zero. Given η &gt; 0, the fact that the linear span of the features (ϕ γ ) γ ∈Γ is dense in L 2 Y (P) implies that there is</p><formula xml:id="formula_143">β η ∈ 2 such that p ε (β η ) &lt; ∞ and E |f β η (X) -Y | 2 ≤ E |f * (X) -Y | 2 + η. Let λ η = η 1+p ε (β η ) , then, for any λ ≤ λ η , E |f λ (X) -f * (X)| 2 ≤ E |f λ (X) -Y | 2 -E |f * (X) -Y | 2 + λp ε (β λ ) ≤ E |f β η (X) -Y | 2 -E |f * (X) -Y | 2 + λp ε (β η ) ≤ η + η.</formula><p>As for the sample error, we let</p><formula xml:id="formula_144">f λ n = f β λ n (so that f n = f λ n n ) and observe that E |f λ (X) -f λ n (X)| 2 = Φ P (β λ n -β λ ) 2 P ≤ κ β λ n -β λ 2 2 .</formula><p>We bound β λ n -β λ 2 by (53) observing that</p><formula xml:id="formula_145">D λ = sup x∈X |f λ (x) -f * (x)| ≤ sup x∈X |f β λ (x)| + sup x∈X |f * (x)| ≤ √ κ β λ 2 + sup x∈X |f * (x)| ≤ D 1 √ λ</formula><p>where D is a suitable constant and where we used the crude estimate</p><formula xml:id="formula_146">λε β λ 2 2 ≤ E λ (β λ ) ≤ E λ (0) = E |Y | 2 .</formula><p>Hence (53) yields</p><formula xml:id="formula_147">Φ * n (f λ -f * ) -Φ * P (f λ -f * ) 2 ≤ √ κδD √ λn + √ 2κδ f λ (X) -f * (X) P √ n .<label>(54)</label></formula><p>Observe that the proof of ( <ref type="formula" target="#formula_106">43</ref>) does not depend on the existence of β ε provided that we replace both Φ P β λ n ∈ L 2 Y (P) and Φ n β λ n ∈ L 2 Y (P n ) with f * , and we take into account that both Φ P β λ ∈ L 2 Y (P) and Φ n β λ ∈ L 2 Y (P n ) are equal to f λ . Hence, plugging (54) and ( <ref type="formula" target="#formula_118">48</ref>) in (43) we have that with probability greater than 1 -4e -δ</p><formula xml:id="formula_148">β λ n -β λ 2 ≤ D √ δ κ 0 + ελ 1 √ n + 1 √ λn + f λ (X) -f * (X) P √ n</formula><p>where D is a suitable constant and δ ≤ n. The thesis now follows by combining the bounds on the sample and approximation errors and repeating the proof of Theorem 2.</p><p>To have an explicit convergence rate, one needs an explicit bound on the approximation error</p><formula xml:id="formula_149">β λ -β ε 2 , for example of the form β λ -β ε 2 = O(λ r</formula><p>). This is out of the scope of the paper.</p><p>We report only the following simple result.</p><p>Proposition 9. Assume that the features ϕ γ are in finite number and linearly independent.</p><formula xml:id="formula_150">Let N * = |supp(β ε )| and w * = sup γ ∈supp(β ε ) w γ , then β λ -β ε 2 ≤ DN * λ. With the choice λ n = 1 √ n , for any δ &gt; 0 and n ∈ N with δ ≤ n β λ n n -β ε 2 ≤ c √ δ √ nκ 0 1 + DN * √ n + DN * √ n , (<label>55</label></formula><formula xml:id="formula_151">)</formula><p>with probability greater than 1 -4e -δ , where D =</p><formula xml:id="formula_152">w * +2ε β ε ∞ 2κ 0 and c = max √ 2κ(σ + L), 3κ<label>.</label></formula><p>Proof. Observe that the assumption on the set of features is equivalent to assuming that κ 0 &gt; 0. First, we bound the approximation error β λ β ε 2 . As usual, with the choice τ = κ 0 +κ 2 , (36) gives</p><formula xml:id="formula_153">β λ -β ε = 1 τ + ελ S λ (τ I -Φ * P Φ P )β λ + Φ * P Φ P β ε -S λ (τ β ε ) + S λ (τ β ε ) -τ β ε - ελ τ + ελ β ε .</formula><p>Property <ref type="bibr" target="#b29">(30)</ref> implies that</p><formula xml:id="formula_154">β λ -β ε 2 ≤ 1 τ + ελ (τ I -Φ * P Φ P )(β λ -β ε ) 2 + S λ (τ β ε ) -τ β ε 2 + ελ τ + ελ β ε 2 . Since τ I -Φ * P Φ P ≤ κ-κ 0 2 , β ε 2 ≤ N * β ε ∞ and S λ (τ β ε ) -τ β ε 2 ≤ w * N * λ 2 , one has β λ -β ε 2 ≤ κ + κ 0 + 2ελ 2(κ 0 + ελ) 2 κ + κ 0 + 2ελ w * N * λ 2 + 2ελ κ 0 + κ + 2ελ β ε 2 ≤ w * + 2ε β ε ∞ 2κ 0 N * λ = DN * λ.</formula><p>Bound (55) is then a straightforward consequence of (52).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptive choice</head><p>In this section, we suggest an adaptive choice of the regularization parameter λ. The main advantage of this selection rule is that it does not require any knowledge of the behavior of the approximation error. To this end, it is useful to replace the approximation error with the following upper bound</p><formula xml:id="formula_155">A(λ) = sup 0&lt;λ ≤λ β λ -β ε 2 . (<label>56</label></formula><formula xml:id="formula_156">)</formula><p>The following simple result holds. Lemma 6. Given ε &gt; 0, A is an increasing continuous function and</p><formula xml:id="formula_157">β λ -β ε 2 ≤ A(λ) ≤ A &lt; ∞ lim λ→0+ A(λ) = 0.</formula><p>Proof. First of all, we show that λ → β λ is a continuous function. Fix λ &gt; 0; for any h such that λ + h &gt; 0, <ref type="bibr" target="#b35">(36)</ref> with τ = κ 0 +κ 2 and Corollary 1 give</p><formula xml:id="formula_158">β λ+h -β λ 2 ≤ T λ+h (β λ+h ) -T λ+h (β λ ) 2 + T λ+h (β λ ) -T λ (β λ ) 2 ≤ κ -κ 0 κ + κ 0 + 2ε(λ + h) β λ+h -β λ 2 + 1 τ + ε(λ + h) S λ+h β - 1 τ + ελ S λ β<label>2</label></formula><p>where β = (τ I -Φ * P Φ P )β λ + Φ * P Y does not depend on h and we wrote T λ to make explicit the dependence of the map T on the regularization parameter. Hence</p><formula xml:id="formula_159">β λ+h -β λ 2 ≤ τ + ε(λ + h) κ 0 + ε(λ + h) 1 τ + ε(λ + h) - 1 τ + ελ β 2 + 1 τ + ελ S λ+h β -S λ β 2 .</formula><p>The claim follows by observing that (assuming for simplicity that h &gt; 0) </p><formula xml:id="formula_160">S</formula><formula xml:id="formula_161">β λ -β ε 2 ≤ β ε 2 + 1 √ ε p ε (β ε ) =: A.</formula><p>Hence A(λ) ≤ A for all λ. Clearly A(λ) is an increasing function of λ; the fact that β λ β ε 2 is continuous and goes to zero with λ ensures that the same holds true for A(λ).</p><p>Notice that we replaced the approximation error with A(λ) just for a technical reason, namely to deal with an increasing function of λ. If we have a monotonic decay rate at our disposal, such as β λ β ε 2 λ a for some a &gt; 0 and for λ → 0, then clearly A(λ) λ a . Now, we fix ε &gt; 0 and δ ≥ 2 and we assume that κ 0 = 0. Then we simplify bound (52) observing that</p><formula xml:id="formula_162">β λ n -β ε 2 ≤ C 1 √ nελ + A(λ)<label>(57)</label></formula><p>where C = c √ δ(1 + A); the bound holds with probability greater than 1 -4e -δ uniformly for all λ &gt; 0.</p><p>When λ increases, the first term in (57) decreases whereas the second increases; hence to have a tight bound a natural choice of the parameter consists of balancing the two terms in the above bound, taking</p><formula xml:id="formula_163">λ opt n = sup λ ∈]0, ∞[| A(λ) = 1 √ nελ . Since A(λ) is continuous, 1 √ nελ opt n</formula><p>= A(λ opt n ) and the resulting bound is</p><formula xml:id="formula_164">β λ n -β ε 2 ≤ 2C √ nελ opt n . (<label>58</label></formula><formula xml:id="formula_165">)</formula><p>This method for choosing the regularization parameter clearly requires the knowledge of the approximation error. To overcome this drawback, we discuss a data-driven choice for λ that allows achieving the rate (58) without requiring any prior information on A(λ). For this reason, such a choice is said to be adaptive. The procedure we present is also referred to as an a posteriori choice since it depends on the given sample and not only on its cardinality n. In other words, the method is purely data-driven.</p><p>Let us consider a discrete set of values for λ defined by the geometric sequence</p><formula xml:id="formula_166">λ i = λ 0 2 i i ∈ N λ 0 &gt; 0.</formula><p>Notice that we may replace the sequence λ 0 2 i by any other geometric sequence λ i = λ 0 q i with q &gt; 1; this would only lead to a more complicated constant in (60). Define the parameter λ + n as follows</p><formula xml:id="formula_167">λ + n = max λ i | β λ j n -β λ j-1 n 2 ≤ 4C √ nελ j-1</formula><p>for all j = 0, . . . , i</p><p>(with the convention that λ -1 = λ 0 ). This strategy for choosing λ is inspired by a procedure originally proposed in <ref type="bibr" target="#b40">[40]</ref> for Gaussian white noise regression and which has been widely discussed in the context of deterministic as well as stochastic inverse problems (see <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">41]</ref>). In the context of nonparametric regression from random design, this strategy has been considered in <ref type="bibr" target="#b42">[42]</ref> and the following proposition is a simple corollary of a result contained in <ref type="bibr" target="#b42">[42]</ref>.</p><p>Proposition 10. Provided that λ 0 &lt; λ opt n , the following bound holds with probability greater than</p><formula xml:id="formula_169">1 -4e -δ β λ + n n -β ε 2 ≤ 20C √ nελ opt n . (<label>60</label></formula><formula xml:id="formula_170">)</formula><p>Proof. The proposition results from Theorem 2 in <ref type="bibr" target="#b42">[42]</ref>. For completeness, we report here a proof adapted to our setting. Let Ω be the event such that (57) holds for any λ &gt; 0; we have that P[Ω] ≥ 1 -4e -δ and we fix a sample point in Ω.</p><p>The definition of λ opt </p><formula xml:id="formula_171">λ i | A(λ i ) ≤ 1 √ nελ i</formula><p>is not empty and we can define</p><formula xml:id="formula_172">λ * n = max λ i | A(λ i ) ≤ 1 √ nελ i</formula><p>.</p><p>The fact that (λ i ) i∈N is a geometric sequence implies that </p><formula xml:id="formula_173">λ * n ≤ λ opt n &lt; 2λ * n ,<label>(61) while</label></formula><formula xml:id="formula_174">β λ + n n -β λ * n n 2 ≤ k-1 =0 β m+1+ n -β m+ n 2 ≤ k-1 =0 4C √ nελ m+ ≤ 4C √ nελ * n ∞ =0 1 2 = 4C √ nελ * n 2.</formula><p>Finally, recalling (61) and (62), we get bound (60):</p><formula xml:id="formula_175">β λ + n n -β ε 2 ≤ β λ + n n -β λ * n n 2 + β λ * n n -β ε 2 ≤ 8C √ nελ * n + 2C √ nελ * n ≤ 20C 1 √ nελ opt n</formula><p>.</p><p>Notice that the a priori condition λ 0 &lt; λ opt n is satisfied, for example, if λ 0 &lt; 1 Aε √ n . To illustrate the implications of the last proposition, let us suppose that We end noting that, if we specialize our analysis to least squares regularized with a pure 2penalty (i.e. setting w γ = 0, ∀γ ∈ Γ ), then our results lead to the error estimate in the norm of the reproducing kernel space H obtained in <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref>. Indeed, in such a case, β ε is the generalized solution β Ď of the equation Φ P β = f * and the approximation error satisfies (63) under the a priori assumption that the regression vector β Ď is in the range of (Φ * P Φ P ) a for some 0 &lt; a ≤ 1 (the fractional power makes sense since Φ * P Φ P is a positive operator). Under this assumption, it follows that β λ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><p>, where L K : L 2 Y (P) → L 2 Y (P) is the integral operator whose kernel is the reproducing kernel K <ref type="bibr" target="#b45">[45]</ref>. Under this condition, the following bound holds</p><formula xml:id="formula_176">f n -f * H ≤ β λ + n n -β ε 2 n -a 2(a+1) ,</formula><p>which gives the same rate as in Theorem 2 of <ref type="bibr" target="#b43">[43]</ref> and Corollary 17 of <ref type="bibr" target="#b44">[44]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The ε-ball with ε &gt; 0 (solid line), the square ( 1 -ball), which is the ε-ball with ε = 0 (dashed line), and the disc ( 2 -ball), which is the ε-ball with ε → ∞ (dotted line).</figDesc><graphic coords="5,137.64,61.37,198.00,169.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Estimators in the two-dimensional example: T = Lasso, P = elastic net and Q = ridge regression. Top panel: θ + &lt; θ &lt; π/2. Bottom panel: π/4 &lt; θ &lt; θ + .</figDesc><graphic coords="6,117.14,61.37,227.88,221.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 2 .</head><label>2</label><figDesc>The functional p ε (•) is a convex, lower semi-continuous (l.s.c.) functional from 2 into [0, ∞]. Given β ∈ 2 , a vector η ∈ ∂p ε (β) if and only if η γ = w γ sgn(β γ ) + 2εβ γ ∀γ ∈ Γ and γ ∈Γ η 2 γ &lt; +∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 D 2 Remark 4 .</head><label>224</label><figDesc>converges and the Borel-Cantelli lemma gives the thesis. The two conditions on λ n in the above theorem are clearly satisfied with the choice λ n = (1/n) r with 0 &lt; r &lt; 1 2 . Moreover, by inspecting the proof, one can easily check that to have the convergence of β λ n n to β ε in probability, it is enough to require that lim n→∞ λ n = 0 and lim n→∞ nλ 2 n = +∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>n and the assumption λ 0 &lt; λ opt n ensure that A(λ 0 ) ≤ 1 √ nελ 0 .</head><label>10</label><figDesc>Hence the set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a ∈]0, 1]. One has then that λ opt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Proposition 1 .</head><label>1</label><figDesc>The map Φ P : 2 → L 2 Y (P), defined by Φ P β = f β , is a Hilbert-Schmidt operator and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. In both cases the convexity of E λ n implies that β is a minimizer if and only if 0 ∈ ∂E λ n (β). Since Φ n β -Y 2 n is continuous, Corollary III.2.1 of [35] ensures that the subgradient is linear. Observing that Φ n β -Y 2 n is differentiable with derivative 2Φ * 2Φ * n Φ n β -2Φ * n Y + λ∂p ε (β), so that (31) follows taking into account the explicit form of ∂p ε (β), Φ * n Φ n β and Φ *</figDesc><table><row><cell>n Y , given by</cell></row><row><cell>Lemma 2 and Proposition 2, respectively.</cell></row></table><note><p>n Φ n β -2Φ * n Y , we get ∂E λ n (β) =</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.</head><label></label><figDesc>Clearly β is a minimizer of E λ , by<ref type="bibr" target="#b28">(29)</ref> with a = 1 τ +ελ , β is a minimizer of E λ n if and only if β = T n β. show that T n is Lipschitz and calculate explicitly a bound on the Lipschitz constant. By assumption we have κ 0 I ≤ Φ * n Φ n ≤ κI; then, by the Spectral Theorem, τ I -Φ *</figDesc><table><row><cell>ThereforeWe</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>n if</cell></row><row><cell cols="3">and only if it is a solution of</cell><cell></cell></row><row><cell>β = S λ τ +ελ</cell><cell>1 -</cell><cell>ελ τ + ελ</cell><cell>β +</cell></row></table><note><p><p><p><p><p><p>n if and only if it is a minimizer of</p>1   </p>τ +ελ E λ n , which means that, in</p><ref type="bibr" target="#b31">(32)</ref></p>, we can replace λ with λ τ +ελ , Φ n by 1</p>√ τ +ελ Φ n and Y by 1 √ τ +ελ Y . Hence β is a minimizer of E λ 1 τ + ελ Φ * n (Y -Φ n β) . n Φ n 2 , 2 ≤ max {|τ -κ 0 |, |τ -κ|} , where • 2 , 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>2 . Since • 2 is l.s.c., lim inf j→∞ β j 2 ≥ β ε 2 . Hence we are left with proving that lim sup j→∞ β j 2 ≤ β ε However, since β → γ ∈Γ w γ |β γ | is l.s.c.</figDesc><table><row><cell cols="3">and, using (51),</cell><cell></cell><cell></cell></row><row><cell cols="2">lim j→∞ γ ∈Γ</cell><cell cols="2">w γ |β j γ | &lt;</cell><cell cols="2">γ ∈Γ</cell><cell>w γ |β ε</cell><cell>|.</cell></row><row><cell cols="3">lim inf j→∞ γ ∈Γ</cell><cell cols="2">w γ |β j γ | ≥</cell><cell>γ ∈Γ</cell><cell>w γ |β ε</cell><cell>|.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 .</cell></row><row><cell cols="6">Assume the contrary. This implies that, possibly passing to a subsequence,</cell></row><row><cell>lim j→∞</cell><cell>β j</cell><cell cols="2">2 &gt; β ε</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>λ+h β -S λ β γ λ≤|β γ |&lt;w γ (λ+h) |β γ -sgn(β γ )w γ λ| 2 + |β γ |≥w γ (λ+h) goes to zero if h tends to zero. Now, by the definition of β λ and β εελ β λ 2 2 ≤ E |Φ P β λ f * (X)| 2 + λp ε (β λ ) ≤ E |Φ P β ε f * (X)| 2 + λp ε (β ε ) = λp ε (β ε ),</figDesc><table><row><cell>2 2 =</cell><cell></cell><cell></cell><cell></cell><cell>w 2 γ h 2</cell></row><row><cell>≤ h 2</cell><cell>w 2 γ ≤ h 2</cell><cell>(β γ /λ) 2</cell><cell>≤ h 2 β</cell><cell>2 2 /λ 2 ,</cell></row><row><cell>|β γ |≥w γ λ</cell><cell></cell><cell>|β γ |≥w γ λ</cell><cell></cell><cell></cell></row><row><cell>so that</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>w which</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>(57)  with the definition of λ *</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n ensures that</cell></row><row><cell>β n -β ε λ  *  n</cell><cell>2</cell><cell>≤ C</cell><cell>√</cell><cell cols="2">1 nελ  *</cell><cell>(62)</cell></row><row><cell>We show that λ  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>β n -β λ  *  n n λ j</cell><cell>2</cell><cell cols="4">≤ β n -β ε λ j</cell><cell>2</cell><cell>+ β n -β ε λ  *  n</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>≤ C</cell><cell cols="2">√</cell><cell>1 nελ j</cell><cell>+ A(λ j ) +</cell><cell>1 nελ  *  √</cell></row></table><note><p><p><p><p><p><p>n + A(λ * n ) ≤ 2C √ nελ * n</p>. n ≤ λ + n . Indeed, for any λ j &lt; λ * n , using (57) twice, we get</p>n + A(λ * n ) ≤ 4C √ nελ j</p>, where the last inequality holds since</p>λ j &lt; λ * n ≤ λ opt n and A(λ) ≤ 1 √ nελ for all λ &lt; λ opt n . Now 2 m λ 0 ≤ λ * n ≤ λ + n = 2 m+k</p>for some m, k ∈ N, so that</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>To compare this bound with the results in the literature, recall that bothf n = f = f β Ď belongto the reproducing kernel Hilbert space H defined in Proposition 3. In particular, one can check that β Ď ∈ ran(Φ * P Φ P ) a if and only if f * ∈ ran L</figDesc><table><row><cell>n</cell><cell>+ n n -β ε 2(a+1) . β -a λ + n n</cell><cell>2 and</cell></row><row><cell cols="2">f  2a+1</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell></row></table><note><p>*    </p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Alessandro Verri for helpful suggestions and discussions. Christine De Mol acknowledges support by the ''Action de Recherche Concertée'' Nb 02/07-281, the VUB-GOA 62 grant and the National Bank of Belgium BNB; she is also grateful to the DISI, Università di Genova for hospitality during a semester in which the present work was initiated. Ernesto De Vito and Lorenzo Rosasco have been partially supported by the FIRB project RBIN04PARL and by the EU Integrated Project Health-e-Child IST-2004-027749.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Dantzig selector: Statistical estimation when p is much larger than n</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2313" to="2351" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A regularized approach to feature selection for face detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACCV07</title>
		<meeting>ACCV07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mosci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Manag. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A sparsity-enforcing method for learning face features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A regularized method for selecting nested groups of relevant genes from microarray data</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mosci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Traskine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/0809.1777" />
	</analytic>
	<monogr>
		<title level="j">J. Comp. Biol</title>
		<imprint/>
	</monogr>
	<note>in press. Preprint available at</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A method for robust variable selection with significance assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mosci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<ptr target="http://www.disi.unige.it/person/MosciS/PAPERS/esann.pdf" />
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>Preprint available at</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regression selection and shrinkage via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Penalized regressions: The bridge versus the lasso</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Statist</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="416" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparsity in penalized empirical risk minimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Koltchinskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l&apos;Institut Henri Poincaré B, Probab. Statist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="57" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Asymptotics for lasso-type estimators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1356" to="1378" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aggregation and sparsity via l1 penalized least squares</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bunea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsybakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Annu. Conference on Comput. Learning Theory</title>
		<meeting>19th Annu. Conference on Comput. Learning Theory</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="379" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive estimation with soft thresholding penalties</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Loubes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Neerlandica</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="454" to="479" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classifiers of support vector machine type with l 1 complexity regularization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tarigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Van De Geer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1076" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Best subset selection, persistence in high-dimensional statistical learning and optimization under l 1 constraint</title>
		<author>
			<persName><forename type="first">E</forename><surname>Greenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2367" to="2386" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-dimensional generalized linear models and the lasso</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Van De Geer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="614" to="645" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On model selection consistency of Lasso</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2541" to="2563" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Least angle regression</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An iterative thresholding algorithm for linear inverse problems with a sparsity constraint</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Defrise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Mol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1413" to="1457" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A robust hybrid of lasso and ridge regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="page" from="59" to="72" />
			<date type="published" when="2007">2007</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>Providence, Rhode Island</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B Series B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recovery algorithms for vector-valued data with joint sparsity constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rauhut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="577" to="613" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spline models for observational data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CBMS-NSF Regional Conference Series in Applied Mathematics</title>
		<meeting><address><addrLine>SIAM, Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive approximation and learning by greedy algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dahmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="94" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularization without preliminary knowledge of smoothness and error behavior</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereverzev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="303" to="317" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On learning vector-valued functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="204" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vector valued regression for iron overload estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gianesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPR 2008</title>
		<meeting>ICPR 2008<address><addrLine>Tampa, FL, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multiple tasks with kernel methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="615" to="637" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal multi-task kernels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Caponnetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1615" to="1646" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wavelet kernel penalized estimation for non-equispaced design regression</title>
		<author>
			<persName><forename type="first">U</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="55" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vector valued reproducing kernel Hilbert spaces of integrable functions and Mercer theorem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName><surname>Toigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Appl. (Singap.)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="408" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wellner</surname></persName>
		</author>
		<title level="m">Weak Convergence and Empirical Processes</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>Springer Series in Statistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Infinite-dimensional Optimization and Convexity</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ekeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turnbull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chicago Lectures in Mathematics</title>
		<meeting><address><addrLine>Chicago</addrLine></address></meeting>
		<imprint>
			<publisher>The University of Chicago Press</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimum bounds for the distributions of martingales in Banach spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pinelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Probab</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1679" to="1706" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Correction: &apos;&apos;Optimum bounds for the distributions of martingales in Banach spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pinelis</surname></persName>
		</author>
		<idno>MR1331198 (96b:60010</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Probab</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1679" to="1706" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Ann. Probab</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2119</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">V</forename><surname>Yurinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sums and Gaussian Vectors</title>
		<title level="s">Lecture Notes in Mathematics</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1617</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Remarks on inequalities for probabilities of large deviations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Pinelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Sakhanenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Probab. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="148" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On a problem of adaptive estimation in Gaussian white noise</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lepskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Probab. Appl</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="454" to="466" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the adaptive selection of the parameter in regularization of ill-posed problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Pereverzev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2060" to="2076" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive learning via the balancing principle</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereverzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<idno>-TR-2008-062</idno>
		<ptr target="http://dspace.mit.edu/bitstream/handle/1721.1/42896/MIT-CSAIL-TR-2008-062.pdf" />
	</analytic>
	<monogr>
		<title level="m">BCL paper-275/CSAIL and</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Preprint available at</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning theory estimates via integral operators and their approximations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constr. Approx</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="172" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On regularization algorithms in learning theory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereverzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="72" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimal rates for regularized least-squares algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Caponnetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="368" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
