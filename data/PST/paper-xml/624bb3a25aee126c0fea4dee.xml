<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prompt-free and Efficient Few-shot Learning with Language Models</title>
				<funder ref="#_VeJW2yH">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-03">3 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">EPFL</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Idiap Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
							<email>james.henderson@idiap.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">Idiap Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
							<email>marzieh@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
							<email>mathiasl@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
							<email>myazdani@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Prompt-free and Efficient Few-shot Learning with Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-03">3 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.01172v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose PERFECT, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. PERFECT makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few shot NLP tasks demonstrate that PERFECT, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https: //github.com/rabeehk/perfect.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent methods for few-shot language model tuning obtain impressive performance but require careful engineering of prompts and verbalizers to convert inputs to a cloze-format <ref type="bibr" target="#b56">(Taylor, 1953)</ref> that can be scored with pre-trained language models (PLMs) <ref type="bibr" target="#b42">(Radford et al., 2018;</ref><ref type="bibr">Radford et al.;</ref><ref type="bibr" target="#b4">Brown et al., 2020;</ref><ref type="bibr">Schick and Sch?tze, 2021a,b)</ref>. For example, as Figure <ref type="figure">1</ref> shows, a sentiment classifier can be designed by inserting the input text x in a prompt template "x It was <ref type="bibr">[MASK]</ref>" where verbalizers (e.g., 'great' and 'terrible') are substituted for the <ref type="bibr">[MASK]</ref> to score target task labels ('positive' or 'negative'). In this paper, we show that such engineering is Existing few-shot fine-tuning methods require manual engineering to reduce new tasks to masked language modeling. PERFECT does not rely on any handcrafting, removing both patterns and verbalizers (see Figure <ref type="figure" target="#fig_3">3</ref>).</p><p>not needed for few-shot learning and instead can be replaced with simple methods for data-efficient fine-tuning with as few as 32 end-task examples.</p><p>More specifically, we propose PERFECT, a Prompt-free and Efficient paRadigm for FEw-shot Cloze-based fine-Tuning. To remove handcrafted patterns, PERFECT uses task-specific adapter layers <ref type="bibr" target="#b15">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b39">Pfeiffer et al., 2020)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( ?3.1).</head><p>Freezing the underlying PLM with millions or billions of parameters <ref type="bibr" target="#b24">(Liu et al., 2019;</ref><ref type="bibr" target="#b44">Raffel et al., 2020)</ref>, and only tuning adapters with very few new parameters saves on memory and storage costs ( ?4.2), while allowing very sample-efficient tuning ( ?4). It also stabilizes the training by increasing the worst-case performance and decreasing variance across the choice of examples in the few shot training sets ( ?4.3).</p><p>To remove handcrafted verbalizers (with variable token lengths), we introduce a new multi-token fixed-length classifier scheme that learns task label embeddings which are independent from the language model vocabulary during fine-tuning ( ?3.2). We show ( ?4) that this approach is sample efficient and outperforms carefully engineered verbalizers from random initialization ( ?4). It also allows us to avoid previously used expensive auto-regressive decoding schemes <ref type="bibr">(Schick and Sch?tze, 2021b)</ref>, by leveraging prototypical networks <ref type="bibr" target="#b53">(Snell et al., 2017)</ref> over multiple tokens. Overall, these changes enable up to 100x faster learning and inference ( ?4.2).</p><p>PERFECT has several advantages: It avoids engineering patterns and verbalizers for each new task, which can be cumbersome. Recent work has shown that even some intentionally irrelevant or misleading prompts can perform as well as more interpretable ones <ref type="bibr" target="#b61">(Webson and Pavlick, 2021)</ref>. Unlike the zero-shot or extreme few-shot case, where prompting might be essential, we argue in this paper that all you need is tens of training examples to avoid these challenges by adopting PERFECT or a similar data-efficient learning method. Experiments on a wide variety of NLP tasks demonstrate that PERFECT outperforms state-of-the-art prompt-based methods while being significantly more efficient in inference and training time, storage, and memory usage ( ?4.2). To the best of our knowledge, we are the first to propose a few-shot learning method using the MLM objective in PLMs that provide state-of-the-art results while removing all per-task manual engineering. k ? Y is the corresponding label, where |Y| = K. We additionally assume access to a development set with the same size as the training data. Note that larger validation sets can grant a substantial advantage <ref type="bibr" target="#b36">(Perez et al., 2021)</ref>, and thus it is important to use a limited validation size to be in line with the goal of few-shot learning. Unless specified otherwise, in this work, we use 16 training examples (N = 16) and a validation set with 16 examples, for a total of 32-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adapters</head><p>Recent work has shown that fine-tuning all parameters of PLMs with a large number of parameters in low-resource datasets can lead to a sub-optimal solution <ref type="bibr" target="#b37">(Peters et al., 2019;</ref><ref type="bibr" target="#b9">Dodge et al., 2020)</ref>. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, <ref type="bibr" target="#b47">Rebuffi et al. (2018)</ref> and <ref type="bibr" target="#b15">Houlsby et al. (2019)</ref> suggest an efficient alternative, by inserting small task-specific modules called adapters within layers of a PLMs. They then only train the newly added adapters and layer normalization, while fixing the remaining parameters of a PLM.</p><p>Each layer of a transformer model is composed of two primary modules: a) an attention block, and b) a feed-forward block, where both modules are followed by a skip connection. As depicted in Figure <ref type="figure" target="#fig_2">2</ref>, adapters are normally inserted after each of these blocks before the skip connection. Adapters are bottleneck architectures. By keeping input and output dimensions the same, they introduce no additional architectural changes. Each adapter, A(.) ? R H , consists of a down-projection, D(.) ? R H?B , a non-linearity, such as GeLU <ref type="bibr" target="#b14">(Hendrycks and Gimpel, 2016)</ref>, and an up-projection U(.) ? R B?H , where H is the dimension of input hidden states x, and B is the bottleneck size. Formally defined as:</p><formula xml:id="formula_0">A(x)=U(GeLU(D(x)))+x,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt-based Fine-tuning</head><p>Standard Fine-tuning: In standard fine-tuning with PLMs <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, first a special <ref type="bibr">[CLS]</ref> token is appended to the input x, and then the PLM maps it to a sequence of hidden representations h = (h 1 , ... , h S ) with h i ? R H , where H is the hidden dimension, and S is the maximum sequence length. Then, a classifier, softmax(W T h [CLS] ), using the embedding of the classification token (h <ref type="bibr">[CLS]</ref> ), is trained end-to-end for each downstream task. The main drawback of this approach is the discrepancy between the pre-training and fine-tuning phases since PLMs have been trained to predict mask tokens in a masked language modeling task <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt-based tuning:</head><p>To address this discrepancy, prompt-based fine-tuning <ref type="bibr">(Schick and Sch?tze, 2021a,b;</ref><ref type="bibr" target="#b11">Gao et al., 2021)</ref> formulates tasks in a clozeformat <ref type="bibr" target="#b56">(Taylor, 1953)</ref>. This way, the model can predict targets with a masked language modeling (MLM) objective. For example, as shown in Figure <ref type="figure">1</ref>, for a sentiment classification task, inputs are converted to:</p><formula xml:id="formula_1">x prompt = [CLS] x . It was pattern [MASK] . [SEP]</formula><p>Then, the PLM determines which verbalizer (e.g., 'great' and 'terrible') is the most likely substitute for the mask in the x prompt . This subsequently determines the score of targets ('positive' or 'negative'). In detail:</p><p>Training strategy: Let M : Y ? V be a mapping from target labels to individual words in a PLM's vocabulary. We refer to this mapping as verbalizers.</p><p>Then the input is converted to x prompt = T (x) by appending a pattern and a mask token to x so that it has the format of a masked language modeling input. Then, the classification task is converted to a MLM objective <ref type="bibr">(Tam et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021a)</ref>, and the PLM computes the probability of the label y as:</p><formula xml:id="formula_2">p(y|x)=p([MASK]=M(y)|x prompt ) = exp(W T M(y) h [MASK] ) v ?V exp(W T v h [MASK] ) ,<label>(2)</label></formula><p>where h <ref type="bibr">[MASK]</ref> is the last hidden representation of the mask, and W v shows the output embedding of the PLM for each verbalizer v ?V. For many tasks, verbalizers have multiple tokens. Schick and Sch?tze (2021b) extended (2) to multiple mask tokens by adding the maximum number of mask tokens M needed to express the outputs (verbalizers) for a task.</p><p>In that case, Schick and Sch?tze (2021b) computes the probability of each class as the summation of the log probabilities of each token in the corresponding verbalizer, and then they add a hinge loss to ensure a margin between the correct verbalizer and the incorrect ones.</p><p>Inference strategy: During inference, the model needs to select which verbalizer to use in the given context. <ref type="bibr">Schick and Sch?tze (2021b)</ref> predicts the verbalizer tokens in an autoregressive fashion. They first trim the number of mask tokens from M to each candidate verbalizer's token length and compute the probability of each mask token. They then choose the predicted token with the highest probability and replace the corresponding mask token. Conditioning We replace patterns using task-specific adapters and design label embeddings for the classes. We only train the green blocks (the label embeddings, adapters, and layer norms).</p><p>on this new token, the probabilities of the remaining mask positions are recomputed. They repeat this autoregressive decoding until they fill all mask positions. This inference strategy is very slow, as the number of forward passes increases with the number of classes and the number of verbalizer's tokens. This formulation obtained impressive few-shot performance with PLMs. However, the success of this approach heavily relies on engineering handcrafted patterns and verbalizers. Coming up with suitable verbalizers and patterns can be difficult <ref type="bibr">(Mishra et al., 2022b,a)</ref>. Additionally, the performance is sensitive to the wording of patterns <ref type="bibr" target="#b65">(Zhao et al., 2021;</ref><ref type="bibr" target="#b36">Perez et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021a;</ref><ref type="bibr" target="#b17">Jiang et al., 2020)</ref> or to the chosen verbalizers <ref type="bibr" target="#b61">(Webson and Pavlick, 2021)</ref>.</p><p>In addition, handcrafted verbalizers cause problems for efficient training: a) they require updating the PLM embedding layer, causing large memory overhead; b) fine-tuning PLMs also requires a very small learning rate (usually 10 -5 ), which slows down tuning the parameters of the verbalizers; c) modeling verbalizers as one of the tokens of the PLM vocabulary (perhaps unintentionally) impacts the input representation during tuning; d) verbalizers have variable token lengths, complicating the implementation in a vectorized format, thereby making it challenging to efficiently fine-tune PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose PERFECT, a verbalizer and pattern free few-shot learning method. We design PERFECT to be close to the pre-training phase, similar to the PET family of models <ref type="bibr">(Schick and Sch?tze, 2021b;</ref><ref type="bibr" target="#b11">Gao et al., 2021)</ref>, while replacing handcrafted patterns and verbalizers with new components that are designed to describe the task and learn the labels. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we first convert each input x input to its masked language modeling (MLM) input containing M mask tokens [MASK]<ref type="foot" target="#foot_0">1</ref> with no added patterns, denoted as x masked = T (x input ).<ref type="foot" target="#foot_1">2</ref> PERFECT then trains a classifier per-token and optimizes the average multi-class hinge loss over each mask position.</p><p>Three main components play a role in the success of PERFECT: a) a pattern-free task description, where we use task-specific adapters to efficiently tell the model about the given task, replacing previously manually engineered patterns ( ?3.1), b) multi-token label-embedding as an efficient mechanism to learn the label representations, removing manually designed verbalizers ( ?3.2). c) an efficient inference strategy building on top of the idea of prototypical networks <ref type="bibr" target="#b53">(Snell et al., 2017)</ref> ( ?3.4), which replaces prior iterative autoregressive decoding methods <ref type="bibr">(Schick and Sch?tze, 2021b)</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we fix the underlying PLM model and only optimize the new parameters that we add (green boxes). This includes the task-specific adapters to adapt the representations for a given task and the multi-token label representations. We detail each of these components below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pattern-Free Task Description</head><p>We use task-specific adapter layers to provide the model with learned, implicit task descriptions. Adapters additionally bring multiple other benefits: a) fine-tuning all weights of PLMs with millions or billions of parameters is sample-inefficient, and can be unstable in low-resource settings <ref type="bibr" target="#b9">(Dodge et al., 2020)</ref>; adapters allow sample-efficient fine-tuning, by keeping the underlying PLM fixed, b) adapters reduce the storage and memory footprints ( ?4.2), c) they also increase stability and performance ( ?4), making them an excellent choice for few-shot fine-tuning. To our knowledge, this is the first approach for using task-specific adapters to effectively and efficiently remove patterns in few-shot learning. Experimental results in ?4 show its effectiveness compared to handcrafted patterns and soft prompts <ref type="bibr" target="#b23">(Li and Liang, 2021;</ref><ref type="bibr" target="#b19">Lester et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Token Label Embeddings</head><p>We freeze the weights of the PLM's embedding layer and introduce a separate label embedding L?R K?M?H , which is a multi-token label representation where M is the number of tokens representing each label, K indicates the number of classes, H is the input hidden dimension. Using a fixed number of tokens M for each label, versus variable-token length verbalizers used in prior work <ref type="bibr">(Schick and Sch?tze, 2021a,b)</ref> substantially simplifies the implementation and accelerates the training ( ?4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training PERFECT</head><p>As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we optimize label embeddings so that the PLM predicts the correct label, and optimize adapters to adapt the PLM for the given task. For label embeddings, PERFECT trains a classifier per token and optimizes the average multi-class hinge loss over all mask positions. Given x masked , let h [MASK] i be the embedding of its i-th mask token from the last layer of the PLM encoder. Additionally, let f(.) : R H ? R K be a per-token classifier that computes the predictions by multiplying the mask token embedding with its corresponding label embedding. Formally defined as:</p><formula xml:id="formula_3">t i =f(h [MASK] i )=L T i h [MASK] i ,</formula><p>where L i ? R K?H shows the label embedding for the i-th mask position. Then, for each mask position, we optimize a multi-class hinge loss between their scores t i and labels. Formally defined as:</p><formula xml:id="formula_4">L(x,y,i)= K k=1,k =y max(0,m-t iy +t ik ) K ,</formula><p>where t ik shows the k-th element of t i , representing the score corresponding to class k, and m is the margin, which we fix to the default value of m = 1.</p><p>Then, the final loss is computed by averaging the loss over all mask tokens and training samples:</p><formula xml:id="formula_5">L= 1 M|D| (x,y)?D M i=1 L(x,y,i) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference with PERFECT</head><p>During evaluation, instead of relying on the prior iterative autoregressive decoding schemes <ref type="bibr">(Schick and Sch?tze, 2021b)</ref>, we classify a query point by finding the nearest class prototype to the mask token embeddings:</p><formula xml:id="formula_6">y =argmax y?Y max i?{1,...,M} exp -d(h q i ,c iy ) , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where d is squared euclidean distance,<ref type="foot" target="#foot_2">3</ref> h q i indicates the embedding of the i-th mask position for the query sample q, and c iy ? R D is the prototype representation of the i-th mask token with class label y, i.e., the mean embedding of i-th mask position in all training samples with label y:</p><formula xml:id="formula_8">c iy = 1 |D y | b?Dy h b i ,<label>(5)</label></formula><p>where h b i shows the embedding of i-th mask position for training sample b, and D y is the training instances with class y. This strategy closely follows prototypical networks <ref type="bibr" target="#b53">(Snell et al., 2017)</ref>, but applied across multiple tokens. We choose this form of inference because prototypical networks are known to be sample efficient and robust <ref type="bibr" target="#b53">(Snell et al., 2017)</ref>, and because it substantially speeds up evaluation compared to prior methods ( ?4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments on a variety of NLP datasets to evaluate the performance of PERFECT and compare it with state-of-the-art few-shot learning.</p><p>Datasets: We consider 7 tasks and 12 datasets: 1) the sentiment analysis datasets SST-2 <ref type="bibr" target="#b54">(Socher et al., 2013)</ref>, SST-5 <ref type="bibr" target="#b54">(Socher et al., 2013)</ref>, MR <ref type="bibr" target="#b35">(Pang and Lee, 2005)</ref>, and CR <ref type="bibr" target="#b16">(Hu and Liu, 2004)</ref>, 2) the subjectivity classification dataset SUBJ <ref type="bibr" target="#b34">(Pang and Lee, 2004)</ref>, 3) the question classification dataset TREC <ref type="bibr" target="#b58">(Voorhees and Tice, 2000)</ref>, 4) the natural language inference datasets CB <ref type="bibr" target="#b7">(De Marneffe et al., 2019)</ref> and RTE <ref type="bibr">(Wang et al., 2019a)</ref>, 5) the question answering dataset QNLI <ref type="bibr">(Rajpurkar et al., 2016), 6)</ref> the word sense disambiguation dataset WiC (Pilehvar and Camacho-Collados, 2019), 7) the paraphrase detection datasets MRPC <ref type="bibr" target="#b10">(Dolan and Brockett, 2005)</ref> and QQP. <ref type="foot" target="#foot_3">4</ref> See datasets statistics in Appendix A.</p><p>For MR, CR, SST-5, SUBJ, and TREC, we test on the original test sets, while for other datasets, since test sets are not publicly available, we test on the original validation set. We sample 16 instances per label from the training set to form training and validation sets.</p><p>Baselines We compare with the state-of-the-art few-shot learning of PET and fine-tuning:</p><p>PET <ref type="bibr">(Schick and Sch?tze, 2021a,b)</ref> is the stateof-the-art few-shot learning method that employs carefully crafted verbalizers and patterns. We report the best (PET-best) and average (PET-average) results among all patterns and verbalizers. <ref type="foot" target="#foot_4">5</ref>FINETUNE The standard fine-tuning <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, with adding a classifier on top of the [CLS] token and fine-tuning all parameters.</p><p>Our method We study the performance of PERFECT and perform an extensive ablation study to show the effectiveness of our design choices:</p><p>PERFECT-rand We randomly initialize the label embedding L from a normal distribution N (0,?) with ? = 10 -4 (chosen based on validation performance, see Appendix D) without relying on any handcrafted patterns and verbalizers. As an ablation, we study the following two variants:</p><p>PERFECT-init We initialize the label embedding with the token embeddings of manually designed verbalizers in the PLM's vocabulary to study the impact of engineered verbalizers.</p><p>prompt+mte To compare the impact of adapters versus soft prompt-tuning for few-shot learning, we append trainable continuous prompt embeddings to the input <ref type="bibr" target="#b19">(Lester et al., 2021)</ref>. Then we only tune the soft prompt and multi-token label embeddings (mte).</p><p>bitfit+mte Following <ref type="bibr" target="#b5">Cai et al. (2020)</ref> and Ravfogel et al. ( <ref type="formula">2021</ref>), we tune biases as an alternative to adapters. We additionally tune multi-token label embeddings.</p><p>Logan Face PyTorch implementation <ref type="bibr" target="#b62">(Wolf et al., 2020)</ref>. For the baselines, we used the carefully manually designed patterns and verbalizers in <ref type="bibr" target="#b11">Gao et al. (2021)</ref>, <ref type="bibr" target="#b30">Min et al. (2021)</ref>, and Schick and Sch?tze (2021b) (usually 5 different options per datasets; see Appendix B). We evaluate all methods using 5 different random samples to create the training/validation sets and 4 different random seeds for training. Therefore, for PET-average, we report the results on 20 x 5 (number of patterns and verbalizers) = 100 runs, while for PET-best and our method, we report the results over 20 runs. The variance in few-shot learning methods is usually high <ref type="bibr" target="#b36">(Perez et al., 2021;</ref><ref type="bibr" target="#b65">Zhao et al., 2021;</ref><ref type="bibr" target="#b26">Lu et al., 2021)</ref>. Therefore, we report average, worst-case performance, and standard deviation across all runs, where the last two values can be important for risk-sensitive applications <ref type="bibr" target="#b1">(Asri et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the performance of all methods. PERFECT obtains state-of-the-art results, improving the performance compared to PET-average by +1.1 and +4.6 points for single-sentence and sentence-pair datasets respectively. It even outperforms PET-best, where we report the best performance of PET across multiple manually engineered patterns and verbalizers. Moreover, PERFECT generally improves the minimum performance and reduces standard deviation substantially. Finally, PERFECT is also significantly more efficient: reducing the training and inference time, memory usage, and storage costs (see ?4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PET-best improves the results over PET-average</head><p>showing that PET is unstable to the choice of patterns and verbalizers; this difference is more severe for sentence-pair benchmarks. This might be because the position of the mask highly impacts the results, and the patterns used for sentence-pair datasets in <ref type="bibr">Schick and Sch?tze (2021b)</ref>   As an ablation, even if we initialize the label embedding with handcrafted verbalizers in PER-FECT-init, it consistently obtains lower performance, demonstrating that PERFECT is able to obtain state-ofthe-art performance with learning from pure random initialization. We argue that initializing randomly close to zero (with low variance ? = 10 -4 ), as done in our case, slightly improves performance, which perhaps is not satisfied when initializing from the manually engineered verbalizers (see Appendix D).</p><p>As a second ablation, when learning patterns with optimizing soft prompts in prompt+mte, we observe high sensitivity to learning rate, as also confirmed in <ref type="bibr" target="#b23">Li and Liang (2021)</ref> and <ref type="bibr">Mahabadi et al. (2021a)</ref>. We experimented with multiple learning rates but performance consistently lags behind PERFECT-rand. This can be explained by the low flexibility of such methods as all the information regarding specifying patterns needs to be contained in the prefixes. As a result, the method only allows limited interaction with the rest of the model parameters, and obtaining good performance requires very large models <ref type="bibr" target="#b19">(Lester et al., 2021)</ref>. In addition, increasing the sequence length leads to memory overhead <ref type="bibr">(Mahabadi et al., 2021a)</ref>, and the number of prompt tokens is capped by the number of tokens that can fit in the maximum input length, which can be a limitation for tasks requiring large contexts.</p><p>As a third ablation, tuning biases with optimizing soft prompts in bitfit+mte obtains lower performance compared to PERFECT, showing that adapters are a better alternative compared to tuning biases to learn task descriptions for few-shot learning.</p><p>We include more ablation results on design choices of PERFECT in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency Evaluation</head><p>In this section, we compare the efficiency of PERFECT with the state-of-the-art few-shot learning method, PET. To this end, we train all methods for ten epochs on the 500-sampled QNLI dataset. We select the largest batch size for each method that fits a fixed budget of the GPU memory (40 GB).</p><p>Due to the auto-regressive inference strategy of PET <ref type="bibr">(Schick and Sch?tze, 2021b)</ref>, all prior work implemented it with a batch size of 1 <ref type="bibr" target="#b36">(Perez et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021b;</ref><ref type="bibr">Tam et al., 2021)</ref>. Additionally, since PET deals with verbalizers of variable lengths, it is hard to implement their training phase in batch mode. We specifically choose QNLI to have verbalizers of the same length and enable batching for comparison purposes (referred to as PET in batch). However, verbalizers are still not of fixed-length for most other tasks, and this speed-up does not apply generally to PET.</p><p>In Table <ref type="table" target="#tab_2">2</ref>, for each method we report the percentage of trained parameters, memory usage, training time, and inference time. PERFECT reduces the number of trained parameters, and therefore the storage requirement, by 99.08%. It additionally reduces the memory requirement by 21.93% compared to PET. PERFECT speeds up training substantially, by 97.22% relative to the original PET's implementation, and 30.85% to our implementation of PET. This is because adapter-based tuning saves on memory and allows training with larger batch sizes. In addition, PERFECT is significantly faster during inference time (96.76% less inference time relative to PET).</p><p>Note that although prompt+mte and bitfit+mte can also reduce the storage costs, by having 0.02M and 0.32 M trainable parameters respectively, they are not expressive enough to learn task descriptions, and their performance substantially lags behind PERFECT (see Table <ref type="table" target="#tab_0">1</ref>).</p><p>Overall, given the size of PLMs with millions and billions of parameters <ref type="bibr" target="#b24">(Liu et al., 2019;</ref><ref type="bibr" target="#b44">Raffel et al., 2020)</ref>, efficient few-shot learning methods are of paramount importance for practical applications. PERFECT not only outperforms the state-of-the-art in terms of accuracy and generally improves the stability (Table <ref type="table" target="#tab_0">1</ref>), but also is significantly more efficient in runtime, storage, and memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Can task-specific adapters replace manually engineered patterns? PERFECT is a pattern-free approach and employs adapters to provide the PLMs with task descriptions implicitly. In this section, we study the contribution of replacing manual patterns with adapters in isolation without considering our other contributions in representing labels, training, and inference. In PET <ref type="bibr">(Schick and Sch?tze, 2021a,b)</ref>, we replace the handcrafted patterns with task-specific adapters (Pattern-Free) while keeping the verbalizers and the training and inference intact<ref type="foot" target="#foot_5">6</ref> and train it with a similar setup as in ?4. Table <ref type="table" target="#tab_3">3</ref> shows the results. While PET is very sensitive to the choice of prompts, adapters provide an efficient alternative to learn patterns robustly by improving the performance (average and worst-case) and reducing the standard deviation. This finding demonstrates that task-specific adapters can effectively replace manually engineered prompts. Additionally, they also save on the training budget by at least 1/number of patterns (normally 1/5) by not requiring running the method for different choices of patterns, and by freezing most parameters, this saves on memory and offers additional speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Study Impact of Removing Adapters</head><p>To study the impact of adapters in learning patterns, we remove adapters, while keeping the label embedding. Handcrafted patterns are not included and we tune all parameters of the model. and unstable on resource-limited datasets <ref type="bibr" target="#b9">(Dodge et al., 2020;</ref><ref type="bibr" target="#b64">Zhang et al., 2020;</ref><ref type="bibr" target="#b33">Mosbach et al., 2021)</ref>. However, by using adapters, we substantially reduce the number of trainable parameters, allowing the model to be better tuned in a few-shot setting.</p><p>Impact of the number of masks In Table <ref type="table" target="#tab_0">1</ref>, to compare our design with PET in isolation, we fixed the number of mask tokens as the maximum number inserted by PET. In table 5, we study the impact of varying the number of inserted mask tokens for a random selection of six tasks. For most tasks, having two mask tokens performs the best, while for MR and RTE, having one, and for MRPC, inserting ten masks improves the results substantially. The number of required masks might be correlated with the difficulty of the task. PERFECT is designed to be general, enabling having multiple mask tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Adapter Layers: Mahabadi et al. (2021b) and <ref type="bibr">?st?n et al. (2020)</ref> proposed to generate adapters' weights using hypernetworks <ref type="bibr" target="#b13">(Ha et al., 2017)</ref>, where <ref type="bibr">Mahabadi et al. (2021b)</ref> proposed to share a small hypernetwork to generate conditional adapter weights efficiently for each transformer layer and task. <ref type="bibr">Mahabadi et al. (2021a)</ref> proposed compacter layers by building on top of ideas of parameterized hypercomplex layers <ref type="bibr" target="#b63">(Zhang et al., 2021)</ref> and low-rank methods <ref type="bibr" target="#b22">(Li et al., 2018;</ref><ref type="bibr" target="#b0">Aghajanyan et al., 2021)</ref>, as an efficient fine-tuning method for PLMs. We are the first to employ adapters to replace handcrafted patterns for few-shot learning.</p><p>Few-shot Learning with PLMs: Le Scao and Rush (2021) showed that prompting provides substantial improvements compared to fine-tuning, especially in low-resource settings. Subsequently, researchers continuously tried to address the challenges of manually engineered patterns and verbalizers: a) Learning the patterns in a continuous space <ref type="bibr" target="#b23">(Li and Liang, 2021;</ref><ref type="bibr" target="#b41">Qin and Eisner, 2021;</ref><ref type="bibr" target="#b19">Lester et al., 2021)</ref>, while freezing PLM for efficiency, has the problem that, in most cases, such an approach only works with very large scale PLMs <ref type="bibr" target="#b19">(Lester et al., 2021)</ref>, and lags behind full fine-tuning in a general setting, while being inefficient and not as effective compared to adapters <ref type="bibr">(Mahabadi et al., 2021a)</ref>. b) Optimizing patterns in a discrete space <ref type="bibr" target="#b52">(Shin et al., 2020;</ref><ref type="bibr" target="#b17">Jiang et al., 2020;</ref><ref type="bibr" target="#b11">Gao et al., 2021)</ref> has the problem that such methods are computationally costly. c) Automatically finding verbalizers in a discrete way <ref type="bibr" target="#b48">(Schick et al., 2020;</ref><ref type="bibr">Schick and Sch?tze, 2021a</ref>) is computationally expensive and does not perform as well as manually designed ones. d) Removing manually designed patterns <ref type="bibr" target="#b25">(Logan IV et al., 2021)</ref> substantially lags behind the expert-designed ones. Our proposed method, PER-FECT, does not rely on any handcrafted patterns and verbalizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed PERFECT, a simple and efficient method for few-shot learning with pre-trained language models without relying on handcrafted patterns and verbalizers. PERFECT employs task-specific adapters to learn task descriptions implicitly, replacing previous handcrafted patterns, and a continuous multi-token label embedding to represent the output classes. Through extensive experiments over 12 NLP benchmarks, we demonstrate that PERFECT, despite being far simpler and more efficient than recent few-shot learning methods, produces state-of-the-art results. Overall, the simplicity and effectiveness of PERFECT make it a promising approach for few-shot learning with PLMs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>Datasets Table <ref type="table" target="#tab_7">6</ref> shows the stastistics of the datasets used. We download SST-2, MR, CR, SST-5, and SUBJ from <ref type="bibr" target="#b11">Gao et al. (2021)</ref>, while the rest of the datasets are downloaded from the HuggingFace Datasets library <ref type="bibr">(Lhoest et al., 2021b,a)</ref>. RTE, CB, WiC datasets are from SuperGLUE benchmark <ref type="bibr">(Wang et al., 2019a)</ref>, while QQP, MRPC and QNLI are from GLUE benchmark <ref type="bibr">(Wang et al., 2019b)</ref> with Creative Commons license (CC BY 4.0). RTE <ref type="bibr">(Wang et al., 2019a</ref>) is a combination of data from RTE1 <ref type="bibr" target="#b6">(Dagan et al., 2005)</ref>, RTE2 <ref type="bibr" target="#b2">(Bar-Haim et al., 2006</ref><ref type="bibr">), RTE3 (Giampiccolo et al., 2007)</ref>, and RTE5 <ref type="bibr" target="#b3">(Bentivogli et al., 2009)</ref>. For WiC (Pilehvar and Camacho-Collados, 2019) sentences are selected from VerbNet <ref type="bibr" target="#b51">(Schuler, 2005)</ref>, WordNet <ref type="bibr" target="#b29">(Miller, 1995)</ref>, and Wiktionary.</p><p>Computing infrastructure We run all the experiments on one NVIDIA A100 with 40G of memory.</p><p>Training hyper-parameters We set the maximum sequence length based on the recommended values in the HuggingFace repository <ref type="bibr" target="#b62">(Wolf et al., 2020)</ref> and prior work <ref type="bibr" target="#b30">(Min et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021b)</ref>, i.e., we set it to 256 for SUBJ, CR, CB, RTE, and WiC, and 128 for other datasets. For all methods, we use a batch size of 32. For FINETUNE and PET, we use the default learning rate of 10 -5 , while for our method, as required by adapter-based methods <ref type="bibr">(Mahabadi et al., 2021a)</ref>, we set the learning rate to a higher value of 10 -4 .<ref type="foot" target="#foot_6">7</ref> Through all experiments, we fix the adapter bottleneck size to 64. Following <ref type="bibr">Pfeiffer et al. (2021)</ref>, we experimented with keeping one of the adapters in each layer for better training efficiency and found keeping the adapter after the feed-forward module in each layer to perform the best.</p><p>For tuning label embedding, we use the learning rate of {10 -1 , 10 -2 , 10 -3 , 10 -4 , 10 -5 } and choose the one obtaining the highest validation performance. For PERFECT-prompt, we tune the continuous prompt for learning rate of {10 -1 ,10 -2 ,10 -3 }.<ref type="foot" target="#foot_7">8</ref> Following <ref type="bibr" target="#b19">Lester et al. (2021)</ref>, for PERFECT-prompt, we set the number of prompt tokens to 20, and initialize them with a random subset of the top 5000 token's embedding of the PLM. We train all methods for 6000 steps. Based on our results, this is sufficient to allow the models to converge. We save a checkpoint every 100 steps for all methods and report the results for the hyper-parameters performing the best on the validation set for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Choice of Patterns and Verbalizers</head><p>For SST-2, MR, CR, SST-5, and TREC, we used 4 different patterns and verbalizers from <ref type="bibr" target="#b11">Gao et al. (2021)</ref>. For CB, WiC, RTE datasets, we used the designed patterns and verbalizers in <ref type="bibr">Schick and Sch?tze (2021b)</ref>. For QQP, MRPC, and QNLI, we wrote the patterns and verbalizers inspired by the ones in <ref type="bibr">Schick and Sch?tze (2021b)</ref>. The used patterns and verbalizers are as follows:</p><p>? For sentiment analysis tasks (MR, CR, SST-2, SST-5), given a sentence s:</p><p>s A &lt;MASK&gt; one.</p><p>s It was &lt;MASK&gt;.</p><p>s All in all &lt;MASK&gt;.</p><p>s A &lt;MASK&gt; piece.</p><p>with "great" as a verbalizer for positive, "terrible" for negative. In case of SST-5 with five labels, we expand it to "great", "good", "okay", "bad", and "terrible".</p><p>with "Yes" and "No" as verbalizers for duplicate and not duplicate respectively.</p><p>? For MRPC, given two sentences s 1 and s 2 : Do s1 and s2 have the same meaning?&lt;MASK&gt;.</p><p>with "Yes" or "true" as verbalizers for equivalent and "No" or "false" for not equivalent.</p><p>s1. Based on the previous sentence, s2? &lt;MASK&gt;.</p><p>with "Yes" or "true" as verbalizers for equivalent and "No" or "false" for not equivalent.</p><p>Based on the following sentence, s1?&lt;MASK&gt;.s2 with "Yes" and "No" as verbalizers for equivalent and not equivalent respectively.</p><p>? For WiC, given two sentences s 1 and s 2 and a word w, the task is to classify whether w is used in the same sense.</p><p>"s1" / "s2". Similar sense of "w"? &lt;MASK&gt;.</p><p>s1 s2 Does w have the same meaning in both sentences? &lt;MASK&gt; With "No" and "Yes" as verbalizers for False, and True.</p><p>w . Sense (1) (a) "s1" (&lt;MASK&gt;) "s2"</p><p>With "2" and "b" as verbalizers for False, and True.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Impact of the Position of Masks in Sentence-pair Datasets</head><p>We evaluate the impact of the position of mask tokens in sentence-pair benchmarks. Given two sentences s 1 and s 2 , we consider the following four locations for inserting mask tokens, where in the case of encoding as two sentences, input parts to the encoder are separated with |: 4. s 1 | s 2 &lt;MASK&gt; Table <ref type="table" target="#tab_8">7</ref> shows how the position of masks impact the results. As demonstrated, pattern 2, inserting mask tokens between the two sentences and encoding both as a single sentence obtains the highest validation performance. We use this choice in all the experiments when removing handcrafted patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Impact of Initialization</head><p>We initialize the label embedding matrix with random initialization from a normal distribution N (0,?). In table 8, we show the development results for different values of ?. We choose the ? obtaining the highest performance on average over average and worst case performance, i.e., ? =10 -4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation Results</head><p>To study the impact of different design choices in PERFECT, we considered the following experiments:</p><p>? -Hinge Loss: In this variant, we replace the hinge loss with multi-class cross entropy loss. ? +Label Emb: We use the trained label embeddings during the inference, substituting the computed prototypes in (5).</p><p>? -Prototypical: Instead of using prototypical networks, during inference, we use the same objective as training, i.e., (4).</p><p>Results are shown in Table <ref type="table" target="#tab_10">9</ref>. Experimental results demonstrate that PERFECT obtains the best results on average. Using multi-class cross-entropy instead of hinge loss, obtains substantially lower minimum performance (67.4 versus 68.1), demonstrating that training with hinge loss makes the model more stable. Using the trained label embeddings (+Label Emb) obtains very close results to PERFECT (slightly worse on average and slightly better on the minimum performance). Using the similar objective as training with replacing prototypical networks (-Prototypical), obtains lower performance on average (75.1 versus 75.6). These results confirm the design choices for PERFECT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>Figure1: Existing few-shot fine-tuning methods require manual engineering to reduce new tasks to masked language modeling. PERFECT does not rely on any handcrafting, removing both patterns and verbalizers (see Figure3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We consider a general problem of fine-tuning language models in a few-shot setting, on a small training set with K unique classes and N examples per class, such that the total number of examples is |D|=N ?K. Let D =? K k=1 D k be the given training set, where D k = {(x i k ,y i k )} N i=1 shows the set of examples labeled with class k and y i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Adapter integration in a PLM. Right: An adapter architecture. Adapters are usually inserted after the feed-forward and self-attention modules. During training, we only optimize the green components</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: We remove handcrafted patterns and verbalizers.We replace patterns using task-specific adapters and design label embeddings for the classes. We only train the green blocks (the label embeddings, adapters, and layer norms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc><ref type="bibr" target="#b25">IV et al. (2021</ref>) Following Logan IV et al.  (2021), we remove patterns and tune the biases in the PET. We use the RoBERTa large model<ref type="bibr" target="#b24">(Liu et al., 2019</ref>) (355M parameters) as the underlying PLM for all methods. We use the Hugging-Performance of all methods on single-sentence and sentence-pair benchmarks. We report average/worst-case accuracy/standard deviation. PERFECT obtains the state-of-the-art results. Bold fonts indicate the best results.</figDesc><table><row><cell>Experimental details:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>exploits this variation by putting the mask in multiple locations (see Appendix B).</figDesc><table><row><cell>Metric</cell><cell cols="2">PET PERFECT ?%</cell></row><row><cell cols="2">Trained params (M) 355.41 3.28</cell><cell>-99.08%</cell></row><row><cell cols="2">Peak memory (GB) 20.93 16.34</cell><cell>-21.93%</cell></row><row><cell cols="2">Training time (min) 23.42 0.65</cell><cell>-97.22%</cell></row><row><cell cols="2">+ PET in batch 0.94 0.65</cell><cell>-30.85%</cell></row><row><cell cols="2">Inference time (min) 9.57 0.31</cell><cell>-96.76%</cell></row><row><cell></cell><cell></cell><cell>Removing patterns and tuning biases in Logan IV</cell></row><row><cell></cell><cell></cell><cell>et al. (2021) is not expressive enough and performs</cell></row><row><cell></cell><cell></cell><cell>substantially worse than PERFECT on average.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Percentage of trained parameters, average peak memory, training, and inference time. ?% is the relative difference with respect to PET. Lower is better.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average performance of PET with five different patterns vs. Pattern-Free that replaces handcrafted patterns with task-specific adapters. We report the average/worstcase performance/and the standard deviation.</figDesc><table><row><cell cols="3">Dataset PET-Average Pattern-Free</cell></row><row><cell cols="2">SST-2 89.7/81.0/2.4</cell><cell>90.5/87.8/1.2</cell></row><row><cell>CR</cell><cell>88.4/68.8/3.0</cell><cell>89.8/87.0/1.4</cell></row><row><cell>MR</cell><cell>85.9/79.0/2.1</cell><cell>86.4/83.0/1.8</cell></row><row><cell cols="2">SST-5 45.9/40.3/2.4</cell><cell>44.8/40.0/2.4</cell></row><row><cell cols="2">SUBJ 88.1/79.6/2.4</cell><cell>85.3/74.7/3.8</cell></row><row><cell cols="2">TREC 85.0/70.6/4.5</cell><cell>87.9/84.6/1.8</cell></row><row><cell>CB</cell><cell>86.9/73.2/5.1</cell><cell>93.0/89.3/1.9</cell></row><row><cell>RTE</cell><cell>60.1/49.5/4.7</cell><cell>63.7/56.3/4.1</cell></row><row><cell cols="2">QNLI 66.5/55.7/6.2</cell><cell>71.3/65.8/2.5</cell></row><row><cell cols="2">MRPC 62.1/38.2/6.8</cell><cell>66.0/54.4/5.6</cell></row><row><cell>QQP</cell><cell>63.4/44.7/7.9</cell><cell>71.8/64.3/3.7</cell></row><row><cell>WiC</cell><cell>51.0/46.1/2.6</cell><cell>53.7/50.3/2.0</cell></row><row><cell>Avg</cell><cell>72.8/60.6/4.2</cell><cell>75.4/69.8/2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of PERFECT w/o adapters, -Adapters. We report the average performance/worst-case performance/and the standard deviation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Test performance for the varying number of mask tokens. Bold fonts indicate the best results in each row.</figDesc><table><row><cell cols="2">Datasets 1</cell><cell>2</cell><cell>5</cell><cell>10</cell></row><row><cell>CR</cell><cell cols="4">90.1 90.2 89.0 87.8</cell></row><row><cell>MR</cell><cell cols="4">86.9 86.1 85.4 85.6</cell></row><row><cell>MRPC</cell><cell cols="4">67.4 68.2 70.1 72.3</cell></row><row><cell>QNLI</cell><cell cols="4">73.7 73.9 73.0 65.1</cell></row><row><cell>RTE</cell><cell cols="4">60.0 57.3 56.2 56.0</cell></row><row><cell>TREC</cell><cell cols="4">90.0 90.9 88.9 88.8</cell></row><row><cell>Avg</cell><cell cols="4">78.0 77.8 77.1 75.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Statistics of datasets used in this work. We sample N ?|Y| instances (with multiple seeds) from the original training set to form the few-shot training and validation sets. The test column shows the size of the test set.</figDesc><table><row><cell cols="2">Dataset Task</cell><cell cols="3">#Train #Test K</cell></row><row><cell></cell><cell cols="2">Single-Sentence Benchmarks</cell><cell></cell></row><row><cell>MR</cell><cell>Sentiment analysis</cell><cell cols="3">8662 2000 2</cell></row><row><cell>CR</cell><cell>Sentiment analysis</cell><cell cols="3">1774 2000 2</cell></row><row><cell cols="2">SST-2 Sentiment analysis</cell><cell cols="3">6920 872 2</cell></row><row><cell cols="2">SST-5 Sentiment analysis</cell><cell cols="3">8544 2210 5</cell></row><row><cell cols="5">SUBJ Subjectivity classification 8000 2000 2</cell></row><row><cell cols="2">TREC Question classification</cell><cell cols="3">5452 500 6</cell></row><row><cell></cell><cell cols="2">Sentence-Pair Benchmarks</cell><cell></cell></row><row><cell>CB</cell><cell cols="2">Natural language inference 250</cell><cell>56</cell><cell>3</cell></row><row><cell>RTE</cell><cell cols="4">Natural language inference 2490 277 2</cell></row><row><cell>WiC</cell><cell cols="4">Word sense disambiguation 5428 638 2</cell></row><row><cell cols="2">MRPC Paraphrase detection</cell><cell cols="3">3668 408 2</cell></row><row><cell cols="2">QNLI Question answering</cell><cell cols="3">104743 5463 2</cell></row><row><cell cols="2">QQP Paraphrase detection</cell><cell cols="3">363846 40430 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Validation performance for sentence-pair benchmarks for different locations of mask tokens. Bold fonts indicate the best results in each row.</figDesc><table><row><cell cols="3">Datasets 1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>CB</cell><cell></cell><cell cols="3">89.8 91.6 88.9 86.5</cell></row><row><cell>RTE</cell><cell></cell><cell cols="3">69.1 69.1 64.5 65.3</cell></row><row><cell>QNLI</cell><cell></cell><cell cols="3">72.0 83.3 77.7 73.1</cell></row><row><cell cols="2">MRPC</cell><cell cols="3">71.6 69.5 66.4 72.0</cell></row><row><cell>QQP</cell><cell></cell><cell cols="3">79.2 82.8 72.5 70.2</cell></row><row><cell>WiC</cell><cell></cell><cell cols="3">60.3 59.5 60.2 59.5</cell></row><row><cell>Avg</cell><cell></cell><cell cols="3">73.7 76.0 71.7 71.1</cell></row><row><cell cols="3">Datasets 10 -2</cell><cell>10 -3</cell><cell>10 -4</cell><cell>10 -5</cell></row><row><cell>CB</cell><cell cols="4">90.0/82.5 92.2/85.0 91.6/87.5 91.6/87.5</cell></row><row><cell>MRPC</cell><cell cols="4">69.8/56.2 70.8/56.2 69.5/56.2 70.8/56.2</cell></row><row><cell>QNLI</cell><cell cols="4">83.3/71.9 82.7/71.9 83.3/71.9 83.1/68.8</cell></row><row><cell>QQP</cell><cell cols="4">82.8/78.1 82.7/75.0 82.8/75.0 83.0/75.0</cell></row><row><cell>RTE</cell><cell cols="4">69.8/62.5 69.2/59.4 69.1/62.5 68.3/62.5</cell></row><row><cell>WiC</cell><cell cols="4">62.2/50.0 59.7/46.9 59.5/53.1 58.9/50.0</cell></row><row><cell>Avg</cell><cell cols="4">76.3/66.9 76.2/65.7 76.0/67.7 76.0/66.7</cell></row><row><cell cols="2">Total Avg 71.6</cell><cell></cell><cell>71.0</cell><cell>71.8</cell><cell>71.3</cell></row></table><note><p>1. s 1 s 2 &lt;MASK&gt; 2. s 1 &lt;MASK&gt; s 2 3. s 1 | &lt;MASK&gt; s 2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Validation performance for different values of ?. We show mean performance/worst-case performance across 20 runs. The last row shows the average of mean performance/worst-case performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Dataset PERFECT -Hinge Loss +Label Emb -Prototypical SST-2 90.7/88.2/1.2 90.0/85.9/1.7 90.6/87.6/1.1 90.4/85.2/1.6 CR 90.0/85.5/1.4 90.1/88.6/0.9 89.7/86.6/1.4 89.9/86.8/1.4 MR 86.3/81.4/1.6 85.2/78.6/2.4 85.8/82.4/1.4 85.7/78.0/2.0 SST-5 42.7/35.1/2.9 43.3/36.8/3.1 41.8/37.1/2.5 Ablation results on the impact of different design choices in PERFECT. We report the average performance/worstcase performance/and the standard deviation.</figDesc><table><row><cell>41.2/35.9/2.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We discuss the general case with inserting multiple masks; for some datasets this improves performance ( ?4.3.1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We insert mask tokens after the input string in singlesentence benchmarks, and after the first sentence in the case of sentence-pair datasets and encode both sentences as a single input, which we found to perform the best (Appendix C).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We also tried with cosine similarity but found a slight improvement with squared Euclidean distance<ref type="bibr" target="#b53">(Snell et al., 2017)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://quoradata.quora.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For a controlled study, we use the MLM variant shown in (2), which has been shown to perform the best(Tam et al., 2021).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Since we don't have patterns, in the case of multiple sets of verbalizers, we use the first set of verbalizers as a random choice.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We have also tried to tune the baselines with the learning rate of 10 -4 but it performed worst.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>We also tried tuning prompts with learning rates of {10 -4 ,10 -5 } but it performed worst, as also observed in prior work(Mahabadi et al., 2021a;<ref type="bibr" target="#b30">Min et al., 2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>Based on the following question, q1?&lt;MASK&gt;.q2</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Sebastian Ruder</rs> and <rs type="person">Marius Mosbach</rs> for their comments on drafts of this paper. This research was partly supported by the <rs type="funder">Swiss National Science Foundation</rs> under grant number <rs type="grantNumber">200021_178862</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VeJW2yH">
					<idno type="grant-number">200021_178862</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>with "subjective" and "objective" as verbalizers.</p><p>? For TREC, given a question q, the task is to classify the type of it:</p><p>q &lt;MASK&gt;: q Q:&lt;MASK&gt;:</p><p>q Answer: &lt;MASK&gt;.</p><p>with "Description", "Entity", "Expression", "Human", "Location", "Number" as verbalizers for question types of "Description", "Entity", "Abbreviation", "Human", "Location", and "Numeric".</p><p>? For entailment task (RTE) given a premise p and hypothesis h:</p><p>with "Yes" as a verbalizer for entailment, "No" for contradiction.</p><p>p question: h True or False? answer: &lt;MASK&gt; with "true" as a verbalizer for entailment, "false" for contradiction.</p><p>? For entailment task (CB) given a premise p and a hypothesis h:</p><p>"h" ? | &lt;MASK&gt;, "p"</p><p>with "Yes" as a verbalizer for entailment, "No" for contradiction, "Maybe" for neutral.</p><p>p question: h true, false or neither? answer: &lt;MASK&gt; with "true" as a verbalizer for entailment, "false" for contradiction, "neither" for neutral.</p><p>? For QNLI, given a sentence s and question q:</p><p>s. Question: q? Answer: &lt;MASK&gt;.</p><p>with "Yes" or "true" as verbalizers for entailment and "No" or "false" for not entailment.</p><p>s. Based on the previous sentence, q? &lt;MASK&gt;.</p><p>with "Yes" or "true" as verbalizers for entailment and "No" or "false" for not entailment.</p><p>Based on the following sentence, q?&lt;MASK&gt;.s with "Yes" and "No" as verbalizers for entailment and not entailment respectively.</p><p>? For QQP, given two questions q 1 and q 2 : Do q1 and q2 have the same meaning?&lt;MASK&gt;.</p><p>with "Yes" or "true" as verbalizers for duplicate and "No" or "false" for not duplicate.</p><p>q1. Based on the previous question, q2? &lt;MASK&gt;.</p><p>with "Yes" or "true" as verbalizers for duplicate and "No" or "false" for not duplicate.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Using machine learning algorithms for breast cancer risk prediction and diagnosis</title>
		<author>
			<persName><forename type="first">Hiba</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hajar</forename><surname>Mousannif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Al Moatassime</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Noel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Procedia Computer Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In TAC</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tinytl: Reduce memory, not parameters for efficient on-device learning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of Sinn und Bedeutung</title>
		<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<title level="m">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>In IWP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Hypernetworks</note>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How can we know what language models know</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How many data points is a prompt worth</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>?a?ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">Canwen</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><surname>Raw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Sylvain Lesage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Carrigan</surname></persName>
		</author>
		<author>
			<persName><surname>Matussi?re</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5639822</idno>
		<imprint/>
		<respStmt>
			<orgName>Leandro von Werra, Lysandre Debut, Stas Bekman</orgName>
		</respStmt>
	</monogr>
	<note>and Cl?ment Delangue. 2021a. huggingface/datasets: 1.15.1</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>?a?ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavitvya</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Matussi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Goehringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Mustar</surname></persName>
		</author>
		<author>
			<persName><surname>Lagunas</surname></persName>
		</author>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Alexander Rush</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>and Thomas Wolf. 2021b. Datasets: A community library for natural language processing</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">2021a. Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">2021b. Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Noisy channel language model prompting for few-shot text classification</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04106</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">2022a. Reframing instructional prompts to gptk&apos;s language</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RepL4NLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Andreas R?ck ?e, Cho Kyunghyun, and Iryna Gurevych. 2021. AdapterFusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adapterhub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP: System Demonstrations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wic: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben-Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient parametrization of multidomain deep neural networks</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In CVPR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">2021a. Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">2021b. It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Verbnet: A broad-coverage, comprehensive verb lexicon</title>
		<author>
			<persName><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Eliciting knowledge from language models using automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Rakesh R Menon</surname></persName>
		</author>
		<author>
			<persName><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11955</idno>
		<title level="m">Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism quarterly</title>
		<imprint>
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Udapter: Language adaptation for truly universal dependency parsing</title>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>?st?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Superglue: a stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Do prompt-based models really understand the meaning of their prompts</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01247</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP: System Demonstrations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Revisiting few-sample bert fine-tuning</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
