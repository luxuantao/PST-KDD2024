<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attending to visual motion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-07-26">26 July 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
							<email>tsotsos@cs.yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision Research</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ont</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueju</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision Research</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ont</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julio</forename><forename type="middle">C</forename><surname>Martinez-Trujillo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Physiology</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Pomplun</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evgueni</forename><surname>Simine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision Research</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ont</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunhao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Vision Research</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ont</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attending to visual motion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-07-26">26 July 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">84473BA3B2E3600A60BA1D18085C3CFA</idno>
					<idno type="DOI">10.1016/j.cviu.2004.10.011</idno>
					<note type="submission">Received 6 February 2004; accepted 5 October 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Attention</term>
					<term>Visual motion analysis</term>
					<term>Feature binding</term>
					<term>Selective tuning</term>
					<term>Affine motion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual motion analysis has focused on decomposing image sequences into their component features. There has been little success at re-combining those features into moving objects.</p><p>Here, a novel model of attentive visual motion processing is presented that addresses both decomposition of the signal into constituent features as well as the re-combination, or binding, of those features into wholes. A new feed-forward motion-processing pyramid is presented motivated by the neurobiology of primate motion processes. On this structure the Selective Tuning (ST) model for visual attention is demonstrated. There are three main contributions:</p><p>(1) a new feed-forward motion processing hierarchy, the first to include a multi-level decomposition with local spatial derivatives of velocity; (2) examples of how ST operates on this hierarchy to attend to motion and to localize and label motion patterns; and (3) a new solution to the feature binding problem sufficient for grouping motion features into coherent object motion. Binding is accomplished using a top-down selection mechanism that does not depend on a single location-based saliency representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Selective Tuning model is a proposal for the explanation at the computational and behavioral levels of visual attention in humans and primates. Key characteristics of the model, all previously detailed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> include: (1) a top-down coarse-to-fine winner-take-all (WTA) selection process, (2) a unique WTA formulation with provable convergence properties, (3) a WTA that is based on region rather than point selection, (4) a task-relevant inhibitory bias mechanism, (5) selective inhibition in both spatial and feature dimensions for elimination of signal interference that leads to a suppressive surround for attended items, and (6) a task-specific executive controller. These characteristics lead to an extensive set of biological predictions many of which have now been supported by experiment. The bulk of the paper will focus on attention to visual motion. Past work will be summarized showing how this is not a well-studied issue. A new model of motion processing is presented and it is demonstrated how ST operates on this representation, with no changes to its previously described definition. In this way three points are made: first, that the weaknesses of previous demonstrations of ST have been remedied; second, that the original statement of ST has generality for a wide variety of visual processing representations; and third, examples of how feature binding can be solved using ST for complex motion patterns.</p><p>It had been suggested that previous demonstrations of the Selective Tuning model were neither biologically plausible nor very useful. In order to demonstrate that ST can indeed operate with realistic representations, the motion domain is chosen because enough is known about motion processing to enable a reasonable attempt at defining the feed-forward pyramid. Moreover, the effort is unique because it seems that no past model has presented a motion hierarchy plus attention to motion <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>The layout of the remainder of this presentation is as follows. The next section will detail the feed-forward motion-processing network. Earlier versions of this network appear in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Following this, an overview of ST is provided because this structure is imposed upon the feed-forward network. ST has been detailed several times in the past; here only a brief presentation is given and the reader is referred to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> for further details. Section 4 will show several examples of the operation of the entire network including feed-forward and feedback components as well as a new solution to the feature-binding problem. A concluding discussion rounds out the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Feed-forward motion processes</head><p>The motion representations and processes that are modeled are informed by current knowledge of motion analysis in the primate cortex. Although the literature is large on the topic, selected experimental observations are used here in order to simplify the models. It is generally accepted that motion processing in the monkey cortex goes through a series of stages, with neural representations in areas V1, MT, MST, and 7a each providing input for the next <ref type="bibr" target="#b18">[19]</ref>. Each of the areas specializes in particular kinds of motions, that is, contains populations of neurons specialized for certain motion features, generally from simple to more complex and with smaller to larger receptive fields higher up in the hierarchy. These different neural properties will be outlined throughout this section, with one sub-section devoted to each area.</p><p>The model aims to explain how a hierarchical feed-forward network consisting of multiple neural populations in the cortical areas V1, MT, MST, and 7a of primates detects and represents different kinds of motion patterns. At best, it is a first-order motion model with much elaboration left for future work. Indeed, some previous motion models cited earlier offer better sophistication at one or another level of processing; however, none cover all these levels and incorporate selective attention processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The feed-forward motion pyramid</head><p>The first component of the motion analysis process is a feed-forward (data-driven) one. The goal is to define a set of processing stages for areas V1, MT, MST, and 7a, corresponding to the areas of the motion processing hierarchy in macaque monkey <ref type="bibr" target="#b18">[19]</ref>, that conform to the basic properties observed in neural populations in those areas <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. A very brief characterization of the processing levels follows:</p><p>• Cells in striate area V1 are selective for a particular local speed and direction of motion in at least three main speed ranges. • Cells in area MT are of two kinds. One kind is tuned for a particular local speed and direction of movement, similar to direction and speed selective cells in V1 but with larger receptive fields. The second kind is selective for a particular angle between local movement direction and spatial velocity gradient. • Cells in area MST are tuned to complex motion patterns: expand or approach, contract or recede, clockwise or counter-clockwise rotation, and combinations of these, and translation but with even larger receptive fields. • Cells in area 7a code four different types of patterns: translation and spiral motion as in MST, full field rotation (regardless of direction), and radial motion (expansion or contraction), within the largest receptive fields.</p><p>There is no claim that these are necessarily the only neural populations within each area in primates; these are simply the only ones modeled here.</p><p>The model includes neurons in the areas V1 (referred to as VF a,v,i and VI a,v,i ), MT (referred to as MT a,v,i and MG a,d,v,i ), MST (referred to as ST a,v,i and SS d,v,i ), and 7a (referred to as AT a,v,i , AS d,v,i , ART v,i , and ARD v,i ). A number of parameters and numerical constraints have been set with guidance from the literature available or by reasonable estimations otherwise.</p><p>Fig. <ref type="figure">1</ref> depicts the full motion hierarchy. This figure emphasizes the scale of the search problem faced by the visual system: to determine which responses within each of these representations belong to the same event. Each layer is now described in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Area V1</head><p>Area V1 receives visual input as a temporal sequence of images. Spatiotemporal filters are used to model the selectivity of V1 neurons for speed and direction of local motion (see <ref type="bibr" target="#b20">[21]</ref>). Our first attempt at this employed the spatiotemporal filter approach of Heeger <ref type="bibr" target="#b29">[30]</ref>; however, for the number and resolution of images in a sequence, the output of these filters was too noisy for the subsequent velocity gradient computation in area MT. Consequently, another computational mechanism for V1 was defined that generates a more appropriate input to the MT neurons. This mechanism is presented in the following paragraphs. Fig. <ref type="figure">1</ref>. The full motion hierarchy. This shows the set of neural selectivities that comprise the entire pyramidal hierarchy covering visual areas V1, MT, MST, and 7a. Each rectangle represents a single type of selectivity applied over the full image at that level of the pyramid. Large grey arrows represent selectivity for direction. Coloured rectangles in area MT represent particular angles between motion and speed gradient. The three rectangles at each direction represent the three speed selectivity ranges in the model. In this way, each single ÔsheetÕ may be considered an expanded view of the ÔhypercolumnsÕ in a visual area. In area V1, for example, the neurons that integrate direction and speed selectivity are represented by the single sheet of grey rectangles. In area MT, there are 13 sheets, the top one representing direction and speed selectivity while the remaining 12 represent the 12 directions of velocity gradient relative to the 12 motion directions. The wheel of coloured arrows represents the colour coding within area MT for speed gradient with respect to local motion, in this case the larger grey arrow pointing upwards. This codes the angle between local motion and speed gradient. MST units respond to patterns of motioncontract, recede, and rotate. The 7a layers represent translational motion, spiral motion, both as in area MST, plus radial and rotation without direction in the topmost set of six rectangles.</p><p>The functionality of layer V1 is realized by two types of artificial neurons, namely those performing spatiotemporal filtering, referred to as VF a,v,i , and those integrating local filter unit activations, referred to as VI a,v,j . The filter units have spatiotemporal RFs that provide access to the intensity values of the T images in the most recent sub-sequence. In the present evaluation of the model, T = 5. The intensity value at position p in the image taken at time t is I (p, t), where t = 1 indicates the first and t = T indicates the most recent image in the sequence.</p><p>In this input space, the RF of a neuron VF a,v,i is oriented in such a way that local motion at its position i in direction a and with speed s v would induce constant intensity across the RF. V1 consists of neurons of three distinct speed selectivity types (following <ref type="bibr" target="#b20">[21]</ref>): type 1 (low speed), type 2 (medium speed), and type 3 (high speed). In the model, these neurons are implemented with three different preferred speeds, which were set to s 1 = 0.5 pixels/frame, s 2 = 1 pixel/frame, and s 3 = 2 pixels/frame. To limit the computational complexity of the model, only 12 different preferred directions were realized (a = 0°, 30°, . . . , 330°), although it is known that a wider range of preferred directions exist in area V1 of macaques <ref type="bibr" target="#b20">[21]</ref>. For a preferred direction a and a preferred speed s v (in pixels per frame), the spatial offset X (a, s v , t) of the line of constant intensity in the image taken at time t can be computed as follows:</p><formula xml:id="formula_0">Xða; s v ; tÞ ¼ s v t À T þ 1 2 cos a sin a .<label>ð1Þ</label></formula><p>Since in most cases this function will not yield integer values, up to four inputs per image are used to estimate actual intensity in defining the line of constant intensity. This can be visualized for a sequence of one-dimensional images (see Fig. <ref type="figure">2</ref>), where two inputs per image can be necessary.</p><p>Here, the darker a pixel, the larger its weight in the computation of the input from its image (because the line is passing through it more centrally). A neuron VF a,v,i Fig. <ref type="figure">2</ref>. Illustration of spatiotemporal energy computation. receives input from each temporal layer and computes an intensity constancy value IC that decreases with increasing standard deviation of the intensity across the layers. In the following quantitative description the real-valued function X (a, s v , t) is used:</p><formula xml:id="formula_1">IC ¼ M VF À ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 T X T t¼1 IðpðiÞ þ Xða; s v ; tÞ; tÞ À P T t¼1 ðIðpðiÞ þ Xða; s v ; tÞ; tÞÞ T ! 2 v u u t ;<label>ð2Þ</label></formula><p>where M VF is the maximum activation of the neuron, and p (i) is the central RF position of neuron VF a,v,i in the visual input. With intensities ranging from 0 to 255, the value of M VF is set to be 128 so that IC assumes only non-negative values. In the current implementation of the model, in order to compensate for noise in the visual input, each filter neuronÕs RF consists of multiple (N = 20) linear arrangements of inputs. The preferred directions a n and speeds s n for these arrangements deviate slightly from the preferred motion (a, s) of the filter neuron. The following probability functions describe the statistical distribution of the variables a n and s n :</p><formula xml:id="formula_2">pða n Þ ¼ 1 30 Á ffiffiffiffiffi ffi 2p p exp À ða n À aÞ 2 2 Á ð30 Þ 2 ( )</formula><p>and</p><formula xml:id="formula_3">pðs n Þ ¼ 1 0.25s Á ffiffiffiffiffi ffi 2p p exp À ðs n À sÞ 2 2 Á ð0.25sÞ<label>2</label></formula><formula xml:id="formula_4">(</formula><p>)</p><formula xml:id="formula_5">for n ¼ 1; . . . ; N .<label>ð3Þ</label></formula><p>Every filter neuron obtains one value IC n for each linear arrangement of inputs, and the activation of a neuron VF a,v,i is then given by</p><formula xml:id="formula_6">VF a;v;i ¼ max i6n6N IC n .<label>ð4Þ</label></formula><p>Obviously, the filter neurons reach a state of high activation if a motion of their preferred orientation and velocity is present in their RF. However, maximum activation is also induced if there is no motion at all in a region of homogeneous intensity in the image sequence. Therefore, the function of the integration units VI a,v,j is not only to reduce the noise of the raw filter unit activations, but also to eliminate such ''pseudo motion'' detected by the filter cells. This is achieved by implementing lateral inhibition between units VI a,v,j with identical positions and speed selectivity, but different preferred directions of motion:</p><formula xml:id="formula_7">VI a;v;j ¼ 1 jRðjÞj X i2RðjÞ VF a;v;i À 1 ðn a À 1Þ Á jRðjÞj X b;b6 ¼a X i2RðjÞ VF b;v;i ;<label>ð5Þ</label></formula><p>where R (j) is the set of neurons whose outputs converge onto the integration unit j, and n a denotes the number of implemented preferred directions of motion; in the present model, n a = 12. In the above formula, a and b assume only these directions.</p><p>To achieve the V1 computation, the model uses one hypercolumn of VF a,v,i neurons for each pixel in the visual field. Each hypercolumn comprises one neuron of each type-because there are three different preferred speeds and 12 different preferred directions of motion, there are 36 units in each hypercolumn. Furthermore, the model employs 64 • 64 evenly distributed VI a,v,j hypercolumns (also 36 units per hypercolumn) that receive input from local filter units. In the present implementation the size of the input images are 256 • 256 pixels and integration units with RFs covering 8 • 8 neighboring filter units are used, thereby creating substantial overlap of RFs. The 64 • 64 hypercolumns of integration units provide the input for the modelÕs MT neurons.</p><p>Figs. <ref type="figure" target="#fig_1">3A-H</ref> show the filter outputs for all of the layers of the hierarchy for an input image sequence using a purely motion-defined object, that is, a pattern of random elements moving within a static random field (Gaussian noise). The motion is a counter-clockwise rotating square. The darker the pixel-value in the representation the stronger is the response; white means zero response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Area MT</head><p>One group of cells in MT is tuned for a particular local speed and direction of movement, similar to V1 cells <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>. Another sub-population of MT neurons is selective for a particular angle between the local direction of movement and the speed gradient <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>. Here, MT has been designed with two different types of neurons: cells with selectivity identical to V1 neurons but larger RFs (detectors of translational motion) and cells selective for the angle between the direction of motion and the velocity gradient (detectors of velocity gradients). MT is implemented as a 30 • 30 array of hypercolumns, 468 neurons each (36 for translation as in V1 and 432 for gradient detection-three speeds, 12 directions, 12 direction/gradient angles). Each MT cell receives input from a 4 • 4 field of V1 neurons with the same direction and speed tuning.</p><p>For a translation neuron i with preferred direction of motion a and speed selectivity of type m, its activation is given by:</p><formula xml:id="formula_8">MT a;m;i ¼ X j2RðiÞ G i;j V a;m;j ;<label>ð6Þ</label></formula><p>where</p><formula xml:id="formula_9">G i;j ¼ 2 pjRðiÞj exp 2ðx 2 j þ y 2 j Þ jRðiÞj ( ) .<label>ð7Þ</label></formula><p>In Eq. ( <ref type="formula" target="#formula_9">7</ref>) and the following equations, R (i) stands for the set of units that constitute the RF of neuron i.G i,j denotes the value of a two-dimensional (2D) Gaussian function at the position (x j , y j ) of neuron j in the RF of neuron i. Here, G i,j represents the connection weights, and x j and y j are measured in unit spaces relative to the center of the RF. As can be seen from Eq. ( <ref type="formula" target="#formula_9">7</ref>), the Gaussian functions were centered on their corresponding RF, and their standard deviation was always chosen to be half the length of the square-shaped RF. For example, since MT neurons have RFs of size 4 • 4, the peak of the Gaussian function is between the second and third RF neurons horizontally and vertically, and its standard deviation is 2.</p><p>The activation of a velocity gradient detector is computed as the product of the activation of V1 cells feeding into its RF with the same speed and direction tuning, and the gradient response. The gradient is determined by oriented RFs (example: RF for detecting upward gradient). For a velocity gradient neuron i with preferred direction of motion a, preferred angle d between motion and gradient speed selectivity of type m: </p><formula xml:id="formula_10">8 &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; :<label>ð8Þ</label></formula><p>H is a threshold, c k are coefficients for the linear reconstruction of absolute speed from the activation of the three types of speed-selective neurons, and D (a, b, m, i) specifies the activation increase in direction b in the receptive field of neuron i for speed selectivity type v and direction a: </p><p>O b,j is an orientation-specific configuration of weights that leads to maximum activation if the inputs increase in direction b. The individual weights are set to either 1 jRij or À 1 jRij in order to make the range of the neuronÕs activation values independent of its RF size. For example, a 4 • 4 RF whose preferred direction of motion is rightward (b = 0) has the following configuration O b,j :</p><p>If in a region of the input image we consistently find the same angle d between motion and speed gradient across all directions of motion, this signifies a particular motion pattern. An angle d of 0°indicates expansion, 90°, indicates clockwise rotation, 180°indicates contraction, and 270°indicates counter-clockwise rotation. Any type of spiral motion can be represented this way; for example, an angle d of 30°stands for expansion and a smaller proportion of clockwise rotation. This is the same coding as used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. The color-coding for the angles used is shown in Fig. <ref type="figure">1</ref>. The output of area MT computations for this input image sequence is shown in Figs. <ref type="figure" target="#fig_1">3B</ref> and<ref type="figure">C</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Area MST</head><p>Cells in MST have larger receptive fields than MT cells and seem tuned to complex motion patterns: expand or approach, contract or recede, and rotation <ref type="bibr" target="#b25">[26]</ref>. Two types of neurons are modeled: translation (as in V1) and spiral motion (clockwise and counter-clockwise rotation, expansion, contraction, and combinations). The reason translational motion is included here (as in area 7a) is so that a full pyramid of translation at all scales is included. MST is implemented as a 5 • 5 array of hypercolumns, 72 neurons each (36 for translation as in MT and 12 types of pattern selectivity for motion patterns with three speeds each). Each MST cell receives input from a 15 • 15 field of MT neurons that have the same tuning as the MST cell.</p><p>The activation of a translation neuron i with preferred direction of motion a and speed selectivity of type v is:</p><formula xml:id="formula_12">ST a;m;i ¼ X j2RðiÞ G i;j MT a;m;j .<label>ð10Þ</label></formula><p>Response for a spiral neuron i selective for a pattern d (angle between the direction and the speed gradient of motion as described in Section 2.3 and of speed selectivity type m is:</p><formula xml:id="formula_13">SS d;m;i ¼ X j2RðiÞ X a G i;j MG a;d;m;j .<label>ð11Þ</label></formula><p>As for area MT, a direction to speed gradient angle of 0°indicates expansion, 90°i ndicates clockwise rotation, 180°indicates contraction, and 270°indicates counter-clockwise rotation and other angles represent combinations of motion types.</p><p>The output of area MST neurons is shown in Figs. <ref type="figure" target="#fig_1">3D</ref> and<ref type="figure">E</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Area 7a</head><p>Area 7a seems to involve at least four different types of computations but with larger RFs than the other areas <ref type="bibr" target="#b22">[23]</ref>: translation and spiral motion, as in MST, rotation (clockwise or counter-clockwise regardless of direction), and radial motion (irrespective of direction, expansion or contraction). Types AT and AS have the same properties as ST and SS, respectively, except for their RF size:</p><formula xml:id="formula_14">AT a;m;i ¼ X j2RðiÞ G i;j ST a;m;j ;<label>ð12Þ</label></formula><formula xml:id="formula_15">AS d;m;i ¼ X j2RðiÞ G i;j SS d;m;j .<label>ð13Þ</label></formula><p>Neuron ART responds to rotation, regardless of the direction of rotation (clockwise or counter-clockwise). The activation of ART neuron i with speed selectivity type m is computed as follows:</p><p>ART</p><formula xml:id="formula_16">m;i ¼ X d X j2RðiÞ G i;j SS d;m;j ½Dðd À 90 Þ þ Dðd À 270 Þ;<label>ð14Þ</label></formula><p>where D is the direction selectivity function defined as a one-dimensional Gaussian function:</p><formula xml:id="formula_17">DðcÞ ¼ 1 r D ffiffiffiffiffi ffi 2p p e Àc 2 =2r 2 D with r D ¼ À45 .<label>ð15Þ</label></formula><p>Neuron ARD responds to radial motion, regardless of whether it is expansion or contraction. The activation of ARD neuron i of speed selectivity type m is given by: ARD</p><formula xml:id="formula_18">m;i ¼ X d X j2RðiÞ G i;j SS d;m;j ½DðdÞ þ Dðd À 180 Þ.<label>ð16Þ</label></formula><p>Area 7a is implemented as a 4 • 4 array of hypercolumns, 78 neurons each (36 for translation, 3 speeds of rotation, 3 speeds of radial motion, and 3 speeds times 12 patterns for spiral motions). Each 7a cell receives input from a 4 • 4 field of MST neurons that have the relevant tuning. The output of area 7a computations is shown in Figs. 3F-H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Motion hierarchy summary</head><p>The motion pyramid is novel in that it contains two separate streams of processing, translation, and generalized spiral motions. It features a decomposition of motion into simpler components including local spatial gradients of velocity. As the previous figures demonstrate the representations that result from the several different neural populations (654 separate full field filter representations) are complex and non-trivial. Although the outputs are noisy, the effect of noise is gradually ameliorated as the signals reach higher levels of the pyramid. It is satisfying to note that even with this complexity of representation, peak responses are right where they should be in terms of correct feature detection and a search strategy (as described in the next section) can successfully find those peaks.</p><p>It is important to acknowledge a weakness of the present work that has resulted from the original motivation for the research described here. This research was motivated by the valid criticism that past demonstrations (as in <ref type="bibr" target="#b1">[2]</ref>) used simple Gaussian pyramids for the features on which the ST mechanisms were demonstrated. This choice did not affect the demonstrations, which did indeed properly show all the characteristics of ST. However, it did make those demonstrations less useful and did allow the possibility that the ST mechanisms might not work as expected with realistic feature pyramids. We chose to address these criticisms within the visual motion domain. Thus, the task of defining motion ÔneuronsÕ along the motion pathway was addressed in a coarse, first-order fashion only and thus the filter definitions (but not the motion decompositions) are perhaps not as strong as they could be. Current research is attempting to improve the motion representation; this in no way will affect the ST demonstration and will only strengthen the motion representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The selective tuning model of visual attention</head><p>The modeling effort described herein must be distinguished from others in at least the following ways: it is not a neural network that learns to attend; it is not a model whose goal is to explain a particular set of quantitative observations; it is not a data fitting exercise; it is not a set of equations whose numerical simulation leads to output functions whose form seems similar to experimental data. In contrast, we are trying to show from Ôfirst principlesÕ what qualitative form visual processing must take and to define a theory and an accompanying computer simulation that can take as input digital images and perform at least qualitatively in the same manner as human or primate vision performs. It features a theoretical foundation of provable properties based in the theory of computational complexity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. The Ôfirst princi-plesÕ arise because vision is formulated as a search problem (given a specific input, what is the subset of neurons that best represent the content of the image?) and complexity theory is concerned with the cost of achieving solutions to such problems. This foundation suggests a specific biologically plausible architecture as well as its processing stages as will be briefly described in this article (a more detailed account can be found in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>). It should be clear that these foundations were derived using the visual search problem. Considerations related to other dimensions of attention functionality remain (however note that in <ref type="bibr" target="#b1">[2]</ref> saccades to peripheral targets are included in the model and that in Zaharescu et al. <ref type="bibr" target="#b34">[35]</ref> added active visual search to ST).</p><p>ST is not compared here to other attention models except where particular points need to be made in order to keep the paper length under control; general comparisons have appeared elsewhere in previous ST publications as cited herein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The model</head><p>The visual processing architecture is pyramidal in structure with units within this network receiving both feed-forward and feedback connections. When a stimulus is presented to the input layer of the pyramid, it activates in a feed-forward manner all of the units within the pyramid with receptive fields (RFs) mapping to the stimulus location; the result is a diverging cone of activity within the processing pyramid. It is assumed that response strength of units in the network is a measure of goodness-ofmatch of the stimulus within the RF to the model that determines the selectivity of that unit.</p><p>Selection relies on a hierarchy of winner-take-all processes. WTA is a parallel algorithm for finding the maximum value in a set. First, a WTA process operates across the entire visual field at the top layer where it computes the global winner, i.e., the units with largest response (see Section 3.3 for details). The WTA can accept guidance to favor areas or stimulus qualities if that guidance is available but operates independently otherwise. The search process then proceeds to the lower levels by activating a hierarchy of WTA processes. The global winner activates a WTA that operates only over its direct inputs to select the strongest responding region within its RF. Next, all of the feed-forward connections in the visual pyramid that do not contribute to the winner are pruned (inhibited). As a result, the input to the higher-level unit changes and thus its output changes. This refinement of unit responses is an important consequence because one of the important goals of attention is to reduce or eliminate signal interference <ref type="bibr" target="#b0">[1]</ref>. By the end of this refinement process, the output of the attended units at the top layer will be the same as if the attended stimulus appeared on a blank field. This strategy of finding the winners within successively smaller RF, layer by layer, in the pyramid and then pruning away irrelevant connections through inhibition is applied recursively through the pyramid. The end result is that from a globally strongest response, the cause of that largest response is localized in the sensory field at the earliest levels. The paths remaining may be considered the pass zone of the attended stimulus while the pruned paths form the inhibitory zone of an attentional beam. The WTA does not violate biological connectivity or relative timing constraints. Fig. <ref type="figure" target="#fig_2">4</ref> gives a pictorial representation of this attentional beam.</p><p>An executive controller is responsible for implementing the following sequence of operations for visual search tasks:</p><p>1. Acquire target as appropriate for the task, store in working memory. 2. Apply top-down biases, inhibiting units that compute task irrelevant quantities. 3. ÔSeeÕ the stimulus, activating feature pyramids in a feed-forward manner. 4. Activate top-down WTA process at top layers of feature pyramids. 5. Implement a layer-by-layer top-down search through the hierarchical WTA based on the winners in the top layer. 6. After completion, permit time for refined stimulus computation to complete a second feed-forward pass. Note that this feed-forward refinement does not begin with the completion of the lowermost WTA process; rather, it occurs simultaneously with completing WTA processes (step 5) as they proceed downwards in the hierarchy. On completion of the lowermost WTA, some additional time is required for the completion of the feed-forward refinement. 7. Extract output of top layers and place in working memory for task verification. 8. Inhibit pass zone connections to permit next most salient item to be processed. 9. Cycle through steps 4-8 as many times as required to satisfy the task. This multi-pass process may seem to not reflect the reality of biological processes that seem very fast. However, it is not claimed that all of these steps are needed for all tasks. Several different levels of tasks may be distinguished, defined as: Detection-is a particular item present in the stimulus, yes or no? Localization-detection plus accurate location; Recognition-localization plus accurate description of stimulus; Understanding-recognition plus role of stimulus in the context of the scene.</p><p>The executive controller is responsible for the choice of task based on instruction. If detection is the task, then the winner after step 4, if it matches the target, will suffice and the remaining steps are not needed. Thus simple detection in this framework requires only a single feed-forward pass ( <ref type="bibr" target="#b0">[1]</ref>, also argued by Thorpe <ref type="bibr" target="#b35">[36]</ref>). If a localization task is required, then all steps up to 7 are required because, as argued in Section 2.2, the top-down WTA is needed to isolate the stimulus and remove the signal interference from nearby stimuli. This clearly takes more time to accomplish. If recognition is the task, then all steps, and perhaps several iterations of the procedure, are needed in order to provide a complete description. The understanding task has similar requirements, although this is not quite within the scope of the model at this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Top-down selection</head><p>ST features a top-down selection mechanism based on a coarse-to-fine WTA hierarchy. Why is a purely feed-forward strategy not sufficient as Riesenhuber and Poggio claim <ref type="bibr" target="#b36">[37]</ref>? There seems to be no disagreement on the need for top-down mechanisms if task/domain knowledge is considered, although few non-trivial schemes seem to exist. Biological evidence, as well as complexity arguments, suggests that the visual architecture consists of a multi-layer hierarchy with pyramidal abstraction. One task of selective attention is to find the value, location, and extent of the most ÔsalientÕ image subset within this architecture. A purely feed-forward scheme operating on such a pyramid with:</p><p>(i) Fixed size receptive fields with no overlap, is able to find the largest single stimulus input with local WTA computations for each receptive field but location is lost and stimulus extent cannot be considered. (ii) Fixed size overlapping receptive fields, suffers from the spreading winners problem due to neural convergence, and although the largest input value can be found, the signal is blurred across the output layer, location is lost, and extent is ambiguous. (iii) All possible RF sizes in each layer, becomes intractable due to combinatorics <ref type="bibr" target="#b0">[1]</ref>.</p><p>While case: (i) might be useful for certain computer vision detection tasks, it cannot be considered as a reasonable proposal for biological vision because it fails to localize targets. Case (iii) is not plausible as it is intractable. Case (ii) reflects a biologically realistic architecture, yet fails at the task of localizing a target. Given this reality, a purely feed-forward scheme is insufficient to describe biological vision. Only a combined bottom-up and top-down strategy can successfully determine the location and extent of a selected stimulus in a constrained pyramidal architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">WTA and saliency</head><p>The Winner-Take-All scheme within ST is defined as an iterative process that can be realized in a biologically plausible manner insofar as time to convergence and connectivity requirements are concerned. It has its roots in Koch and UllmanÕs model <ref type="bibr" target="#b37">[38]</ref> (but also see <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>) but provides a complete redefinition with proofs of convergence and convergence properties fully described in <ref type="bibr" target="#b1">[2]</ref>. The basis for its distinguishing characteristic comes from the fact that it implicitly creates a partitioning of the set of unit responses into bins of width determined by a task-specific parameter, h. The partitioning arises because inhibition between units is not based on the value of a single unit but rather on the absolute value of the difference between pairs of unit values. Further, this WTA process is not restricted to converging to single points as all other formulations. The winning bin of the partition, whose determination is now described, is claimed to represent the strongest responding contiguous region in the image (this is formally proved in <ref type="bibr" target="#b1">[2]</ref>).</p><p>First, the WTA implementation uses an iterative algorithm with unit response values updated after each step until convergence is achieved. Competition depends linearly on the difference between unit strengths in the following way. Unit A will inhibit unit B in the competition if the response of A, denoted by q (A) satisfies |q(A) À q(B)| &gt; h. Otherwise A will not inhibit B. The overall impact of the competition on unit B is the weighted sum of all inhibitory effects, each of whose magnitude is determined by |q(A) À q(B)|. It has been shown <ref type="bibr" target="#b1">[2]</ref> that this WTA is guaranteed to converge, has well-defined properties with respect to finding strongest items, and has well-defined convergence characteristics. The time to convergence, in contrast to any other iterative or relaxation-based method is specified by a simple relationship involving h and the maximum possible value, Z, across all unit responses. The reason for this is that the partitioning procedure uses differences of values. All larger units will inhibit the units with the smallest responses, while no units will inhibit the largest valued units. As a result the small response units are reduced to zero very quickly while the time for the second largest units to be eliminated depends only on the values of those units and the largest units. As a result, a two-unit network is easy to characterize. The time to convergence is given by log 2 ð AÀh AÀB Þ where A is the largest value and B the second largest value. This is also quite consistent with behavioral evidence; the closer in response strength two units are, the longer it takes to distinguish them.</p><p>Second, the competition depends linearly on the topographical distance between units, i.e., the features they represent. The larger the distance between units, the greater the inhibition. This strategy will find the largest, most spatially contiguous subset within the winning bin. A spatially large and contiguous region will inhibit a contiguous region of similar response strengths but of smaller spatial extent because more units from the large region apply inhibition to the smaller region than inhibit the larger region from the smaller one. At the top layer, this is a global competition; at lower layers, it only takes place within receptive fields. In this way, the process does not require implausible connectivity lengths. For efficiency reasons, this is currently only implemented for the units in the winning bin. With respect to the weighted sums computed, in practice the weights depend strongly on the types of computations the units represent. There may also be a task-specific component included in the weights. Finally, a rectifier is needed for the whole operation to ensure that no unit values go below zero. The iterative update continues until there is only one bin of positive response values remaining and all other bins contain units whose values have fallen below h. Note that even the winning bin of positive values must be of a value greater than some threshold in order to eliminate false detections due to noise.</p><p>The key question is how is the root of the WTA process hierarchy determined? The following is a conceptual description of this where the ÔmaxÕ function used below is implemented using the iterative process just described. Let F be the set of feature maps at the output layers overall, and F i , i = 1 to n, be particular feature maps. Values at each x, y location within map i are represented by M i x;y . The root of the WTA computation is set by a competition at the top layers of the pyramid depending on network configuration (task biases can weight each computation). The winning value is W, and this is determined by: 1. If there is only a single active feature pyramid f,</p><formula xml:id="formula_19">W ¼ max x;y M f x;y .<label>ð17Þ</label></formula><p>2. If F contains more than one feature map, representing mutually exclusive features, then</p><formula xml:id="formula_20">W ¼ max x;y À max i2F M i x;y Á .<label>ð18Þ</label></formula><p>3. If F contains more than one feature map representing features that can co-exist at each point, then there is more than one WTA process, all rooted at the same location but operating through different feature pyramids</p><formula xml:id="formula_21">W ¼ max x;y X i2F M i x;y .<label>ð19Þ</label></formula><p>4. If F contains subsets representing features that are mutually exclusive (the set A, as in case 2 above) as well as complementary (the set B, as in case 3 above), the winning locations are determined by the sum of the strongest response among set B (following method 3) plus the strongest response within set A (using method 2). Thus, a combination of the above strategies is used. There is more than one WTA process, all rooted at the same location but operating through different feature pyramids:</p><formula xml:id="formula_22">W ¼ max x;y X b2B M b x;y þ max a2A ðM a x;y Þ ! .<label>ð20Þ</label></formula><p>At the ÔtopÕ of the overall processing layers Eq. ( <ref type="formula" target="#formula_22">20</ref>) applies and includes all representations. At all other layers of the hierarchy, the equation used is determined by the receptive field properties of the neurons in each representation. As a result, there is no single saliency map in this model as there is in most other models <ref type="bibr">([42,38,43]</ref> and others). Notable exceptions are the models of Hamker <ref type="bibr" target="#b43">[44]</ref> and of Deco and Zihl <ref type="bibr" target="#b44">[45]</ref> both of which claim no salience map. Nevertheless, the similarity ends there. The Hamker strategy considers attentional effects in V4 only, does not provide a mechanism for how attentional control signals are generated, and says nothing about the contribution to overall perception by attentional modulation in all the other visual areas. In Deco and Zihl, the Neurodynamical Model implicitly codes saliency as a distribution of modulation across the feature maps. Feature maps relevant for the task are enhanced and/or distracters are inhibited, the dynamics of the network producing winners without the need for explicit representation of salience. Selection occurs through inhibitory competition within neuronal pools. They do not consider a realistic implementation since simple saliency matrices form their input.</p><p>Although the WTA in ST was introduced in the preceding paragraphs for the top layer only, in ST there is no single WTA process necessarily, but several simultaneous WTA threads that extend through the hierarchy to all layers. Eqs. ( <ref type="formula" target="#formula_19">17</ref>) and ( <ref type="formula" target="#formula_20">18</ref>) lead to a single WTA thread moving from top layer through the hierarchy; Eqs. <ref type="bibr" target="#b18">(19)</ref> and ( <ref type="formula" target="#formula_22">20</ref>) lead to multiple top-down WTA threads, one for each of the representations that may co-exist. Each of those will localize their features. Saliency is a dynamic, local, distributed, and task-specific determination and one that may differ even between processing layers as required. Although it is known that feature combinations of high complexity do exist in the higher levels of cortex, the above does not assume that all possible combinations must exist. Features are encoded separately in a pre-defined set of maps and the relationships of competition or cooperation among them provide the potential for combinations. The above four types of competitions then select which combinations are to be further explored. This flexibility allows for a solution (at least in part) to the binding issues that arise for this domain.</p><p>The WTA process is implemented utilizing a top-down hierarchy of units. There are two main unit types: gating control units and gating units. Gating control units are associated with each competition in each layer and at the top, are activated by the executive controller in order to begin the WTA process. An additional network of top-down bias units can also provide task-specific bias if it is available. They communicate downwards to gating units that form the competitive gating network for each WTA within a receptive field. Whether the competition uses Eqs. ( <ref type="formula" target="#formula_19">17</ref>)- <ref type="bibr" target="#b18">(19)</ref>, or <ref type="bibr" target="#b19">(20)</ref> depends on the nature of the inputs to the receptive field. Once a particular competition converges, the gating control unit associated with that unit sends downward signals for the competition to begin at the next layer down in the pyramid. The process continues until all layers have converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The simulation</head><p>The model has been implemented and tested in several labs applying it to computer vision and robotics tasks. The current model structure is shown in Fig. <ref type="figure" target="#fig_3">5</ref>. The executive controller and working memory, the motion pathway (V1, MT, MST, and 7a), the peripheral target area PO, the gaze WTA and gaze controller have all been implemented and examples of performance can be found in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Work is currently underway to extend the implementation to the object pathway (V1, V2, V4, and IT) and to binocular stimuli as well as extensions of the executive controller and recognition layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">A full hierarchy example</head><p>Fig. <ref type="figure">6</ref> shows an example using a purely motion-defined object, that is, a pattern of random elements moving within a static random field (Gaussian noise). The motion is a counter-clockwise rotating square, the same as for the sequence of Fig. <ref type="figure" target="#fig_0">3</ref>. The figure also illustrates how separate features in different locations and represented in different maps are bound together into a whole, a process that will be described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Biological and behavioral predictions</head><p>The first description of the overall structure of the model appeared along with most of the basic predictions in 1990 <ref type="bibr" target="#b0">[1]</ref>. These included (with support that has appeared since):</p><p>• Suppression around attended items in spatial as well as in the feature dimension <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>. • Attention is a top-down process; attentional guidance and control are integrated into the visual processing hierarchy, rather than being centralized in some external brain structure implying that the latency of attentional modulations decreases from lower to higher visual areas <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>• Attentional modulation appears wherever there is many-to-one, feed-forward neural convergence, something that in 1990 had no support at all <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59</ref>]. • Topographic distance between attended items and distractors affects amount of attentional modulation <ref type="bibr" target="#b50">[51]</ref>.</p><p>Additional predictions and supporting arguments can be found in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. These counter-intuitive predictions made well before any hints of experimental evidence, provide the strongest possible argument for the biological realism of the theory behind the ST model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Using ST to attend to and localize motion patterns</head><p>Most of the computational models of primate motion perception that have been proposed concentrate on feed-forward, classical types of processing and do not address attentional issues. However, there is strong evidence that the responses of motion neurons in at least areas MT, MST, and 7a are modulated by attention <ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref>. As a result of the modelÕs feed-forward computations, the neural responses in the high-level areas (MST and 7a) indicate the kind of motion patterns presented as an input but do not localize the spatial position of the patterns. The ST model was then applied to this feed-forward pyramid, adding the required feedback connections, hierarchical WTA processes, and gating networks as originally defined. The model Fig. <ref type="figure">6</ref>. Attending to a motion defined object. This shows the structure of the attention beam that localizes and labels the rotating square whose feed-forward outputs are shown in Fig. <ref type="figure" target="#fig_0">3</ref>. The beam color is green, which signifies counter-clockwise rotation. (see the colour wheel in Fig. <ref type="figure">1</ref>). Note also the fact that its root is in a single representation of 7a (spiral neurons), and then the beam splits to include all the components of the rotating object localizing those components in each of the MT and MST representations. The beam then reunifies at the input image, binding together the pieces into a whole. The top of the figure shows the active beam pass zone structure; the bottom of the figure shows the localization of the motion in the image. In this figure all layers of the pyramid are clearly visible, the active representations within each layer only are shown. In the figures that follow, the pyramid will be tilted into the page so that the input can be shown together with the beam structure all in one figure. However, the top layer is thus occluded from view; it is however still part of the overall beam. attends to motion, whether it exhibits a single motion or a combination of motion types, and serially focuses on each motion, sequentially, in order of response strength.</p><p>The hierarchical WTA described earlier finds the globally most active region. Then for this region, WTA processes are activated as described in Section 3. Translational and spiral motion patterns can co-exist for the same object (Eq. ( <ref type="formula" target="#formula_20">18</ref>)). The remaining processing proceeds as described earlier for each of the winning patterns. The model also includes processes detecting onset and offset events (start and stop), but these are not described here (see <ref type="bibr" target="#b46">[47]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature binding</head><p>A major contribution of the demonstration of how ST can operate within such a complex hierarchy is the method of grouping features (known as the binding problem in computational neuroscience <ref type="bibr" target="#b62">[63]</ref>). It is not claimed that this particular strategy has sufficient generality to solve all possible issues within the binding problem; however it seems to solve the limited cases that occur in image sequences of simple motion patterns. As such, it is the first instance of such a solution and further work will investigate its generality.</p><p>Quoting Roskies <ref type="bibr" target="#b63">[64]</ref>, ''the canonical example of binding is the one suggested by Rosenblatt <ref type="bibr" target="#b64">[65]</ref> in which one sort of visual feature, such as an objectÕs shape, must be correctly associated with another feature, such as its location, to provide a unified representation of that object.'' Such explicit association (''binding'') is particularly important when more than one visual object is present, in order to avoid incorrect combinations of features belonging to different objects, otherwise known as Ôillusory conjunctionsÕ <ref type="bibr" target="#b65">[66]</ref>. Several other examples of the varieties of binding problems in the literature appear in a special issue of Neuron edited by Roskies <ref type="bibr" target="#b62">[63]</ref>. At least some authors <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref> suggest that specialized neurons that code feature combinations (introduced as cardinal cells by Barlow <ref type="bibr" target="#b68">[69]</ref>) may assist in binding. The solution in this paper does indeed include such cells; however, they do not suffice on their own as will be described because they alone cannot solve the localization problem.</p><p>What is demonstrated here through the use of localized saliency and WTA decision processes, is precisely what the binding problem requires: neurons in different representations that respond to different features and in different locations are selected together, the selection being in location and in feature space, and are thus bound together via the ÔpassÕ zone(s) of the attention mechanism. Even if there is no single neuron at the top of the pyramid that represents the concept, the WTA allows for multiple threads bound through location by definition in Eqs. ( <ref type="formula" target="#formula_19">17</ref>)- <ref type="bibr" target="#b19">(20)</ref>.</p><p>Part of the difficulty facing research on binding is the confusion over definitions and the wide variety of tasks included in binding discussions. For example, in Feature Integration Theory (FIT) <ref type="bibr" target="#b69">[70]</ref> location is a feature because FIT assumes it is faithfully represented in a master map of locations. But this cannot be true; location precision changes layer to layer in any pyramid representation. In the cortex, it is not accurate in a Euclidean sense almost anywhere, although the topography seems qualitatively preserved <ref type="bibr" target="#b18">[19]</ref>. The wiring pattern matters in order to get the right image bits to the right neurons. Thus binding needs to occur layer to layer and is not simply a problem for high-level consideration. Features from different representations with different location coding properties converge onto single cells and this seems to necessitate an active search process.</p><p>This proposal is supported by the architecture described by Felleman et al. <ref type="bibr" target="#b70">[71]</ref> for the object recognition pathway of V1, V2, V4, and IT. They suggest that specific patterns of inter-cortical input and cortical circuitry may permit new and more complex receptive field properties for extrastriate cortical neurons. This appears true for both feed-forward as well as feedback connectivity. Projections display a complicated submodular selectivity with the modules being inter-digitating, non-overlapping, and highly intermixed. This structure necessitates a different view of how neural inputs are handled, each of these different inputs perhaps dealt with differently. The strategy presented in this paper is a step toward providing a computational framework for this architecture.</p><p>For the purposes of this argument, consider the following:</p><p>1. Location is not a feature, rather, it is the anchor that permits features to be bound together. Location is defined broadly, may be single points or groups of contiguous points, and may be differently organized in each visual area; in practice it is considered to be local coordinates within a visual area (think of an array of hypercolumns, each with its own local coordinates). 2. A grouping of features not coincident by location cannot be considered as a unitary group unless there is a unit to represent that group with a receptive field definition that takes in input from the different feature representations providing a ÔtemplateÕ for the group. 3. Features that compose a group may be in different locations and represented in different visual areas as long as they converge onto units that represent the group. 4. If the group is attended, then the WTA of Section 3.3 will find and attend to each of its parts regardless of their location or feature map representation. This is a solution to the aspect of binding that attends to groups and finds parts of groups. It applies equally well for object recognition: faces are good examples of a grouping of features. In the demonstrations below, the groups are motion patterns. There are several components to this solution. The first has to do with the particular representations chosen for motion patterns. Our representation is hierarchical with each layer being defined using components from the previous. Note how a constant speed-rotating object exhibits constant velocity gradient across location with respect to local motion. A neuron higher in the hierarchy then can be selective to regions that are homogeneous for this value and this is an easy selectivity to define and implement. As shown in Eq. ( <ref type="formula" target="#formula_13">11</ref>), a motion pattern detector in layer MST simply sums responses of the corresponding MT units that feed it. An example is in order using the figure sequence in Fig. <ref type="figure" target="#fig_0">3</ref>. In layer MT, neurons sensitive to local motion gradients respond as shown (Fig. <ref type="figure" target="#fig_1">3C</ref>). Across all directions in the representation, one sees that the object has been deconstructed-Ôcut into pie piecesÕ-one for each local motion direction. That is, the tuning properties of the neurons have decom-posed the flow field into distinct areas of constant velocity gradient. Note that these have also been partitioned depending on speed. Then, at the MST layer, the neurons whose selectivity is for rotation within this particular speed band will receive input from these MT representations (and not from the others). The MST neuron whose receptive field is best centered on the object will fire strongest if it receives sufficient stimulation, which in this case means that it sees all pieces of the pie. This best responding neuron can now be considered as having grouped the pie pieces and re-assembled the pie, that is, to have bound together the representations at the MT layer which otherwise are neither co-incident by location nor feature type. This is the feed-forward part of this process-an implicit binding action. If the task of the system were to simply detect the presence of a particular motion pattern, this representation would suffice as long as the top-level global WTA selects this region. However, if the systemÕs task is to localize or recognize, then the job is not complete. As is clear in the figure, there are many MST neurons that respond. The feedback process of top-down attention selects the best of these responses, and actively sub-selects the particular regions of MT neurons that correspond to that best firing, and thus best fitting the pattern selectivity of the neuron. The unique aspect here is that the receptive field of the MST neuron is defined by a spatial region as well as a subset of features computed within that spatial region, each feature contributing a component across that spatial region (as specified by Eq. ( <ref type="formula" target="#formula_13">11</ref>); however, it is easily seen that if spatial distributions different from uniform motion are of interest, variations of this equation can be set up to permit any spatial combination of local gradients). There are 468 feature maps in the MT layer that feed the 72 MST layer units and these can be organized for a huge variety of distinct motions. Translation and spiral motions can co-exist for the same spatial object, and thus there is a WTA thread for translation and another one for spiral motion. The overall peak then uses the strategy described in Section 3.3 to grow the full region corresponding to the moving object. At layer 7a, the same is seen. Full-field motion takes precedence over object motion in these representations. Thus, full-field rotations, full-field contractions, and spiral motions cannot co-exist. They can co-exist with translation however. All feature maps are not complementary nor do they all play equivalent roles in the WTA process. This shows the need for a more flexible view on saliency and WTA computations than has been previously shown in other models (all other models use the definition and structure first presented by Koch and Ullman <ref type="bibr" target="#b37">[38]</ref>). No other model currently includes such a distributed definition of saliency.</p><p>What if a more complex binding problem is considered, one where multiple motion patterns appear in an image sequence? As can be seen in Fig. <ref type="figure" target="#fig_4">7</ref>, the two moving objects (this time in an image sequence of a real scene) activate many representations within the hierarchy and both can be seen within each of several representations. The rectangular object is approaching the camera while the circular one is rotating counter-clockwise. This is again a classic binding problem. Although the representation at the output layer (7a) is more complex due to multiple stimuli, the WTA is still able to choose a peak, and use that location information as well as the tuning properties (i.e., feed-forward connectivity) of the winning units to sub-select the correct components of the winning pattern. The full sequence is shown in the figure in order to also show the sequential nature of the selection process that attends to each of the patterns (two passes through the algorithm in Section 3.1).  <ref type="figure">(F-G</ref>) The output of area MT in the second feed-forward pass, after the inhibition of return is applied, in summary form as described for Fig. <ref type="figure" target="#fig_1">3C</ref>. (H) The second attentional fixation, in green, the colour for counter-clockwise rotation. The inhibition of return seems to not be perfect, that is, not all pixels due to the approaching object are eliminated and so some additional responses remain. The reason for this is that the IOR is set using the attended location. The object however continues to move and since it is approaching, its appearance is beyond that original attended and thus inhibited location. Future work would enable inhibition not only of location but of the attended object.</p><p>Finally, an even more complex binding example would be one where objects are not spatially separated but rather overlap. An example is shown of two spatially overlapping motion patterns in Fig. <ref type="figure" target="#fig_6">8</ref> where two hexagonal disks are rotating one against the other. The two attentional fixations are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Other motion types in the model</head><p>The model includes more motion types than just the ones described above. The methods of detecting onset and offset of events are included in the model and have been described previously <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b1">2]</ref>. This method has proven to be effective within any of the above representations. For example, assume an object is displaying a clockwise rotating motion, then stops and changes direction. The offset of the rotation signifies the end of the motion within the clockwise rotation representation. The onset within the counter-clockwise rotation representation would denote its beginning. Each of the representations above has a corresponding onset and offset computation and representation. It should be clear then that any motion processing model must include methods to detect the initiation and cessation of motions, yet most do not.</p><p>The previous examples were of motions generated by objects that did not change position in the image. Of course, objects in motion do change position (or camera motion induces such a motion) and motion occurs not only in pre-defined short image sequences but also continuously. To generate continuous output from continuous input, the straightforward solution might be to repeat the entire process of the algorithm in Section 3.1, that is, feed the network with time varying image sequences, perform the feed-forward computation and then feedback attentional selection. This idea that re-computing the pyramid and the beam for each image subset will definitely work, but is not efficient and has doubtful biological consistency due to the extra processing time required. A better strategy would be to modify the beam locally only as much as is needed to enable it to track the changes. The local beam structure could be defined in a dynamic manner, re-directed by new gating control signals at the top that propagate downwards <ref type="bibr" target="#b71">[72]</ref>.</p><p>In this approach, signals flow continuously through the pyramid. Input is continuous and flows upwards continuously. As WTA processes complete at the top, gating control signals begin their downward journey implementing the top-down WTA hierarchy. However, signals are potentially changing as this occurs and the selections made will not be the exact ones that led to the top winner, at least in precise location. Remember that the WTA as defined is guaranteed to find winners anywhere in the receptive field. The winner at the top is not restricted as a single location but rather can be a region. As long as motion speed and speed of signal propagation is matched, the system is blind to small location changes and operates correctly as expected. However the gating signals that would be generated at the top of the pyramid require time to propagate downwards and thus the control they may exert on lower levels of processing will reflect the past and not the present. This strategy features a time delay and a time period of ÔblindnessÕ: discontinuities in motion that occur with a shorter duration than the propagation time are missed. This has been tested successfully on motion-defined object translation. The localization is rather simple because only the translation pyramidal stream is activated and the motion feature is uniform across the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>This paper makes several points: (1) it presents a new feed-forward motion processing hierarchy, <ref type="bibr" target="#b1">(2)</ref> it presents examples of how the ST model can operate on this hierarchy to localize and label motion patterns, and (3) it shows how some aspects of recognition that require feature grouping (or binding) may be accomplished using a top-down attentional selection mechanism that does not depend on a single locationbased saliency representation.</p><p>First, a new feed-forward motion analysis hierarchy is presented. The structure and computations are strongly inspired by biology, and the resulting network has a good degree of biological realism, although it is not biologically accurate in several ways. Due to the incorporation of functionally diverse neurons in the motion hierarchy, the output of the present model encompasses a wide variety of selectivities at different resolutions. This enables the computer simulation of the model to detect and classify various motion patterns in artificial and natural image sequences showing one or more moving objects as well as single objects undergoing complex, multiple motions. Most other models of biological motion perception focus on a single cortical area. For instance, the models by Zemel and Sejnowksi <ref type="bibr" target="#b9">[10]</ref>, Simoncelli and Heeger <ref type="bibr" target="#b3">[4]</ref>, and Beardsley and Vaina <ref type="bibr" target="#b4">[5]</ref> are biologically relevant approaches that explain some specific functionality of MT or MST neurons, but do not include the embedding hierarchy in the motion pathway. On the other hand, there are hierarchical models for the detection of motion. Meese and Andersen <ref type="bibr" target="#b6">[7]</ref> do not provide a computationally plausible version of the motion processing hierarchy. Giese and Poggio <ref type="bibr" target="#b5">[6]</ref> describe a sophisticated, biologically motivated, and complex hierarchy for processing human movement patterns. However, they did not include any attentional influences. Further, they provide early input to their algorithm manually. Hand-tracked body joint positions were manually converted to stick figures where optic flow is easily computed. They cannot handle complex, overlapping, dense flow or discontinuous motions and certainly cannot process real image sequences directly. Lu and Sperling <ref type="bibr" target="#b72">[73]</ref> present a motion hierarchy as well as attentive processes, but the model is not a computational one. However, it has strong biological plausibility in its function. They proposed that human visual motion perception is served by three separate motion systems: a first-order system that responds to moving luminance patterns, a second-order system that responds to moving modulations of feature types, and a third-order system that computes the motion of marked locations in a salience map. This third-order system of Lu and Sperling seems to be similar to the process of attending to motion in ST but without the computational details, it is difficult to draw too close a comparison.</p><p>Of course, this is only the beginning and we are actively pursuing several avenues of further work. The tuning characteristics of each of the neurons only coarsely model current knowledge of primate vision. The model includes little cooperative or competitive processing among units within a layer other than V1. Experimental work examining the relationship of this particular structure to human vision is also ongoing.</p><p>It is important to put this new motion analysis framework into context of classic literature on motion starting with Koenderink and Van Doorn <ref type="bibr" target="#b73">[74]</ref> and Longuet-Higgins and Prazdny <ref type="bibr" target="#b74">[75]</ref>. On the assumptions that a moving object can be modeled (at least piecewise) by rigid planar patches, and that there is a fixation point on the surface in question, the motion may be estimated using an affine transformation. The affine model can be described on the basis of four quantities: image translation, image rotation, divergence, and shear. The first two terms specify respectively a rigid 2D translation and rotation of the fixated object. The third term describes an isotropic expansion (or contraction) that specifies a change in scale or a pure deformation.</p><p>The shear term results in a distortion of the image pattern and corresponds to an expansion in a specified direction and a simultaneous contraction in the perpendicular one in such a way that the area of the pattern is preserved. Under perspective projection, the velocity vector at each point of an image is given by computing the derivative in time of the gray value changes at each pixel (x, y) and yields (u, v). This The two attentional fixations for the two hexagons. The lower one was attended first, localized quite well and labeled in green for counter-clockwise rotation, and the upper one second, localized well except for the overlap region and labeled in red for clockwise rotation. Note that even though the objects were overlapping, the motion labels were correct and the object localizations reasonable given the overlap. Note how the responses in area 7a are completely merged and no simple scheme could possibly disentangle this using only that output. However, the top-down search and feature binding strategy described here successfully separates the signals and localizes and labels the moving objects. represents translational motion. Spatial derivatives are then taken of each velocity component u and v in the x and y directions (u x , u y , v x , v y ). Combinations of these derivatives provide definitions of each of the remaining three affine motions, rotation, deformation, and divergence. Divergence is represented by l = u x + v y . Deformation or shear has two components q and r and these are given by q = u x À v y and r = u y + v x . Finally, rotation is expressed as k = u y À v x . The extraction of the affine estimates has essentially two components: the identification of an appropriate set of 2D spatial patches to represent each surface in a scene, and the tracking of the patches through the image sequence. The main point here is that the setting of a fixation point or the identification of the 2D patches to track is central to the definition and the majority if not all past uses of affine estimation make assumptions about where this fixation comes from. In this paper, we show a method for how it might be determined. More importantly, this is the first model to explicitly use local spatial derivates of velocity as an intermediate representation between local flow vectors and affine motion patterns. Evidence for such neurons was presented by Treue and Andersen <ref type="bibr" target="#b24">[25]</ref>. Elsewhere, we show that this structure is biologically realistic by experimentally confirming that there are neural correlates in humans for the several layers of motion processing implied by these affine motion definitions <ref type="bibr" target="#b30">[31]</ref>.</p><p>Second, this paper shows that the earlier criticisms of the ST demonstrations, namely that the simple feature pyramids computed using Gaussian blurring were not biologically realistic nor useful, can now be forgotten. Although those criticisms were completely valid, they do not affect the original definition of ST, only those early demonstrations. As has been shown, ST operates perfectly well in this significantly more complex representation.</p><p>Another strength of our model is its mechanism of visual attention. To our knowledge, there are only three other computational motion models that address attention for motion. The earliest ones are due to Nowlan and Sejnowski <ref type="bibr" target="#b7">[8]</ref> and Daniilidis <ref type="bibr" target="#b2">[3]</ref>. In Nowlan and Sejnowski, processing is much in the same spirit as ours but very different in form. They compute motion energy with the goal of modeling MT neurons. This energy is part of a hierarchy of processes that include softmax for local velocity selection. They suggest that the selection permits processing to be focused on the most reliable estimates of velocity. There is neither top-down component nor a full processing hierarchy nor binding for complex patterns. Attentional modulation in motion neurons was described experimentally in <ref type="bibr" target="#b60">[61]</ref> and appeared after their model was presented; thus, of course it is not developed and does not appear to be within the scope of their model. Based on the optical flow, Daniilidis computed 3D motion and structure. He fixated on an object to estimate ego motion in the presence of translation and rotation of the observer from the flow in the log-polar periphery. Computation of time to collision was a goal, not the definition of an attentive motion hierarchy. Although he used attentive fixations to advantage, the motion processing there was quite specific and based on log-polar representations and the connection to affine motion, implicated by the need to fixate, was not recognized. Finally, Grossberg et al. <ref type="bibr" target="#b8">[9]</ref> present an integration and segmentation model for motion capture. Called the Formotion BCS model, their goal is to integrate motion information across the image and segment motion cues into a unified global percept. They employ models of translational processing in areas V1, V2, MT, and MST and do not consider motion patterns. Competition determines local winners among neural responses and the MST cells encoding the winning direction have an excitatory influence on MT cells tuned to the same direction. A variety of motion illusions are illustrated but no real image sequences are attempted. This model seems to be closest to ST here in goal and methodology. None of these models has the breadth of processing in the motion domain or in attentional selection as the current work.</p><p>An interesting comparison can be drawn between the performance of ST on this motion hierarchy and the population code strategy of neuronal representation. In the standard approach of population encoding, an assumption is made that there exists a unique and unambiguous value within the population that represents the stimulus (see <ref type="bibr" target="#b75">[76]</ref> for review). A contrasting view put forward by Zemel and Dayan is that a population code can also contain additional information such as multiple values and uncertainty thus obviating the need for the restrictive assumption <ref type="bibr" target="#b76">[77]</ref>. Is there a relationship of one or both of these approaches to the methods presented in this paper? The short answer is yes, there is a rather strong relationship at least at a qualitative level with the Zemel and Dayan approach. One of the examples presented earlier will be used to illustrate. The set of figures within Fig. <ref type="figure" target="#fig_4">7</ref> show the representations at each level of the hierarchy that arise due to two moving items within the visual field. Since there is no other clutter it will serve well to illustrate the representations for each item and how the overall population encodes these. First, the term Ôpopu-lationÕ must be clarified. In most population coding work, an assumption is made about where the neural responses come from and to what stimuli they respond. In Zemel and Dayan for example, responses from MT neurons in monkey are used; as is clear from the present paper, there is not only a single type of neuron in area MT. The Zemel and Dayan scheme does not separate the features, something that has been shown to have computational advantages <ref type="bibr" target="#b0">[1]</ref>. Also, as the review by Pouget et al. shows, responses of behaving monkeys performing a task requiring attention are not considered. In other words, the population of neurons in the Zemel and Dayan work represent the combined result of all the feature maps of the first feed-forward pass as described in this paper, an unnecessarily large population. Population-coding strategies then, from this first pass encoding, attempt to remove noise and make inferences. ST employs attention for this task. The WTA process operates over a population in order to estimate the best response. This is an estimate because it is not necessarily the correct response. The next stage of processing, applying the top-down inhibitory surround around the selected location removes noise and permits a more precise representation of only the attended stimulus. A match of this result to a task representation then verifies or rejects the estimate. If there is more than one item in the visual field as in Fig. <ref type="figure" target="#fig_4">7</ref>, the response populations due to each overlap as seen in the figure since the two peaks are readily visible. Importantly, Figs. 7F and G show the population code after the first attended stimulus has been removed by the attentive inhibition of return process. It has been sufficiently (but not completely) cleaned up so that the second peak in the population is correctly found. In summary, the distributional population-coding scheme of Zemel and Dayan and the ST scheme address very similar problems and accomplish very similar goals. The methods differ and the underlying assumption differs. That ST includes attention and the time course of attentional modulation within the process makes it a closer biological match. However, it would still be very interesting to do a more detailed analysis of the underlying mathematics for each to see where equivalences and differences can be found.</p><p>Inspection of the patterns of activation shown in the figures also provides some perspective on the issue of the location of Ôthe saliency map.Õ The search for a neural correlate to the concept of a saliency map has led to much interesting experimental work each providing evidence for one or another particular location (superior colliculus <ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref>; LGN <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>; V1 <ref type="bibr" target="#b82">[83]</ref>; V1 and V2 <ref type="bibr" target="#b83">[84]</ref>; pulvinar <ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref>; FEF <ref type="bibr" target="#b87">[88]</ref>; parietal <ref type="bibr" target="#b88">[89]</ref>). In each of these, the correlate is found by locating maxima of response within a neural population that corresponds to the attended location. Consider the patterns of response shown in any of the figures in this paper involving visual areas MST or higher. In each case, this criterion of a maximum corresponding to an attended location can be seen. In each area, both feed-forward and feedback influences take effect. Perhaps this is why evidence has been found in so many areas for the neural correlate to the saliency map? Maybe saliency is a distributed computation, as shown in this paper, and like attention itself, evidence reflecting these computations can be found in many, if not all, neural populations.</p><p>Finally, our attention strategy also demonstrates a key aspect of the recognition process, that is, the separate computation of parts and their subsequent re-assembly guided by attention. The key to this solution is the abandonment of the single, location-based saliency representation that supports a single point-based WTA, a feature of most other attention models. Even for those that do not use a single map, attention is an emergent, stochastic feature. Here, saliency is a local and distributed, deterministic phenomenon and the WTA processes are hierarchical, region-based, and dynamically defined depending on task and neural selectivity. Although it would not be entirely inappropriate to claim this is a solution to the classic binding problem, it is too early to justify this claim. The solution here, however, does appear to have the right elements to solve the limited aspects of binding required for this domain.</p><p>This strategy for attention to motion can be considered as a precursor to more detailed analysis in order to extract precise velocity and direction of motion. For example, in a computer vision application, a determination of precise velocity depends on good object localization and elimination of outliers. An attentive process such as that presented here could provide a first estimate of object location and extent as well as point out which type of motion is present. New algorithms for velocity extraction could be developed that take advantage of the reduced search space.</p><p>Attention to motion has not received sufficient study in computer vision even though it is a critical component of a general solution to visual information processing <ref type="bibr" target="#b89">[90]</ref>. Significant enough advances have been made in early attentive algorithms to warrant a closer look at how current models of attention, such as ST, can be usefully directed at complex computer vision problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Output of area computations. This series of figures gives in detail all of the filter outputs throughout the full motion hierarchy. The input is a square of random noise rotating counter-clockwise in place on a background of random noise. (A) The overall output of area V1, that is, the output of the integrative units. (B) Translation output in area MT. (C) Speed gradient output in area MT. This is a summary representation of the 12 different speed gradients at each local speed and direction. Each coloured dot is the maximum value across the 12 representations. This is not used for any decision process; it is only for a simpler visualization. (D) Translation output in area MST. (E) Generalized spiral output in area MST. (F) Translation output in area 7a. (G) Generalized spiral output in area 7a. (H) Rotation and radial output in area 7a.</figDesc><graphic coords="8,100.56,107.28,357.48,359.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. (continued)</figDesc><graphic coords="9,87.87,107.28,352.80,354.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Attentional beam. This shows the rationale for suppression around attended items that is a feature of ST.</figDesc><graphic coords="14,158.25,108.45,253.44,285.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. ST full hierarchy. The full visual processing hierarchy on which ST operates is depicted. This paper focuses on the motion pathway-areas V1, MT, MST, and 7a. Several other components have been demonstrated previously while others are current research topics.</figDesc><graphic coords="19,93.54,186.66,340.34,418.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Grouping across location for motion pattern detection. (A) In this example, real images are used of two textured objects against a cluttered background, the rectangle is approaching the camera while the circle is rotating counter-clockwise. (B and C) The output of area MT in the first feed-forward pass in summary form as described for Fig. 3C. (D) Localization of the first winning area in red, the colour signifying ÔapproachÕ (E) Inhibition of return on the attended pathways.(F-G) The output of area MT in the second feed-forward pass, after the inhibition of return is applied, in summary form as described for Fig.3C. (H) The second attentional fixation, in green, the colour for counter-clockwise rotation. The inhibition of return seems to not be perfect, that is, not all pixels due to the approaching object are eliminated and so some additional responses remain. The reason for this is that the IOR is set using the attended location. The object however continues to move and since it is approaching, its appearance is beyond that original attended and thus inhibited location. Future work would enable inhibition not only of location but of the attended object.</figDesc><graphic coords="25,87.87,291.10,354.24,171.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 7 .</head><label>7</label><figDesc>Fig 7. (continued)</figDesc><graphic coords="26,97.70,285.15,174.20,169.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Overlapping motion patterns-a feature binding example. (A) Two overlapping textured hexagons, the left one rotating clockwise while the right one rotates counter-clockwise. (B-J) The complete representation of all first pass feed-forward outputs is shown. (K)The two attentional fixations for the two hexagons. The lower one was attended first, localized quite well and labeled in green for counter-clockwise rotation, and the upper one second, localized well except for the overlap region and labeled in red for clockwise rotation. Note that even though the objects were overlapping, the motion labels were correct and the object localizations reasonable given the overlap. Note how the responses in area 7a are completely merged and no simple scheme could possibly disentangle this using only that output. However, the top-down search and feature binding strategy described here successfully separates the signals and localizes and labels the moving objects.</figDesc><graphic coords="29,267.87,287.77,171.36,171.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 8 .</head><label>8</label><figDesc>Fig 8. (continued)</figDesc><graphic coords="30,109.06,107.28,341.28,515.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 8 .</head><label>8</label><figDesc>Fig 8. (continued)</figDesc><graphic coords="31,170.08,107.28,184.32,232.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,106.23,107.28,347.40,254.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="21,124.72,107.28,277.56,367.92" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.K. Tsotsos et al. / Computer Vision and Image Understanding 100 (2005) 3-40</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>J.K. Tsotsos et al. / Computer Vision and Image Understanding 100 (2005) 3-40</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>A number of people have contributed to this effort in important ways and we wish to express our thanks for their assistance: Sean Culhane, Stefan Treue, Guy Orban, Albert Rothenstein, Winky Wai, Yuzhong Liu, and Michael Tombu. Funding for this research was gratefully received from the Natural Sciences and Engineering Council of Canada, Communications and Information Technology Ontario, a Province of Ontario Centre of Excellence, and from the Institute for Robotics and Intelligent Systems, A Network of Centers of Excellence of the Government of Canada through grants to J.K.T. J.K.T. holds the Canada Research Chair in Computational Vision. J.C.M.-T. holds the Canada Research Chair in Neuroscience.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing vision at the complexity level</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Brain Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="445" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nuflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="507" to="547" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attentive visual motion processing: computations in the log-polar plane</title>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing (Suppl.)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A model of neuronal responses in visual area MT</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computational modeling of optic flow selectivity in MSTd neurons</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Vaina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="467" to="493" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural mechanisms for the recognition of complex movements and actions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Giese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spiral mechanisms are required to account for summation of complex motion components</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Meese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1073" to="1080" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A selection model for motion processing in area MT of primates</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1195" to="1214" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural dynamics of motion integration and segmentation within and across apertures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2521" to="2553" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A model for encoding multiple object motions and self-motion in area MST of primate visual cortex</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="531" to="547" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural model of smooth pursuit control and motion perception by cortical area MST</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognit. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="120" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emulating the visual receptive field properties of MST neurons with a template model of heading estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="5958" to="5975" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neurally-inspired model for detecting and localizing simple motion patterns in image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pomplun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez-Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Workshop on Dynamic Perception</title>
		<meeting>4th Workshop on Dynamic Perception<address><addrLine>Bochum, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">November 14-15, 2002</date>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attending to Motion: Localizing and Labeling Simple Motion Patterns in Image Sequences</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pomplun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez-Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2525</biblScope>
			<biblScope unit="page" from="439" to="452" />
			<date type="published" when="2002">2002</date>
			<publisher>Springer-Verlag Berlin</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An inhibitory beam for attentional selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial Vision in Humans and Robots</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Harris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Jenkin</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="313" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards a computational model of visual attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Early Vision and Beyond</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Papathomas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Chubb</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gorea</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kowler</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press/Bradford Books</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complexity, vision and attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision and Attention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Harris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Jenkin</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From theoretical foundations to a hierarchical circuit for selective attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cutzu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Attention and Cortical Circuits</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Braun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed hierarchical processing in the primate visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Felleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speed and direction selectivity of Macaque middle temporal neurons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raiguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Orban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="39" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Velocity sensitivity and direction sensitivity of neurons in areas V1 and V2 of the monkey: influence of eccentricity</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Orban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bullier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="462" to="480" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The speed tuning of medial superior temporal (MST) cell responses to optic-flow components</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Orban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raiguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="285" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analysis of optic flow in the monkey parietal area 7a</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="327" to="346" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Motion-responsive regions of the human brain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sunaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Hecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Orban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Exp. Brain Res</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="370" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural responses to velocity gradients in macaque cortical area MT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="804" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MST neurons respond to speed patterns in optic flow</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Wurtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2839" to="2851" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Receptive field properties of neurons in middle temporal visual area (MT) of owl monkeys</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Felleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kaas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="488" to="513" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tuning of MST neurons to spiral motions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Snowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The middle temporal visual area in the macaque: myeloarchitecture, connections, functional properties and topographic organization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Maunsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bixby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Neurol</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="326" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optical flow using spatiotemporal filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="302" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Martinez-Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pomplun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Hopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selectivity for speed gradients in human area MT/V5</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="435" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Ôcomplexity levelÕ analysis of vision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Internat. Conf. on Computer Vision</title>
		<meeting>1st Internat. Conf. on Computer Vision<address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="346" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The complexity of perceptual search tasks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internat. Joint Conf. on Artificial Intelligence</title>
		<meeting>Internat. Joint Conf. on Artificial Intelligence<address><addrLine>Detroit</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1571" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the relative complexity of passive vs active visual search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="141" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards a biologically plausible active visual search model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rothenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV WAPCV 2004</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>ECCV WAPCV 2004</meeting>
		<imprint>
			<publisher>Springer-Verlag Berlin</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3368</biblScope>
			<biblScope unit="page" from="133" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ultra-rapid scene categorisation with a wave of spikes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biologically Motivated Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bulthoff</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2525</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Are cortical models really bound by the Ôbinding problemÕ?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural activity in early visual cortex reflects behavioral experience and higher order perceptual saliency</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="589" to="597" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parallelism in comparison problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="355" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<editor>M.A. Arbib</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="1056" to="1060" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Winner-take-all mechanisms</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Guided search: an alternative to the feature integration model for visual search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol.: Hum. Percept. Perform</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="419" to="433" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A dynamic model of how feature cues can guide spatial attention</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Hamker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="501" to="521" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Top-down selective visual attention: a neurodynamical approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cogn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="119" to="140" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An attentional prototype for early vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd European Conf. on Computer Vision</title>
		<title level="s">LNCS-Series</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Sandini</surname></persName>
		</editor>
		<meeting>2nd European Conf. on Computer Vision<address><addrLine>Santa Margherita Ligure, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag Berlin</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Directing attention to onset and offset of image events for eye-head movement control</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR Conf. on Pattern Recognition</title>
		<meeting>IAPR Conf. on Pattern Recognition<address><addrLine>Jerusalem</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attentional selection by distractor suppression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guerra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="669" to="689" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attentional interference at small spatial separations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Bahcall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention-dependent suppression of metabolic activity in the early stages of the macaque visual system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Vanduffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B H</forename><surname>Tootell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Orban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="126" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The selective tuning model of visual attention: testing the predictions arising from the inhibitory surround mechanism</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cutzu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="205" to="219" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Effects of search efficiency on surround suppression during visual selection in frontal eye field</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Schall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaughn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chi-Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="2765" to="2769" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attentional capture by abrupt onsets and feature singletons produces inhibitory surrounds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mounts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1485" to="1493" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention to orientation results in an inhibitory surround in orientation space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tombu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<ptr target="http://www.science.mcmaster.ca/~BBCS/2004/viewabstract.php?id=170" />
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Annual Meeting of Behaviour</title>
		<meeting>14th Annual Meeting of Behaviour</meeting>
		<imprint>
			<date type="published" when="2004">June 12-14, 2004</date>
		</imprint>
	</monogr>
	<note>St. JohnÕs Newfoundland</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The attentional ÔspotlightÕsÕ penumbra: center-surround modulation in striate cortex</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroreport</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="977" to="980" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention modulates responses in the human lateral geniculate nucleus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oõconnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinsk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kastner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1203" to="1209" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Intermodal selective attention in monkeys. I: distribution and timing of effects across visual areas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ulbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="358" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mechanisms of directed attention in the human extrastriate cortex as revealed by functional MRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Weerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungerleider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<biblScope unit="page" from="108" to="111" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is everywhere</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Britten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="issue">6591</biblScope>
			<biblScope unit="page" from="497" to="498" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The influence of attentive fixation upon the excitability of the light-sensitive neurons off the posterior parietal cortex</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mountcastle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Motter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1218" to="1225" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attentional modulation of visual motion processing in cortical areas MT and MST</title>
		<author>
			<persName><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H R</forename><surname>Maunsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="539" to="541" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Feature-based attention influences motion processing gain in macaque visual cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Martinez-Trujillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<biblScope unit="issue">6736</biblScope>
			<biblScope unit="page" from="575" to="579" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Roskies</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The binding problem-introduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roskies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="7" to="9" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Principles of Neurodynamics: Perceptions and the Theory of Brain Mechanisms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>Spartan Books</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Illusory conjunctions in the perception of objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Psychol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="107" to="141" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Specialized representations review in visual cortex: a role for binding?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maunsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The what and why of binding: review the modelerÕs perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Single units and cognition: a neurone doctrine for perceptual psychology</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="371" to="394" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="99" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Modular organization of occipito-temporal pathways: cortical connections between visual area 4 and visual area 2 and posterior inferotemporal ventral area in macaque monkeys</title>
		<author>
			<persName><forename type="first">D</forename><surname>Felleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcclendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3185" to="3200" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Modeling Motion with the Selective Tuning Model</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, York University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MSc. Thesis</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The functional architecture of human visual motion perception</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sperling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2697" to="2722" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Local structure of movement parallax of the plane</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="717" to="723" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The interpretation of a moving retinal image</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Prazdny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Royal Soc. London B</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page" from="385" to="397" />
			<date type="published" when="1173">1173. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Inference and computation with population codes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="381" to="410" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Distributional population codes and multiple motion models, NIPS-11: Adv</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="174" to="180" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Shared neural control of attentional shifts and eye movements</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kustov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page" from="74" to="77" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Saccade target selection in the superior colliculus during a visual search task</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mcpeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="2019" to="2034" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Separate signals for target selection and movement specification in the superior colliculus</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Newsome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A theoretical analysis of the electrical properties of an X-cell in the catÕs LGN: does the spine-triad circuit subserve selective visual attention?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Memo</title>
		<imprint>
			<biblScope unit="volume">787</biblScope>
			<date type="published" when="1984-02">February, 1984</date>
		</imprint>
		<respStmt>
			<orgName>MIT, Artificial Intelligence Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The control of retinogeniculate transmission in the mammalian lateral geniculate nucleus</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Exp. Brain Res</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A saliency map in primary visual cortex</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cognit. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Attention activates winner-take-all competition among visual filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="375" to="381" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Contributions of the pulvinar to visual spatial attention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="97" to="105" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The attention system of the human brain</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The pulvinar and visual salience</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Dissociation of visual discrimination from saccade programming in macaque frontal eye field</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Schall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1046" to="1050" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The representation of visual salience in monkey posterior parietal cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kusunoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="page" from="481" to="484" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Motion understanding: task-directed attention and representations that link perception with action</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="280" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
