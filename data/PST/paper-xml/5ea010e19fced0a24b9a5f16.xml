<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retinal Area Detector From Scanning Laser Ophthalmoscope (SLO) Images for Diagnosing Retinal Diseases</title>
				<funder ref="#_Rs3tpE4">
					<orgName type="full">EPSRC-DHPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Salman</forename><surname>Muhammad</surname></persName>
							<email>muhammad.s.haleem2@stu.mmu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangxiu</forename><surname>Haleem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jano</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baihua</forename><surname>Van Hemert</surname></persName>
							<email>jvanhemert@optos.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Fleming</surname></persName>
							<email>afleming@optos.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Haleem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Han</surname></persName>
							<email>l.han@mmu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Li</surname></persName>
							<email>b.li@mmu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Mathemat-ics and Digital Technology</orgName>
								<orgName type="institution">Manchester Metropolitan University</orgName>
								<address>
									<postCode>M1 5GD</postCode>
									<settlement>Manchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retinal Area Detector From Scanning Laser Ophthalmoscope (SLO) Images for Diagnosing Retinal Diseases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/JBHI.2014.2352271</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature selection</term>
					<term>retinal artefacts extraction</term>
					<term>retinal image analysis</term>
					<term>scanning laser ophthalmoscope (SLO)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scanning laser ophthalmoscopes (SLOs) can be used for early detection of retinal diseases. With the advent of latest screening technology, the advantage of using SLO is its wide field of view, which can image a large part of the retina for better diagnosis of the retinal diseases. On the other hand, during the imaging process, artefacts such as eyelashes and eyelids are also imaged along with the retinal area. This brings a big challenge on how to exclude these artefacts. In this paper, we propose a novel approach to automatically extract out true retinal area from an SLO image based on image processing and machine learning approaches. To reduce the complexity of image processing tasks and provide a convenient primitive image pattern, we have grouped pixels into different regions based on the regional size and compactness, called superpixels. The framework then calculates image based features reflecting textural and structural information and classifies between retinal area and artefacts. The experimental evaluation results have shown good performance with an overall accuracy of 92%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E ARLY detection and treatment of retinal eye diseases is critical to avoid preventable vision loss. Conventionally, retinal disease identification techniques are based on manual observations. Optometrists and ophthalmologists often rely on image operations such as change of contrast and zooming to interpret these images and diagnose results based on their own experience and domain knowledge. These diagnostic techniques are time consuming. Automated analysis of retinal images has the potential to reduce the time, which clinicians need to look at the images, which can expect more patients to be screened and more consistent diagnoses can be given in a time efficient manner <ref type="bibr" target="#b0">[1]</ref>.</p><p>The 2-D retinal scans obtained from imaging instruments [e.g., fundus camera, scanning laser ophthalmoscope (SLO)] may contain structures other than the retinal area; collectively regarded as artefacts. Exclusion of artefacts is important as a preprocessing step before automated detection of features of retinal diseases. In a retinal scan, extraneous objects such as the eyelashes, eyelids, and dust on optical surfaces may appear bright and in focus. Therefore, automatic segmentation of these artefacts from an imaged retina is not a trivial task. The purpose of performing this study is to develop a method that can exclude artefacts from retinal scans so as to improve automatic detection of disease features from the retinal scans.</p><p>To the best of our knowledge, there is no existing work related to differentiation between the true retinal area and the artefacts for retinal area detection in an SLO image. The SLO manufactured by Optos <ref type="bibr" target="#b1">[2]</ref> produces images of the retina with a width of up to 200 ? (measured from the centre of the eye). This compares to 45 ? -60 ? achievable in a single fundus photograph. Examples of retinal imaging using fundus camera and SLO are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Due to the wide field of view (FOV) of SLO images, structures such as eyelashes and eyelids are also imaged along with the retina. If these structures are removed, this will not only facilitate the effective analysis of retinal area, but also enable to register multiview images into a montage, resulting in a completely visible retina for disease diagnosis.</p><p>In this study, we have constructed a novel framework for the extraction of retinal area in SLO images. The three main steps for constructing our framework include:</p><p>1) determination of features that can be used to distinguish between the retinal area and the artefacts; 2) selection of features which are most relevant to the classification; 3) construction of the classifier which can classify out the retinal area from SLO images. For differentiating between the retinal area and the artefacts, we have determined different image-based features which reflect grayscale, textural, and structural information at multiple resolutions. Then, we have selected the features among the large feature set, which are relevant to the classification. The feature selection process improves the classifier performance in terms of computational time. Finally, we have constructed the classifier for discriminating between the retinal area and the artefacts. Our prototype has achieved average classification accuracy of 92% on the dataset having healthy as well as diseased retinal images.</p><p>The rest of this paper is organised as follows. Section II introduces the previous work for feature determination and classification. Section III discusses our proposed method. Section IV provides the quantitative and visual results of our proposed method. Section V summarizes and concludes the method with future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE SURVEY</head><p>Our literature survey is initiated with the methods for detection and segmentation of eyelids and eyelashes applied on images of the front of the eye, which contains the pupil, eyelids, and eyelashes. On such an image, the eyelashes are usually in the form of lines or bunch of lines grouped together. Therefore, the first step of detecting them was the application of edge detection techniques such as Sobel, Prewitt, Canny, Hough Transform <ref type="bibr" target="#b2">[3]</ref>, and Wavelet transform <ref type="bibr" target="#b3">[4]</ref>. The eyelashes on the iris were then removed by applying nonlinear filtering on the suspected eyelash areas <ref type="bibr" target="#b4">[5]</ref>. Since eyelashes can be in either separable form or in the form of multiple eyelashes grouped together, Gaussian filter and Variance filter were applied in order to distinguish among both forms of eyelashes <ref type="bibr" target="#b5">[6]</ref>. The experiment showed that separable forms of eyelashes were most likely detected by applying Gaussian filter, whereas Variance filters are more preferable for multiple eyelash segmentation <ref type="bibr" target="#b6">[7]</ref>. Initially, the eyelash candidates were localized using active shape modeling, and then, eight-directional filter bank was applied on the possible eyelash candidates. Kang and Park <ref type="bibr" target="#b7">[8]</ref> used focus score in order to vary the size of convolution kernels for eyelash detection. The size variation of the convolution kernels also differentiated between separable eyelashes and multiple eyelashes. Min and Park <ref type="bibr" target="#b8">[9]</ref> determined the features based on intensity and local standard variation in order to determine eyelashes. They were thresholded using Otsu's method, which is an automatic threshold selection method based on particular assumptions about intensity distribution. All of these methods have been applied on CASIA database [10], which is an online database of Iris images. In an image obtained from SLO, the eyelashes show as either dark or bright region compared to retinal area depending upon how laser beam is focused as it passes the eyelashes. The eyelids show as reflectance region with greater reflectance response compared to retinal area. Therefore, we need to find out features, which can differentiate among true retinal area and the artefacts in SLO retinal scans. After visual observation in Fig. <ref type="figure" target="#fig_0">1</ref>(b), the features reflecting the textural and structural difference could have been the suggested choice. These features have been calculated for different regions in fundus images, mostly for quality analysis.</p><p>The characterisation of retinal images were performed in terms of image features such as intensity, skewness, textural analysis, histogram analysis, sharpness, etc., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[11]</ref>, <ref type="bibr" target="#b10">[12]</ref>. Dias et al. <ref type="bibr" target="#b11">[13]</ref> determined four different classifiers using four types of features. They were analyzed for the retinal area including colour, focus, contrast, and illumination. The output of these classifiers were concatenated for quality classification. For classification, the classifiers such as partial least square (PLS) <ref type="bibr" target="#b12">[14]</ref> and support vector machines (SVMs) <ref type="bibr" target="#b13">[15]</ref> were used. PLS selects the most relevant features required for classification. Apart from calculating image features for whole image, grid analysis containing small patches of the image has also been proposed for reducing computational complexity <ref type="bibr" target="#b9">[11]</ref>. For determining image quality, the features of region of interest of anatomical structures such as optic nerve head (ONH) and Fovea have also been analyzed <ref type="bibr" target="#b14">[16]</ref>. The features included structural similarity index, area, and visual descriptor etc. Some of the above mentioned techniques suggest the use of grid analysis, which can be an time effective method to generate features of particular region rather than each pixel. But grid analysis might not be an accurate way to represent irregular regions in the image. Therefore, we decided the use of superpixels <ref type="bibr" target="#b15">[17]</ref>- <ref type="bibr" target="#b18">[20]</ref>, which group pixels into different regions depending upon their regional size and compactness.</p><p>Our methodology is based on analyzing the SLO image-based features, which are calculated for a small region in the retinal image called superpixels. The determination of feature vector for each superpixel is computationally efficient as compared to feature vector determination for each pixel. The superpixels from the images in the training set are assigned the class of either retinal area or artefacts depending upon the majority of pixels in the superpixel belonging to the particular class. The classification is performed after ranking and selection of features in terms of effectiveness in classification. The details of the methods are discussed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The block diagram of the retina detector framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The framework has been divided into three stages, namely training stage, testing and evaluation stage, and deployment stage. The training stage is concerned with building of classification model based on training images and the annotations reflecting the boundary around retinal area. In the testing and evaluation stages, the automatic annotations are performed on the "test set" of images and the classifier performance is evaluated against the manual annotations for the determination of accuracy. Finally, the deployment stage performs the automatic extraction of retinal area.</p><p>The subtasks for training, testing, and deployment stages are briefly described as follows:</p><p>1) Image Data Integration: It involves the integration of image data with their manual annotations around true retinal area. 2) Image Preprocessing: Images are then preprocessed in order to bring the intensity values of each image into a particular range. 3) Generation of Superpixels: The training images after preprocessing are represented by small regions called superpixels. The generation of the feature vector for each superpixel makes the process computationally efficient as compared to feature vector generation for each pixel. 4) Feature Generation: We generate image-based features which are used to distinguish between the retinal area and the artefacts. The image-based features reflect textural, grayscale, or regional information and they were calculated for each superpixel of the image present in the training set. In testing stage, only those features will be generated which are selected by feature selection process. 5) Feature Selection: Due to a large number of features, the feature array needs to be reduced before classifier construction. This involves features selection of the most significant features for classification. 6) Classifier Construction: In conjunction with manual annotations, the selected features are then used to construct the binary classifier. The result of such a classifier is the superpixel representing either the "true retinal area" or the "artefacts." 7) Image Postprocessing: Image postprocessing is performed by morphological filtering so as to determine the retinal area boundary using superpixels classified by the classification model. The elements of our detection framework are discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Preprocessing</head><p>Images were normalized by applying a Gamma (?) adjustment to bring the mean image intensity to a target value. ? was calculated using ? = log 10 (? target )log 10 (255) log 10 (? orig )log 10 (255)</p><p>where ? orig is the mean intensity of the original image and ? target is the mean intensity of the target image. For image visualization, ? target is set to 80. Finally, the Gamma adjustment of the image is given as</p><formula xml:id="formula_1">I norm = I 255 ? .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generation of Superpixels</head><p>The superpixel algorithm groups pixels into different regions, which can be used to calculate image features while reducing the complexity of subsequent image processing tasks. Superpixels capture image redundancy and provide a convenient primitive image pattern. As far as fundus retinal images are concerned, the superpixels have been generated for analyzing anatomical structures <ref type="bibr" target="#b19">[21]</ref> and retinal hemorrhage detection <ref type="bibr" target="#b20">[22]</ref>. For retinal hemorrhage detection, the superpixels were generated using watershed approach but the number of superpixels generated in our case need to be controlled. The watershed approach sometimes generates number of superpixels of the artefacts more than desired. The superpixel generation method used in our retina detector framework is simple linear iterative clustering <ref type="bibr" target="#b15">[17]</ref>, which was shown to be efficient not only in terms of computational time, but also in terms of region compactness and adherence. The algorithm is initialized by defining number of superpixels to be generated. The value was set to 5000 as a compromise between computational stability and prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Generation</head><p>After the generation of superpixels, the next step is to determine their features. We intend to differentiate between the retinal area and artefacts using textural, grayscale gradient, and regional based features. Textural and gradient based features are calculated from red and green channels on different Gaussian blurring scales, also known as smoothing scales <ref type="bibr" target="#b21">[23]</ref>. In SLO images, the blue channel is set to zero; therefore, no feature was calculated for the blue channel. The regional features are determined for the image irrespective of the colour channel. The details of these features are described as follows 1) Textural Features: Texture can be analyzed using Haralick features <ref type="bibr" target="#b22">[24]</ref> by gray level co-occurrence matrix (GLCM) analysis. GLCM determines how often a pixel of a gray scale value i occurs adjacent to a pixel of the value j. Four angles for observing the pixel adjacency, i.e., ? = 0 ? , 45 ? , 90 ? , 135 ? are used. These directions are shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. GLCM also needs an offset value D, which defines pixel adjacency by certain distance. In our case, offset value is set to 1. Fig. <ref type="figure" target="#fig_2">3(b</ref>) illustrates the process of creating GLCM using the image I. The features, which are calculated using GLCM matrix are summarized in Table <ref type="table">I</ref>. The mean value in each direction was taken for each Haralick feature and they were calculated from both red and green channels.</p><p>2) Gradient Features: The reason for including gradient features was illumination nonuniformity of the artefacts. In order to calculate these features, the response from Gaussian filter bank <ref type="bibr" target="#b21">[23]</ref> is calculated. The Gaussian filter bank includes Gaussian N (?), its two first-order derivatives N x (?) and N y (?) and three second-order derivatives N xx (?), N xy (?), and N y y (?) in horizontal (x) and vertical (y) directions. After convolving the image with the filter bank at a particular channel, the mean value is taken over of each filter response over all pixels of each superpixel.</p><p>3) Regional Features: The features used to define regional attributes were included because superpixels belonging artefacts have irregular shape compared to those belonging the retinal area in an SLO image. Table <ref type="table">II</ref> represents the features describing regional attributes.</p><p>The image features are calculated for each superpixel of the images present in the training set and they form a matrix of the form as</p><formula xml:id="formula_2">F M = A tex R A tex G A g R A g G A re B tex R B tex G B g R B g G B re<label>(3)</label></formula><p>where A and B represent class of true retinal area and class of artefacts, superscripts tex, re, g represent textural features, regional features, and gradient based features, respectively, and subscript R and G represent the red and green channel, respectively. For determining features at different smoothing scales, both red and green channels of images are convolved with the Gaussian <ref type="bibr" target="#b21">[23]</ref> at scales ? = 1, 2, 4, 8, 16. The textural features are calculated at the original scale, as well as at five different smoothing scales so as to accommodate their image response in the training set after blurring. In this way, the total number of columns in both channels of A tex and B tex will be 114 making it 228 altogether. The gradient features has six columns in each scale making 30 columns for each channel of A g and B g so 60 columns in total for each superpixel. As far as regional features are concerned, except I ? , they are independent of channel variation. Therefore, they are calculated only once for the superpixel so seven columns for A re and B re (I ? is calculated for both red and green channels). In this way, there are the total number of 295 features in the feature matrix for each superpixel of the image present in the training set. Each column of the feature matrix calculated for the particular image is normalized using z-score normalization <ref type="bibr" target="#b24">[26]</ref>. Z-score normalization returns the scores of the column with zero mean and unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Selection</head><p>The main purposes for feature selection are reducing execution time, determination of features most relevant to the classification and dimensionality reduction. For feature selection, we have selected sequential forward selection (SFS) approach.</p><p>In the "SFS approach," the interaction among features is taken into account. From the available set of features, the feature with the highest area under the curve (AUC) <ref type="bibr" target="#b25">[27]</ref> is selected. The next feature is chosen in such a way that when it is used along with the first selected feature, it will give the highest AUC compared to other nonselected features. The process is repeated until ten features were selected, since a higher number of features resulted in a very small improvement in AUC.</p><p>The performance of the SFS has been compared against other feature selection approaches such as "Filter approach" and "Filter and SFS" approach. In the filter approach, the features are ranked with respect to their effectiveness in classification and higher ranked features are thresholded out. In order to determine most relevant features, an independent evaluation criterion for binary classification is used <ref type="bibr" target="#b26">[28]</ref> and AUC is selected as its evaluation measure <ref type="bibr" target="#b25">[27]</ref>. The features with higher AUC are ranked higher, and the features with AUC greater than 0.9 are selected. In this way, 33 features are selected for classifier construction. The "Filter and SFS" approach is similar to SFS approach except that it is applied on the filtered feature set rather than complete feature set.</p><p>The individual and collective performance of the features selected in the feature sets from the above mentioned approaches are shown in Figs. <ref type="figure">4</ref> and<ref type="figure" target="#fig_4">5</ref>. The axis of "Feature Index" in Fig. <ref type="figure">4</ref> is ordered according to descending independent evaluation criterion. The axis of "Number of Selected Features" in Fig. <ref type="figure" target="#fig_4">5</ref> represents the order with which the features are selected using the SFS approach. We have not applied SFS on the "Filter approach;" therefore, axis of "Number of Selected Features" (i -H s u m )p x + y (i) Higher weights that differ from entropy value of marginal GLCM (i, j ) represent rows and columns, respectively; N g is the number of distinct gray levels in the quantized image; p(i, j ) is the element from normalized GLCM matrix; p x (i) and p y (j ) are the marginal probabilities of matrix obtained by summing rows and columns of GLCM, respectively, i.e., p x (i) =</p><formula xml:id="formula_3">N g j = 1 p(i, j ), p y (j ) = N g i = 1 p(i, j ), p x + y (k ) = N g i = 1 N g j = 1 p(i, j ), k = i + j -1 = 1, 2, 3, . . . , 2N g and p x -y (k ) = N g i = 1 N g j = 1</formula><p>p(i, j ), k = |ij | + 1 = 1, . . . , N g ; H x and H y are entropies of p x and p y , respectively, H x y = i j p x (i)p y (j )log(p x (i)p y (j )), and H x y 2 =i j p(i, j )log(p x (i)p y (j )).</p><p>for "Filter Approach" would be same as that of "Feature Index" in Fig. <ref type="figure">4</ref>. The features represented by "Feature Index" and "Number of Selected Features" are shown in Table <ref type="table">III</ref>. SFS is computationally intensive as it required 5 min/feature on filtered feature set and 30 min/feature on complete feature set. But the results show that the SFS approach performed better compared to other two approaches despite of the fact that the feature set also consists of those features which ranked low in independent evaluation criterion. The Table <ref type="table" target="#tab_2">IV</ref> represents the percentage of different types of features selected in each feature set. The table shows clear dominance of textural features compared to gradient features and regional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Classifier Construction</head><p>The classifier is constructed in order to determine the different classes in a test image. In our case, it is a two class problem: true retinal area and artefacts. We have applied Artificial Neural Networks (ANNs). The ANN is the classification algorithm that is inspired by human and animal brain. It is composed of many interconnected units called artificial neurons. ANN takes training samples as input and determines the model that best fits to the training samples using nonlinear regression. Consider the Fig. <ref type="figure">6</ref> which shows three basic blocks of ANN, i.e., input, hidden layer (used for recoding or providing representation for input), and output layer. More than one hidden layer can be used but in our case, there is only one hidden layer with ten neurons. The output of each layer is in the form of matrix of floating values, which can be obtained by sigmoid function as  Ratio of area to convex area Fig. <ref type="figure">4</ref>. Plot of independent evaluation criterion. The features are ranked in descending order of independent evaluation criterion value. In top figure, red dots for "Filter and SFS approach" represent the ten features selected by applying SFS on "Filter approach" set. By applying SFS on complete feature set, ten out of 295 features have been selected as shown in bottom figure ("SFS approach").</p><formula xml:id="formula_4">h W (x) = 1 1 + exp(-W T x + b) (4)</formula><p>where b is the bias value and W are the weights of input x. These weights can be determined by backpropagation algorithm, which tends to minimize mean square error value between desired output and actual output as </p><formula xml:id="formula_5">err = 1 2 (t -y) 2<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III FEATURE SETS OBTAINED USING DIFFERENT FEATURE SELECTION APPROACHES</head><p>Feature Selection Method Feature Symbols</p><p>Filter Approach (feature index and number of selected features)</p><formula xml:id="formula_6">? s u m R (16), ? s u m R (16), ? s u m R (8), N R (16), ? s u m R (4), ? s u m R (8), ? s u m R (2), ? s u m R (1), ? s u m R , ? s u m R (4), ? s u m R (2), N R (8), ? s u m R (1), acorr R (16), ? s o s R (16), ? s u m R , N R (4), N R (2), N R (1), N y y R (1), I ? R , N x x R (1), acorr R (8), ? s o s R (8), acorr R (4), ? s o s R (4), N y y R (2), acorr R (2), ? s o s R (2), acorr R (1), ? s o s R (1), acorr R , ? s o s R Filter and SFS Approach (feature index) ? s u m R (16), ? s u m R (16), ? s u m R (8), ? s u m R , ? s u m R (4), ? s u m R , acorr R (8), ? s o s R (8), acorr R (1), ? s o s R (1)</formula><p>Filter and SFS Approach (number of selected features)</p><formula xml:id="formula_7">? s u m R (16), ? s o s R (1), ? s u m R (8), ? s o s R (8), ? s u m R (16), ? s u m R , ? s u m R , acorr R (8), acorr R (1), ? s u m R (4) SFS Approach (feature index) ? s u m R (16), acorr R (8), ? s o s R (8), ? s u m G , acorr G , ? s o s G , H G (8), N y R (16), H G (1), H d iff G (1) SFS Approach (Number of Selected Features) ? s u m R (16), ? s o s G , H G (8), N y R (16), ? s o s R (8), H d iff G (1), acorr G , acorr R (8), ? s u m G , H G (1)</formula><p>"Feature index" represents the order of highest independent evaluation criterion measure, and "number of selected features" represent the sequence of feature selection in the feature set. R and G subscripts represent red and green channel, respectively. where t and y represent the target output and actual output of the output layer. The minimization of ( <ref type="formula" target="#formula_5">5</ref>) can be represented as</p><formula xml:id="formula_8">?err ?W i = (y -t)y(1 -y)x i . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>Since it is an iterative process, therefore weights are updated by delta rule as</p><formula xml:id="formula_10">?w i = ?(t -y)x i (7)</formula><p>? represents the step size. The weights were updated until 1000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Image Postprocessing</head><p>After classification of the test image, the superpixels are refined using morphological operation <ref type="bibr" target="#b2">[3]</ref>, so as to remove misclassified isolated superpixels. The morphological closing was performed so as to remove small gaps among superpixels. The size of disk structuring element can be a smaller value, say 10. For better results, we can perform area opening so as to remove one or two misclassified isolated superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison Study</head><p>After the construction of our classifier, we have compared its performance against different classifiers in terms of accuracy and computational time. The classifiers have been applied across different feature sets, which are obtained by using different feature selection procedures as mentioned in Section III-D. The classifiers we have selected for comparing the performance of our classifier are SVMs and k Nearest Neighbours (kNNs) <ref type="bibr" target="#b24">[26]</ref>.</p><p>The idea behind kNN method is to find out samples whose feature are similar to the classes to be detected. The function, which we are following in order to determine the similarity of the features with true retinal area is "Euclidean distance." SVM finds a separating hyperplane with the maximal margin in higher dimensional space. In our comparison study, we are using nonlinear SVM with radial-based function kernel with default parameter of (number of features) -1 = 0.1 <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>The images for training and testing have been obtained from Optos <ref type="bibr" target="#b1">[2]</ref> and are acquired using their ultrawide field SLO. Each image has a FOV of up to 200 ? of the retina in a resolution of 14 ?m. The device captures the retinal image without dilation, through a small pupil of 2 mm. The image has two channels: red and green. The green channel (wavelength: 532 nm) provides information about the sensory retina to retinal pigment epithelium, whereas the red channel (wavelengh: 633 nm) shows deeper structures of the retina toward the choroid. Each image has a dimension of 3900 ? 3072 and each pixel is represented by 8-bit on both red and green channels. The dataset is composed of healthy and diseased retinal images; most of the diseased retinal images are from Diabetic Retinopathy patients. The system has been trained with 28 images and tested against 76 images. Fig. <ref type="figure" target="#fig_5">7</ref> compares the classification power of different feature sets with the help of receiver operating characteristics (ROC). One of those feature sets include all features calculated. The rest of other feature sets include features selected by the approaches discussed in Section III-D. By using SFS approach, ten features out of 295 features have been selected and their calculation time is 25 s per image, whereas calculating the complete feature set can take around 10 min per image. The ROC curves and AUC values reveal that if the features are selected using the SFS approach, they can have a classification power almost similar to the complete feature set while reducing the computational time.</p><p>The visual results and the accuracies of different classifiers among different feature sets has been presented using Dice Coefficient as evaluation metric. The Dice Coefficient is the degree of overlap between the framework output and the benchmark obtained from the clinician. The Dice Coefficient is defined as</p><formula xml:id="formula_11">D(A, B) = 2|A ? B| |A| + |B| (8)</formula><p>where A and B are the segmented images obtained from the framework and the benchmark, respectively, |.| represents number of samples of the region, and ? denotes the intersection. Its value varies between 0 and 1, where a higher value indicates an increased degree of overlap. Let RA 1 and AR 1 represent samples from the retinal area and the artefact area obtained from the framework, respectively, and RA 2 and AR 2 be these samples from the benchmark. The class of superpixels in the benchmark was decided based on majority of pixels in the superpixel belonging to particular class. Also, </p><formula xml:id="formula_12">|RA 1 | + |AR 1 | = |RA 2 | + |AR 2 | = N sample , i.e.,</formula><formula xml:id="formula_13">D I = (|RA 1 ? RA 2 | + |AR 1 ? AR 2 |) N sample . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>The Dice Coefficient for the retinal area D R and artefacts D A will be given as   Table V compares the performance of different classifiers across different feature sets. As far as classification accuracy is concerned, there is a little difference among the outputs of different classifiers. The advantage of using ANN is its highcomputational efficiency in terms of testing time as shown in Table <ref type="table" target="#tab_4">VI</ref>. Although the training time of ANN is longer compared to its other two counterparts, the training time is once in a lifetime process and once the model is deployed, it can process any image. Fig. <ref type="figure" target="#fig_7">9</ref> represents the total time taken by an image to be processed for automatic annotations. The block diagram and the Table VI shows that while using ANN, couple of seconds can be saved per image during automatic-annotation process. As shown in Table <ref type="table" target="#tab_4">V</ref>, SVM although performed better on SFS feature set compared to ANN and kNN, ANN has the highest classification accuracy in other two feature sets. This shows that classification accuracy is highly dependent on type of features selected.</p><formula xml:id="formula_15">D R = 2|RA 1 ? RA 2 | |RA 1 | + |RA 2 | , D A = 2|AR 1 ? AR 2 | |AR 1 | + |AR 2 | . (<label>10)</label></formula><p>Fig. <ref type="figure" target="#fig_6">8</ref> shows superpixel classification results and final output after postprocessing of different examples of healthy and diseased retinal images. ANN is able to achieve the average accuracy nearer to that of other two classifiers, while saving significant computational time when processing millions of images for automatic annotations.</p><p>V. DISCUSSION AND CONCLUSION Distinguishing true retinal area from artefacts in SLO images is a challenging task, which is also the first important step toward computer-aided disease diagnosis. In this study, we have proposed a novel framework for automatic detection of true retinal area in SLO images. We have used superpixels to represent different irregular regions in a compact way and reduce the computing cost. Feature selection enables the most significant features to be selected and, thus, reduces computing cost too. A classifier has been built based on selected features to extract out the retina area. It has been compared to other two classifiers and was compatible while saving the computational time. The experimental evaluation result shows that our proposed framework can achieve an accuracy of 92% in segmentation of the true retinal area from an SLO image.</p><p>Feature selection is necessary so as to reduce computational time during training and classification. Among different approaches used for feature selection, the performance of our feature selection approach surpassed the filter approach and "Filter and SFS" approaches in terms of classification power. The comparison of different feature selection approaches shows that selection of features based on their mutual interaction can provide the classification power close to that of feature set with all features. Feature selection is once in a life-time process and we can compromise on computational time for feature selection on account of accuracy.</p><p>As far as the classifier is concerned, the testing time of ANN was the lowest compared to other two classifiers. Although the overall accuracy of SVM was the highest compared to other two classifiers, the training and testing time is quite long. Although kNN has the shortest training time, the testing time can be quite high compared to ANN while processing millions of images. Compared to SVM, we can tradeoff the overall accuracy of 0.1% on average while saving the testing time of 8 s per image. As far as images with lesions are concerned [see Fig. <ref type="figure" target="#fig_6">8</ref>(a), (c), and (e)], ANN misclassified 1 or 2 superpixels at the corners, but they are corrected using morphological postprocessing as shown in Fig. <ref type="figure" target="#fig_6">8(e)</ref>.</p><p>Our retina detection framework serves as the first step toward the processing of ultrawidefield SLO images. A complete retinal scan is possible if the retina is imaged from different eye-steered angles using an ultrawidefield SLO and, then, montaging the resulting image. Montaging is possible only if the artefacts are removed before.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of (a) a fundus image and (b) an SLO image annotated with true retinal area and ONH.</figDesc><graphic url="image-1.png" coords="1,308.16,193.09,244.80,105.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of retina detector framework.</figDesc><graphic url="image-2.png" coords="3,45.15,66.07,244.80,209.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) GLCM directions and offset. (b) GLCM process using image I [25].</figDesc><graphic url="image-3.png" coords="4,102.45,66.52,384.96,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ratio of area to number of pixels in the bounding box Orientation ? s Superpixel angle with respect to x-axis Solidity Sol = N s N s c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Plot of AUC by selecting the features one by one in different feature set.</figDesc><graphic url="image-5.png" coords="6,303.95,214.49,244.80,189.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) ROC on the test sets. (b) Magnified version of (a).</figDesc><graphic url="image-7.png" coords="8,54.95,66.64,480.00,200.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Superpixel classification result of two examples of SLO images. Columns represent different examples of retinal images. Left column are retinal scans with lesions, whereas right column is the retinal scan from healthy subject. (a) and (b) represent the test images divided into superpixels. (c) and (d) represent superpixel classification results and (e) and (f) represent output after postprocessing.</figDesc><graphic url="image-8.png" coords="9,89.15,66.75,420.00,543.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Block diagram of deployment stage along with execution time of each block.</figDesc><graphic url="image-9.png" coords="10,40.94,66.72,244.80,92.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV PERCENTAGE</head><label>IV</label><figDesc>OF DIFFERENT TYPES OF FEATURES ACROSS DIFFERENT FEATURE SET</figDesc><table><row><cell>Feature Set</cell><cell>Textural Features</cell><cell>Gradient Features</cell><cell>Regional Features</cell></row><row><cell>SFS Approach</cell><cell>90%</cell><cell>10%</cell><cell>0%</cell></row><row><cell>Filter Approach</cell><cell>72.73%</cell><cell>24.24%</cell><cell>3.03%</cell></row><row><cell>Filter and SFS</cell><cell>100%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>Approach</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Fig. 6. AANs diagram.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="5">AVERAGE CLASSIFICATION ACCURACY</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classifier</cell><cell></cell><cell>Filter Approach</cell><cell></cell><cell cols="3">Filter/SFS Approach</cell><cell></cell><cell>SFS Approach</cell><cell></cell></row><row><cell></cell><cell>D I</cell><cell>D R</cell><cell>D A</cell><cell>D I</cell><cell>D R</cell><cell>D A</cell><cell>D I</cell><cell>D R</cell><cell>D A</cell></row><row><cell>ANN</cell><cell>89.36%</cell><cell>89.49%</cell><cell>89.22%</cell><cell>88.88%</cell><cell>89.00%</cell><cell>88.75%</cell><cell>90.48%</cell><cell>90.28%</cell><cell>90.68%</cell></row><row><cell>SVM</cell><cell>88.48%</cell><cell>88.48%</cell><cell>88.47%</cell><cell>88.41%</cell><cell>88.36%</cell><cell>88.46%</cell><cell>90.93%</cell><cell>90.89%</cell><cell>90.96%</cell></row><row><cell>k NN</cell><cell>88.35%</cell><cell>88.53%</cell><cell>88.17%</cell><cell>88.09%</cell><cell>88.24%</cell><cell>87.94%</cell><cell>90.34%</cell><cell>90.17%</cell><cell>90.52%</cell></row><row><cell cols="6">Degree of overlap has been calculated by taking superpixels as samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would also like to thank the anonymous reviewers, who provided detailed and constructive comments on an earlier version of this paper.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported by <rs type="funder">EPSRC-DHPA</rs> funded Project "<rs type="projectName">Automatic Detection of Features in Retinal Imaging to Improve Diagnosis of Eye Diseases</rs>" and Optos plc.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Rs3tpE4">
					<orgName type="project" subtype="full">Automatic Detection of Features in Retinal Imaging to Improve Diagnosis of Eye Diseases</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Liangxiu Han received the Ph.D. degree in computer science from Fudan University, Shanghai, China, in 2002.</p><p>She is currently a Reader at the school of Computing, Mathematics and Digital Technology, Manchester Metropolitan University, Manchester, U.K., where he leads the Future Network and Distributed Systems Research Group. Her research interests include the development of novel architectures for large-scale networked distributed systems (e.g., cloud/grid/service-oriented computing/internet), large-scale data mining (application domains include web mining, biomedical images, environmental sensor data, network traffic data, cyber security, etc.), and knowledge engineering. As a Principal or Coprincipal Investigator, her research were funded by research councils, industries, and charity bodies, in her research areas. She is a Member of Engineering and Physical Sciences Research Council Peer Review College and as an Expert for Horizon 2020 proposal evaluation. She is also a regular Reviewer for several prestigious journals and international conferences in the field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jano van</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic extraction of retinal features from colour retinal images for glaucoma diagnosis: A review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Haleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Hemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imag. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="581" to="596" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Optos</surname></persName>
		</author>
		<ptr target="www.optos.com" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Digital Image Processing</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eyelid and eyelash segmentation based on wavelet transform for iris recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Aligholizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Nadooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kangarloo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Congr. Image Signal Process</title>
		<meeting>4th Int. Congr. Image Signal ess</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1231" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eyelash removal method for human iris recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Monro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rakshit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="285" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iris recognition system with accurate eyelash segmentation and improved FAR, FRR using textural and topological features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Mire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Dhote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="975" to="8887" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating useful and distinguishing features around the eyelash region</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th IEEE Workshop Appl. Imag. Pattern Recog</title>
		<meeting>37th IEEE Workshop Appl. Imag. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A robust eyelash detection based on iris focus assessment</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1630" to="1639" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyelid and eyelash detection method in the normalized iris image using the parabolic Hough model and Otsus thresholding method</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<ptr target="http://www.cbsr.ia.ac.cn/IrisDatabase.htm" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1138" to="1143" />
			<date type="published" when="2005">2009. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision-based, real-time retinal image quality assessment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soliz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd IEEE Int. Symp</title>
		<meeting>22nd IEEE Int. Symp</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated image quality evaluation of retinal fundus photographs in diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Agurto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Nemeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soliz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zamora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Southwest Symp. Image Anal</title>
		<meeting>IEEE Southwest Symp. Image Anal</meeting>
		<imprint>
			<publisher>Interpretation</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retinal image quality assessment using generic image quality indicators</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A M P</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A D S</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Partial least squares for discrimination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rayens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chemom</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="166" to="173" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated quality assessment of retinal fundus photos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assisted Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="557" to="564" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retinal image quality analysis for automatic diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th SIBGRAPI Conf. Graph., Patterns Images</title>
		<meeting>25th SIBGRAPI Conf. Graph., Patterns Images</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Superpixel lattices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Superpixels and supervoxels in an energy optimization framework</title>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mehrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Eur. Conf. Comput. Vis</title>
		<meeting>11th Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Watersheds in digital spaces: An efficient algorithm based on immersion simulations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="598" />
			<date type="published" when="1991-06">Jun. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superpixel classification based optic disc and optic cup segmentation for glaucoma screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1019" to="1032" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Splat feature classification with application to retinal hemorrhage detection in fundus images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="375" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated segmentation of the optic disc from stereo color photographs using physiologically plausible features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abr?moff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Greenlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fingert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1665" to="1673" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973-11">Nov. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Computer and Robot Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature selection using ROC curves on classification problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Magdalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Feature Selection for Knowledge Discovery and Data Mining</title>
		<editor>H. Liu and H. Motoda</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A practical guide to support vector classification</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Taiwan Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The title of his Ph.D. thesis is Automatic Detection of Features to Assist Diagnosis of Retinal Diseases. For this project, he received the prestigious Dorothy Hodgkin Postgraduate Award from the Engineering and Physical Sciences Research Council. His areas of research interests include computer vision, image processing, machine learning, and data mining</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Salman Haleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">B</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He is currently working toward the Ph.D. degree as a Research Student at the School of Computing, Mathematics and Digital Technology</title>
		<meeting><address><addrLine>Karachi, Pakistan; Chicago, IL, USA; Manchester, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>NED University of Engineering and Technology ; Manchester Metropolitan University</orgName>
		</respStmt>
	</monogr>
	<note>2008, and the M.S. degree in electrical engineering from the Illinois Institute of Technology</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
