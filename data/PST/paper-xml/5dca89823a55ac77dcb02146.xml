<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphDefense: Towards Robust Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-11">11 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyun</forename><surname>Wang</surname></persName>
							<email>xiywang@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
							<email>xqliu@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<email>chohsieh@cs.ucla.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphDefense: Towards Robust Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-11">11 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.04429v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the robustness of graph convolutional networks (GCNs). Despite the good performance of GCNs on graph semi-supervised learning tasks, previous works have shown that the original GCNs are very unstable to adversarial perturbations. In particular, we can observe a severe performance degradation by slightly changing the graph adjacency matrix or the features of a few nodes, making it unsuitable for security-critical applications. Inspired by the previous works on adversarial defense for deep neural networks, and especially adversarial training algorithm, we propose a method called GraphDefense to defend against the adversarial perturbations. In addition, for our defense method, we could still maintain semi-supervised learning settings, without a large label rate. We also show that adversarial training in features is equivalent to adversarial training for edges with a small perturbation. Our experiments show that the proposed defense methods successfully increase the robustness of Graph Convolutional Networks. Furthermore, we show that with careful design, our proposed algorithm can scale to large graphs, such as Reddit dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The GCN model takes in both feature matrix X and the adjacency matrix A, the original model consists of two fully connected layers parameterized by W (1) and W (2) , together with a final softmax layer to do the per-node classification. In specific, we can formulate the whole model as Z = softmax Âσ( ÂXW (1) )W (2)   = softmax f (X, A) ,</p><p>where A is the original adjacency matrix and Â = D−1/2 Ã D−1/2 , is the normalized adjacency matrix. Ã = A + I is the original graph plus "self-connection" and D = diag{ Ã1} is the degree matrix of each node. Although it looks tempting to try augmenting with more layers so that the information can be diffused to further nodes in deeper layers, experimental results in <ref type="bibr" target="#b9">[10]</ref> shows that a two layer network is the most effective setting. One limitation of the original GCN is that it directly aggregates the feature vector of a certain node with its neighboring nodes, also the optimization algorithm requires to do full batch gradient descent, this is very inefficient when the training dataset is very large.</p><p>To deal with this problem, neighbourhood sampling method came out <ref type="bibr" target="#b6">[7]</ref>. GraphSAGE samples a fixed size of neighbours for each nodes, and aggregates sampled neighbourhood features then concatenates with it own feature. After that, they use mini-batch during training. In this way, the memory bottleneck caused by randomness access is solved, thus working with large scale datasets and fast training become possible. The aggregation process for each node v would be written as</p><formula xml:id="formula_1">h k N (v) ← AGGREGAT E k ({h k−1 u , ∀u ∈ N (v)})<label>(2)</label></formula><p>Where N (v) is the sampled fixed number of neighbour of node v, k is depth, and h is the features vector or aggregate features vectors of nodes. AGGREGAT E is an aggregate function, we will use mean aggregator with GCN setting during our experiments:</p><formula xml:id="formula_2">M EAN ({h k−1 v } {h k−1 u , ∀u ∈ N (v)}).<label>(3)</label></formula><p>Despite that GCN and its variants are suitable to deal with graph data, recently people found that they are also prone to adversarial perturbations. It is worth noting that such perturbations are unlike random noises, instead, they are usually created dedicatedly by maximizing the loss metric. By convention, we call the people who create such adversarial perturbations as "attacker" and the side who apply the model to testset as "user". For example, suppose the user is doing pernode classification, then it would be reasonable for attacker to maximize the negative cross-entropy loss over a testing example. The overall idea of finding adversarial perturbation can be described as a constraint optimization problem as follows δ = arg max</p><formula xml:id="formula_3">δ∈S J f (x + δ), y ,<label>(4)</label></formula><p>where J(ŷ, y) is the loss function and f (x) is our model. S is the constraint depending of the goals of attack, two common choices are {δ| δ 2 ≤ r} and {δ| δ ∞ ≤ r}, both of them aim at creating an invisible perturbation if r is small enough.</p><p>For deep neural networks on image recognition task, there are several ways to solve Eq. ( <ref type="formula" target="#formula_3">4</ref>) efficiently. The simplest one is called fast gradient sign method (FGSM) <ref type="bibr" target="#b5">[6]</ref>, where we do one step gradient descent starting from origin, that is, δ = η • sign ∇ δ J f (x + δ), y , here we need to choose a step size η properly such that δ ∈ S. It is shown that although simple, this method is quite effective for finding an adversarial perturbation for images. Moreover, it is straightforward to improve FGSM method by running it iteratively, and that is essentially projected gradient descent (PGD) attack <ref type="bibr" target="#b14">[15]</ref>.</p><p>As to adversarial defense methods, we can roughly divide them into two groups: first method is to inject noises to each layer during both training and testing time, and hope that the additive noise can "cancel out" the adversarial pattern, examples include random-self ensemble <ref type="bibr" target="#b13">[14]</ref>; Second method is to augment the training set with adversarial data, this is also called adversarial training <ref type="bibr" target="#b14">[15]</ref>. Generally, for adversarial defense in image domain, adversarial training (the latter one) is slightly better than noise injection (the former one). However, in terms of adversarial training on graph data, there are several challenges that impede us from directly applying it to graph domain:</p><p>• Low label rates for semi-supervised learning setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attacks and Defense on CNNs</head><p>Adversarial examples of computer vision have been studied extensively. <ref type="bibr" target="#b5">[6]</ref> discovered that deep neural networks are vulnerable to adversarial attacks-a carefully designed small perturbation can easily fool a neural network. Several algorithms have been proposed to generate adversarial examples for image classification tasks, including FGSM <ref type="bibr" target="#b5">[6]</ref>, IFGSM <ref type="bibr" target="#b10">[11]</ref>, C&amp;W attack <ref type="bibr" target="#b0">[1]</ref> and PGD attack <ref type="bibr" target="#b14">[15]</ref>. In the black-box setting, it has also been reported that an attack algorithm can have high success rate using finite difference techniques <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and several algorithms are recently proposed to reduce query numbers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Adversarial training is a popular way for improving robustness. It's based on the idea of including adversarial examples in the training phase to make neural networks robust against those examples. For instance, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref> generate and append adversarial examples found by attack algorithms to training dataset. Other methods modifying structures of neural networks such as modifying ReLU activation layers and adding noises into images to original training dataset <ref type="bibr" target="#b23">[24]</ref>, modifying softmax layers and then use prediction probability to train "student" networks <ref type="bibr" target="#b16">[17]</ref>. Adding noises to images and using random self-ensemble helps with defensing white box attacks <ref type="bibr" target="#b13">[14]</ref>. Dropping or adding edges to graphs could be viewed as mapping adding noises methods for images to graphs.</p><p>Most of the above-mentioned works are focusing on problems with continuous input space (such as images), directly applying these methods to Graph Convolutional Networks can only improves robustness marginally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nodes classifications tasks with GCNs</head><p>GCNs widely are used for node classification tasks, the original one is introduced in <ref type="bibr" target="#b9">[10]</ref>, after that tons of works came out. From the large scale training aspect, sampling from just a few neighbourhoods is a standard way to scale the algorithm to big datasets. Different sampling methods are introduced with different papers, such as uniform sampling <ref type="bibr" target="#b6">[7]</ref>, importance sampling <ref type="bibr" target="#b1">[2]</ref>, sampling from random walk through neighbours <ref type="bibr" target="#b22">[23]</ref>.</p><p>Different structure of GCNs also have been explored, for example more layers <ref type="bibr" target="#b12">[13]</ref>, change ReLU to Leaky ReLU <ref type="bibr" target="#b18">[19]</ref>. And variety of aggregate functions have also been apply to GCNs, such as max pooling, LSTM, and other different pooling methods <ref type="bibr" target="#b6">[7]</ref> [22] <ref type="bibr" target="#b15">[16]</ref>. Original GCN could be used as a kind of mean aggregator inside of GraphSAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attacks and Defense on GCNs</head><p>The wide applicability of GCNs motivates recent studies about their robustness. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b24">[25]</ref> recently proposed algorithms to attack GCNs by changing existing nodes' links and features. <ref type="bibr" target="#b24">[25]</ref> developed an FGSM-based method that optimizes a surrogate model to choose the edges and features that should be manipulated. <ref type="bibr" target="#b4">[5]</ref> proposed several attacking approaches including, gradient ascent, Genetic algorithm and reinforcement learning; <ref type="bibr" target="#b4">[5]</ref> also showed experiments of using drop edges and adversarial training for defensing, and claimed that dropping edges is a cheap way for increasing robustness. <ref type="bibr" target="#b20">[21]</ref> learned graphs from a continuous function for attacking, also claimed that deeper GCNs have better robustness. Recently more defense methods come out, besides adversarial training, <ref type="bibr" target="#b19">[20]</ref> used graph encoder refining and adversarial contrasting learning, this paper explores robustness on both original GCN and GraphSAGE for small datasets, large graphs' robustness has not been discussed yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEFENSE FRAMEWORK</head><p>In this paper, we propose a framework for adversarial training for graphs to increase the robustness of GCNs. We will first introduce the adversarial training framework for GCN, and then discuss how to scale it up to large graphs and the connection between feature perturbation and graph perturbation in GCN adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework</head><p>Unlike previous defense work for CNN, GCN has some unique characteristics that will cause difficulties for improving robustness of GCNs.</p><p>• Low labeling rate: For most cases, GCN is used for classification nodes in graphs with semi-supervised setting, with lower labelling rate than supervised learning. It will lead to a problem if we directly apply adversarial training on it. For example, when attacking a GCN, the perturbations of edges and features will be limited almost limited to training datasets and their neighbours. Directed attacks are more powerful than indirected ones <ref type="bibr" target="#b24">[25]</ref>. Thus during adversarial training only a few nodes get adversarial examples. For example a node in testing dataset may at least be the 4-hop neighbour of the training nodes. While GCNs are usually 2 layers or 3 layers, thus transfer adversarial training information will be impossible for that nodes, so adversarial training fail to work in this case.</p><p>• Less of transferability for adversarial training : Consider depth K GCN, the adjacency matrix A is multiplied K times, and each node could get information for k-hop neighbour, but as the result of matrix multiplications, the further nodes (more-hop nodes) have less influence. Thus after adversarial training, if a testing nodes that are far from all adversarial examples, it will be more vulnerable than the nodes in the training test or close the them.</p><p>For most cases, GCN is used for classification nodes in graphs with semi-supervised setting, with lower labelling rate than supervised learning. It will lead to a problem if we directly apply adversarial training on it. For example, when attacking a GCN, the perturbations of edges and features will be limited almost limited to training datasets and their neighbours. Directed attacks are more powerful than indirected ones <ref type="bibr" target="#b24">[25]</ref>. Thus during adversarial training only a few nodes get adversarial examples. For example a node in testing dataset may at least be the 4-hop neighbour of the training nodes. While GCNs are usually 2 layers or 3 layers, thus transfer adversarial training information will be impossible for that nodes, so adversarial training fail to work in this case.</p><p>Proposed algorithm. It has been reported in <ref type="bibr" target="#b4">[5]</ref> that directly applying existing methods can only marginally improve the robustness of GCN. Due to lack of connectivity between training set and tested nodes that are being attacked, i.e. (they are in different connected components or they are not directly connected), the loss gradient of training set hard to be transmit to targets nodes. That is because when multiply adjacency matrix in GCNs, the further nodes will have small values and closer ones will have larger values (It is similar to Katz Similarity.), Thus targets nodes that is not in the same connected components will not get benefit from the adversarial training. And these far away from the adversarial training set only benefit marginally from the adversarial training. (definition for distance is the shortest pass from the targets node to any node in the adversarial training set.)</p><p>With small labeling weight for semi-supervised learning, lack of connectivity is very common. <ref type="bibr" target="#b12">[13]</ref> shows that using part of predicted labels as training labels could increase the accuracy for prediction when label rate is low. This gives us intuition to relief the less of transferability problem during adversarial training.</p><p>Thus, we introduce the proposed adversarial training objective function as: min W (1) ,W (2)   { max ||A −A||&lt; J(A σ(A XW (1) )W (2) , y)}, <ref type="bibr" target="#b4">(5)</ref> where A is the modified adjacency matrix. For efficiency, we do not constraint elements of A to be discrete. </p><formula xml:id="formula_4">= (u * , v * ) ← arg max ∇ A i∈N odes adv loss(ŷ i , z i ) Let e * drop = (u * , v * ) ← arg min ∇ A i∈N odes adv loss(ŷ i , z i ) if |∇ A [ i∈N odes adv loss(ŷ i , z i )] e * add | &gt; |∇ A [ i∈N odes adv loss(ŷ i , z i )] e * drop | : A ← A + e * add</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>else:</head><p>A ← A − e * drop retrain: A function J is defined as</p><formula xml:id="formula_5">J(A , X) = i∈L loss(y i , z i ) + α j∈U loss( ŷi , z i ) loss(y i , z i ) = max [f (X, A )] i,: − [f (X, A )] i,yi ,</formula><p>where L is labeled nodes set and U is unlabeled nodes set. The loss of labeled data and unlabeled data are combined with a weight α. Using fitted label for unlabeled data will resolve the connectivity problem. We use this method to give each nodes a label(the label maybe correct or incorrect), thus during the adversarial training, each node are able to be in the training set.</p><p>There are different ways of getting adversarial examples: (1) adversarial perturbation that constrained in the discrete space.</p><p>(2) the proposed GraphDefense perturbation in the continuous space.</p><p>Adversarial training and adversarial attacking are different situations for GCNs. During adversarial attacking the values of adjacency matrix and features are constraint on some certain space. For example if the adjacency matrix is normalized by row, them the sum of each after adversarial attacks on Algorithm 3 our algorithm: GraphDefense Input: Adjacency matrix A; feature matrix X; A classifier f with loss function loss; targeted nodes N odes adv . Output: adversarial example A Compute A = arg max i∈N odes adv loss(ŷ i , z i ) A ← βA + (1 − β)A T retrain: A adjacency matrix should always be 1; if the adjacency matrix is discrete, adversarial attacks are not able to add a continuous weight edge (say 1.23) into the graph. While for adversarial training, when generating adversarial example, there is not such a constraint, the values could be either discrete or continuous or even negative, which gives us a larger research space for adversarial examples.</p><p>To solve the inner max problem in <ref type="bibr" target="#b4">(5)</ref>, we use gradient descent on the adjacency matrix. The time complexity overhead compared to the original backpropagation is O(|V | 2 ) per update, where |V | is the number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scaling to Large Datasets</head><p>To scale up our attack and defense, we conduct experiments using GraphSage with GCN aggregator. The difficulties are:</p><p>• For large GCN training with SGD, all the efficient methods rely on sampled neighborhood expansion. Examples include GraphSage <ref type="bibr" target="#b6">[7]</ref>, FastGCN <ref type="bibr" target="#b1">[2]</ref> and many others. Unfortunately, currently is no attack developed for the sampled neighborhood expansion process and it will introduce difficulty in backpropagation in adversarial training.</p><p>• Due to the large number of edges, the existing greedy methods are very time consuming. For example <ref type="bibr" target="#b4">[5]</ref> [26] needs to check the gradient values for all the edges at each iteration, which requires O(|V | 2 ) time. • Due to the large number of nodes, adversarial edge changing examples in the adversarial training process, may not appear in the testing process. Thus the robustness will be affected.</p><p>In our implementation we consider the neighborhood expansion used in GraphSAGE with the GCN aggregator. The aggregator could be written as:</p><formula xml:id="formula_6">agg := A 1 σ(A 2 XW 1 )W 2 , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where σ is activation function; A 1 is sparse matrix containing neighborhood list : B 1 in Figure <ref type="bibr" target="#b6">[7]</ref>, A 2 is a sparse matrix containing neighbor's neighborhood list B 2 ; other matrices are dense; we note predicted labels ŷ = sof tmax(agg).</p><p>For large dataset adversarial training, we could still use the framework above, by only changing GCN function f to GraphSAGE aggregator agg and using mini-batch during training. The time complexity for each epoch is O(B * B * N 1 ), where N 1 is number of sampled 1-hop neighbours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial training in features</head><p>For large scale graph convolutional networks, neighborhood sampling is a common way to scale up to large graphs. The basic idea to aggregate features of 1-hop and 2-hop neighbour then doing nodes classification. This gives us an intuition for doing adversarial training faster and for large-scale graphs. We could generate adversarial features and using these features for adversarial training. We could prove that any small perturbation in discrete edge space are all included in features perturbations in continuous space. The time complexity for retraining features is in each batch O(|B * N f eatures |). Adversarial training on features will speedup adversarial process especially for large batch training, furthermore GCNs will also be more robust on edge perturbations. When considering modifying feature matrix X with δ perturbation, the formula of GCN in Eq 1 will be: (1) )W (2)   = softmax Âσ( ÂXW (1) + ÂδW (1) )W (2)  (7)</p><formula xml:id="formula_8">Z = softmax Âσ( Â(X + δ)W</formula><p>For perturbation on graph A, the formula of GCN in Eq 1 will be:</p><formula xml:id="formula_9">Z = softmax (A + )σ( (A + )XW (1) )W (2)<label>(8)</label></formula><p>Consider surrogate models without activation functions,</p><formula xml:id="formula_10">δ = Â−1 [ Â−1 ( Â + )( Â + )X − ÂX]<label>(9)</label></formula><p>IV. EXPERIMENTS</p><p>We use Cora, Citeseer, and Reddit attribute graphs as benchmarks. For Cora and Citeseer, we split the data into 15% for training, 35% for validation, and 50% for testing; For Reddit dataset, we use the same setting as GraphSAGE paper, which is 65 % for training 11 % for validation and 24 % for testing. Dataset descriptions could be find in Table <ref type="table" target="#tab_2">I</ref> We conduct experiments on both single node and a group of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Defense for GCN</head><p>We conduct experiments to test the robustness of GCNs with different retrain GCN : drop edges, naive adversarial training in Algorithm 2 and our method GraphDefense in Algorithm 3 with our framework in Algorithm 1. Drop edges training is a cheap way to increase the robustness of GCN; Retraining with adversarial samples also works for defense attacks <ref type="bibr" target="#b4">[5]</ref>. Our GraphDefense method gets the best results among these methods when defending adversarial defense in most cases.</p><p>Table <ref type="table" target="#tab_3">II</ref> shows defense a 100 nodes group defense using different methods, for Cora dataset, the number of changed edges is 100 for each group of 100 nodes; for Citeseer dataset, due to the graph density is lower than Cora dataset, we chose   When attacking singe nodes, for each targeted node, we modify 1 edge in the graph. Table <ref type="table" target="#tab_4">III</ref> shows the result for single node attacks by only dropping edges, adding edges or both. We notice that in both Table <ref type="table" target="#tab_3">II</ref> and Table <ref type="table" target="#tab_4">III</ref> dropping edges method is the least robustness expect for original GCN. That is because adding edges are more efficient when attacking GCN, thus although dropping edges is a very fast way, the improvement of robustness is not significant compared with other methods. We also notice that for the Cora dataset, during single node attacks, discrete adversarial training is better than GraphDefense, the reason might be discrete adversarial training is more suitable for single node attacks. We will discuss this in the latter part.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure">2</ref> shows more details of attacking with different amount of modified edges. With the number of modified edges increases, Our GraphDefense method remains more stable than discrete adversarial samples retrain and dropping edges. To investigate deeper in the reason why these methods perform differently, we use to study the different degrees of nodes accuracy corresponding to attacks.</p><p>Figure <ref type="figure">3</ref> shows the correctly predicted nodes and incor-Fig. <ref type="figure">2</ref>: Accuracy of GCN under different defense methods for Citeseer, with modifying 0 to 70 edges. Fig. <ref type="figure">6</ref>: accuracy increase for nodes' degree, compare with our defense method for Citeseer dataset; X axis: different degree of nodes; Y axis: accuracy improvement times rectly predicted nodes with the original GCN. It indicates that the lower degree nodes are more vulnerable. Figure <ref type="figure">4</ref> show accuracy ratio after attack with our GraphDefense method. The accuracy increases a lot for lower degree nodes. With degree follows power law distribution for most graph increasing lower degree nodes robustness is crucial for keep robustness of the GCNs. For Cora and Citeseer datasets, our graphDefense method works well for improving lower degree nodes robustness. Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref> shows the accuracy improvement when compared with original GCNs. In the most case, our method gives a lower degree of nodes a boost on robustness after attacks.</p><p>Next, we are discussing how accuracy increasing for different methods. We use the Citeseer dataset as an example. Figure <ref type="figure">7a</ref> 7b 7c show accuracy improvement for single node attacks, and Figure <ref type="figure">7d</ref> 7e 7f is for attacking groups of 100 nodes. Our method not only keeps the higher degree nodes accuracy but also boost the lower degree ones. When comparing attacking groups of nodes and attacking single node, we find there our GraphDefense method results stay inconstant for different kinds of attacks, and the accuracy for degree 2 nodes improved by 6X for attacking groups nodes compared with 3X for attacking single node. While for the other 2 methods, the accuracy drops in some larger degree nodes for attacking groups of 100 nodes. More Bar plots are listed in Figure <ref type="figure">9</ref>, which shows each case how the accuracy changes before and after attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Large scale and feature adversarial training</head><p>For large scale data, we use GraphSAGE compare with our GraphDefense method, discrete edges adversarial training, and adversarial training on features. We did not do a comparison with discrete adversarial training and dropping edges because previous experiments show they are far away behind our method. GraphSAGE is more difficult to attack, because there is a neighbourhood sampling function in these algorithms, directly adding or deleting edges on the original graphs method becomes less effective than on GCN. The reason is when training GraphSAGE (or other large scale graph neural networks), sampling neighbourhood could be view as dropping edges during training, <ref type="bibr" target="#b4">[5]</ref> shows that use dropping edges while training is a cheap method to increase the robustness of GCN. As a result, attacking GraphSAGE (or other large scale graph neural networks) is more difficult than attacking GCN. Also since attacking a single node by modifying only one edge is not a significant attack, in this part, we show attacking groups of 128 nodes instead.</p><p>Because the Reddit dataset is an inductive dataset, using our framework Algorithm 1 is important, otherwise, the adversarial training on the training dataset is very hard to transmit to the testing part through the edges, as the result, testing data will remain vulnerable. Figure <ref type="figure">8</ref> and Table <ref type="table" target="#tab_5">IV</ref> show attacking after different adversarial training methods. The result matches our claim in Section III-C. Adversarial training in features has a similar result as in edges when facing attacks on edges, also adversarial training in features is faster than adversarial training in edges. The performance of adversarial training in features might be related to the data type. For example, Reddit dataset features are continuous while Cora and Citeseer are discrete. Although the result for adversarial training in feature for Cora is not as good as our GraphDefense method, it is still quite better than others, Cora dataset could remain 51 % accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Sensitivity</head><p>In this section, we will discuss the weight between adversarial examples and clean data during the adversarial training process in Algorithm 1. Table <ref type="table" target="#tab_6">V</ref> shows that choosing an appropriate ratio between adversarial examples and clean (c) increasing accuracy rate for attacking single node by each degree for our method for attacking single node  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a new defense algorithm call GraphDefense to improve the robustness of Graph Convolutional Networks against adversarial attacks on graph structures. We further show that adversarial training on features is equivalent to adversarial training on graph structures, which could be used as a fast method of adversarial training without losing too much performance. Our experimental results that our defense method successfully defense white-box graph structure attacks for not only small datasets but also large scale datasets with GraphSAGE <ref type="bibr" target="#b6">[7]</ref> training. We also discuss what characteristics of defense methods are crucial to improve the robustness.  (h) attack a group of 100 nodes, Citeseer dataset, using GraphDefense to defense Fig. <ref type="figure">9</ref>: Attack Citseer dataset with different method. Green: correctly predicted nodes; Brown incorrectly predicted nodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Accuracy of GCN under different defense methods for Cora, with modifying 0 to 100 edges.</figDesc><graphic url="image-1.png" coords="5,48.96,215.13,236.47,153.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :Fig. 5 :</head><label>345</label><figDesc>Fig. 3: Classification performance, after attack for Original GCN, with different degrees of nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>accuracy improvement times for nodes' degree, compare with our defense method and original GCN for attacking single node</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 7: Accuracy improvement for different attacks using discrete adversarial training, drop edges training and our GraphDefense method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>attack single nodes, Citeseer dataset, clean model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Due to the semi-supervised learning nature of GCN, if the nodes been perturbed are in testing group, then adversarial training may not work: this is because propagating the gradients to the nodes been attacked may require to go through several nodes, but in plain GCN model, each node can only access its 2-hop neighbourhoods.</figDesc><table /><note>• Inductive learning is even more difficult, it remains unknown whether adversarial training on certain graph can successfully generalize to other graphs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The lossAlgorithm 1 Framework for adversarial trainingInput: Graph adjacency matrix A and features X, and classifier f and adversarial generating function adv, and loss function J Output: Robust classifier f . Predict nodes labels using GCN : ŷ ← f (A, X) for : t = 0 to T-1 do Randomly sample w group of nodes noted as N odes adv and N odes clean ;get adversarial examples of the group of a nodes, A ← adv(A, X, f, N odes adv ) retrain f with loss function J = i∈L loss(y i , z i ) + α j∈N odes adv N odes clean loss( ŷi , z i ); Note the retrained f as f retrain f</figDesc><table><row><cell>Algorithm 2 adversarial training using discrete adjacency</cell></row><row><cell>matrix</cell></row></table><note>Input: Adjacency matrix A; feature matrix X; A classifier f with loss function loss = crossEntropy; targeted nodes N odes adv Output: adversarial example A Let e * add</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Description for Cora Citeseer and Reddits</figDesc><table><row><cell>Datasets</cell><cell>Nodes</cell><cell>Edges</cell><cell>Features</cell><cell>Classes</cell><cell>Features Type</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>discrete</cell></row><row><cell>Citeseer</cell><cell>3,312</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>discrete</cell></row><row><cell>Reddits</cell><cell>232,965</cell><cell>11,606,919</cell><cell>41</cell><cell>602</cell><cell>continuous</cell></row><row><cell>nodes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Result of average accuracy with different defense methods on GCN for attacking a group of 100 nodes</figDesc><table><row><cell>Dataset</cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell></row><row><cell></cell><cell>before attack</cell><cell>after attack</cell><cell>before attack</cell><cell>after attack</cell></row><row><cell>Clean</cell><cell>0.8449</cell><cell>0.408</cell><cell>0.7434</cell><cell>0.396</cell></row><row><cell>drop edges</cell><cell>0.8338</cell><cell>0.474</cell><cell>0.7409</cell><cell>0.410</cell></row><row><cell>discrete adversarial training A</cell><cell>0.8301</cell><cell>0.492</cell><cell>0.7385</cell><cell>0.404</cell></row><row><cell>Our method</cell><cell>0.8486</cell><cell>0.692</cell><cell>0.7409</cell><cell>0.628</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Result of average accuracy with different defense methods on GCN for attacking singe nodes</figDesc><table><row><cell>Dataset</cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell></row><row><cell></cell><cell>before attack</cell><cell>after attack</cell><cell>before attack</cell><cell>after attack</cell></row><row><cell>Clean</cell><cell>0.8449</cell><cell>0.370</cell><cell>0.7434</cell><cell>0.440</cell></row><row><cell>drop edges</cell><cell>0.8338</cell><cell>0.374</cell><cell>0.7409</cell><cell>0.452</cell></row><row><cell>discrete adversarial training A</cell><cell>0.8301</cell><cell>0.554</cell><cell>0.7385</cell><cell>0.552</cell></row><row><cell>Our method</cell><cell>0.8486</cell><cell>0.540</cell><cell>0.7409</cell><cell>0.632</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Result of average accuracy with different defense methods on GraphSAGE</figDesc><table><row><cell></cell><cell>before attack</cell><cell>50 edges</cell><cell>100 edges</cell><cell>150 edges</cell><cell>200 edges</cell><cell>300 edges</cell></row><row><cell>Clean</cell><cell>0.9422</cell><cell>0.8</cell><cell>0.6953</cell><cell>0.5688</cell><cell>0.4781</cell><cell>0.3531</cell></row><row><cell>Feature retrain X</cell><cell>0.8641</cell><cell>0.8406</cell><cell>0.8281</cell><cell>0.7859</cell><cell>0.7422</cell><cell>0.6797</cell></row><row><cell>Our method on A</cell><cell>0.9188</cell><cell>0.8391</cell><cell>0.8016</cell><cell>0.7828</cell><cell>0.7625</cell><cell>0.6969</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Different number of Adversarial examples and clean examples during retrain process in Algorithm 1, Cora dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>examples during the adversarial training process is important.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Too large portions of adversarial examples will cause lower</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>accuracy, thus lead to bad performance after attacks.</cell></row><row><cell>Adversarial</cell><cell>Clean</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell></row><row><cell>100</cell><cell></cell><cell>0.634</cell><cell>0.692</cell><cell>0.616</cell><cell>0.622</cell></row><row><cell>200</cell><cell></cell><cell>0.522</cell><cell>0.54</cell><cell>0.555</cell><cell>0.55</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
				<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-22">2017. May 22-26, 2017. 2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno>CoRR, abs/1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS 2017</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS 2017<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-03">November 3, 2017. 2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Query-efficient hard-label black-box attack: An optimization-based approach</title>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ruitong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03034</idno>
		<title level="m">Learning with a Strong Adversary</title>
				<imprint>
			<date type="published" when="2015-11">Nov 2015</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Black-box Adversarial Attacks with Limited Queries and Information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Adversarial Machine Learning at Scale. ArXiv e-prints</title>
				<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<title level="m">Adversarial Machine Learning at Scale. arXiv e-prints</title>
				<imprint>
			<date type="published" when="2016-11">Nov 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07606</idno>
		<imprint>
			<date type="published" when="2018-01">Jan 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random self-ensemble</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno>CoRR, abs/1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Graph convolutional networks with argument-aware pooling for event detection</title>
		<author>
			<persName><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04508</idno>
		<title level="m">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. arXiv e-prints</title>
				<imprint>
			<date type="published" when="2015-11">Nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Query-limited black-box attacks to classifiers</title>
		<author>
			<persName><forename type="first">Fnu</forename><surname>Suya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<idno>CoRR, abs/1712.08713</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph Attention Networks. arXiv eprints</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03679</idno>
		<title level="m">Adversarial Defense Framework for Graph Neural Network</title>
				<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Adversarial Examples on Graph Data: Deep Insights into Attack and Defense</title>
				<imprint>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation Learning on Graphs with Jumping Knowledge Networks. arXiv e-prints</title>
				<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-19">2018. August 19-23. 2018. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Irina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06728</idno>
		<imprint>
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Efficient Defenses Against Adversarial Attacks. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial Attacks on Neural Networks for Graph Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08412</idno>
		<title level="m">Adversarial Attacks on Graph Neural Networks via Meta Learning</title>
				<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
