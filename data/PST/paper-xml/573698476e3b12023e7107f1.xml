<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Collaborative Filtering via Marginalized Denoising Auto-encoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
							<email>shengli@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University Boston</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
							<email>kawale@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research San Jose</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Northeastern University Boston</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Collaborative Filtering via Marginalized Denoising Auto-encoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2806416.2806527</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Data Mining</term>
					<term>H.3.3 [Information Search Retrieval]: Information Filtering Collaborative filtering</term>
					<term>matrix factorization</term>
					<term>deep learning</term>
					<term>denoising auto-encoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collaborative filtering (CF) has been widely employed within recommender systems to solve many real-world problems. Learning effective latent factors plays the most important role in collaborative filtering. Traditional CF methods based upon matrix factorization techniques learn the latent factors from the user-item ratings and suffer from the cold start problem as well as the sparsity problem. Some improved CF methods enrich the priors on the latent factors by incorporating side information as regularization. However, the learned latent factors may not be very effective due to the sparse nature of the ratings and the side information. To tackle this problem, we learn effective latent representations via deep learning. Deep learning models have emerged as very appealing in learning effective representations in many applications. In particular, we propose a general deep architecture for CF by integrating matrix factorization with deep feature learning. We provide a natural instantiations of our architecture by combining probabilistic matrix factorization with marginalized denoising stacked auto-encoders. The combined framework leads to a parsimonious fit over the latent features as indicated by its improved performance in comparison to prior state-of-art models over four large datasets for the tasks of movie/book recommendation and response prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recommendation is a fundamental problem that has gained utmost importance in the modern era of information overload. The goal of recommendation is to help users find the item that they maybe potentially interested in from a large repository of items. Recommender systems are widely used by websites (e.g., Amazon, Google News, Netflix, and Last.fm) in various contexts to target customers and provide them with useful information. A widely used setting of recommendation system is to predict how a user would rate an item (such as a movie) if only given the past rating history of the users. Many classical recommendation methods have been proposed during the last decade. The two broad categories of recommendation systems are content filtering approaches and collaborative filtering (CF) based methods. The CF based methods have attracted more attention due to their impressive performance <ref type="bibr" target="#b0">[1]</ref>. Among various CF methods, matrix factorization has emerged as a powerful tool to perform recommendations in large datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Learning effective latent factors plays the most important role in matrix factorization based CF methods. Traditional matrix factorization methods for CF directly learn the latent factors from the user-item rating matrix <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. One of the main challenges faced by these systems is to provide a rating when a new user/item arrives in the system, which is also known as the cold start scenario. The cold start problem is circular in nature as -the system will not recommend an item unless it has some ratings for it and unless the system recommends it will not get ratings for it. Another practical challenge is learning the appropriate latent factors when the rating matrix is sparse, which is often the case in many real world scenarios. In order to overcome these challenges, researchers have suggested to incorporate additional sources of information about the users or items, also known as the side information. This side information could be obtained from the user/item profiles, for example, demographics of a user, genre of a movie, etc. The user demographics could be used to infer the relationships between the users and similarly the item similarity can be used to automatically assign ratings to new items. The use of side information to aid matrix factorization has been successfully applied by various prior work, for example <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. These methods, however, only utilize the side information as regularizations in the model, and the learned latent factors may not be very effective due to the sparse nature of the ratings and the side information. In order to make matrix factorization based methods effective in such a setting, it is highly desirable to learn and extract discriminative features from the datasets.</p><p>One of the powerful approaches to capture feature interactions and complex relations that has emerged in the recent past is deep learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Deep learning has attracted a lot of attention because of its promising performance to learn representations on various tasks. Deep neural networks have been shown to achieve stateof-the-art results in computer vision, speech recognition and machine translation. The application of deep learning in recommen-dation systems, however, is very recent. With large-scale data and rich-side information available, it is now practicable to learn latent factors through deep architectures. Researchers have invested in modifying deep learning algorithms like Restricted Botzmann Machines or Convolutional Neural Networks or Deep Belief Networks directly for the task of collaborative filtering <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. However, there are no prior work that bridge together matrix factorization with deep learning methods with the notable exception of <ref type="bibr" target="#b14">[15]</ref>. In this paper, we present a deep learning model for collaborative filtering that tightly couples matrix factorization based collaborative filtering with deep learning algorithm namely marginalized denoising auto-encoders (mDA) <ref type="bibr" target="#b15">[16]</ref>. Unlike <ref type="bibr" target="#b14">[15]</ref> which integrates collaborative topic regression and bayesian stacked denoising auto-encoders and requires learning of a large number of hyper parameters using an EM style algorithm, our approach uses a much more efficient architecture based upon mDA and stochastic gradient descent and is thus computationally efficient and highly scalable. This paper makes the following contributions:</p><p>• We propose a general deep architecture named deep collaborative filtering (DCF), which integrates matrix factorization and deep feature learning. It models the mappings between the latent factors used in CF and the latent layers in deep models.</p><p>• We present a practical instantiation (i.e., mDA-CF and mSDA-CF) of the proposed architecture, by utilizing the probabilistic matrix factorization and mDA. The scalability and low computational cost of the mDA makes it a highly attractive deep learning tool, which is unlike the prior work <ref type="bibr" target="#b14">[15]</ref>.</p><p>• We evaluate the performance of our model on three realworld applications, movie recommendation, book recommendation and response prediction. Our model outperforms conventional CF methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In general, our work is closely related to the following topics: matrix factorization based collaborative filtering, and deep learning based collaborative filtering. We will discuss the two in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matrix Factorization for Collaborative Filtering</head><p>The importance of accurate recommendation techniques motivated by wide ranging applications has fuelled a great amount of academic as well as industrial research in this area <ref type="bibr" target="#b16">[17]</ref>. Recommender systems are most often based on collaborative filtering and there are typically two approaches that are widely used. In neighborhood methods, the similarity between users based on the content they have consumed and rated is the basis of a new recommendation. A related but intrinsically more powerful approach has been the use of latent factor models. Matrix factorization (MF) is the most popular technique to derive latent factor models and their success at the Netflix competition have highlighted their strength <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref>. For example, the given matrix X ∈ R N ×M consisting of the item preferences of the users can be decomposed as a product of two low dimensional matrices U and V . The decomposition can be carried out by a variety of methods ranging from SVD based approaches <ref type="bibr" target="#b18">[19]</ref> to the relatively new non-negative matrix factorization approach <ref type="bibr" target="#b19">[20]</ref>. One classical MF method is probabilistic matrix factorization (PMF) <ref type="bibr" target="#b3">[4]</ref>. The underlying assumption behind this method is that the prior probability distribution of the latent factors and the probability of the observed ratings given the latent factors follows a Gaussian distribution. Many algorithms have been developed to enhance the performance of PMF, by designing the Bayesian versions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, or incorporating side information, such as social relationships <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Although promising, matrix factorization methods suffer from the problem of cold-start, i.e. what recommendations to make when a new user/item arrives in the system. Another problem often presented in many real world applications is data sparsity or reduced coverage. Incorporating side information has shown promising performance in collaborative filtering in such scenarios. Porteous et al. proposed a Bayesian matrix factorization (BMF) approach with side information and Dirichlet process mixtures <ref type="bibr" target="#b25">[26]</ref>. A variational BMF method and a hierarchical BMF method that utilizes side information were also proposed in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref>, respectively. Hu et al. proposed a cross-domain triadic factorization (CDTF) method <ref type="bibr" target="#b28">[29]</ref>, which leverages the information from other domains. The methods discussed above are proposed for addressing recommendation problems. Recently, MF based collaborative filtering is also applied to response prediction <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. The afore-mentioned approaches can alleviate the problem of cold start and data sparsity but might still suffer when the side information is sparse. Learning effective features is critical in matrix factorization. Recently, deep learning based methods have emerged as a powerful tool for learning representation and are widely used in many applications ranging from computer vision to speech recognition and machine translation. In this paper, our goal is to combine deep learning based methods with matrix factorization for collaborative filtering. In the next subsection, we survey the application of deep learning based methods for collaborative filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning for Collaborative Filtering</head><p>The application of deep learning models to the task of collaborative filtering is very new and there are not much attempts in this direction. Salakhutdinov et al. <ref type="bibr" target="#b9">[10]</ref> were the first to apply deep learning to the task of collaborative filtering. They modified the restricted Boltzmann machines as a two-layer undirected graphical model consisting of binary hidden units and softmax visible units for the task of collaborative filtering. They designed an efficient learning procedure called the Contrastive Divergence (CD) to maximize an approximation to the true likelihood function. They also proposed a conditional RBM model and inference procedures. They tested the performance of the model on the Netflix dataset for movie recommendation and showed that their model performs well as compared to the baseline methods.</p><p>Truyen et al. <ref type="bibr" target="#b13">[14]</ref> proposed ordinal Boltzmann machines for collaborative filtering. They studied the parameterizations for handling the ordinal nature of ratings, and presented the integration of multiple Boltzmann machines for user-based and item-based processes.</p><p>Recently, some deep learning models learn latent factors from content information such as raw features of audio or articles <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Wang et al. <ref type="bibr" target="#b11">[12]</ref> utilized deep belief nets (DBN) for music recommendation, which unifies feature extraction and recommendation of songs in a joint framework. They assumed that a user has a feature vector βu drawn from a Gaussian prior and the songs have a feature vector xv. They automatically learned the feature vectors of the songs using a deep belief network which is a generative probabilistic graphical model with hidden nodes and observation. It has millions of parameters to be learned from the training data. The authors used stacked layers of Restricted Boltzmann Machines for pretraining in an unsupervised fashion, and then employed the Maximum Likelihood Estimation (MLE) for supervised learning.</p><p>Oord et al. <ref type="bibr" target="#b12">[13]</ref> addressed the music recommendation problem using the convolutional neural networks. They first conducted a weighted matrix factorization to handle implicit feedback and obtained latent factors for all songs. After that they used deep learning to map audio content to those latent factors. In particular, they extracted local features from audio signals and aggregated them into a bag-of-words representation. Finally, the deep convolutional network was employed to map this feature representation to the latent factors. They tested their algorithm on the Million song dataset and showed that their model improved the recommendation performance by augmenting the audio signals.</p><p>All the previously mentioned approaches mainly modify the deep learning algorithms for the task of collaborative filtering and do not directly couple matrix factorization with deep learning models. Most recently, Wang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a hierarchical Bayesian model called collaborative deep learning (CDL) which tightly couples stacked denoising auto-encoders (SDA) and collaborative topic regression (CTR). This work is the closest to our work but differs from ours in many significant ways as follows -(i) CDL utilized a Bayesian formulation of SDA. The generative process of CDL consists of drawing samples for CDL uses an EM-style algorithm for obtaining the MAP estimates of Bayesian SDA, and thus it has to learn a large number of parameters. Our model employs a more efficient architecture, marginalized SDA (mSDA), which computes the parameters in closed form and is thus highly efficient and scalable. (ii) CDL only extracts deep features for items, whereas our model learns deep features for both items and users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRELIMINARIES</head><p>Before we describe our general framework, we discuss the preliminaries as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Matrix Factorization</head><p>Matrix Factorization (MF) is the most effective collaborative filtering approach. It allows us to discover the latent factors of useritem interactions by factorizing the interactions matrix into a joint latent space of user and item features respectively. It proceeds by decomposing the original rating matrix R ∈ R m×n consisting of ratings by m users for n items into two low-rank matrices U ∈ R m×d and V ∈ R n×d consisting of the user and item features respectively of rank d.</p><p>The system learns the latent factors by minimizing the following objective function -</p><formula xml:id="formula_0">arg min U,V l(R, U, V ) + β( U 2 F + V 2 F ),<label>(1)</label></formula><p>where l(R, U, V ) is the loss function of predicting rating using the latent factors U and V and the last two terms are the regularizations used to avoid overfitting. • F denotes the Frobenius norm. Many MF-based methods have been proposed, by designing a sophisticated loss function l(R, U, V ). Existing works usually pose some assumptions on the latent factors U and V in (1). Probabilistic matrix factorization (PMF) <ref type="bibr" target="#b3">[4]</ref> provides a probabilistic foundation by assuming a probabilistic linear model with Gaussian observation noise and Gaussian priors on the latent factors.</p><formula xml:id="formula_1">p(R|U, V, σ 2 ) = M i=1 N j=1 N (Rij|U T i Vj, σ 2 ) I ij (2) p(U |σ 2 u ) = M i=1 N (Ui|0, σ 2 u ) ; p(V |σ 2 v ) = N j=1 N (Vj|0, σ 2 v ). (<label>3</label></formula><formula xml:id="formula_2">)</formula><p>The model is fitted by finding a MAP estimate of the parameters. Maximizing the log posterior leads to the following objective function that can be solved using stochastic gradient descent (SGD):</p><formula xml:id="formula_3">arg minU,V E = R − U V 2 F + β( U 2 F + V 2 F ).</formula><p>To improve the recommendation performance of PMF, Bayesian probabilistic matrix factorization method (BPMF) <ref type="bibr" target="#b33">[34]</ref> considers a full Bayesian treatment of the parameter space instead of a point estimate used in PMF. In the weighted matrix factorization (WMF),</p><formula xml:id="formula_4">l(R, U, V ) = C (R − U V ) 2</formula><p>F , where C is the weight matrix <ref type="bibr" target="#b34">[35]</ref>. Our model is based upon the probabilistic matrix factorization approach as it has shown to have a very good performance on several datasets and at the same time is computationally more efficient as compared to BPMF.</p><p>When side information are available, some MF methods make use of these additional information via regression to predict the ratings <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Marginalized Denoising Auto-encoder (mDA)</head><p>As a specific form of neural network, an autoencoder takes a given input and maps it (encodes) to a hidden representation via a deterministic mapping. Denoising autoencoders reconstruct the input from a corrupted version of the data with the motivation of learning a more robust mapping from the data. Various types of autoencoders have been developed in the literature and have shown promising results in several domains <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Moreover, denoising autoencoders can be stacked to construct a deep network also known as stacked denoising autoencoder (SDA) which allows learning higher level representations <ref type="bibr" target="#b37">[38]</ref>. Despite their state-ofthe-art performance, one of the main drawbacks of SDA is the high computational cost of training, as they rely upon the iterative and numerical optimization techniques to learn a large amount of model parameters.</p><p>Marginalized denoising auto-encoder (mDA) <ref type="bibr" target="#b15">[16]</ref> is a variant of SDA that avoids the high computational cost by marginalizing out the random feature corruption and thus has a closed-form solution to learn model parameters. Therefore, mDA is highly scalable and faster than SDA. It proceeds as follows:</p><p>Given a sample set X = [x1, • • • , x k ], mDA considers multiple passes (e.g., c-times) of random corruptions over X to obtain X. It then reconstructs the input with a mapping W that minimizes the squared loss as follows:</p><formula xml:id="formula_5">L(W ) = 1 2ck c j=1 k i=1 xi − W xij 2 ,<label>(4)</label></formula><p>where xij represents the j th corrupted version of the original input xi and W represents the mapping that is expected to minimizes the loss function.</p><p>The above objective can be rewritten in the matrix form as</p><formula xml:id="formula_6">L(W ) = X − W X 2 F ,<label>(5)</label></formula><p>where X = [X, • • • , X] is the c-times repeated version of X, and X is the corresponding corrupted version. This problem is similar to the ordinary least squares problem and has the analytical solution as given by W = SQ −1 , where S = X XT , Q = X XT When c → ∞ in (4), we can derive the expectations of Q and P , and obtain the closed form solution of the mDA <ref type="bibr" target="#b15">[16]</ref>. Further, multiple mDAs can be stacked to form a deep architecture, marginalized stacked denoising auto-encoder (mSDA). mSDA usually enhances the performance of mDA. Most recently, a nonlinear version of mDA is presented <ref type="bibr" target="#b38">[39]</ref>. </p><formula xml:id="formula_7">U ∈ R m×d Latent factors of users V ∈ R n×d Latent factors of items X ∈ R p×m Side information of users Y ∈ R q×n</formula><p>Side information of items W1 ∈ R p×p Mapping function for X in auto-encoder P1 ∈ R p×d Projection matrix for U</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OUR APPROACH</head><p>As we noted earlier, deep learning models have been proven to be very effective in extracting high-level representations from the raw input data in several learning tasks. The learned features represent high-level knowledge. In the collaborative filtering problem, we face a similar challenge of inferring effective latent and high-level knowledge on user preferences from the raw inputs, including the rating matrix and related features. MF based CF methods are able to capture the implicit relationship between the users and the items successfully, but they suffer from the cold start and data sparsity problems. Therefore, it is reasonable to draw strength from the deep models to assist the collaborative filtering process.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the symbols used in our approach. Next, we describe a general framework that integrates matrix factorization and deep feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Collaborative Filtering (DCF): A General Framework</head><p>In this section, we introduce the proposed deep collaborative filtering (DCF) framework which unifies the deep learning models with MF based collaborative filtering. Fig. <ref type="figure">1</ref> illustrates the idea of our DCF framework. DCF is a hybrid model, which makes use of both rating matrix and side information and bridges together matrix factorization and feature learning.</p><p>Given a user-item rating matrix R, the user side information X and the item side information Y , DCF jointly decomposes R and learns latent factors (i.e., U , V ) from ratings and side information (i.e., X and Y ) through the following formulation:</p><formula xml:id="formula_8">arg min U,V l(R, U, V ) + β( U 2 F + V 2 F ) +γL(X, U ) + δL(Y, V ),<label>(6)</label></formula><p>where β, γ and δ are the trade-off parameters.</p><p>There are two key components in the DCF framework: (i) the function l(R, U, V ) for decomposing the rating matrix R into the two latent matrices; (ii) the functions L(X, U ) and L(Y, V ) that connect the user/item contextual features with the latent factors. The first component derived through matrix factorization extracts latent knowledge from the rating matrix. The second component devised using deep learning models establishes connections of the side information with the latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DCF using PMF + mDA</head><p>A natural instantiation of DCF is combining probabilistic matrix factorization (PMF) with marginalized denoising auto-encoders (mDA). PMF is a widely applied CF approach with excellent performance, and mDA is a powerful tool in extracting high-level features from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝐿(𝑌, 𝑉)</head><p>Figure <ref type="figure">1</ref>: Illustration of DCF framework. The inputs are useritem rating matrix R, the user feature set X and the item feature set Y . Our approach jointly decomposes R and learns latent factors (i.e., U , V ) from ratings and side information (i.e., X and Y ). In particular, the latent factors are extracted from the hidden layer of deep networks. raw inputs. The combination of the two leverages their benefits for learning even richer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">mDA based Collaborative Filtering (mDA-CF)</head><p>Let X ∈ R p×cm and Ȳ ∈ R q×cn denote the c-times repeated versions of X and Y respectively and let X and Ỹ denote their corrupted versions. As discussed before, we utilize the loss function of PMF to decompose rating matrix R, i.e., l(R, U,</p><formula xml:id="formula_9">V ) = A (R − U V ) 2</formula><p>F , where A is the indicator matrix indicating the non-empty entries in R and denotes the Hadamard or pointwise product. The objective function of mDA-CF is formulated as follows:</p><formula xml:id="formula_10">arg min U,V,W 1 , W 2 ,P 1 ,P 2 LU (W1, P1, U ) + LV (W2, P2, V )+ α A (R − U V ) 2 F + β( U 2 F + V 2 F ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">LU (W1, P1, U ) = X − W1 X 2 F + λ P1U − W1X 2 F , LV (W2, P2, V ) = Ȳ − W2 Ỹ 2 F + λ P2V − W2Y 2 F ,</formula><p>W1 ∈ R p×p and W2 ∈ R q×q are reconstructive mappings, P1 ∈ R p×d and P2 ∈ R q×d are projection matrices, α, β and λ are tradeoff parameters. Note that, we set γ and δ in (6) to 1 for simplicity. The first term in LU (W1, P1, U ) denotes the learning process in marginalized denoising auto-encoder. It measures the reconstruction error between input user features X and the mapped features of corrupted inputs, i.e., W1 X. W1 is the learned mapping that is expected to minimize the loss. The second term connects the hidden layer feature W1X and the latent factor U . Generally, the latent factor has much lower dimension than the raw features. Therefore, we add a low-dimensional projection P1 that maps latent factor to the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>Although the optimization problem in <ref type="bibr" target="#b6">(7)</ref> is not jointly convex in all the variables, it is convex to each of them when fixing the others. Hence, we can alternately optimize for each of the variables in <ref type="bibr" target="#b6">(7)</ref>. The detailed procedures are provided below.</p><p>First, we derive a solution to solve W1 and W2 using <ref type="bibr" target="#b15">[16]</ref>. By ignoring the variables irrelevant to W1, the objective (7) can be rewritten as</p><formula xml:id="formula_12">arg min W 1 X − W1 X 2 F + λ P1U − W1X 2 F .<label>(8)</label></formula><p>Inspired by mDA, we consider the infinitely many copies of noisy data, and obtain the optimal solution</p><formula xml:id="formula_13">W1 = E[S1]E[Q1] −1 ,<label>(9)</label></formula><p>where S1 = X X + λP1U X and Q1 = X X + λXX . An efficient solver for calculating the expectations E[S1] and E[Q1] is provided in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Similarly, we can derive the closed-form solution to W2</p><formula xml:id="formula_14">W2 = E[S2]E[Q2] −1 ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_15">S2 = Ȳ Ỹ + λP2V Y and Q2 = Ȳ Ỹ + λY Y .</formula><p>Next, by dropping the irrelevant variables w.r.t. P1, the objective function becomes arg min</p><formula xml:id="formula_16">P 1 λ P1U − W1X 2 F .<label>(11)</label></formula><p>We can obtain the closed-form solution as</p><formula xml:id="formula_17">P1 = W1XU (U U ) −1 .<label>(12)</label></formula><p>Similarly, the optimal solution of P2 is</p><formula xml:id="formula_18">P2 = W2Y V (V V ) −1 .<label>(13)</label></formula><p>To solve for the latent factors U and V , we use the popular stochastic gradient descent (SGD) algorithm. In particular, when other variables irrelevant to U and V are fixed, we use f (U, V ) to denote the objective in <ref type="bibr" target="#b6">(7)</ref>. The update rules are:</p><formula xml:id="formula_19">ui = ui − η ∂ ∂u i f (U, V ), vj = vj − η ∂ ∂v f (U, V ), (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where η is the learning rate , and the detailed derivatives are defined as</p><formula xml:id="formula_21">∂f (U,V ) ∂u i =λ(P 1 (P1ui − (W1X)i)) + βui − α (i,j)∈A (Ri,j − uiv j )vj. (<label>15</label></formula><formula xml:id="formula_22">) ∂f (U,V ) ∂v j =λ(P 2 (P2vj − (W2Y )j)) + βvj − α (i,j)∈A (Ri,j − uiv j )ui.<label>(16)</label></formula><p>The above steps are repeated until convergence. Finally, we obtain the latent factors U and V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Complexity</head><p>The steps of our mDA-CF approach are summarized in Algorithm 1. The learned latent factors U and V can be used to predict missing entries in the rating matrix. In Algorithm 1, we have analytical solutions of Steps 3-6 which are efficient to compute. The matrix multiplication and inversion used in <ref type="bibr">Step</ref>  max{m, n, d}, the time complexity of Algorithm 1 is mainly determined by O(tN ). Hence, our approach owns a good scalability. We discuss the settings of the parameters in the experimental section. To further reduce the computational cost, some advanced distributed optimization algorithms could be applied to our model <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Stacked mDA based Collaborative Filtering (mSDA-CF)</head><p>Existing literature show that stacking multiple deep learning layers together can usually generate rich features in the form of hidden layers, and therefore results in better performance for various learning tasks. Inspired by the marginalized stacked denoising auto-encoders (mSDA) <ref type="bibr" target="#b15">[16]</ref>, we stack multiple mDA together, and present the mSDA-CF approach.</p><p>We assume that only one hidden layer should be close to the latent factor. The reasons are two-fold. First, latent factors are high-level representations, which should correspond to the deeper layers in deep models. Secondly, latent factors should be unique, but different hidden layers have various representations. Therefore, enforcing the similarity between multiple hidden layers and latent factors is unreasonable.</p><p>In our mSDA-CF model, we assume that the latent factors are generated from the l+1 2 layer, given the total number of layers is l. When we train the model for the rest layers, the parameters λ, α and β are simply set to 0. In particular, if i = l+1 2 , we only need to update W i 1 and W i 2 and ignore the other steps, where W i 1 and W i 2 denote the mappings in the i-th layer. One benefit of such setting is the time efficiency, as we do not increase too much computational burden when adding multiple layers.</p><p>Another interesting problem is how to set the number of layers. The number of layers implies the model complexity, which is usually related to the learning task and the size of training data. In the experiments we will discuss the influence of different number of layers. The detailed procedures of mSDA-CF are summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>We notice that most existing deep learning based collaborative filtering methods can be unified in our DCF framework. For example, Oord et al. <ref type="bibr" target="#b12">[13]</ref> use deep convolutional neural networks (CNN) to predict latent factors from music audio using the following ob-Algorithm 2. mSDA-CF Algorithm Input: Rating matrix R, user features X, item features Y λ, α, β, layers l.</p><formula xml:id="formula_23">Output: Latent factors U , V 1: for i 1 : l, do 2: if i = l+1 2 , do 3:</formula><p>Update U and V using Algorithm 1, by setting valid values to λ, α and β; 4: otherwise 5:</p><p>Update W i 1 and W i 2 using Algorithm 1, by setting λ = 0, α = 0 and β = 0; 6: end if 7: end for jective function:</p><formula xml:id="formula_24">min θ u,i cu,i(pu,i − x u y i ) 2 , (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>where xu, the latent factor of user, is estimated from weighted matrix factorization beforehand, and y i , the latent factors of audio, are learned from CNN, i.e., y i = CNN(fv, Ω). Here, fv denotes audio contents, and Ω denotes the model parameters in CNN. Eqn ( <ref type="formula" target="#formula_24">17</ref>) can be interpreted in our DCF formulation (i.e., Eqn ( <ref type="formula" target="#formula_8">6</ref>)). First, it utilizes the weighted matrix factorization as the loss function l(•).</p><p>Secondly, the regularization function L(Y, V ) is implemented by CNN.</p><p>Wang et al. <ref type="bibr" target="#b11">[12]</ref> utilize deep belief networks (DBN) to build a hybrid model for collaborative filtering. The objective function is</p><formula xml:id="formula_26">LHybrid = u,v∈I (ruv − β u xv − r u yv) 2 + λ β β − µ 2 F +λγ γ 2 F + λy y 2 F .<label>(18)</label></formula><p>where xv, the latent factor of item, is obtained from DBN, i.e., xv = DBN(fv, Ω). Also, we can interpret model <ref type="bibr" target="#b17">(18)</ref> in our DCF framework. First, loss function l(•) is implemented in a hybrid way, i.e., rating ruv is predicted by the sum of the CF part r u yv and the content part β u xv. Secondly, DBN is employed to map the content features fv and latent factor xv, which can be formulated by L(Y, V ) in DCF framework. Meanwhile, γ in ( <ref type="formula" target="#formula_8">6</ref>) is set to 0.</p><p>Wang et al. <ref type="bibr" target="#b14">[15]</ref> propose a model with Bayesian stacked denoising auto-encoders (SDAE) and collaborative topic regression are integrated. The objective function is</p><formula xml:id="formula_27">L = − i,j c ij 2 (rij − u i vj) 2 − λn 2 j fr(X0,j * , W + ) − Xc,j * 2 2 − λv 2 j vj − fe(X0,j * , W + ) 2 2 − freg,<label>(19)</label></formula><p>where fe(•) and fr(•) denote the encoding and decoding functions in SDAE, λn and λv denote the trade-off parameters, and freg denote the regularization terms that prevent overfitting.</p><p>Obviously, the model ( <ref type="formula" target="#formula_27">19</ref>) can also be interpreted in the DCF framework. The loss function for decomposing rating matrix in <ref type="bibr" target="#b18">(19)</ref> is in the standard matrix factorization fashion. Further, the second and the third terms in <ref type="bibr" target="#b18">(19)</ref> infer the item latent factor using SDAE, which can be abstracted as L(Y, V ) in DCF.</p><p>In summary, the existing deep collaborative filtering methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> can be unified in a common framework, DCF. We also notice that the existing models only infer latent factors of items using deep models, whereas the latent factors of users are generated in traditional ways. Compared to existing works, our DCF framework provides a more flexible way to explore effective latent factors for users and/or items via deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>We evaluate the performance of our mDA-CF and mSDA-CF approaches on three challenging tasks that are movie recommendation, book recommendation and response prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Movie Recommendation</head><p>For movie recommendation, we conduct experiments on two benchmark datasets MovieLens-100K and MovieLens-1M<ref type="foot" target="#foot_0">1</ref> , which are commonly used for evaluating collaborative filtering algorithms.</p><p>The MovieLens-100K dataset contains 100K ratings of 943 users and 1682 movies, and the MovieLens-1M dataset consists of about 1 million ratings of 6040 users and 3706 movies. Each rating is an integer between 1 (worst) and 5 (best). The ratings are highly sparse. Table <ref type="table" target="#tab_3">2</ref> summarizes the statistics of datasets. We extract the features from side information of users and movies to construct X and Y . To summarize, the user information which consists of the user's age, gender and occupation were encoded into a binary valued vector of length 28. Similarly, the item feature information which consists of the 18 category of movie genre were encoded into a binary valued vector of length 18. Ratings were normalized to be zero-mean.</p><p>As our model ( <ref type="formula" target="#formula_10">7</ref>) is an extension of the representative collaborative filtering method PMF, we mainly compare our approach with PMF and its several variants, such as the Biased PMF <ref type="bibr" target="#b40">[41]</ref> and sparse covariance matrix factorization (SCMF) <ref type="bibr" target="#b22">[23]</ref>. PMF and Biased PMF are special cases of our approach mDA-CF. For example, by adding zero weight to the first two terms in <ref type="bibr" target="#b6">(7)</ref>, mDA-CF degrades to PMF. Further, since our approach takes advantage of the side information of users and movies, we also compare our approach with the collaborative filtering method that incorporates side information, such as the Bayesian matrix factorization with side information (BMFSI) <ref type="bibr" target="#b25">[26]</ref>.</p><p>We employ the root mean squared error (RMSE) as the evaluation metric. RMSE is defined as:</p><formula xml:id="formula_28">RMSE = 1 N i,j Z P ij (Rij − Rij) 2 , (<label>20</label></formula><formula xml:id="formula_29">)</formula><p>where Rij is the ground-truth rating of user i for item j, Rij denotes the corresponding predicted rating, N is the total number of ratings in the test set, and Z P ij is a binary matrix that indicates test ratings.</p><p>For all the compared methods, we set the regularization parameters (e.g., λ, α and β) via 5-fold cross validation. Following the experimental settings in <ref type="bibr" target="#b22">[23]</ref>, we train each compared method with different percentages (50%, 80%, and 99%) of ratings. The training data are randomly chosen from each dataset, and the remaining data are used for testing. This process is repeated five times, and we report the average RMSE.</p><p>For the MovieLens-100K dataset, the parameters α, β and λ are set to 0.7, 0.004 and 0.2, respectively. The learning rate used in SGD is set to 0.002. Table <ref type="table" target="#tab_4">3</ref> shows the average RMSE (with standard deviations) of baselines PMF, Biased PMF, BMFSI, SCMF and our approaches, mDA-CF and mSDA-CF, on the MovieLens-100K dataset. For each method, we have two settings for the dimensions of latent factors, including d = 10 and d = 20. We can observe from Table <ref type="table" target="#tab_4">3</ref> that (a) Our approaches (mDA-CF and mSDA-CF) achieve much better performance than PMF and Biased PMF, which are special cases of our approach. It demonstrates the effectiveness of incorporating side information and deep architectures. 99% 80% 50% Method d=10 d=20 d=10 d=20 d=10 d=20 PMF <ref type="bibr" target="#b3">[4]</ref> 0.9184±0.0265 0.9164±0.0261 0.9223±0.0056 0.9190±0.0052 0.9524±0.0023 0.9506±0.0024 Biased PMF <ref type="bibr" target="#b40">[41]</ref> 0.8953±0.0189 0.8923±0.0150 0.9135±0.0039 0.9087±0.0030 0.9388±0.0029 0.9337±0.0020 BMFSI <ref type="bibr" target="#b25">[26]</ref> 0.8912±0.0127 0.8905±0.0154 0.9114±0.0031 0.9065±0.0029 0.9371±0.0023 0.9335±0.0025SS SCMF <ref type="bibr" target="#b22">[23]</ref> 0.8891±0.0146 0.8896±0.0198 0.9092±0.0033 0.9068±0.0036 0.9334±0.0025 0.9331±0.0021 mDA-CF (Ours) 0.8874±0.0142 0.8861±0.0153 0.9043±0.0043 0.9040±0.0045 0.9312±0.0026 0.9311±0.0025 mSDA-CF (Ours) 0.8852±0.0135 0.8849±0.0167 0.9035±0.0028 0.9024±0.0030 0.9309±0.0026 0.9308±0.0028</p><p>(b) BMFSI is a Bayesian matrix factorization method that utilizes side information, so it performs better than PMF and Biased PMF that ignore such information. Our approach outperforms BMFSI, which validates the strengths of the latent factors learned by marginalized denoising auto-encoders.</p><p>(c) Usually, deep models with multiple layers lead to better performance. Our mSDA-CF slightly enhances the performance of mDA-CF. We will show the influence of different number of layers in the next section.</p><p>(d) Note that the basic component in our approach is PMF. Actually, DCF is a general framework for collaborative filtering. When we implement l(R, U, V ) in ( <ref type="formula" target="#formula_8">6</ref>) as some advanced MF methods (e.g., weighted matrix factorization), the results could be further improved.</p><p>For the MovieLens-1M dataset, the parameters α, β and λ are set to 0.8, 0.003 and 0.3, respectively. Table <ref type="table" target="#tab_5">4</ref> shows the average RMSE (with standard deviations) of our approach and compared methods on the MovieLens-1M dataset. Basically, Table <ref type="table" target="#tab_5">4</ref> shows similar phenomenon to that we observed from Table <ref type="table" target="#tab_4">3</ref>. The proposed mDA-CF and mSDA-CF approaches consistently achieve lower RMSE than compared methods. As before, we evaluate each method in two settings with different dimension of latent factors. Usually, d = 20 generates better results than d = 10 on the two MovieLens datasets.</p><p>In addition, we compare our approaches with the state-of-the-art deep learning based collaborative filtering method, a joint user-item based restricted Boltzmann machine (UI-RBM) <ref type="bibr" target="#b10">[11]</ref>. To conduct fair comparisons with UI-RBM, we adopt the mean absolute error (MAE) used in <ref type="bibr" target="#b10">[11]</ref> as evaluation metric. The MAE is defined as follows</p><formula xml:id="formula_30">MAE = L i |gi − pi| L , (<label>21</label></formula><formula xml:id="formula_31">)</formula><p>where gi is the ground truth rating, pi is the predicted rating, and L is the total number of ratings. We follow the experimental settings in <ref type="bibr" target="#b10">[11]</ref>, and conduct 5-fold cross-validations. The dimension of latent factors is set to 20. Ta-Table <ref type="table">5</ref>: MAE of compared methods on MovieLens-100K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MAE PMF <ref type="bibr" target="#b3">[4]</ref> 0.793 U-RBM <ref type="bibr" target="#b10">[11]</ref> 0.779 I-RBM <ref type="bibr" target="#b10">[11]</ref> 0.775 I-RBM+INB <ref type="bibr" target="#b10">[11]</ref> 0.699 UI-RBM <ref type="bibr" target="#b10">[11]</ref> 0.690 mDA-CF (Ours) 0.683 mSDA-CF (Ours) 0.680 ble 5 shows the average MAE of compared methods on the MovieLens-100K dataset. U-RBM and I-RBM denote the user-based model and item-based model, respectively. They are the baselines used in <ref type="bibr" target="#b10">[11]</ref>. Table <ref type="table">5</ref> shows that UI-RBM achieves much better performance than traditional methods like PMF, as it takes advantage of the deep feature learning. Our mDA-CF and mSDA-CF approaches obtain lower MAE than UI-RBM, demonstrating the effectiveness of our DCF framework compared to the RBM based deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Book Recommendation</head><p>For book recommendation, we utilize the Book-Crossing dataset<ref type="foot" target="#foot_1">2</ref> , which contains 1149780 ratings for 271379 books from 278858 users. The rating scale is from 0 to 10 with the higher score indicating the more preference. Some attributes of users and books are also provided in this dataset. These attributes are encoded to binary vectors, which form the feature sets for users and books. Table <ref type="table" target="#tab_3">2</ref> shows some statistics of this dataset.</p><p>We follow the settings in <ref type="bibr" target="#b42">[43]</ref>, and conduct 5-fold cross-validation. The baselines include PMF, the implicit social matrix factorization (ISMF) <ref type="bibr" target="#b41">[42]</ref> and the coupled item-based matrix factorization (CIMF) <ref type="bibr" target="#b42">[43]</ref>. ISMF incorporates the implicit social relationships </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>99%</head><p>80% 50% Method d=10 d=20 d=10 d=20 d=10 d=20 PMF <ref type="bibr" target="#b3">[4]</ref> 0.8424±0.0071 0.8388±0.0059 0.8559±0.0022 0.8512±0.0017 0.8790±0.0009 0.8745±0.0011 Biased PMF <ref type="bibr" target="#b40">[41]</ref> 0.8408±0.0070 0.8367±0.0067 0.8531±0.0019 0.8493±0.0020 0.8766±0.0015 0.8722±0.0012 BMFSI <ref type="bibr" target="#b25">[26]</ref> 0.8391±0.0067 0.8340±0.0069 0.8503±0.0017 0.8478±0.0019 0.8742±0.0016 0.8703±0.0010 SCMF <ref type="bibr" target="#b22">[23]</ref> 0.8364±0.0065 0.8323±0.0065 0.8496±0.0019 0.8465±0.0018 0.8707±0.0013 0.8678±0.0007 mDA-CF (Ours) 0.8335±0.0064 0.8317±0.0062 0.8449±0.0015 0.8429±0.0013 0.8655±0.0007 0.8645±0.0006 mSDA-CF (Ours) 0.8320±0.0063 0.8304±0.0057 0.8416±0.0014 0.8407±0.0011 0.8628±0.0005 0.8613±0.0006 Table <ref type="table">6</ref>: RMSE of compared methods on Book-Crossing data.</p><p>Method RMSE (d = 10) RMSE (d = 50) PMF <ref type="bibr" target="#b3">[4]</ref> 3.7483 3.7452 ISMF <ref type="bibr" target="#b41">[42]</ref> 3.7440 3.7415 CIMF <ref type="bibr" target="#b42">[43]</ref> 3.7398 3.7372 mDA-CF (Ours)</p><p>3.6610 3.6528 mSDA-CF (Ours)</p><p>3.6592 3.6513 between users and between items. CIMF makes use of the attributes of books in the model. Table <ref type="table">6</ref> shows the RMSE of all compared methods. We can observe that ISMF and CIMF obtain better results than the conventional PMF method, as they incorporate side information to their models. Our approaches obtain much lower RMSE than PMF, ISMF and CIMF in different settings. mSDA-CF achieves the best results among all competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Response Prediction</head><p>Response prediction is another interesting application of collaborative filtering <ref type="bibr" target="#b29">[30]</ref>. In particular, we consider a specific type of response prediction, which is click prediction. Given an online advertisement, our task is to predict whether a given user will click it or not in the near future.</p><p>Previous research works have proved that collaborative filtering (CF) methods are suitable for addressing the click prediction problem. Unfortunately, there are few datasets available for evaluating the click prediction performance of CF models. To evaluate the performance of our model in real-world applications, we collected an advertising dataset at a large software company. The dataset is collected from its website, which contains the click responses for advertisements over 3 million users. For our purpose, we analyze the data from a 2-month period, from October 1, 2013 to November 30, 2013.</p><p>The dataset used in our experiments contains 737 ads and 448,158 users. It also has the impression and click data of the advertisements. For each click event, we have the user ID, day, ad ID, page ID, country, browser, advertiser ID, and size of ad. Each record can be uniquely identified by a (user, ad, day) triplet. In addition, we have information about the user profiles. For each user, we have some attributes such as country, domain, etc. Apart from the user information, our dataset contains the meta-information of the ads such as ad size. In the experiments, we encode the demographic information (e.g., country, state, domain) into a binary valued vector for each user. The attributes of ads (e.g., advertiser, ad size) are also encoded into binary vectors. Some statistics of this dataset can be found in Table <ref type="table" target="#tab_3">2</ref>.  The advertising dataset used in our experiments is very sparse, which contains 880,569 click responses (density: 0.27%). We use the first 50% click responses for training, and the remaining data for testing. Following <ref type="bibr" target="#b43">[44]</ref>, we use the receiver operating characteristic (ROC) curve and the area under ROC (AUC-ROC) as our evaluation metrics. The true positive rate (TPR) and false positive rate (FPR) used for generating ROC curves are defined as follows:</p><formula xml:id="formula_32">T P R = T P T P +F N , F P R = F P F P +T N ,<label>(22)</label></formula><p>where T P represents the true positives, F N represents the false negatives, T N represents the true negatives, and F P represents the false positives. We evaluate the performance of each compared method on the Advertising dataset. The major parameters α, β and λ are set to 0.8, 0.02 and 0.12, respectively. Two settings are utilized, by setting the latent factor dimension to 10 and 20, respectively. Fig. <ref type="figure" target="#fig_1">2</ref> shows the ROC curves of PMF, Biased PMF, BMFSI, SCMF, mDA-CF and mSDA-CF. Table <ref type="table" target="#tab_6">7</ref> shows the corresponding AUC of all compared methods. We can observe that our approaches obtain higher AUC than other methods, which demonstrates the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>So far, we have seen that our approach outperforms the existing approaches on the different datasets. We also analyze the convergence property and parameter settings of our approach on the   <ref type="bibr" target="#b3">[4]</ref> 0.7651 0.7716 Biased PMF <ref type="bibr" target="#b40">[41]</ref> 0.7692 0.7724 BMFSI <ref type="bibr" target="#b25">[26]</ref> 0.7720 0.7805 SCMF <ref type="bibr" target="#b22">[23]</ref> 0.7782 0.7866 mDA-CF (Ours) 0.7961 0.8023 mSDA-CF (Ours) 0.8057 0.8115</p><p>MovieLens-100K dataset. In Fig. <ref type="figure" target="#fig_2">3</ref>(a), we show the RMSE of mDA-CF and mSDA-CF with different levels of noise. We observe an interesting phenomena that the RMSE of mDA-CF slightly decreases when increasing the noise level for input samples at first; mSDA-CF achieves the best performance when adding 50% noise. However, when the percentage of noise is larger than 50%, mDA-CF outperforms mSDA-CF. It shows that the latent factors learned from multi-layer models might be unreliable if there are too much noise contained in the input samples. Fig. <ref type="figure" target="#fig_2">3</ref>(b) shows the RMSE in different iterations of PMF and our approaches. We see that PMF overfits the data after 100 epochs. Although our approaches have larger RMSE from epoch 40 to epoch 120, they keep reducing the RMSE even after 400 epochs. It shows that our approach enjoys better stability. Also, by incorporating the side information of users and items, the learned model has a good generalization ability.</p><p>Another important property in our approach is the setting of stacked layers. Fig. <ref type="figure" target="#fig_2">3(c</ref>) shows the RMSE of mSDA-CF with different number of layers. In general, stacking multiple auto-encoder will lead to better performance. But Fig. <ref type="figure" target="#fig_2">3(c</ref>) shows that the improvement becomes marginal when there are more than 5 layers. The reasons are two-fold. First, the model capacity is related to the size of training data. In the experiments, training a very complicated deep model (with many layers) with the employed datasets may be unreasonable. Secondly, accessing rich features of users is always a challenging issue in reality. In our model, these features are the inputs of deep models. Our model achieves better performance than existing works by using these features. However, the features are not extensive enough to train deep models. In a word, this parameter should be carefully tuned in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we propose a deep collaborative filtering (DCF) framework, which bridges matrix factorization and deep feature learning. DCF is a hybrid collaborative filtering model, as it learns effective latent factors from both user-item ratings and side information. Using this framework, we present the mDA-CF and mSDA-CF approaches by incorporating the probabilistic matrix factorization and marginalized denoising auto-encoders. We also design efficient optimization algorithms to solve the models. Extensive experimental results on the MovieLens-100K, MovieLens-1M, Book-Crossing and Advertising datasets demonstrate the effectiveness of the latent factors learned by our model. Our mDA-CF and mSDA-CF approaches outperform related methods on the tasks of movie recommendation, book recommendation and response prediction. They also achieve better performance than the existing deep learning based collaborative filtering method such as UI-RBM. In addition, the convergence property and parameter settings of our model are discussed in the experiments.</p><p>A part of the future work is to extend other deep learning and matrix factorization methods using our DCF framework and evaluate their performance for collaborative filtering. Another future direction is to apply the distributed optimization algorithms to further reduce the computational costs of our algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ROC curves of our approach and compared methods on Advertising dataset (d=10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental analysis on MovieLens-100K dataset: (a) RMSE with different level of noise; (b) RMSE with epochs; (c) RMSE of mSDA-CF with different number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of notations.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>m</cell><cell>Number of users</cell></row><row><cell>n</cell><cell>Number of items</cell></row><row><cell>d</cell><cell>Dimension of latent factors</cell></row><row><cell>p</cell><cell>Dimension of user features</cell></row><row><cell>q</cell><cell>Dimension of item features</cell></row><row><cell cols="2">R ∈ R m×n Rating matrix</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>5 and Step 6 cost O(p 2 m + pmd + d 3 ) and O(q 2 n + qnd + d 3 ), respectively. The Steps 8-9 are implemented in a batch-learning fashion, and cost Algorithm 1. mDA-CF Algorithm Input: Rating matrix R, user features X, item features Y , parameters λ, α, β. to evaluate the gradients, where t is the number of iterations and N is the number of training ratings/responses in R. Considering that N</figDesc><table><row><cell cols="2">Output: Latent factors U , V</cell></row><row><cell cols="2">1: Initialize U , V , P1 and P2;</cell></row><row><cell cols="2">2: while validation error decreases, do</cell></row><row><cell cols="2">3: Update W1 using (9);</cell></row><row><cell cols="2">4: Update W2 using (10);</cell></row><row><cell cols="2">5: Update P1 using (12);</cell></row><row><cell cols="2">6: Update P2 using (13);</cell></row><row><cell cols="2">7: for each observed Rij, do</cell></row><row><cell>8:</cell><cell>Update ui using (14);</cell></row><row><cell>9:</cell><cell>Update vj using (14);</cell></row><row><cell cols="2">10: end for</cell></row><row><cell cols="2">11: end while</cell></row><row><cell>O(tN )</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Users #Items Sparsity</cell><cell>User Features</cell><cell>Item Features</cell></row><row><cell>MovieLens-100K</cell><cell>943</cell><cell>1682</cell><cell>93.7%</cell><cell>Age, gender, and occupation</cell><cell>Genres</cell></row><row><cell>MovieLens-1M</cell><cell>6040</cell><cell>3706</cell><cell>95.8%</cell><cell>Age, gender, and occupation</cell><cell>Genres</cell></row><row><cell>Book-Crossing</cell><cell cols="2">278858 271379</cell><cell>99.9%</cell><cell>Age, country, city, etc.</cell><cell>Title, year, publisher, etc.</cell></row><row><cell>Advertising</cell><cell>448158</cell><cell>737</cell><cell>99.7%</cell><cell>Age, geolocation, domain, etc.</cell><cell>Ad size, advertisor, etc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average RMSE (with standard deviation) of compared methods with different percentages of training data on MovleLens-100K dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Average RMSE (with standard deviation) of compared methods with different percentages of training data on MovleLens-1M dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>AUC of compared methods on Advertising dataset. Method AUC (d = 10) AUC (d = 20) PMF</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://grouplens.org/datasets/movielens/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://www2.informatik.uni-freiburg.de/ ~cziegler/BX/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported in part by the National Science Foundation (NSF) CNS award 1314484, Office of Naval Research (ONR) award N00014-12-1-1028, ONR Young Investigator Award N00014-14-1-0484, Naval Postgraduate School (NPS) award N00244-15-1-0041 and U.S. Army Research Office Young Investigator Award W911NF-14-1-0218.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Artificial Intellegence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian multitask collaborative filtering</title>
		<author>
			<persName><forename type="first">Sotirios</forename><surname>Chatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2149" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ajit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A bayesian matrix factorization model for relational data</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ajit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Gordon</surname></persName>
		</author>
		<idno>CoRR, abs/1203.3517</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A non-iid framework for collaborative filtering with restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Kostadin</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1148" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving content-based and hybrid music recommendation using deep learning</title>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ordinal boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">Tran</forename><surname>The Truyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno>CoRR, abs/1409.2944</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recommender systems handbook</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
				<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Sebastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast max-margin matrix factorization with data augmentation</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SCMF: sparse covariance matrix factorization for collaborative filtering</title>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporating side information in probabilistic matrix factorization with gaussian processes</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prescott Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leveraging social connections to improve personalized ranking for collaborative filtering</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian matrix factorization with side information and dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">U</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scalable variational bayesian matrix factorization with side information</title>
		<author>
			<persName><forename type="first">Yong-Deok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian matrix factorization with side information</title>
		<author>
			<persName><forename type="first">Sunho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Deok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Personalized recommendation via cross-domain triadic factorization</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="595" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Response prediction using collaborative filtering with hierarchies and side-information</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Prasad</forename><surname>Chitrapura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagaraj</forename><surname>Kota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting user behavior in display advertising via dynamic collective matrix factorization</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep modeling of group preferences for group-based recommendation</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1861" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autoencoder-based collaborative filtering</title>
		<author>
			<persName><forename type="first">Yuanxin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="284" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning invariant features through topographic filter maps</title>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Marcâ Ȃ Źaurelio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1605" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Marginalized denoising auto-encoders for nonlinear representations</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1476" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed stochastic ADMM for matrix factorization</title>
		<author>
			<persName><forename type="first">Zhi-Qin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing-Jian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1259" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An experimental study on implicit social recommendation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coupled item-based matrix factorization</title>
		<author>
			<persName><forename type="first">Fangfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WISE</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable hierarchical multitask learning algorithms for conversion optimization in display advertising</title>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
