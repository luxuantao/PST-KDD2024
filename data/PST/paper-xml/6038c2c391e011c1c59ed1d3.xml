<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIXSPEECH: DATA AUGMENTATION FOR LOW-RESOURCE AUTOMATIC SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-25">25 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linghui</forename><surname>Meng</surname></persName>
							<email>menglinghui2019@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
							<email>jindong.wang@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MIXSPEECH: DATA AUGMENTATION FOR LOW-RESOURCE AUTOMATIC SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-25">25 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2102.12664v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech Recognition</term>
					<term>Data Augmentation</term>
					<term>Low-resource</term>
					<term>Mixup</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose MixSpeech, a simple yet effective data augmentation method based on mixup for automatic speech recognition (ASR). MixSpeech trains an ASR model by taking a weighted combination of two different speech features (e.g., mel-spectrograms or MFCC) as the input, and recognizing both text sequences, where the two recognition losses use the same combination weight. We apply MixSpeech on two popular end-to-end speech recognition models including LAS (Listen, Attend and Spell) and Transformer, and conduct experiments on several low-resource datasets including TIMIT, WSJ, and HKUST. Experimental results show that MixSpeech achieves better accuracy than the baseline models without data augmentation, and outperforms a strong data augmentation method SpecAugment on these recognition tasks. Specifically, MixSpeech outperforms SpecAugment with a relative PER improvement of 10.6% on TIMIT dataset, and achieves a strong WER of 4.7% on WSJ dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic speech recognition (ASR) has achieved rapid progress with the development of deep learning. Advanced models such as DNN <ref type="bibr" target="#b0">[1]</ref>, CNN <ref type="bibr" target="#b1">[2]</ref>, RNN <ref type="bibr" target="#b2">[3]</ref> and end-to-end models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> result in better recognition accuracy compared with conventional hybrid models <ref type="bibr" target="#b5">[6]</ref>. However, as a side effect, deep learning-based models require a large amount of labeled training data to combat overfitting and ensure high accuracy, especially for speech recognition tasks with few training data. Therefore, a lot of data augmentation methods for ASR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> were proposed, mainly on augmenting speech data. For example, speed perturbation <ref type="bibr" target="#b6">[7]</ref>, pitch adjust <ref type="bibr" target="#b7">[8]</ref>, adding noise <ref type="bibr" target="#b8">[9]</ref> and vocal tract length perturbation increases the quantity of speech data by adjusting the speed or pitch of the audio, or by adding noisy audio on the original clean audio, or by transforming spectrograms. Recently, SpecAugment <ref type="bibr" target="#b9">[10]</ref> was proposed to mask the mel-spectrogram along the time and frequency axes, and achieve good improvements on recognition accuracy. Furthermore, <ref type="bibr" target="#b12">[13]</ref> masks the speech sequence in the time domain according to the alignment with text to explore the semantical relationship. As can be seen, most previous methods focus on augmenting the speech input while not changing the corresponding label (text), which needs careful tuning on the augmentation policy. For example, SpecAugment needs a lot of hyper-parameters (the time warp parameter W , the time and frequency mask parameters T and F , a time mask upper bound p, and the number of time and frequency mask m F and m T ) to determine how to perform speech augmentation, where improper parameters may cause much information loss and thus cannot generate text correctly, or may have small changes on the speech and thus have no augmentation effect. <ref type="bibr" target="#b13">[14]</ref> is proposed to improve the generalization limitation of empirical risk minimization (ERM) <ref type="bibr" target="#b14">[15]</ref>. Vanilla mixup randomly chooses a weight from a distribution and combines two samples and corresponding labels with the same weight. Recent works apply mixup on different tasks, including image classification <ref type="bibr" target="#b15">[16]</ref>, and sequence classification <ref type="bibr" target="#b16">[17]</ref>. Different from classification task with one-hot labels where mixup can be easily incorporated, conditional sequence generation tasks such as automatic speech recognition, image captioning, and handwritten text recognition cannot directly apply mixup on the target sequence due to different lengths. <ref type="bibr" target="#b17">[18]</ref> directly apply mixup to train a neural acoustic model like LF-MMI <ref type="bibr" target="#b18">[19]</ref>, which simply integrates inputs and targets frame-by-frame with shared mixture weight because the input feature and label are aligned frame-wisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recently, mixup technique</head><p>In this paper, we propose MixSpeech, a simple yet effective data augmentation method for automatic speech recognition. MixSpeech trains an ASR model by taking a weighted combination of two different speech sequences (e.g., melspectrograms or MFCC) as input, and recognizing both text sequences. Different from mixup <ref type="bibr" target="#b13">[14]</ref> that uses the weighted combination of two labels as the new label for the mixed input, MixSpeech uses each label to calculate the recognition loss and combines the two losses using the same weight as in the speech input. MixSpeech is much simple with only a single hyper-parameter (the combination weight λ), unlike the complicated hyper-parameters used in SpecAugment. Meanwhile, MixSpeech augments the speech input by introducing another speech, which acts like a contrastive signal to force the ASR model to better recognize the correct text of the corresponding speech instead of misled by the other speech signal. Therefore, MixSpeech is more effective than previous augmentation methods that only change the speech input with masking, wrapping, pitch, and duration adjusting, without introducing contrastive signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>In this section, we first briefly recap the concept of mixup <ref type="bibr" target="#b13">[14]</ref>, which is an efficient augmentation approach for the singlelabel task. In order to handle sequence generation tasks (e.g., ASR), then we introduce MixSpeech. At the last, we describe two popular models on which MixSpeech is implemented to verify the effectiveness of MixSpeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mixup Recap</head><p>Mixup <ref type="bibr" target="#b13">[14]</ref> is an effective data augmentation method for supervised learning tasks. It trains a model on a convex combination of pairs of inputs and their targets to make the model more robust to adversarial samples. The mixup procedure can be demonstrated as:</p><formula xml:id="formula_0">X mix = λX i + (1 − λ)X j , Y mix = λY i + (1 − λ)Y j ,<label>(1)</label></formula><p>where X i and Y i are the i-th input and target of the data sample, X mix and Y mix represent the mixup data by combining a pair of data samples (i and j), and λ ∼ Beta(α, α) with α ∈ (0, ∞), is the combination weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MixSpeech</head><p>Mixup can be easily applied to classification tasks where the target is a single label. However, it is difficult to apply into sequence generation tasks such as ASR due to the following reasons: 1) Two text sequences (Y i , Y j ) may have different lengths and thus cannot be directly mixed up as in Eq. 1;</p><formula xml:id="formula_1">Model λ 1 -λ Y i Output Loss j X j X i Loss i Y j λ 1 -λ Loss mix X mix</formula><p>Fig. <ref type="figure">1</ref>. The pipeline of MixSpeech. X mix is the mixture of two mel-spectrograms, X i and X j . Loss i is calculated from the output of X mix and the corresponding label Y i . Loss mix is the weighted combination of two losses Loss i and Loss j .</p><p>2) Text sequences are discrete and cannot be directly added;</p><p>3) If adding two text sequences in the embedding space, the model will learn to predict a mixture embedding of two text sequences, which will confuse the ASR model and may hurt the performance since its final goal is to recognize a single text from the speech input.</p><p>To ensure effective mixup for the sequential data (speech and text) in ASR, we propose MixSpeech, which mixes two speech sequences in the input and mixes two loss functions regarding the text output, as shown in Figure <ref type="figure">1</ref>. The formulation of MixSpeech is as follows:</p><formula xml:id="formula_2">X mix = λX i + (1 − λ)X j , L i = L(X mix , Y i ) L j = L(X mix , Y j ), L mix = λL i + (1 − λ)L j ,<label>(2)</label></formula><p>where X i and Y i are the input speech sequence and target text sequence of i-th sample, X mix is the mixup speech sequence by adding the two speech sequences frame-wisely with weight λ. L(•, •) calculates the ASR loss (which also includes the recognition process), and L mix combines the two losses with the same weight λ as in the speech input during the training phase. Following the original mixup, we choose λ ∼ Beta(α, α) where α ∈ (0, ∞).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Structure</head><p>LAS <ref type="bibr" target="#b3">[4]</ref> and Transformer <ref type="bibr" target="#b19">[20]</ref> architectures have been widely used and achieved great performance on ASR tasks. In this paper, we implement MixSpeech on these two popular models to demonstrate its effectiveness. The two models both leverage the joint CTC-attention <ref type="bibr" target="#b20">[21]</ref> structure to train the model, which consists of two loss functions: 1) CTC (Connectionist Temporal Classification) <ref type="bibr" target="#b21">[22]</ref> loss on the encoder output and 2) Cross-Entropy (CE) loss at the output of decoder at each timestep. To train the CTC part, a sequence of labels, denoted as l, are used to compute loss with an effective algorithm such as the Baum-Welch algorithm <ref type="bibr" target="#b22">[23]</ref>. The CTC loss can be written as:</p><formula xml:id="formula_3">L CT C (ŷ, l) = − log   π∈β −1 (l) T t=1 y t πt   , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where β is the function that removes repeated labels and blank labels, π t represents intermediate label sequence containing blank label and l is the target sequence without blank. The Cross-Entropy loss can be written as:</p><formula xml:id="formula_5">L CE = − u log(P (y u |x, y 1:u−1 )),<label>(4)</label></formula><p>where y u is the u-th target token. In LAS <ref type="bibr" target="#b3">[4]</ref>, the encoder and decoder are the stacked BiLSTM and LSTM respectively. In Transformer based structure <ref type="bibr" target="#b19">[20]</ref>, they are stacked of multihead attention and feed-forward network.</p><p>During training, following <ref type="bibr" target="#b20">[21]</ref>, we leverage the multitask learning by combining CTC and Cross-Entropy loss as follows:</p><formula xml:id="formula_6">L M T L = βL CT C +(1 − β)L CE ,<label>(5)</label></formula><p>where β ∈ [0, 1] is a tunable hyper-parameter. By combining with MixSpeech in Eq. 2, the final training objective can be written as:</p><formula xml:id="formula_7">L mix = λL M T L (X mix , Y i )+(1−λ)L M T L (X mix , Y j ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluate our proposed MixSpeech on several datasets including TIMIT <ref type="bibr" target="#b23">[24]</ref>, WSJ <ref type="bibr" target="#b24">[25]</ref> and the Mandarin ASR corpus HKUST <ref type="bibr" target="#b25">[26]</ref>. For TIMIT dataset, we use the standard train/test split, where we randomly sample 10% of the training set for validation and regard the left as the training set. WSJ <ref type="bibr" target="#b24">[25]</ref> is a corpus of read news articles with approximately 81 hours of clean data. Following the standard process, we set the Dev93 for development and Eval92 for evaluation.</p><p>For HKUST dataset, the development set contains 4,000 utterances (5 hours) extracted from the original training set with 197,387 utterances (173 hours) and the rest are used for training, besides 5,413 utterances (5 hours) for evaluation.</p><p>The speech sentences of these corpora with 16k sampling rate extract the fbank features as the model input. For TIMIT dataset, fbank features with 23 dimensions are extracted at each frame, while for the other two datasets 80 dimensions fbank features are extracted at each frame following the common practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Settings</head><p>We implement MixSpeech based on the ESPnet codebase <ref type="foot" target="#foot_0">1</ref> .</p><p>MixSpeech is designed for extracting and splitting targets from mixed inputs to train a more generalizable model, which is a more difficult task than training without MixSpeech. Thus we give more training time to the model enhanced with MixSpeech<ref type="foot" target="#foot_1">2</ref> . The baselines of LAS <ref type="bibr" target="#b3">[4]</ref> and Transformer <ref type="bibr" target="#b19">[20]</ref> are implemented as the following settings. For the LAS (Listen Attend and Spell), the encoder has 4-layer Bi-LSTM with width 320 and the decoder has a single layer LSTM with width 300. For the Transformer based architecture, the encoder has 12 layers and the decoder has 6 layers with width 256, where each layer is composed of multi-head attention and fully connected layer. CTC and decoder cross-entropy loss are used for jointly decoding using beam search with beam size 20. The default α for Beta distribution is set to 0.5 following <ref type="bibr" target="#b13">[14]</ref>. The multi-task learning parameter β is set to 0.3. In the practice, we randomly select part of paired data within one batch to train the model with MixSpeech and other data within one batch to train the model as usual.</p><p>The proportion of one batch data to train using MixSpeech is denoted as τ , which is set to 15% as default and achieve good performance according to our experiments in Table <ref type="table" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results of MixSpeech</head><p>We compare MixSpeech with two settings: 1) models trained without MixSpeech (Baseline) and 2) models trained with SpecAugment <ref type="bibr" target="#b9">[10]</ref> (SpecAugment), an effective data augmentation technique. Table <ref type="table" target="#tab_0">1</ref> shows the performance of MixSpeech on TIMIT dataset with the LAS model, comparing with Baseline and SpecAugment. Furthermore, Table <ref type="table" target="#tab_1">2</ref> shows the comparison of our method with Baseline and SpecAugment on three different low resource datasets by different metrics with Transformer. We can find that MixSpeech improves the baseline model on TIMIT dataset relatively by 10.6% on LAS and 6.9% on Transformer. As shown in the Table <ref type="table" target="#tab_1">2</ref>, MixSpeech achieves 4.7% WER on WSJ, outperforming the baseline model alone and the baseline model with SpecAugment. MixSpeech also achieves better performance on HKUST <ref type="bibr" target="#b25">[26]</ref>, which is a large Mandarin corpus.</p><p>The results demonstrate that MixSpeech can consistently improve the recognition accuracy across datasets with different languages and data sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Method Analysis</head><p>Both LAS and Transformer based models leverage multi-task learning to boost the performance and β in Equation 5 is a hyper-parameter to adjust the weight of them. When β is set to 0 or 1, there is only Cross-Entropy loss or CTC loss. We vary β and get the results of the baseline as shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>The results show that the model cannot perform well with only one target and requires alignment information guided by CTC loss to help the attention decoder. In Table <ref type="table" target="#tab_2">3</ref>, β = 0.3 gets the lowest PER score, and thus is set as default in the baselines and models enhanced with MixSpeech. As described in Sec. 3.2, we randomly select part of paired data within one batch (the proportion is denoted as τ ) to train the model with MixSpeech and other data within one batch to train the model as usual. We further study whether the proportion τ of a batch data has much influence on the results and the results are shown in Table <ref type="table" target="#tab_3">4</ref>. we can see that the model enhanced with MixSpeech can consistently outperform the baseline (τ = 0) by varying τ , which shows that MixSpeech is not sensitive to τ .</p><p>Rather than mixup on two inputs, we can extend mixup to more inputs. Here we randomly select three inputs, named Tri-mixup, and set the mixup weight λ to 1/3<ref type="foot" target="#foot_2">3</ref> . Similar to MixSpeech, the proportion for mixup τ is also set to 15%. Table <ref type="table" target="#tab_4">5</ref> shows that the performance of Tri-mixup drops dramatically, comparing with the baseline and MixSpeech. Trimixup may introduce too much information beyond the original ones, which hinders the model from learning useful patterns from the data and leads to the performance drop. Noise regularization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, which incorporates noise on the inputs, is a common data augmentation method to make the model more robust to noise and more generalizable to unseen data. To compare it with MixSpeech, we add Gaussian noise with SNR 5dB on the training set and report the results in Table <ref type="table" target="#tab_4">5</ref>. We can see that MixSpeech outperforms the noise regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed MixSpeech, a new data augmentation method to apply the mixup technique on ASR tasks for low-resource scenarios. Experimental results show that our method improves the recognition accuracy compared with the baseline model and previous data augmentation method SpecAgument, and achieves competitive WER performance on WSJ dataset. In the future, we will consider another mixup strategy by concatenating different segments of speech and also their corresponding text sequences, which can create more data samples with novel distributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work is supported by the National Key Research and Development Program of China under No.2017YFB1002102</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The phone error rate (PER) results of LAS (Listen, Attend and Spell), and LAS enhanced with SpecAugment or MixSpeech on TIMIT dataset.</figDesc><table><row><cell>Method</cell><cell>PER</cell></row><row><cell>Baseline</cell><cell>21.8%</cell></row><row><cell>+SpecAugment</cell><cell>20.5%</cell></row><row><cell>+MixSpeech</cell><cell>19.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The results of Transformer, and Transformer enhanced with SpecAugment or MixSpeech on TIMIT, WSJ, and HKUST dataset. The metrics of the three datasets are phone error rate (PER) and word error rate (WER) respectively.</figDesc><table><row><cell>Method</cell><cell cols="3">TIMIT P ER WSJ W ER HKUST W ER</cell></row><row><cell>Baseline</cell><cell>23.1%</cell><cell>5.9%</cell><cell>23.9%</cell></row><row><cell>+SpecAugment</cell><cell>22.5%</cell><cell>5.4%</cell><cell>23.5%</cell></row><row><cell>+MixSpeech</cell><cell>21.8%</cell><cell>4.7%</cell><cell>22.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The phone error rate (PER) results by varying the multi-task learning parameters β on TIMIT dataset.</figDesc><table><row><cell>β</cell><cell>0</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell></row><row><cell cols="5">PER 26.6% 23.0% 23.1% 23.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The phone error rate (PER) results by varying proportion τ of a batch data to conduct MixSpeech on TIMIT dataset.</figDesc><table><row><cell>τ</cell><cell>0%</cell><cell>15%</cell><cell>20%</cell><cell>30%</cell></row><row><cell cols="5">PER 23.1% 21.8% 21.9% 22.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The phone error rate (PER) results of different data augmentation methods including Tri-mixup, Noise Regularization and MixSpeech on TIMIT dataset. The Baseline refers to the Transformer model without data augmentation. Trimixup means that we select three inputs and conduct mixup with weight λ =1/3. The noise regularization is to add Gaussian noise to make the model more robust to noise.</figDesc><table><row><cell></cell><cell cols="2">Baseline Tri-mixup</cell><cell>Noise Regularization</cell><cell>MixSpeech</cell></row><row><cell>PER</cell><cell>23.1%</cell><cell>35.8%</cell><cell>22.0%</cell><cell>21.8%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/espnet/espnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">More training time for the baseline model cannot get better results according to our preliminary experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><ref type="bibr" target="#b13">[14]</ref> points out that complex prior distribution like the Dirichlet distribution to generate λ fails to provide further gain but increase the computation cost. Thus we choose λ =1/3.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on audio, speech, and language processing</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for lvcsr</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transformers with convolutional context for asr</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of hybrid ann/hmm models for automatic speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Trentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="91" to="126" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pitch-adaptive front-end features for robust children&apos;s asr</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shahnawazuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3459" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A perceptually inspired data augmentation method for noise robust cnn acoustic models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tóth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kovács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Compernolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Speech and Computer</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Almost unsupervised text to speech and automatic speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lrspeech: Extremely low-resource speech synthesis and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2802" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic mask for transformer based end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03010</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An overview of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="988" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network with mixup for environmental sound classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An investigation of mixup training strategies for acoustic models in asr</title>
		<author>
			<persName><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Khokhlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zatvornitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2903" to="2907" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for asr based on lattice-free mmi</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
				<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">NASA STI/Recon technical report n</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The design for the wall street journalbased csr corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop</title>
				<meeting><address><addrLine>Harriman, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">February 23-26, 1992, 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hkust/mts: A very large scale mandarin telephone speech corpus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Chinese Spoken Language Processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="724" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An investigation of deep neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7398" to="7402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A minimum-mean-square-error noise reduction algorithm on mel-frequency cepstra for robust speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="4041" to="4044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for noise reduction in robust asr</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
