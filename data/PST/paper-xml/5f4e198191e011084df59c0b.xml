<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pairwise Learning for Name Disambiguation in Large-Scale Heterogeneous Academic Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-30">30 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingyun</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<email>penghao@act.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
							<email>szwang@nuaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
							<email>dongxiangyu@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangxuan</forename><surname>Zhao</surname></persName>
							<email>zhaolx@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<region>Chicago</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Lehigh University</orgName>
								<address>
									<settlement>Bethlehem</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pairwise Learning for Name Disambiguation in Large-Scale Heterogeneous Academic Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-30">30 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2008.13099v1[cs.DL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Name disambiguation</term>
					<term>graph embedding</term>
					<term>pairwise learning</term>
					<term>heterogeneous information network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Name disambiguation aims to identify unique authors with the same name. Existing name disambiguation methods always exploit author attributes to enhance disambiguation results. However, some discriminative author attributes (e.g., email and affiliation) may change because of graduation or job-hopping, which will result in the separation of the same author's papers in digital libraries. Although these attributes may change, an author's co-authors and research topics do not change frequently with time, which means that papers within a period have similar text and relation information in the academic network. Inspired by this idea, we introduce Multi-view Attention-based Pairwise Recurrent Neural Network (MA-PairRNN) to solve the name disambiguation problem. We divided papers into small blocks based on discriminative author attributes and blocks of the same author will be merged according to pairwise classification results of MA-PairRNN. MA-PairRNN combines heterogeneous graph embedding learning and pairwise similarity learning into a framework. In addition to attribute and structure information, MA-PairRNN also exploits semantic information by meta-path and generates node representation in an inductive way, which is scalable to large graphs. Furthermore, a semantic-level attention mechanism is adopted to fuse multiple meta-path based representations. A Pseudo-Siamese network consisting of two RNNs takes two paper sequences in publication time order as input and outputs their similarity. Results on two real-world datasets demonstrate that our framework has a significant and consistent improvement of performance on the name disambiguation task. It was also demonstrated that MA-PairRNN can perform well with a small amount of training data and have better generalization ability across different research areas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Namesake problem <ref type="bibr" target="#b0">[1]</ref> poses a huge challenge on many applications, e.g., information retrieval, bibliographic data analysis. When searching for academic publications by author name, the results may contain a long list of publications of multiple authors with the same name. Some Qingyun Sun and Hao Peng contributed equally to this work. Jianxin Li is corresponding author. digital libraries (e.g., DBLP <ref type="foot" target="#foot_0">1</ref> and Google Scholar<ref type="foot" target="#foot_1">2</ref> ) list candidates after name disambiguation with corresponding homepage, email and affiliation to make it easier to obtain all publications of one particular author. The academic impacts of researchers are always measured by impacts of their publications in the research community. Therefore, it is important to keep publication data in digital libraries accurate, consistent, and up to date.</p><p>Name disambiguation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, which aims to identify unique persons with the same name, has been studied for decades but remains largely unsolved. Most of the existing solutions utilize author attributes, including name, affiliation, email, homepage, etc., to generate paper representations or further validate disambiguation results. However, these discriminative attributes, especially email and affiliation, may change because of graduation or job-hopping. We take Jian Pei, the well known leading researcher in data science, as an example to show the change of discriminative attributes in Fig. <ref type="figure" target="#fig_0">1</ref>. Jian Pei's papers from 2003 to 2005 are associated with jianpei@cse.buffalo.edu and State University of New York at Buffalo. His papers from 2005 to 2020 are associated with jpei@cs.sfu.ca and Simon Fraser University. The change of discriminative attributes may lead to the paper separation problem <ref type="bibr" target="#b3">[4]</ref>, i.e., papers of an author are regarded as belonging to different authors, which commonly occurs in digital libraries. To address this issue, name disambiguation methods should perform well even when discriminative attributes change.</p><p>Even though discriminative attributes may have changed, researchers often have a fixed co-author set and a few specific research areas that do not change frequently over time, which can also be exploited to solve the name disambiguation problem. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, even Jian Pei has different affiliations and emails in two time periods, his close co-authors (e.g., Jiawei Han, Ke Wang) are fixed and his research areas (e.g., Data mining, Time series) are also  There are several challenges that should be overcome: (1) Heterogeneity of academic network. The academic network is a heterogeneous network that contains multiple entities (e.g., author, paper, venue) and multiple relationships (e.g., writing, publishing) as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It is challenging to preserve diverse structural and semantic information simultaneously. (2) Inductive capability. Many real-world applications encounter a large number of new papers every day. It is challenging for name disambiguation methods to have the inductive capability that can generate representations of new papers efficiently. (3) Uncertain number of authors. It is challenging to determine the number of authors with the same name.</p><p>In existing clustering based name disambiguation methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, the number of authors (i.e., cluster size) is usually a pre-specified parameter. Current works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> did not efficiently handle the change of discriminative attributes and inductive paper embedding problem in the heterogeneous academic network simultaneously. In this work, we propose a novel Multi-view Attention-based Pair Recurrent Neural Network framework, namely MA-PairRNN, to solve name disambiguation problem. The intuitive idea is that an author's papers during a period of time should have more similar representations since the co-authors and research interests of most authors are consistent despite attributes change. Inspired by this idea, we take name disambiguation as a pairwise paper set classification problem that does not require to estimate the number of authors with the same name. We divide papers into small blocks according to discriminative author attributes to reduce the search space of the name disambiguation algorithm. Then small blocks are merged based on pairwise classification result and each block after merging is the paper set of an author. We represent each paper block as a sequence in publication time order and solve the pairwise classification problem by comparing sequence similarity. MA-PairRNN combines multiple multiview graph embedding layers, a semantic-level attention layer, and a Pseudo-Siamese recurrent neural network layer, to learn node embedding and node sequence pair similarity simultaneously. Specifically, multi-view graph embedding layer generates meta-path based embeddings of papers in the heterogeneous academic network. Then, semantic-level attention layer fuses these meta-path based embeddings into a vector. Finally, Pseudo-Siamese recurrent neural network layer learns the similarity of a node sequence pair. We elaborate on the three components as follows:</p><p>Multi-view graph embedding layer. Multi-view graph embedding layer incorporates meta-paths to capture rich semantic information in the heterogeneous network. The heterogeneous network is converted into multiple relation view according to meta-paths. For each view, we learn K aggregator functions to incorporate the K-hop neighborhood of each node. In this way, node embeddings are generated by enhancing node feature with semantics.</p><p>Semantic attention layer. Semantic attention layer captures the importance of meta-paths by an attention mechanism and fuse semantic information for specific tasks.</p><p>Pseudo-Siamese recurrent neural network layer. Pseudo-Siamese recurrent neural network composes of two recurrent neural networks, which are used to learn inherent relations of paper sequences. It takes two sequences of paper embedding as input and outputs their similarity.</p><p>The main contributions are summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will briefly review name disambiguation methods and graph embedding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Name Disambiguation</head><p>Name disambiguation methods can be divided into supervised <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, unsupervised <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref> and graph-based ones <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Graph-based works exploit graph topological features in the academic network to enhance the representation of papers. For instance, GHOST <ref type="bibr" target="#b1">[2]</ref> constructs document graph based on co-authorship. <ref type="bibr" target="#b4">[5]</ref> leverages only relational data in the form of anonymized graphs to preserve author privacy. Pairwise classification methods are applied to estimate the probability of a pair of author mentions belonging to the same author and are essential in the name disambiguation task. <ref type="bibr" target="#b5">[6]</ref> first learns representation for every name mention in a pairwise or tripletwise way and refines the representation by a graph auto-encoder, but this method neglects linkage between paper and author and coauthorship. <ref type="bibr" target="#b6">[7]</ref> addresses the pairwise classification problem by extracting both structure-aware features and global features without considering semantic features. In this paper, we focus on the paper set level pairwise classification problem and exploit attribute, structure, and semantic features to form better representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Embedding</head><p>Graph embedding aims to represent a graph as a low dimensional vector while preserving graph structure and properties. Recently, Graph Neural Network (GNN) <ref type="bibr" target="#b9">[10]</ref> has attracted rising attention due to effective representation ability. While most GNN works <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> focus on transductive setting, there have been some recent works adopting an inductive learning setting. DeepGL <ref type="bibr" target="#b12">[13]</ref> aggregates a set of base graph features by relational functions that can generalize across networks. GraphSage <ref type="bibr" target="#b13">[14]</ref> samples a fixed number of neighbors and generate node embeddings by aggregating their features. Both DeepGL and GraphSage are designed for homogeneous graphs. LAN <ref type="bibr" target="#b14">[15]</ref> aggregates neighbors with both rule-based and network-based attention weights for knowledge graphs.</p><p>Heterogeneous information networks <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> have been studied in recent years. Meta-path is designed to preserve diverse semantic information of node type and edge type <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. GTN <ref type="bibr" target="#b22">[23]</ref> converts heterogeneous graph to new graph structures which involve identifying task-specific meta-paths and multi-hop connections. HAN <ref type="bibr" target="#b23">[24]</ref> includes both nodelevel and semantic-level attention to take the importance of nodes and meta-paths into consideration simultaneously.</p><p>In this paper, we propose an inductive graph embedding method utilizing rich heterogeneous information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>In this section, we formally define Heterogeneous Academic Network and the problem of Name Disambiguation.</p><p>Definition 1 (Heterogeneous Academic Network): Heterogeneous Academic Network is defined as G = {V, E}, where V and E denote the set of nodes and edges, respectively. A Heterogeneous Academic Network is associated a node type mapping function f v : V → O and an edge type mapping function f e : E → R. O = {P, A, T, V } denotes node types set and R = {A writes P, P cites P, P is related to T, P is published in V} denotes edge types set, where P, A, T, V denote the type of Paper, Author, Topic and Venue, respectively.</p><p>Definition 2 (Name Disambiguation): Given a name a, D a = {d a 1 , d a 2 , . . . , d a N } is a set of papers with name mention a. Every paper d a k consists of some metadata including paper attributes (e.g. title, year, venue, keywords) and author attributes (e.g. name, email, affiliation). The objective of name disambiguation is to partition all name mentions into a set of unique authors C a = {c a 1 , c a 2 , . . . , c a n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Architecture</head><p>In this section, we propose a novel framework named MA-PairRNN for name disambiguation. As described above, the main intuition is that papers of the same author within a period should have similar representations in the academic network since the author's research and scholar relation is consistent. We divide the set of papers D a into small blocks by discriminative author attributes in metadata. These small blocks will be merged based on pairwise classification results of MA-PairRNN. First, the multi-view inductive graph embedding layer is designed to generate the paper representation of each meta-path. Then a semantic attention layer is designed to learn importance of metapaths and fuse meta-path based representations. Finally, papers in every block are arranged as a sequence denoted as s ∈ S according to their publication time. Two sequences of paper embedding are fed into a Pseudo-Siamese network with two RNNs for pairwise similarity learning. The overall architecture of MA-PairRNN is shown in Fig. <ref type="figure" target="#fig_2">3</ref> C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Multi-View Graph Embedding Layer</head><p>Multi-view graph embedding layer generates node representations inductively by learning a function to aggregate attribute and topology information from local neighborhoods. To exploit rich semantic information in the heterogeneous academic network, we proposed the concept of metapath based view. Given a heterogeneous academic network G = {V, E} and a meta-path p, a meta-path based view G p is derived from a type of proximity or relationship between nodes characterized by a meta-path. It can capture different aspects of structure information through meta-paths and is potential to add new nodes dynamically.</p><p>For each meta-path based view, similar to GraphSage <ref type="bibr" target="#b13">[14]</ref>, node representations are generated by aggregating features of meta-path based neighbors and propagating information across K layers. Node v's representation based on meta-path p is generated as below. First, in the k-th layer, each node aggregates its own representation and representations of its 1-hop neighborhood N i generated by (k-1)-th layer into a single vector z</p><formula xml:id="formula_0">(k) p (N i ) as (1): z (k) p (N i ) = mean({z (k−1) p (v j ), ∀v j ∈ v i ∪ N i }),<label>(1)</label></formula><p>where z</p><formula xml:id="formula_1">(k−1) p (v j ) denotes representation of v j in (k-1)-th layer. When k = 0, z<label>(0)</label></formula><p>p (v j ) is defined as original feature x(v j ) of v j . Then a weight matrix W p are used to transfer information between layers as (2):</p><formula xml:id="formula_2">z (k) p (v i ) = σ(W (k) p • z (k−1) p (N i ) + b (k) p ).<label>(2)</label></formula><p>To extend the algorithm to a mini-batch setting, we first sample the l-egonet of papers in the batch. The l-egonet of node v is defined as the set of its l-hop neighbors and all edges between nodes in the set. For each batch, multiview subgraphs are constructed based on the union of legonets of all paper nodes in this batch. Then we generate meta-path based representation of every node in these multiview subgraphs. For more convenient notation, we denote v i 's final representation based meta-path p after K layers as</p><formula xml:id="formula_3">z p (v i ) ≡ z (K) p (v i ), where z p (v i ) ∈ R d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Semantic Attention Layer</head><p>For each paper, multiple meta-path based representations are obtained and they can collaborate with each other. Since we assume that the importance of meta-paths varies, an attention mechanism is adopted to capture their contribution and fuse meta-path based node representations.</p><p>We first introduce a meta-path preference vector a p ∈ R |P| * d ′ for each meta-path p to guide the semantic attention mechanism. For meta-path based representation z (k) p and meta-path preference vector a p , the more similar they are, the greater weight will be assigned to z </p><formula xml:id="formula_4">z ′ p (v i ) = σ(W p • z p (v i ) + b p ).<label>(3)</label></formula><p>where W p ∈ R |P| * d ′ is the weight parameter and b p ∈ R d ′ is the bias parameter of transformation. z ′ p (v i ) ∈ R d ′ is the node representation of v i based meta-path p after transformation. The similarity of transformed representation vector and preference vector ω p (v i ) is calculated as (4):</p><formula xml:id="formula_5">ω p (v i ) = a T p • z ′ p (v i ) a p • z ′ p (v i ) , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where • is the L2 normalization of vectors. The weight of meta-path p for node v i is defined using a softmax unit as follows:</p><formula xml:id="formula_7">ω ′ p (v i ) = exp(ω p (v i )) p ′ ∈P exp(ω p ′ (v i )) . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Final representation of node v i is generated by fusing all meta-path based representations in the weighted sum form:</p><formula xml:id="formula_9">z(v i ) = p ′ ∈P ω ′ p ′ (v i ) * z p ′ (v i )).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Pseudo-Siamese Recurrent Neural Network Layer</head><p>We designed a Pseudo-Siamese recurrent neural network layer to capture inherent relations of papers and measure similarity of two paper sets. Pseudo-Siamese recurrent neural network layer is a Pseudo-Siamese network consisting of two RNNs with different parameters to generate representations of two node sequences. Specifically, we feed two sequence of paper embeddings into two RNNs respectively. The learned paper embedding of the paper is taken as the input of RNN units. The output of each RNN unit can be formalized as:</p><formula xml:id="formula_10">h t = RNN(z t , θ t ),<label>(7)</label></formula><p>where θ t means parameters of RNN unit. Here we apply the popular LSTM to capture inherent relations of paper sequences and learn their similarity. Note that the paper sequence published earlier is in published time order and the other sequence is in reverse. This setting is based on the assumption that an author's research topics and co-authors are stable during the period of attribute changing. All outputs of RNN units are aggregated by a GlobalP ool function to generate the representation of paper sequence as follows:</p><formula xml:id="formula_11">h = GlobalP ool({h t , t = 1, 2, • • • , |s|}),<label>(8)</label></formula><p>where |•| denotes the length of sequence. We apply a simple averaging strategy as the GlobalP ool function here. The final representations of two paper sequences h (1) and h (1)  are concatenated and then fed into a multiple fully connected neural network:</p><formula xml:id="formula_12">ŷs = σ(MLP([h (1) , h (2) ])),<label>(9)</label></formula><p>where σ(•)denotes the softmax function and [•, •] represents the concatenation operation. Since our task is classification, the loss function L classif y can be defined as the Cross-Entropy over all labeled node sequence pairs between the ground-truth and the predict results. The proposed framework can be trained on a set of example pairs. For each pair of paper sequences, a cosine score function is applied to measure the similarity of the two paper sequence representations as <ref type="bibr" target="#b9">(10)</ref>.</p><p>L sim = sim(h (1) , h (2) ) = h (1) • h (2)  h (1) • h (2) .</p><p>The pairwise similarity loss function encourages node sequences of the same author to have similar representations, and enforces that of different authors to be highly distinct.</p><p>The model is then trained to minimize the sum of classification loss as follows:</p><formula xml:id="formula_14">L = L classif y + η * L sim ,<label>(11)</label></formula><p>where η denotes the coefficient of pair similarity loss. The overall process of MA-PairRNN is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>For our experiments we used two datasets: Aminer-AND and Semantic Scholar.   </p><formula xml:id="formula_15">, G p2 , • • • , G pM }; 4 z (0) p (v i ) = x(v i ), ∀v i ∈ V; 5 while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics and Baselines</head><p>We apply pairwise Precision, Recall and F1 score in Aminer-AND and apply averaged Accuracy, F1 score and AUC in Semantic Scholar to measure the performance of all methods. We compare with attribute based methods as well as attribute and structure based methods to demonstrate the effectiveness of our model. To verify the effectiveness of each component including meta-path based views, semanticlevel attention and Pseudo-Siamese structure, we also test three variants of MA-PairRNN.</p><p>• MLP <ref type="bibr" target="#b24">[25]</ref>: It's s multilayer perceptron that directly projecting input features into a low dimensional vector.</p><p>• Deepwalk <ref type="bibr" target="#b25">[26]</ref>: Deepwalk captures contextual information of neighborhood via uniform random walks for node embedding in homogeneous network.</p><p>• GraphSage <ref type="bibr" target="#b13">[14]</ref>: GraphSage samples node neighborhoods to generate node embeddings for unseen data in an inductive way and is designed for homogeneous network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>In Aminer-AND, the selected meta-paths of our method consist of Paper-Author-Paper, Paper-Topic-Paper and Paper-Venue-Paper. We use the author's affiliation as the discriminative attribute to separate papers into small blocks and we use the same trainset and testset as in <ref type="bibr" target="#b5">[6]</ref>.</p><p>In Semantic Scholar, the selected meta-paths of our method consist of Paper-Paper, Paper-Author-Paper, Paper-Topic-Paper, and Paper-Venue-Paper. We use the author's email as the discriminative attribute to separate papers into small blocks. To evaluate the learning ability of models, we test them on Semantic Scholar with different training ratios {20%, 40%, 60%, 80%}.</p><p>The common training parameters are set as learning rate = 5e − 4 and dropout = 0.2. The node embedding dimension is set to 64 and the classifiers of all methods is a three-layer fully-connected neural network with a ReLU function. In our proposed model MA-PairRNN LSTM , K is set to 2 and the dimension of meta-path preference vector a is set to 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and Discussions</head><p>The performance of different methods on some sampled names of Aminer-AND is reported in Table <ref type="table" target="#tab_3">II</ref>. The results on Semantic Scholar is reported in Table <ref type="table" target="#tab_3">III</ref>. Major findings from experimental results can be summarized as follows:</p><p>Performance Comparison. As shown in Table <ref type="table" target="#tab_3">II</ref> and Table <ref type="table" target="#tab_3">III</ref>, by incorporating attribute, structure and semantic information, MA-PairRNN LSTM outperforms all baselines in both datasets. Generally, GNN based methods that combine the attribute and structure information usually perform better than those methods which only exploit attribute information. Compared to simply concatenate representations of nodes, the Pseudo-Siamese RNN network can better extract inherent relations of paper sequence. Compared to taking the graph as homogeneous, M-PairRNN LSTM and MA-PairRNN LSTM can exploit semantic information successfully and show their superiority. It demonstrates that combined use of attribute, structure, and semantic features better capture the similarities between papers. In addition, the semanticlevel attention mechanism in MA-PairRNN LSTM can exploit semantic information more properly.   The performances of all methods get worse as the training ratio decrease. Our method MA-PairRNN LSTM and its vari-ants suffer less performance degradation than others, which shows better learning ability.</p><p>Siamese Network v.s Pseudo-Siamese Network. As mentioned above, Pseudo-Siamese neural network component consists of two RNNs with different parameters. We also test three variations including a Pseudo-Siamese network with two BiLSTM (MA-PairRNN BiLSTM ), a Siamese network with two parameter-shared LSTM (MA-RNN LSTM ), and a Siamese network with two parameter-shared BiL-STM (MA-RNN BiLSTM ). Results on Semantic Scholar are shown in Table <ref type="table">.</ref> IV. We can see that Pseudo-Siamese Network models have a better performance than the other two Siamese Network models. Based on our assumption that papers during the period of discriminative attributes changing have similar text and structure features, the paper sequence published earlier is fed into RNN in publication time order and the other is in reverse order. Pseudo-Siamese network may better capture the changing trend of research topic and scholar relationship.</p><p>Impact of Different Meta-paths. To verify the ability of semantic-level attention, we report F1 scores of MA-PairRNN LSTM using single meta-path and corresponding attention values on Semantic Scholar in Fig. <ref type="figure" target="#fig_10">7</ref>. Obviously,  there is a positive correlation between the performance of each meta-path and its attention value. Among four metapaths, MA-PairRNN LSTM gives PVP the highest weight, which means that PVP is considered as the most critical meta-path in paper representation. It makes sense because authors research areas are highly correlated with venues where their papers are published. Meanwhile, PP is also given a high weight. It also makes sense because author's papers are often closely related and have similar references.</p><p>Generalization ability across research areas. On Semantic Scholar, our models are trained on papers of medical area. To verify the generalization ability of models across different research areas, we collected data of 100 authors from biology, chemistry, computer science, and mathematics area, respectively. The performance of all models on these  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameters Analysis</head><p>In this section, we will investigate how dimension of node embedding and attention preference vector and coefficient of similarity loss can affect classification performance. The results on Semantic Scholar are reported in Fig. <ref type="figure" target="#fig_12">9</ref>.</p><p>Dimension of the final node embedding z. The representation ability of graph embedding methods is affected by the dimension of node embedding z. We explore its impact with various dimension {16, 32, 64, 128, 256}. As shown in Fig. <ref type="figure" target="#fig_12">9</ref> (a), the performance firstly improves with the increase of node embedding dimension, then degenerates slowly, and achieves the best performance at the dimension of 64. The reason may be that larger dimension could introduce some additional redundancies. Dimension of semantic attention vector a. We evaluate the effect of semantic attention vector a's dimension in the set of {16, 32, 64, 128, 256}. As shown in Fig. <ref type="figure" target="#fig_12">9 (b)</ref>, the F1 score has minor changes, which shows that MA-PairRNN LSTM is not very sensitive to the dimension of attention preference vector.</p><p>Coefficient η of cosine similarity loss. The impact of similarity loss item is controlled by η. We vary η ∈ {0, 0.25, 0.5, 1, 1.5, 2, 4}. As shown in Fig. <ref type="figure" target="#fig_12">9</ref> (c), optimal performance is obtained near η = 1, indicating that η cannot be set too small or too large in order to prevent overfitting and underfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Case Study</head><p>We specifically choose three author variants named Jian Pei in Semantic Scholar as a study case and we denote them as Jian Pei 1, Jian Pei 2, Jian Pei 3. Statistics of selected three author variants are shown in Table <ref type="table">.</ref> V. Our model classifies Jian Pei 1 and Jian Pei 2 as the same person while Jian Pei 3 is another person, which is consistent with the ground truth. We visualize the subgraph of the academic network that three author variants are in. The visualized subgraph includes papers and co-authors of the three author variants, and topics their papers related to. Papers of three author variants are colored blue, green, and red respectively and other nodes are colored by their type. Paper nodes of Jian Pei 1 colored blue and paper nodes of Jian Pei 2 colored green tend to be closely connected physically and many of them are connected by same topics (e.g., Data mining, Social Network) and same venues (e.g., KDD, TKDE). Jian Pei 3's paper nodes are connected to paper nodes of the other two by topic nodes such as Algorithm and Simulation experiment, which are used in many research areas.</p><p>V. CONCLUSION AND FUTURE WORK In this paper, we propose MA-PairRNN, a novel pairwise node sequence classification framework for name disambiguation, in which multi-view graph embedding layer is designed to generate node representation inductively, and Pseudo-Siamese recurrent neural network is designed to learn sequence pair similarity. Our proposed method can learn node representation and sequence pair similarity simultaneously, and can scale to large graphs for its inductive capability. Experimental results on two real-world datasets demonstrate the effectiveness of our method. By analyzing the learned attention weights of meta-paths, MA-PairRNN has proven its potentially good interpretability. By testing on data of unseen areas, MA-PairRNN has also proven its good generalization ability. In the future, we plan to leverage hierarchical clustering to address the problem that an author has diverse research areas and works with non-overlapping sets of co-authors corresponding to each research area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of the change of Jian Pei's discriminative attributes.</figDesc><graphic url="image-1.png" coords="2,32.09,72.19,288.82,162.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Academic network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An overview of our overall network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(k) p . We use a non-linear function to transform the d-dimension meta-path based embedding into d ′ -dimension as (3):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Length Statistics of Paper sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>•</head><label></label><figDesc>Zhang et al. [5]: This method learns paper embedding by sampling triplets from three graphs constructed by relations of authors and papers and cluster them by hierarchical agglomerative algorithm.• GHOST<ref type="bibr" target="#b1">[2]</ref>: GHOST use affinity propagation algorithm for clustering on a co-authors graph where the node distance is measured based on the number of valid paths. • Louppe et al. [3]: This method trains a pairwise distance function based on similarity features and use a semisupervised HAC algorithm for clustering. • Aminer [6]: This method first learns supervised global embeddings and then refines the global embeddings for each candidate set based on the local contexts. • Kim et al. [7]: It is a hybrid pairwise classification method which generates paper representation by extracting both structure-aware features and global features. • PairRNN LSTM : A variation of MA-PairRNN LSTM , which directly feed node feature into a Pseudo-Siamese recurrent neural network layer with two LSTMs. • G-PairRNN LSTM : A variation of MA-PairRNN LSTM , which neglects the heterogeneity of academic network and generates representation on the original graph. • M-PairRNN LSTM : A variation of MA-PairRNN LSTM , which removes semantic-level attention layer and assigns the same importance to each meta-path. • MA-PairRNN LSTM : The proposed model that fuses attribute, structure and semantic feature for node embedding generation with an semantic attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>shows F1 scores of MA-PairRNN LSTM on different partition versions of Semantic Scholar with training ratio of 80%. After adequate rounds of training, the performance of MA-PairRNN LSTM on each dataset partition version has</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Performance of MA-PairRNNLSTM on different Semantic Scholar partition version with training ratio of 80%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>F1Figure 6 .</head><label>6</label><figDesc>Figure 6. Performance with different training ratio on Semantic Scholar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Performance of single meta-path and corresponding attention value.</figDesc><graphic url="image-1324.png" coords="8,361.29,276.19,184.32,102.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Performance (F1 score %) in different research areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Parameter sensitivity: Dimension of node embedding z, Dimension of semantic attention vector a and Coefficient η of cosine similarity loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The overall process of MA-PairRNN Input: Paper set D, heterogeneous graph G = {V, E}, node features {x(v), ∀v ∈ V}, meta-path set P = {p 1 , p 2 , • • • , p M }, number of multi-view graph embedding layer K Output: meta-path based node representation {z p1 , z p1 , • • • , z p1 } 1 Separate paper set D into small blocks according discriminative author attributes; 2 Arrange papers in every block as sequence s ∈ S; 3 Construct meta-path based view {G p1</figDesc><table /><note>• Aminer-AND<ref type="bibr" target="#b5">[6]</ref>: This dataset contains 70,285 records of 12,798 unique authors with 100 ambiguous name references.Algorithm 1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II THE</head><label>II</label><figDesc>DETAILED RESULTS (%) ON AMINER-AND</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Attr.</cell><cell></cell><cell></cell><cell>Struc.</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attr. + Struc.</cell><cell></cell><cell></cell><cell cols="3">Attr. + Struc. + Sem.</cell></row><row><cell>Name</cell><cell></cell><cell></cell><cell>Louppe et al.</cell><cell></cell><cell></cell><cell>Zhang et al.</cell><cell></cell><cell></cell><cell>GHOST</cell><cell></cell><cell></cell><cell>Aminer</cell><cell></cell><cell cols="3">MA-PairRNNLSTM</cell></row><row><cell></cell><cell></cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell cols="2">Hongbin Li</cell><cell>19.48</cell><cell>85.96</cell><cell>31.77</cell><cell>54.66</cell><cell>53.05</cell><cell>53.84</cell><cell>56.29</cell><cell>29.12</cell><cell>38.39</cell><cell>77.20</cell><cell>69.21</cell><cell>72.99</cell><cell>88.89</cell><cell cols="2">65.98</cell><cell>75.74</cell></row><row><cell>Hua Bai</cell><cell></cell><cell>36.39</cell><cell>41.33</cell><cell>38.70</cell><cell>58.58</cell><cell>35.90</cell><cell>44.52</cell><cell>83.06</cell><cell>29.54</cell><cell>43.58</cell><cell>71.49</cell><cell>39.73</cell><cell>51.08</cell><cell>89.22</cell><cell cols="2">70.54</cell><cell>78.79</cell></row><row><cell>Kexin Xu</cell><cell></cell><cell>91.26</cell><cell>98.35</cell><cell>94.67</cell><cell>90.02</cell><cell>82.47</cell><cell>86.08</cell><cell>92.90</cell><cell>28.52</cell><cell>43.64</cell><cell>91.37</cell><cell>98.64</cell><cell>94.87</cell><cell>85.19</cell><cell cols="2">71.88</cell><cell>77.97</cell></row><row><cell>Lu Han</cell><cell></cell><cell>30.25</cell><cell>46.65</cell><cell>36.70</cell><cell>47.88</cell><cell>20.62</cell><cell>28.82</cell><cell>69.72</cell><cell>17.39</cell><cell>27.84</cell><cell>51.78</cell><cell>28.05</cell><cell>36.39</cell><cell>92.43</cell><cell cols="2">69.62</cell><cell>79.42</cell></row><row><cell>Lin Huang</cell><cell></cell><cell>24.86</cell><cell>71.32</cell><cell>36.87</cell><cell>71.84</cell><cell>34.17</cell><cell>46.31</cell><cell>86.15</cell><cell>17.25</cell><cell>28.74</cell><cell>77.10</cell><cell>32.87</cell><cell>46.09</cell><cell>88.26</cell><cell cols="2">73.44</cell><cell>80.17</cell></row><row><cell cols="2">Meiling Chen</cell><cell>58.32</cell><cell>47.14</cell><cell>52.14</cell><cell>59.36</cell><cell>28.80</cell><cell>38.79</cell><cell>86.11</cell><cell>23.85</cell><cell>37.35</cell><cell>74.93</cell><cell>44.70</cell><cell>55.99</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Min Zheng</cell><cell>25.86</cell><cell>32.67</cell><cell>28.87</cell><cell>54.76</cell><cell>19.70</cell><cell>28.98</cell><cell>80.50</cell><cell>15.21</cell><cell>25.58</cell><cell>57.65</cell><cell>22.35</cell><cell>32.21</cell><cell>86.07</cell><cell cols="2">82.03</cell><cell>84.00</cell></row><row><cell>Qiang Shi</cell><cell></cell><cell>35.31</cell><cell>47.18</cell><cell>40.39</cell><cell>43.84</cell><cell>36.94</cell><cell>40.10</cell><cell>53.72</cell><cell>26.80</cell><cell>35.76</cell><cell>52.20</cell><cell>36.15</cell><cell>42.72</cell><cell>80.25</cell><cell cols="2">69.15</cell><cell>74.29</cell></row><row><cell>Rong Yu</cell><cell></cell><cell>38.85</cell><cell>91.43</cell><cell>54.53</cell><cell>65.48</cell><cell>40.85</cell><cell>50.32</cell><cell>92.00</cell><cell>36.41</cell><cell>52.17</cell><cell>89.13</cell><cell>46.51</cell><cell>61.12</cell><cell>90.67</cell><cell cols="2">68.69</cell><cell>78.16</cell></row><row><cell>Tao Deng</cell><cell></cell><cell>40.46</cell><cell>51.38</cell><cell>45.27</cell><cell>53.04</cell><cell>29.89</cell><cell>38.23</cell><cell>73.33</cell><cell>24.50</cell><cell>36.73</cell><cell>81.63</cell><cell>43.62</cell><cell>56.86</cell><cell>88.42</cell><cell cols="2">65.12</cell><cell>75.00</cell></row><row><cell>Wei Quan</cell><cell></cell><cell>37.86</cell><cell>63.41</cell><cell>47.41</cell><cell>64.45</cell><cell>47.66</cell><cell>54.77</cell><cell>86.42</cell><cell>27.80</cell><cell>42.07</cell><cell>53.88</cell><cell>39.02</cell><cell>45.26</cell><cell>75.76</cell><cell cols="2">78.13</cell><cell>76.92</cell></row><row><cell cols="2">Xudong Zhang</cell><cell>72.38</cell><cell>79.83</cell><cell>75.92</cell><cell>70.20</cell><cell>23.35</cell><cell>35.04</cell><cell>85.75</cell><cell>7.23</cell><cell>13.34</cell><cell>62.40</cell><cell>22.54</cell><cell>33.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Xu Xu</cell><cell></cell><cell>22.55</cell><cell>64.40</cell><cell>33.40</cell><cell>48.16</cell><cell>41.87</cell><cell>44.80</cell><cell>61.34</cell><cell>21.79</cell><cell>32.15</cell><cell>74.18</cell><cell>45.86</cell><cell>56.68</cell><cell>78.68</cell><cell cols="2">79.08</cell><cell>78.88</cell></row><row><cell cols="2">Yanqing Wang</cell><cell>29.64</cell><cell>79.08</cell><cell>43.11</cell><cell>60.40</cell><cell>51.97</cell><cell>55.87</cell><cell>80.79</cell><cell>40.39</cell><cell>53.86</cell><cell>71.52</cell><cell>75.33</cell><cell>73.37</cell><cell>77.42</cell><cell cols="2">64.86</cell><cell>70.59</cell></row><row><cell>Yong Tian</cell><cell></cell><cell>32.08</cell><cell>63.71</cell><cell>42.67</cell><cell>70.74</cell><cell>56.85</cell><cell>63.04</cell><cell>86.94</cell><cell>54.58</cell><cell>67.06</cell><cell>76.32</cell><cell>51.95</cell><cell>61.82</cell><cell>87.80</cell><cell cols="2">70.59</cell><cell>78.26</cell></row><row><cell>Average</cell><cell></cell><cell>57.09</cell><cell>77.22</cell><cell>63.10</cell><cell>70.63</cell><cell>59.53</cell><cell>62.81</cell><cell>81.62</cell><cell>40.43</cell><cell>50.23</cell><cell>77.96</cell><cell>63.03</cell><cell>67.79</cell><cell>87.93</cell><cell cols="2">77.74</cell><cell>82.53</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="11">QUANTITATIVE RESULTS AND STANDARD DEVIATION (%) ON SEMANTICSCHOLAR</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Attr.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attr. + Struc.</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Attr. + Struc. + Sem.</cell></row><row><cell>Metrics</cell><cell cols="2">Training</cell><cell>MLP</cell><cell cols="3">PairRNNLSTM Deepwalk</cell><cell cols="2">GraphSage</cell><cell>Aminer</cell><cell cols="2">Kim et al.</cell><cell cols="2">G-PairRNNLSTM</cell><cell cols="2">M-PairRNNLSTM</cell><cell>MA-PairRNNLSTM</cell></row><row><cell></cell><cell>20%</cell><cell cols="2">92.24±1.36</cell><cell cols="2">94.78±0.74</cell><cell cols="6">92.26±0.62 95.56±0.35 96.73±0.35 96.88±0.46</cell><cell cols="2">95.93±0.57</cell><cell>96.40±0.54</cell><cell></cell><cell>96.95±1.36</cell></row><row><cell>Accuracy</cell><cell>40% 60%</cell><cell cols="2">93.88±1.01 94.43±0.69</cell><cell cols="2">96.46±1.12 97.34±1.08</cell><cell cols="6">93.85±0.65 96.27±0.18 96.59±0.33 96.80±0.16 94.47±0.46 97.49±0.32 97.48±0.24 97.54±0.35</cell><cell cols="2">96.34±0.61 97.19±0.71</cell><cell>96.73±0.69 97.56±0.26</cell><cell></cell><cell>97.01±0.45 97.91±0.18</cell></row><row><cell></cell><cell>80%</cell><cell cols="2">94.24±1.42</cell><cell cols="2">97.56±0.26</cell><cell cols="6">94.50±0.74 97.85±0.29 97.75±0.23 97.38±0.23</cell><cell cols="2">97.88±0.84</cell><cell>97.81±0.38</cell><cell></cell><cell>98.50±0.41</cell></row><row><cell></cell><cell>20%</cell><cell cols="2">92.14±1.49</cell><cell cols="2">95.05±0.66</cell><cell cols="6">92.37±0.54 95.62±0.35 96.84±0.32 96.94±0.54</cell><cell cols="2">96.10±0.53</cell><cell>96.54±0.50</cell><cell></cell><cell>97.04±1.30</cell></row><row><cell>F1 Score</cell><cell>40% 60%</cell><cell cols="2">93.91±1.00 94.43±0.74</cell><cell cols="2">96.58±1.06 97.40±1.05</cell><cell cols="6">93.92±0.59 96.33±0.17 96.66±0.33 96.84±0.16 94.18±0.77 97.54±0.31 97.53±0.23 97.59±0.34</cell><cell cols="2">96.48±0.57 97.28±0.63</cell><cell>96.84±0.64 97.63±0.23</cell><cell></cell><cell>97.12±0.43 97.96±0.17</cell></row><row><cell></cell><cell>80%</cell><cell cols="2">94.24±1.42</cell><cell cols="2">97.66±0.27</cell><cell cols="6">94.57±0.75 97.90±0.30 97.83±0.20 97.42±0.24</cell><cell cols="2">97.94±0.81</cell><cell>97.81±0.23</cell><cell></cell><cell>98.54±0.37</cell></row><row><cell></cell><cell>20%</cell><cell cols="2">92.24±1.36</cell><cell cols="2">97.61±0.38</cell><cell cols="6">92.26±0.62 96.10±1.93 98.02±0.59 97.48±1.94</cell><cell cols="2">92.85±9.56</cell><cell>97.96±1.13</cell><cell></cell><cell>98.12±1.08</cell></row><row><cell>AUC</cell><cell>40% 60%</cell><cell cols="2">93.88±1.01 94.43±0.69</cell><cell cols="2">95.38±5.23 98.54±0.39</cell><cell cols="6">93.85±0.65 96.63±1.49 97.29±0.38 95.39±5.72 94.27±0.85 97.91±0.86 98.32±0.56 97.73±1.13</cell><cell cols="2">97.65±0.86 98.86±0.43</cell><cell>95.57±6.60 99.07±0.30</cell><cell></cell><cell>98.55±1.05 99.31±0.45</cell></row><row><cell></cell><cell>80%</cell><cell cols="2">94.24±1.42</cell><cell cols="2">98.43±0.57</cell><cell cols="6">94.50±0.74 98.12±0.20 98.73±0.36 97.70±0.59</cell><cell cols="2">98.76±0.74</cell><cell>98.27±0.22</cell><cell></cell><cell>99.18±0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON (%) OF DIFFERENT SEQUENCE REPRESENTATION MODEL ON SEMANTIC SCHOLAR</figDesc><table><row><cell>Models</cell><cell>Accuracy</cell><cell>F1 score</cell><cell>AUC</cell></row><row><cell>MA-PairRNNLSTM</cell><cell>98.50</cell><cell>98.54</cell><cell>99.18</cell></row><row><cell>MA-PairRNNBiLSTM</cell><cell>98.47</cell><cell>98.52</cell><cell>99.17</cell></row><row><cell>MA-RNNLSTM</cell><cell>97.88</cell><cell>97.96</cell><cell>99.00</cell></row><row><cell>MA-RNNBiLSTM</cell><cell>98.25</cell><cell>98.28</cell><cell>99.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://dblp.uni-trier.de/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://scholar.google.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.semanticscholar.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by the the National Key R&amp;D Program of China (2018YFC0830804), NSFC No.61872022, NSF of Jiangsu Province BK20171420, NSF of Guangdong Province (2017A030313339) and CCF-Tencent Open Research Fund, and in part by NSF under grants III-1526499, III-1763325, III-1909323, and SaTC-1930941.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two supervised learning approaches for name disambiguation in author citations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCDL</title>
		<imprint>
			<biblScope unit="page" from="296" to="305" />
			<date type="published" when="2004">2004</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On graph-based name disambiguation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JDIQ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ethnicity sensitive author disambiguation using semi-supervised learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Maguire</surname></persName>
		</author>
		<editor>KESW</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="272" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective and scalable solutions for mixed and split citation problems in digital libraries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IQIS</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Name disambiguation in aminer: Clustering, maintenance, and human in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid deep pairwise classification for author name disambiguation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2369" to="2372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Author disambiguation by hierarchical agglomerative clustering with adaptive stopping criterion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Dragut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="741" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep feature learning for graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08829</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Logic attention based neighborhood aggregation for inductive knowledge graph embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7152" to="7159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mmrate: inferring multi-aspect diffusion networks with multi-pattern cascades</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1246" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving stock market prediction via heterogeneous information fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KBS</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="236" to="247" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarialnas: Adversarial neural architecture search for gans</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5680" to="5689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-information source hin for medical concept embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAKDD</title>
		<imprint>
			<biblScope unit="page" from="396" to="408" />
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained event categorization with heterogeneous graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3238" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hetespaceywalk: a heterogeneous spacey random walk for heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">970</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilayer perceptron, fuzzy sets, and classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="683" to="697" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
