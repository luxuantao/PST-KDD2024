<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The limits and potentials of deep learning for robotics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
							<email>niko.suenderhauf@roboticvision.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technol-ogy (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Brock</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Robotics and Biology Laboratory</orgName>
								<orgName type="institution">Technische Universit?t Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><surname>Scheirer</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J?rgen</forename><surname>Leitner</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technol-ogy (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Oxbotica Ltd</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Milford</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technol-ogy (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Corke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technol-ogy (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Queensland University of Technology (QUT)</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>4000 QLD</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The limits and potentials of deep learning for robotics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1177/0278364918770733</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Robotics</term>
					<term>deep learning</term>
					<term>machine learning</term>
					<term>robotic vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of roboticsspecific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A robot is an inherently active agent that interacts with the real world, and often operates in uncontrolled or detrimental conditions. Robots have to perceive, decide, plan, and execute actions, all based on incomplete and uncertain knowledge. Mistakes can lead to potentially catastrophic results that will not only endanger the success of the robot's mission, but can even put human lives at risk, e.g. if the robot is a driverless car.</p><p>The application of deep learning in robotics therefore motivates research questions that differ from those typically addressed in computer vision: How much trust can we put in the predictions of a deep learning system when misclassifications can have catastrophic consequences? How can we estimate the uncertainty in a deep network's predictions and how can we fuse these predictions with prior knowledge and other sensors in a probabilistic framework? How well does deep learning perform in realistic unconstrained open-set scenarios where objects of unknown class and appearance are regularly encountered?</p><p>If we want to use data-driven learning approaches to generate motor commands for robots to move and act in the world, we are faced with additional challenging questions: How can we generate enough high-quality training data? Do we rely on data solely collected on robots in real-world scenarios or do we require data augmentation through simulation? How can we ensure the learned policies transfer well to different situations, from simulation to reality, or between different robots?</p><p>This leads to further fundamental questions: How can the structure, the constraints, and the physical laws that govern robotic tasks in the real world be leveraged and exploited by a deep learning system? Is there a fundamental difference between model-driven and data-driven problem solving, or are these rather two ends of a spectrum?</p><p>This paper explores some of the challenges, limits, and potentials for deep learning in robotics. The invited speakers and organizers of the workshop on The Limits and Potentials of Deep Learning for Robotics at the 2016 edition of the Robotics: Science and Systems (RSS) conference <ref type="bibr">(S?nderhauf et al., 2016)</ref> provide their thoughts and opinions, and point out open research problems and questions that are yet to be answered. We hope this paper will offer the interested reader with an overview of where we believe important research needs to be done, and where deep learning can have an even bigger impact in robotics over the coming years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Uncertainty estimation.</head><p>To fully integrate deep learning into robotics, it is important that deep learning systems can reliably estimate the uncertainty in their predictions. This would allow robots to treat a deep neural network in the same way as any other sensor, and use the established Bayesian techniques <ref type="bibr" target="#b51">(Kaess et al., 2012;</ref><ref type="bibr" target="#b56">K?mmerle et al., 2011;</ref><ref type="bibr" target="#b115">Thrun et al., 2005)</ref> to fuse the network's predictions with prior knowledge or other sensor measurements, or to accumulate information over time. Deep learning systems, e.g. for classification or detection, typically return scores from their softmax layers that are proportional to Fig. <ref type="figure">1</ref>. Current challenges for deep learning in robotic vision. We can categorize these challenges into three conceptually orthogonal axes: learning, embodiment, and reasoning. the system's confidence, but are not calibrated probabilities, and therefore not useable in a Bayesian sensor fusion framework.</p><p>Current approaches towards uncertainty estimation for deep learning are calibration techniques <ref type="bibr" target="#b36">(Guo et al., 2017;</ref><ref type="bibr" target="#b42">Hendrycks and Gimpel, 2017)</ref>, or Bayesian deep learning <ref type="bibr">(MacKay, 1992;</ref><ref type="bibr" target="#b76">Neal, 1995)</ref> with approximations such as dropout sampling <ref type="bibr" target="#b26">(Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b52">Kendall and Gal, 2017)</ref> or ensemble methods <ref type="bibr" target="#b59">(Lakshminarayanan et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Identify unknowns. A common assumption in deep</head><p>learning is that trained models will be deployed under closed-set conditions <ref type="bibr" target="#b9">(Bendale and Boult, 2015;</ref><ref type="bibr" target="#b118">Torralba and Efros, 2011)</ref>, i.e. the classes encountered during deployment are known and exactly the same as during training. However, robots often have to operate in ever-changing, uncontrolled real-world environments, and will inevitably encounter instances of classes, scenarios, textures, or environmental conditions that were not covered by the training data.</p><p>In these so-called open-set conditions <ref type="bibr" target="#b9">(Bendale and Boult, 2015;</ref><ref type="bibr">Scheirer et al., 2013a)</ref>, it is crucial to identify the unknowns: the perception system must not assign highconfidence scores to unknown objects or falsely recognize them as one of the known classes. If, for example, an object detection system is fooled by data outside of its training data distribution <ref type="bibr" target="#b33">(Goodfellow et al., 2014;</ref><ref type="bibr">Nguyen et al., 2015a)</ref>, the consequences for a robot acting on false, but high-confidence detections can be catastrophic. One way to handle the open-set problem and identify unknowns is to utilize the epistemic uncertainty <ref type="bibr" target="#b26">(Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b52">Kendall and Gal, 2017)</ref> of the model predictions to reject predictions with low confidence <ref type="bibr" target="#b74">(Miller et al., 2017)</ref>. The system is able to select the most informative samples for incremental learning on its own in a data-efficient way, e.g. by utilizing its estimated uncertainty in a prediction. It can ask the user to provide labels. 4</p><p>Class-incremental learning</p><p>The system can learn new classes, preferably using low-shot or one-shot learning techniques, without catastrophic forgetting. The system requires the user to provide these new training samples along with correct class labels. 3 Incremental learning</p><p>The system can learn off new instances of known classes to address domain adaptation or label shift. It requires the user to select these new training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Identify unknowns</head><p>In an open-set scenario, the robot can reliably identify instances of unknown classes and is not fooled by out-of-distribution data. 1</p><p>Uncertainty estimation</p><p>The system can correctly estimate its uncertainty and returns calibrated confidence scores that can be used as probabilities in a Bayesian data fusion framework. Current work on Bayesian deep learning falls into this category. 0</p><p>Closed-set assumptions</p><p>The system can detect and classify objects of classes known during training. It provides uncalibrated confidence scores that are proportional to the system's belief of the label probabilities. State-of-the-art methods, such as YOLO9000, SSD, and Mask R-CNN, are at this level. As an extension of active vision, the system can manipulate the scene to aid perception. For example, it can move an occluding object to gain information about objects hidden underneath.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Active vision</head><p>The system has learned to actively control the camera movements in the world, for example it can move the camera to a better viewpoint to improve its perception confidence or better deal with occlusions. 2 Spatial embodiment</p><p>The system can exploit aspects of spatial coherency and incorporate views of objects taken from different viewpoints to improve its perception, while handling occlusions. 1 Temporal embodiment</p><p>The system learned that it is temporally embedded and consecutive images are strongly correlated.</p><p>The system can accumulate evidence over time to improve its predictions. Appearance changes over time can be coped with. 0</p><p>None The system has no understanding of any form of embodiment and treats every image as an independent from previously seen images. Object and scene geometry</p><p>The system learned to reason about the geometry and shape of individual objects, and about the general scene geometry, such as absolute and relative object pose, support surfaces, and object continuity under occlusions and in clutter. 1</p><p>Object and scene semantics</p><p>The system can exploit prior semantic knowledge to improve its performance. It can utilize priors about which objects are more likely to occur together in a scene, or how objects and overall scene type are correlated. 0 None The system does not perform any sophisticated reasoning, e.g. it treats every detected object as independent from other objects or the overall scene. Estimates of semantics and geometry are treated as independent.</p><p>2.1.3. Incremental learning. For many robotics applications the characteristics and appearance of objects can be quite different in the deployment scenario compared with the training data. To address this domain adaptation problem <ref type="bibr" target="#b21">(Csurka, 2017;</ref><ref type="bibr" target="#b28">Ganin et al., 2015;</ref><ref type="bibr" target="#b81">Patel et al., 2015)</ref>, a robotic vision system should be able to learn from new training samples of known classes during deployment and adopt its internal representations accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Class-incremental learning.</head><p>When operating in open-set conditions, the deployment scenario might contain new classes of interest that were not available during training. A robot therefore needs the capability to extend its knowledge and efficiently learn new classes without forgetting the previously learned representations <ref type="bibr" target="#b32">(Goodfellow et al., 2013)</ref>. This class-incremental learning would preferably be data-effcient by using one-shot <ref type="bibr" target="#b11">(Bertinetto et al., 2016;</ref><ref type="bibr" target="#b58">Lake et al., 2015;</ref><ref type="bibr" target="#b90">Rezende et al., 2016;</ref><ref type="bibr">Santoro et al., 2016;</ref><ref type="bibr" target="#b122">Vinyals et al., 2016)</ref> or low-shot <ref type="bibr" target="#b25">(Finn et al., 2017;</ref><ref type="bibr" target="#b39">Hariharan and Girshick, 2016;</ref><ref type="bibr" target="#b124">Wang and Hebert, 2016)</ref> learning techniques. Semi-supervised approaches <ref type="bibr" target="#b54">(Kingma et al., 2014;</ref><ref type="bibr" target="#b80">Papandreou et al., 2015;</ref><ref type="bibr" target="#b87">Rasmus et al., 2015)</ref> that can leverage unlabeled data are of particular interest. Current techniques for class-incremental learning <ref type="bibr" target="#b73">(Mensink et al., 2012;</ref><ref type="bibr" target="#b88">Rebuffi et al., 2017)</ref> still rely on supervision in the sense that the user has to specifically tell the system which samples are new data and therefore should be incorporated. The next challenge in our list, active learning, aims to overcome this and automatically selects new training samples from the available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5.">Active learning.</head><p>A robot should be able to select the most informative samples for incremental learning techniques on its own. Since it would have to ask the human user for the true label for these selected samples, dataefficiency is key to minimize this kind of interaction with the user. Active learning <ref type="bibr" target="#b19">(Cohn et al., 1996)</ref> can also comprise retrieving annotations from other sources such as the web.</p><p>Some current approaches <ref type="bibr" target="#b22">(Dayoub et al., 2017;</ref><ref type="bibr" target="#b27">Gal et al., 2017)</ref> leverage the uncertainty estimation techniques based on approximate Bayesian inference (see Section 2.1.1) to choose the most informative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Embodiment challenges</head><p>Embodiment is a cornerstone of what constitutes robotic vision, and what sets it apart from computer vision. Along this axis we describe four embodiment challenges: understanding and utilizing temporal and spatial embodiment helps to improve perception, but also enables robotic vision to perform active vision, and even targeted manipulation of the environment to further improve perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Temporal embodiment.</head><p>In contrast to typical recent computer vision systems that treat every image as independent, a robotic vision system perceives a stream of consecutive and therefore strongly correlated images. Whereas current work on action recognition, learning from demonstration, and similar directions in computer vision work on video data (e.g. by using recurrent neural networks or by simply stacking consecutive frames in the input layers), the potential of temporal embodiment to improve the quality of the perception process for object detection or semantic segmentation, is currently rarely utilized: a robotic vision system that uses its temporal embodiment can, for example, accumulate evidence over time (preferably using Bayesian techniques, if uncertainty estimates are available as discussed in Section 2.1.1) or exploit small viewpoint variations that occur over time in dynamic scenes.</p><p>The new CORe50 dataset <ref type="bibr" target="#b67">(Lomonaco and Maltoni, 2017)</ref> is one of the few available datasets that encourages researchers to exploit temporal embodiment for object recognition, but the robotic vision research community should invest more effort to fully exploit the potentials of temporal embodiment.</p><p>A challenging aspect of temporal embodiment is that the appearance of scenes changes over time. An environment can comprise dynamic objects such as cars or pedestrians moving through the field of view of a camera. An environment can also change its appearance caused by different lighting conditions (day/night), structural changes in objects (summer/winter), or differences in the presence and pose of objects (e.g. an office during and after work hours). A robotic vision system has to cope with all of those effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Spatial embodiment.</head><p>In robotic vision, the camera that observes the world is part of a larger robotic system that acts and moves in the world: the camera is spatially embodied. As the robot moves in its environment, the camera will observe the scene from different viewpoints, which poses both challenges and opportunities to a robotic vision system: observing an object from different viewpoints can help to disambiguate its semantic properties, improve depth perception, or segregate an object from other objects or the background in cluttered scenes. On the other hand, occlusions and the resulting sudden appearance changes complicate visual perception and require capabilities such as object unity and object permanence <ref type="bibr" target="#b83">(Piaget, 2013)</ref> that are known to develop in the human visual system <ref type="bibr" target="#b31">(Goldstein and Brockmole, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Active vision.</head><p>One of the biggest advantages robotic vision can draw from its embodiment is the potential to control the camera, move it, and change its viewpoint to improve its perception or gather additional information about the scene. This is in stark contrast to most computer vision scenarios, where the camera is a passive sensor that observes the environment from where it was placed, without any means of controlling its pose.</p><p>Some work is undertaken in the area of next-best viewpoint prediction to improve object detection <ref type="bibr" target="#b3">(Atanasov et al., 2014;</ref><ref type="bibr" target="#b23">Doumanoglou et al., 2016;</ref><ref type="bibr" target="#b71">Malmir et al., 2017;</ref><ref type="bibr">Wu et al., 2015b)</ref> or path planning for exploration on a mobile robot <ref type="bibr" target="#b12">(Bircher et al., 2016)</ref>, but a more holistic approach to active scene understanding is still missing from current research. Such an active robotic vision system could control camera movements through the world to improve the system's perception confidence, resolve ambiguities, mitigate the effect of occlusions, or reflections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Manipulation for perception.</head><p>As an extension of active vision, a robotic system could purposefully manipulate the scene to aid its perception. For example, a robot could move occluding objects to gain information about object hidden underneath. Planning such actions will require an understanding of the geometry of the scene, the capability to reason about how certain manipulation actions will change the scene, and if those changes will positively affect the perception processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Reasoning challenges</head><p>In his influential 1867 book on physiological optics, Von Helmholtz (1867) formulated the idea that humans use unconscious reasoning, inference or conclusion, when processing visual information. Since then, psychologists have devised various experiments to investigate these unconscious mechanisms <ref type="bibr" target="#b31">(Goldstein and Brockmole, 2016)</ref>, modernized Helmholtz's original ideas <ref type="bibr" target="#b92">(Rock, 1983)</ref>, and reformulated them in the framework of Bayesian inference <ref type="bibr" target="#b53">(Kersten et al., 2004)</ref>.</p><p>Inspired by their biological counterparts, we formulate the following three reasoning challenges, addressing separate and joint reasoning about the semantics and geometry of a scene and the objects therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Reasoning about object and scene semantics.</head><p>The world around us contains many semantic regularities that humans use to aid their perception <ref type="bibr" target="#b31">(Goldstein and Brockmole, 2016)</ref>: objects tend to appear more often in a certain context than in other contexts (e.g. it is more likely to find a fork in a kitchen or on a dining table, but less likely to find it in a bathroom), some objects tend to appear in groups, some objects rarely appear together in a scene, and so on. Semantic regularities also comprise the absolute pose of object in a scene, or the relative pose of an object with respect to other objects.</p><p>Although the importance of semantic regularities and contextual information for human perception processes is well known in psychology <ref type="bibr" target="#b31">(Goldstein and Brockmole, 2016;</ref><ref type="bibr" target="#b79">Oliva and Torralba, 2007)</ref>, current object detection systems <ref type="bibr" target="#b40">(He et al., 2017;</ref><ref type="bibr">Liu et al., 2016b;</ref><ref type="bibr" target="#b89">Redmon and Farhadi, 2016)</ref> do not exploit this rich source of information. If the many semantic regularities present in the real world can be learned or otherwise made available to the vision system in the form of prior knowledge, we can expect an improved and more robust perception performance: Context can help to disambiguate or correct predictions and detections.</p><p>The work by Lin et al. ( <ref type="formula">2013</ref>) is an example of a scene understanding approach that explicitly models and exploits several semantic and geometric relations between objects and the overall scene using conditional random fields. A combination of place categorization and improved object detection utilizing learned scene-object priors has been demonstrated in <ref type="bibr">S?nderhauf et al. (2016)</ref>. In more recent work by <ref type="bibr" target="#b134">Zhang et al. (2017)</ref> a method was devised to perform holistic scene understanding using a deep neural network that learns to utilize context information from training data.</p><p>2.3.2. Reasoning about object and scene geometry. Many applications in robotics require knowledge about the geometry of individual objects, or the scene as a whole. Estimating the depth of the scene from a single image has become a widely researched topic <ref type="bibr" target="#b29">(Garg et al., 2016;</ref><ref type="bibr" target="#b30">Godard et al., 2017;</ref><ref type="bibr">Liu et al., 2016a)</ref>. Similarly, there is a lot of ongoing work on estimating the 3D structure of objects from a single or multiple views without having depth information available <ref type="bibr" target="#b18">(Choy et al., 2016;</ref><ref type="bibr" target="#b38">H?ne et al., 2017;</ref><ref type="bibr" target="#b129">Yan et al., 2016;</ref><ref type="bibr">Zhu et al., 2017)</ref>. These methods are typically evaluated on images with only one or a few prominent and clearly separated objects. However for robotic applications, cluttered scenes are very common.</p><p>The previously discussed problems of uncertainty estimation and coping with unknown objects apply here as well: a robotic vision system that uses the inferred geometry, for example to grasp objects, needs the ability to express uncertainty in the inferred object shape when planning grasp points. Similarly, it should be able to exploit its embodiment to move the camera to a better viewpoint to efficiently collect new information that enables a more accurate estimate of the object geometry.</p><p>As an extension of reasoning over individual objects, inference over the geometry of the whole scene is important for robotic vision, and closely related to the problems of object-based mapping or object-based simultaneous localization and mapping (SLAM) <ref type="bibr">(Cadena et al., 2016;</ref><ref type="bibr" target="#b84">Pillai and Leonard, 2015;</ref><ref type="bibr" target="#b96">Salas-Moreno et al., 2013;</ref><ref type="bibr">S?nderhauf et al., 2017)</ref>. Exploiting semantic and prior knowledge can help a robotic vision system to better reason about the scene structure, for example the absolute and relative poses of objects, support surfaces, and object continuity despite occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Joint reasoning about semantics and geometry.</head><p>The ability to extract information about objects, environmental structures, their various complex relations, and the scene geometry in complex environments under realistic, open-set conditions is increasingly important for robotics. Our final reasoning challenge for a robotic vision system therefore is the ability to reason jointly about the semantics and the geometry of a scene and the objects therein. Since semantics and geometry can co-inform each other, a tightly coupled inference approach can be advantageous over loosely coupled approaches where reasoning over semantics and geometry is performed separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Are we getting evaluation right in deep learning for robotics?</head><p>Why does real-world deep learning performance fail to match the published performance on benchmark datasets? This is a vexing question currently facing roboticists, and the answer has to do with the nature of evaluation in computer vision. Robotics is different from much of computer vision in that a robot must interact with a dynamic environment, not just images or videos downloaded from the Internet. Therefore, a successful algorithm must generalize to numerous novel settings, which shifts the emphasis away from a singular focus on computing the best summary statistic (e.g. average accuracy, area under the curve, precision, recall) over a canned dataset. Recent catastrophic failures of autonomous vehicles relying on convolutional neural networks <ref type="bibr" target="#b66">(Lohr, 2016)</ref> highlight this disconnect: when a summary statistic indicates that a dataset has been solved, it does not necessarily mean that the problem itself has been solved. The consequences of this observation are potentially far reaching if algorithms are deployed without a thorough understanding of their strengths and weaknesses <ref type="bibr" target="#b2">(Anthony, 2016)</ref>.</p><p>Whereas there are numerous flaws lurking in the shadows of deep learning benchmarks <ref type="bibr" target="#b10">(Bendale and Boult, 2016;</ref><ref type="bibr" target="#b20">Cox and Dean, 2014;</ref><ref type="bibr" target="#b78">Nguyen et al., 2015b;</ref><ref type="bibr">Szegedy et al., 2013)</ref>, two key aspects are worth discussing here: ( <ref type="formula">1</ref>) the open set nature of decision making in visual recognition problems related to robotics; and (2) the limitations of traditional dataset evaluation in helping us understand the capabilities of an algorithm. Open set recognition refers to scenarios where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during its operation <ref type="bibr">(Scheirer et al., 2013b)</ref>. It is absolutely critical to ask what the dataset is not capturing before setting a trained model loose to perform in the real world. Moreover, if a claim is made about the humanlevel (or, as we have been hearing lately, superhuman-level) performance of an algorithm, human behavior across varying conditions should be the frame of reference, not just a comparison of summary statistics on a dataset. This leads us to suggest visual psychophysics as a sensible alternative for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The importance of open set recognition</head><p>In an autonomous vehicle setting, one can envision an object detection model trained to recognize other cars, while rejecting trees, signs, telephone poles, and any other non-car object in the scene. The challenge in obtaining good performance from this model is in the necessary generalization to all non-car objects, both known and unknown. Instead of casting such a detection task as a binary decision problem like most popular classification strategies would do, it is perhaps more useful to think about it within the context of the following taxonomy <ref type="bibr">(Scheirer et al., 2014b)</ref>, inspired by some memorable words spoken by <ref type="bibr" target="#b93">Rumsfeld (2002)</ref>.</p><p>? Known classes: the classes with distinctly labeled pos-</p><p>itive training examples (also serving as negative examples for other known classes). These samples are the most problematic for machine learning.</p><p>It is not the case that the feature space produced by a deep learning method should help us with the unknown classes? After all, the advantage of deep learning is the ability to learn separable feature representations that are strongly invariant to changing scene conditions. The trouble we find is not necessarily with the features themselves, but in the read-out layer used for decision making. Consider the following problems with three popular classifiers used as read-out layers for convolutional neural networks when applied to recognition tasks where unknown classes are present. A linear support vector machine (SVM) separates the positive and negative classes by a single linear decision boundary, establishing two half-spaces. These half-spaces are infinite in extent, meaning unknown samples far from the support of known training data can receive a positive label <ref type="bibr">(Scheirer et al., 2014b)</ref>. The Softmax function is a common choice for multi-class classification, but computing it requires calculating a summation over all of the classes. This is not possible when unknown classes are expected at testing time <ref type="bibr" target="#b10">(Bendale and Boult, 2016)</ref>. Along these same lines, when used to make a decision, cosine similarity requires a threshold, which can only be estimated over known data. The difficulty of establishing decision boundaries that capture a large measure of intraclass variance while rejecting unknown classes underpins several well-known deficiencies in deep learning architectures <ref type="bibr" target="#b78">(Nguyen et al., 2015b;</ref><ref type="bibr">Szegedy et al., 2013)</ref>.</p><p>It is readily apparent that we do not understand decision boundary modeling as well as we should. Accordingly, we suggest that researchers give more attention to decision making at an algorithmic level to address the limitations of existing classification mechanisms. What is needed is a new class of machine learning algorithms that minimize the risk of the unknown. Preliminary work exploring this idea has included slab-based linear classifiers to limit the risk of half-spaces <ref type="bibr">(Scheirer et al., 2013b)</ref>, nearest nonoutlier models <ref type="bibr" target="#b9">(Bendale and Boult, 2015)</ref>, and extreme value theory-based calibration of decision boundaries <ref type="bibr" target="#b10">(Bendale and Boult, 2016;</ref><ref type="bibr">Scheirer et al., 2014b;</ref><ref type="bibr" target="#b133">Zhang and Patel, 2016)</ref>. Much more work is needed in this direction, including algorithms that incorporate the risk of the unknown directly into their learning objectives, and evaluation protocols that incorporate data which is both known and unknown to a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The role visual psychophysics should play</head><p>One need not resort to tricky manipulations such as noise patterns that are imperceptible to humans <ref type="bibr">(Szegedy et al., 2013)</ref> or carefully evolved images <ref type="bibr" target="#b78">(Nguyen et al., 2015b)</ref> to fool recognition systems based on deep learning. Simple transformations such as rotation, scale, and occlusion will do the job just fine. Remarkably, a systematic study of a recognition model's performance across an exhaustive range of object appearances is typically not done during the course of machine learning research. This is a major shortcoming of evaluation within the field. Turning to the study of biological vision systems, psychologists and neuroscientists do perform such tests on humans and animals using a set of concepts and procedures from the discipline of psychophysics. Psychophysics allows scientists to probe the inner mechanisms of visual processing through the controlled manipulation of the characteristics of visual stimuli presented to a subject. The careful management of stimulus construction, ordering, and presentation allows a perceptual threshold, the inflection point at which perception transitions from success to failure, to be determined precisely. As in biological vision, we would like to know under what conditions a machine learning model is able to operate successfully, as well as where it begins to fail. If this is to be done in an exhaustive manner, we need to leverage item response theory <ref type="bibr" target="#b24">(Embretson and Reise, 2000)</ref>, which will let us map each stimulus condition to a performance point (e.g. model accuracy). When individual item responses are collected to form a curve, an exemplar-by-exemplar summary of the patterns of error for a model becomes available, allowing us to point exactly to the condition(s) that will lead to failure.</p><p>Psychophysics is commonplace in the laboratory, but how exactly can it be applied to models? One possibility is through a computational pipeline that is able to perturb 2D natural images or 3D rendered scenes at a massive scale (e.g. millions of images per image transformation being studied) and submit them to a model, generating an item-response curve from the resulting recognition scores <ref type="bibr" target="#b91">(RichardWebster et al., 2016)</ref>. Key to the interpretability of the results is the ability to identify a model's preferred view. Work in vision science has established that humans possess an internalized canonical view (the visual appearance that is easiest to recognize) for individual object classes <ref type="bibr" target="#b13">(Blanz et al., 1999)</ref>. Similarly, recognition models have one or more preferred views of an object class, each of which leads to a maximum (or minimum) score output. A preferred view, thus, forms a natural starting place for model assessment. Through perturbation, the results will at best stay the same, but more likely will degrade as visual appearance moves outside the variance learned from the training dataset. With respect to the stimuli used when performing psychophysics experiments on models, there is a growing trend in robotics and computer vision to make use of simulations rendered via computer graphics. In line with this, we believe that procedurally rendered graphics hold much promise for psychophysics experiments, where the position of objects can be manipulated in three dimensions, and aspects of the scene, such as lighting and background, changed at will.</p><p>Instead of comparing summary statistics related to benchmark dataset performance for different models, relative performance can be assessed by comparing the respective item-response curves. Importantly, not only can any gaps between the behaviors of different models be assessed, but also potential gaps between human and model behavior.</p><p>Validation by this procedure is necessary if a claim is going to made about a model matching (or exceeding) human performance. Summary statistics only reflect one data point over a mixture of scene conditions, which obscures the patterns of error we are often most interested in. Through experimentation, we have found that human performance vastly exceeds model performance even in cases where a problem has been assumed to be solved (e.g. human face detection <ref type="bibr">(Scheirer et al., 2014a)</ref>). Whereas the summary statistics in those cases indicated that both humans and models were at the performance ceiling for the dataset at hand, the item-response curves from psychophysics experiments showed a clear gap between human and model performance. However, psychophysics need not entirely replace datasets. After all, we still need a collection of data from which to train the model, and some indication of performance on a collection of web-scale data is still useful for model screening. Steps should be taken to explore strategies for combining datasets and visual psychophysics to address some of the obvious shortcomings of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The role of simulation for pixel-to-action robotics</head><p>Robotics, still dominated by complex processing stacks, could benefit from a similar revolution as seen in computer vision which would clear a path directly from pixels to torques and enable powerful gradient-driven end-to-end optimization. A critical difference is that robotics constitutes an interactive domain with sequential actions where supervised learning from static datasets is not a solution.</p><p>Deep reinforcement learning is a new learning paradigm that is capable of learning end-to-end robotic control tasks, but the accomplishments have been demonstrated primarily in simulation, rather than on actual robot platforms <ref type="bibr" target="#b35">(Gu et al., 2016;</ref><ref type="bibr" target="#b41">Heess et al., 2015;</ref><ref type="bibr" target="#b60">Levine and Abbeel, 2014;</ref><ref type="bibr" target="#b62">Lillicrap et al., 2015;</ref><ref type="bibr" target="#b75">Mnih et al., 2016;</ref><ref type="bibr" target="#b104">Schulman et al., 2015</ref><ref type="bibr" target="#b105">Schulman et al., , 2016))</ref>. However, demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods. This poses a significant challenge, given the long, data-hungry training paradigm of pixel-based deep robotic learning methods and the relative frailty of research robots and their human handlers.</p><p>To make the challenge more concrete, consider a simple pixel-to-action learning task: reaching to a randomly placed target from a random start location, using a threefingered Jaco robot arm (see Figure <ref type="figure" target="#fig_0">2</ref>). Trained in the MuJoCo simulator using Asynchronous Advantage Actor-Critic (A3C) <ref type="bibr" target="#b75">(Mnih et al., 2016)</ref>, the current state-of-theart robotic learning algorithm, full performance is only achieved after substantial interaction with the environment, on the order of 50 million steps, a number which is infeasible with a real robot. The simulation training, compared with the real robot, is accelerated because of fast rendering, multi-threaded learning algorithms, and the ability to continuously train without human involvement. We calculate that learning this task, which trains to convergence in 24 hours using a CPU compute cluster, would take 53 days on the real robot even with continuous training for 24 hours a day. Moreover, multiple experiments in parallel were used to explore hyperparameters in simulation; this sort of search would compound further the hypothetical real robot training time.</p><p>Taking advantage of the simulation-learnt policies to train real robots is thus critical, but there is a reality gap that often separates a simulated task and its real-world analog, especially for raw pixel inputs. One solution is to use transfer learning methods to bridge the reality gap that separates simulation from real-world domains. There exist many different paradigms for domain transfer and many approaches designed specifically for deep neural models, but substantially fewer approaches for transfer from simulation to reality for robot domains. Even rarer are methods that can be used for transfer in interactive, rich sensor domains using end-to-end (pixel-to-action) learning. growing body of work has been investigating the ability of deep networks to transfer between domains. Some research <ref type="bibr" target="#b82">(Peng et al., 2015;</ref><ref type="bibr" target="#b108">Su et al., 2015)</ref> considers simply augmenting the target domain data with data from the source domain where an alignment exists. Building on this work, <ref type="bibr" target="#b68">Long et al. (2015)</ref> started from the observation that as one looks at higher layers in the model, the transferability of the features decreases quickly. To correct this effect, a soft constraint is added that enforces the distribution of the features to be more similar. <ref type="bibr" target="#b68">Long et al. (2015)</ref> proposed a "confusion" loss that forces the model to ignore variations in the data that separate the two domains <ref type="bibr" target="#b120">(Tzeng et al., 2015b</ref><ref type="bibr" target="#b121">(Tzeng et al., , 2014))</ref>, and <ref type="bibr">Tzeng et al. (2015a)</ref> attempted to address the simulation to reality gap by using aligned data. The work was focused on pose estimation of the robotic arm, where training happens on a triple loss that looks at aligned simulation to real data, including the domain confusion loss. The paper does not show the efficiency of the method on learning novel complex policies. Partial success on transferring from simulation to a real robot has been reported <ref type="bibr" target="#b6">(Barrett et al., 2010;</ref><ref type="bibr" target="#b46">James and Johns, 2016;</ref><ref type="bibr" target="#b128">Zhang et al., 2015;</ref><ref type="bibr" target="#b136">Zhu et al., 2016)</ref>. They focus primarily on the problem of transfer from a more restricted simpler version of a task to the full, more difficult version. Another promising recent direction is domain randomization <ref type="bibr" target="#b95">(Sadeghi and Levine, 2016;</ref><ref type="bibr" target="#b116">Tobin et al., 2017)</ref>.</p><p>A recent simulation-to-real approach relies on the progressive nets architecture <ref type="bibr" target="#b94">(Rusu et al., 2016)</ref>, which enables transfer learning through lateral connections that connect each layer of previously learnt deep networks to new networks, thus supporting deep compositionality of features (see Figure <ref type="figure" target="#fig_1">3</ref>). Progressive networks are well suited for simulation-to-real transfer of policies in robot control domains for multiple reasons. First, features learnt for one task may be transferred to many new tasks without destruction from fine-tuning. Second, the columns may be heterogeneous, which may be important for solving different tasks, including different input modalities, or simply to improve learning speed when transferring to the real robot. Third, progressive nets add new capacity, including new input connections, when transferring to new tasks. This is advantageous for bridging the reality gap, to accommodate dissimilar inputs between simulation and real sensors.</p><p>Experiments with the Jaco robot showed that the progressive architecture is valuable for simulation-to-real transfer. The progressive second column gets to 34 points, whereas the experiment with fine-tuning, which starts with the simulation-trained column and continues training on the robot, does not reach the same score as the progressive network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep learning and physics-based models</head><p>The predominant approach to perception, planning, and control in robotics is to use approximate models of the physics underlying a robot, its sensors, and its interactions with the environment. These model-based techniques often capture properties such as the mass, momentum, shape, and surface friction of objects, and use these to generate controls that change the environment in a desirable way <ref type="bibr" target="#b55">(Kuindersma et al., 2016;</ref><ref type="bibr" target="#b57">Kunze and Beetz, 2015;</ref><ref type="bibr" target="#b103">Schmidt et al., 2015;</ref><ref type="bibr" target="#b117">Todorov et al., 2012)</ref>. Whereas physics-based models are well suited for planning and predicting the outcome of actions, to function on a real robot they require that all relevant model parameters are known with sufficient accuracy and can be tracked over time. This requirement poses overly challenging demands on system identification and perception, resulting in systems that are brittle, especially when direct interaction with the environment is required.</p><p>Humans, on the other hand, operate under intuitive rather than exact physical models <ref type="bibr">(Baillargeon et al., 20111, 2012;</ref><ref type="bibr" target="#b8">Battaglia et al., 2013;</ref><ref type="bibr" target="#b43">Hespos et al., 2009;</ref><ref type="bibr" target="#b72">McCloskey, 1983;</ref><ref type="bibr" target="#b86">Povinelli, 2000)</ref>. Whereas these intuitive models have many well-documented deficiencies and inaccuracies, they have the crucial property that they are grounded in real world experience, are well suited for closed-loop control, and can be learned and adapted to new situations. As a result, humans are capable of robustly performing a wide variety of tasks that are well beyond the reach of current robot systems, including dexterous manipulation, handling vastly different kinds of ingredients when cooking a meal, or climbing a tree.</p><p>Recent approaches to end-to-end training of deep networks forgo the use of explicit physics models, learning predictive models and controls from raw experiences <ref type="bibr" target="#b7">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b14">Byravan and Fox, 2017;</ref><ref type="bibr" target="#b16">C. Finn and Levine, 2016;</ref><ref type="bibr" target="#b34">Greff et al., 2017;</ref><ref type="bibr">Wu et al., 2015a;</ref><ref type="bibr" target="#b130">Yildirim et al., 2017)</ref>. Whereas these early applications of large-scale deep learning are just the beginning, they have the potential to provide robots with highly robust perception and control mechanisms, based on an intuitive notion of physics that is fully grounded in a robot's experience.</p><p>The properties of model-based and deep-learned approaches can be measured along multiple dimensions, including the kind of representations used for reasoning, how generally applicable their solutions are, how robust they are in real-world settings, how efficiently they make use of data, and how computationally efficient they are during operation. Model-based approaches often rely on explicit models of objects and their shape, surface, and mass properties, and use these to predict and control motion through time. In deep learning, models are typically implicitly encoded via networks and their parameters. As a consequence, model-based approaches have wide applicability, since the physics underlying them are universal. However, at the same time, the parameters of these models are difficult to estimate from perception, resulting in rather brittle performance operating only in local basins of convergence. Deep learning, on the other hand, enables highly robust performance when trained on sufficiently large data sets that are representative of the operating regime of the system. However, the implicit models learned by current deep learning techniques do not have the general applicability of physics-based reasoning. Model-based approaches are significantly more data efficient, related to their smaller number of parameters. The optimizations required for modelbased approaches can be performed efficiently, but the basin of convergence can be rather small. In contrast, deeplearned solutions are often very fast and can have very large basins of convergence. However, they do not perform well if applied in a regime outside the training data. Table <ref type="table" target="#tab_4">4</ref> summarizes the main properties.</p><p>Different variants of deep learning have been shown to successfully learn predictive physics models and robot control policies in a purely data-driven way <ref type="bibr" target="#b0">(Agrawal et al., 2016;</ref><ref type="bibr" target="#b15">Byravan et al., 2018;</ref><ref type="bibr" target="#b49">Jonschkowski et al., 2017;</ref><ref type="bibr" target="#b125">Watter et al., 2015)</ref>. Although such a learning-based paradigm could potentially inherit the robustness of intuitive physics reasoning, current approaches are nowhere near human prediction and control capabilities. Key challenges toward achieving highly robust, physics-based reasoning and control for robots are as follows. (1) Learn general, predictive models for how the environment evolves and how it reacts to a robot's actions. Although the first attempts in this direction show promising results, these only capture very specific scenarios and it is not clear how they can be made to scale to general predictive models. (2) Leverage existing physics-based models to learn intuitive models from less data. Several systems approach this problem in promising ways, such as using physics-based models to generate training data for deep learning or developing deep network structures that incorporate insights from physics-based reasoning. (3) Learn models and controllers at multiple levels of abstractions that can be reused in many contexts. Rather than training new network structures for each task, such an approach would enable robots to fully leverage previously learned knowledge and apply it in new contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Towards an automation of informatics</head><p>Deep learning will change the foundations of computer science. Already, the successes of deep learning in various domains are calling into question the dominant problemsolving paradigm: algorithm design. 1 This can easily be seen in the area of image classification, where deep learning has outperformed all prior attempts of explicitly programming image-processing algorithms. In contrast to most other applications of machine learning that require the careful design of problem-specific features, deep learning approaches require little to no knowledge of the problem domain. Sure, the search for a suitable network architectures and training procedures remains but the amount of domain-specific knowledge required to apply deep learning methods to novel problem domains is substantially lower than for programming a solution explicitly. As a result, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good in local regime</head><p>Highly efficient once trained the amount of problem-specific expertise required to solve complex problems has reached an all-time low. Whether this is good or bad remains to be seen (it is probably neither and both). However, it might seem that deep learning is currently the winner in the competition between "traditional" programming and the clever use of large amounts of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Programming versus data</head><p>Solutions to computational problems lie on a spectrum along which the relative and complementary contributions of programming and data vary. On one end of the spectrum lies traditional computer science: human experts program problem-specific algorithms that require no additional data to solve a particular problem instance, e.g. quicksort. On the other extreme lies deep learning. A generic approach to learning leverages large amounts of data to find a computational solution automatically. In between these two extremes lie algorithms that are less generic than deep learning and less specific than quicksort, including maybe decision trees for example. It is helpful to look at the two ends of the spectrum in more detail. The act of programming on one end of the spectrum is replaced by training on the other end. The concept of program is turned into learning weights of the network. The programming language, i.e. the language in which a solution is expressed, is replaced by network architecture, loss function, training procedure, and data. Please note that the training procedure itself is again seen as a concrete algorithm, on the opposing end of the spectrum. This already alludes to the fact that solutions to challenging problems probably must combine sub-solutions from the entire spectrum spanned by programming and deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Does understand imply one end of the spectrum?</head><p>For a programmer to solve a problem through programming, we might say that they have to understand the problem. Computer programs therefore reflect human understanding. We might also say that the further a particular solution is positioned towards the deep-learning end of the spectrum, the less understanding about the problem it requires. As science strives for understanding, we should ultimately attempt to articulate the structure of our solutions explicitly, relying on as little data as possible for solving a particular problem instance. There are many reasons for pursuing this goal: robustness, transfer, generality, verifiability, re-use, and ultimately insight, which might lead to further progress. Consider, for example, the problem of tracking the trajectory of a quad-copter. We can certainly come up with a deep-learning solution to this problem. However, would we not expect the outcome of learning, given an arbitrary amount of data and computational resources, to be some kind of Bayes filter? Either we believe that the Bayes filter captures the computational structure inherent to this problem (recursive state estimation), and then a learned solution eventually has to discover and represent this solution. However, at that point we might simply use the algorithm instead of the deep neural network. If, on the other hand, the deep neural network represents something else than a Bayes filter, something outperforming the Bayes filter, then we discovered that Bayes filters do not adequately capture the structure of the problem at hand. We will naturally be curious as to what the neural network discovered.</p><p>From this, we should draw three conclusions. First, our quest for understanding implies that we must try to move towards the programming-end of the spectrum, whenever we can. Second, we need to be able to leverage generic tools, such as deep learning, to discover problem structure; this will help us derive novel knowledge and to devise algorithms based on that knowledge. Third, we should understand how problems can be divided into parts: those parts for which we know the structure (and, therefore, can write algorithms for) and those for which we would like to discover the structure. This will facilitate the component-wise movement towards explicit understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Generic tools might help us identify new structure</head><p>When we do not know how to program a solution for a problem and instead apply a generic learning method, such as deep learning, and this generic method delivers a solution, then we have implicitly learned something about the problem. It might be difficult to extract this knowledge from a deep neural network but that should simply motivate us to develop methods for extracting this knowledge. Towards this goal, our community should (a) report in detail on the limitations of deep networks and (b) study in similar detail the dependencies of deep-learning solutions on various parameters. This will lead the way to an ability of "reading" networks so as to extract algorithmifiable information.</p><p>There have been some recent results about "distilling" knowledge from neural networks, indicating that the extraction of problem structure from neural networks might be possible <ref type="bibr" target="#b44">(Hinton et al., 2015)</ref>. Such distilled knowledge is still far away from being algorithmifiable, but this line of work seem promising in this regard. The idea of distillation can also be combined with side information <ref type="bibr" target="#b69">(Lopez-Paz et al., 2016)</ref>, further facilitating the identification of relevant problem structure.</p><p>On the other hand, it was shown that our insights about generalization, an important objective for machine learning algorithms, might not transfer easily to neural networks <ref type="bibr">(Zhang et al., 2016)</ref>. If it turns out that the deep neural networks we learn today simply memorize training data and then interpolate between them <ref type="bibr">(Zhang et al., 2016)</ref>, then we must develop novel regularization methods to enforce the extraction of problem structure instead of memorization, possibly through the use of side information <ref type="bibr">(Jonschkowski et al., 2015)</ref>. Otherwise, if neural networks are only good for memorization, they are not as powerful as we thought. There might be evidence, however, that neural networks do indeed find good representations, i.e. problem structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Complex problems should be solved by decomposition and re-composition</head><p>In many cases, interesting and complex problems will exhibit complex structure because they are composed of sub-problems. For each of these sub-problems, computational solutions are most appropriate that lie on different points along the programming/data-spectrum; this is because we may have more or less understanding of the sub-problem's inherent structure. It would therefore make sense to compose solutions to the original problem from sub-solutions that lie on different points on the programming/data-spectrum <ref type="bibr" target="#b48">(Jonschkowski and Brock, 2016)</ref>. For many sub-problems, we already have excellent algorithmic solutions, e.g. implementations of quicksort. Sorting is a problem on one end of the spectrum: we understand it and have codified that understanding in an algorithm. However, there are many other problems, such as image classification, where human programs are outperformed by deep neural networks. Those problem should be solved by neural networks and then integrated with solutions from other parts of the spectrum.</p><p>This re-composition of component solutions from different places on the spectrum can be achieved with differentiable versions of existing algorithms (one end of the spectrum) that are compatible solutions obtained with back-propagation (other end of the spectrum) <ref type="bibr" target="#b14">(Byravan and Fox, 2017;</ref><ref type="bibr" target="#b37">Haarnoja et al., 2016;</ref><ref type="bibr" target="#b48">Jonschkowski and Brock, 2016;</ref><ref type="bibr" target="#b106">Shi and Griffiths, 2009;</ref><ref type="bibr" target="#b114">Tamar et al., 2016;</ref><ref type="bibr" target="#b126">Wilson and Finkel, 2009)</ref>. For example, <ref type="bibr" target="#b48">Jonschkowski and Brock (2016)</ref> solve the aforementioned localization problem for quad-copters by combining a histogram filter with back-propagation-learned motion and sensing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Decomposability of problems</head><p>In the previous section, we argued that complex problems often are decomposable into sub-problems that can be solved independently. A problem is called decomposable or near-decomposable <ref type="bibr" target="#b107">(Simon, 1996)</ref> if there is little complexity in the interactions among sub-problems and most of the complexity is handled within those sub-problems. However, are all problems decomposable in this manner? For example, Schierwagen (2012) argued that the brain is not decomposable because the interactions between its components still contain much of the complexity of the original problem. Furthermore, many interpret results on end-toend learning of deep visuomotor policies to indicate that modular sub-solutions automatically lead to poor solutions <ref type="bibr" target="#b61">(Levine et al., 2015)</ref>. Of course, a sub-optimal factorization of a problem into sub-problems will lead to sub-optimal solutions. However, the results presented by <ref type="bibr" target="#b61">Levine et al. (2015)</ref> do not lend strong support to this statement. The authors showed that end-to-end learning, i.e. giving up strict boundaries between sub-problems improves their solution. However, it is unclear whether this is an artifact of overfitting, an indication of a poor initial factorization, or an indication of the fact that even correct factorizations may exclude parts of the solution space containing the optimal solution.</p><p>Irrespective of the degree of decomposability of a problem (and the suitable degree of modularity of the solution), we suspect that there are optimal factorizations of problems for a defined task, agent, and environment. Such a factorization may not always lead to simple interfaces between sup-problems but always facilitates finding an optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Automating programming</head><p>we are able to (1) decompose problems into subproblems, (2) solve those sub-problems with solutions from different points along the programming/data-spectrum, 3) recompose the solutions to sub-problems, and (4) extract algorithmic information from data-driven solutions, we might as well automate programming (computer science) altogether. Programming should be easy to automate, as it takes place entirely within the well-defined world of the computer. If we can successfully apply generic methods to complex problems, extract and algorithmify structural knowledge from the resulting solutions, use the resulting algorithms to solve sub-problems of the original problem, thereby making that original problem more easily solvable, and so forth, then we can also imagine an automated way of deriving computer algorithms from problem-specific data. A key challenge will be the automatic decomposition or factorization of the problem into suitably solvable sub-problems.</p><p>This view raises some fundamental questions about the differences between program in programming and weights in deep learning. Really, this view implies that there is no qualitative difference between them, only a difference of expressiveness and the amount of prior assumptions reflected in them. Programs and weights, in this view, are different instances of the same thing, namely of parameters that specify a solution, given a framework for expressing such solutions. Now it seems plausible that we can incrementally extract structure from learned parameters (weights), leading to a less generic representation with fewer parameters, until the parameters are so specific that we might call them a program.</p><p>However, the opposite is also possible. It is possible that problems exists that do not exhibit algorithmifiable structure. It is possible that these problems can (only) be solved in a data-driven manner. To speculate about this, comparisons with biological cognitive capabilities might be helpful. Can these capabilities (in principle) be encoded in a program? Do these capabilities depend on massive amounts of data? These are difficult questions that artificial intelligence researchers have asked themselves for many years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Priors to reduce the amount of data</head><p>A natural concern for this kind of reasoning is the necessity to acquire large amounts of data. This can be very costly, especially when these data have to be acquired from interaction with the real world, as is the case in robotics. It will then become necessary to reduce the required amount of data by incorporating appropriate priors into learning <ref type="bibr">(Jonschkowski and Brock, 2015)</ref>. These priors reduce all possible interpretations of data to only those consistent with the prior. If sufficiently strong priors are available, it will become possible to extract (and possibly algorithmify) the problem structure from reasonable amounts of data.</p><p>It might also be difficult to separate acquired data into those groups associated with a single task. Recent methods have shown that this separation can be performed automatically <ref type="bibr" target="#b45">(H?fer et al., 2016)</ref>. Now data can be acquired in lessrestrictive settings and the learning agent can differentiate the task associated with a datum by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8.">Where will this lead?</head><p>Maybe in the end, the most lasting impact of deep learning will not be deep learning itself but rather the effect it had. The successes of deep learning, achieved by leveraging data and computation, have made computer scientists realize that there is a spectrum, rather than a dichotomy, between programming and data. This realization may pave the way for a computer science that fully leverages the entire breadth of this spectrum to automatically derive algorithms from reasonable amounts of data and suitable priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>The rather skeptical attitude towards deep learning at the RSS conference in Rome 2015 motivated us to organize a workshop at RSS 2016 with the title "Are the Skeptics Right? Limits and Potentials of Deep Learning in Robotics" <ref type="bibr">(S?nderhauf et al., 2016)</ref>. As it turned out, by then there were hardly any skeptics left. The robotics community had accepted deep learning as a very powerful tool and begun to utilize and advance it. A follow-up workshop on "New Frontiers for Deep Learning in Robotics" <ref type="bibr">(S?nderhauf et al., 2017)</ref> at RSS 2017 concentrated more on some of the robotics-specific research challenges we discussed in this paper. A surge of deep learning in robotics was seen in 2017: workshops at CVPR <ref type="bibr" target="#b1">(Angelova et al., 2017)</ref> and NIPS <ref type="bibr" target="#b85">(Posner et al., 2016)</ref> built bridges between the robotics, computer vision, and machine learning communities. Over 10% of the papers submitted to ICRA 2018 used Deep learning in robotics and automation as a keyword, making it the most frequent keyword. Furthermore, a whole new Conference on Robot Learning (CoRL) 2 was initiated. much ongoing work in deep learning for robotics concentrates on either perception or acting, we hope to see more integrated approaches in the future: robots that learn to utilize their embodiment to reduce the uncertainty in perception, decision making, and execution. Robots that learn complex multi-stage tasks, while incorporating prior model knowledge or heuristics, and exploiting a semantic understanding of their environment. Robots that learn to discover and exploit the rich semantic regularities and geometric structure of the world, to operate more robustly in realistic environments with open-set characteristics.</p><p>Deep learning techniques have revolutionized many aspects of computer vision over the past 5 years and have been rapidly adopted into robotics as well. However, robotic perception, robotic learning, and robotic control are demanding tasks that continue to pose severe challenges on the techniques typically applied. Our paper discussed some of these current research questions and challenges for deep learning in robotics. We pointed the reader into different directions worthwhile for further research and hope our paper contributes to the ongoing advancement of deep learning for robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016). Oliver Brock was supported by the DFG (grant number 329426068). Walter Scheirer acknowledges the funding provided by IARPA (contract number D16PC00002). Michael Milford was partially supported by an Australian Research Council Future Fellowship (FT140101229).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes</head><p>1. The term algorithm refers to the Oxford Dictionary definition: "a process or set of rules to be followed in calculations or other problem-solving operations." Here, it includes physics formulas, computational models, probabilistic representations and inference, etc. 2. See http://www.robot-learning.org</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Sample images from the real camera input image (left) and the MuJoCo-rendered image (right), demonstrating the reality gap between simulation and reality even for a simple reaching task.</figDesc><graphic url="image-2.png" coords="8,54.69,73.69,108.02,107.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detailed schematic of progressive recurrent network architecture, where the left column is trained in simulation, then the weights are frozen while the second column is trained on the real robot. A third column may then be trained on an additional task, taking advantage of the policies and features learnt and frozen in the first two columns.</figDesc><graphic url="image-3.png" coords="8,172.79,73.89,108.12,107.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Learning challenges for robotic vision</figDesc><table><row><cell cols="2">Level Name</cell><cell>Description</cell></row><row><cell>5</cell><cell>Active learning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Embodiment challenges for robotic vision</figDesc><table><row><cell cols="2">Level Name</cell><cell>Description</cell></row><row><cell>4</cell><cell>Active</cell></row><row><cell></cell><cell>manipulation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Reasoning challenges for robotic vision</figDesc><table><row><cell cols="2">Level Name</cell><cell>Description</cell></row><row><cell>3</cell><cell>Joint reasoning</cell><cell>The system jointly reasons about semantics and geometry in a tightly coupled way, allowing semantics</cell></row><row><cell></cell><cell></cell><cell>and geometry to co-inform each other.</cell></row><row><cell>2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Models versus deep learning</figDesc><table><row><cell></cell><cell>Model-based</cell><cell>Deep learning</cell></row><row><cell>Representation</cell><cell>Explicit: based on or inspired by physics</cell><cell>Implicit: network structure and parameters</cell></row><row><cell>Generality</cell><cell>Broadly applicable: physics are universal</cell><cell>Only in trained regime: risk of overfitting</cell></row><row><cell>Robustness</cell><cell>Small basin of convergence: requires good models and</cell><cell>Large basin of convergence: highly robust in trained</cell></row><row><cell></cell><cell>estimates thereof</cell><cell>regime</cell></row><row><cell>Data efficiency</cell><cell>Very high: only needed for system identification</cell><cell>Training requires significant data collection effort</cell></row><row><cell>Computational effi-</cell><cell></cell><cell></cell></row><row><cell>ciency</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to poke by poking: Experiential learning of intuitive physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><forename type="middle">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://juxi.net/workshop/deep-learning-robotic-vision-cvpr-2017" />
		<title level="m">Computer Vision and Pattern Recognition (CVPR) Workshop on Deep Learning in Robotic Vision</title>
		<imprint>
			<date type="published" when="2017-03">2017. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The trollable self-driving car</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Anthony</surname></persName>
		</author>
		<ptr target="http://www.slate.com/articles/technology/future_tense/2016/03/google_self_driving_cars_lack_a_human_s_intuition_for_what_other_drivers.html" />
		<imprint>
			<date type="published" when="2016-03">2016. March 2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note>Slate. Available at</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonmyopic view planning for active object classification and pose estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1078" to="1090" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">How do infants reason about physical events?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baillargeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gertner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date>20111</date>
			<publisher>Blackwell</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Wiley-Blackwell Handbook of Childhood Cognitive Development</orgName>
		</respStmt>
	</monogr>
	<note>nd edn</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object individuation and physical reasoning in infancy: An integrative account</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baillargeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stavans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Learning and Development</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="46" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer learning for reinforcement learning on a physical robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Autonomous Agents and Multiagent Systems -Adaptive Learning Agents Workshop (AAMAS -ALA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename><forename type="middle">P</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Receding horizon &quot;next-best-view&quot; planner for 3D exploration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bircher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><forename type="middle">K</forename><surname>Oleynikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1462" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What object attributes determine canonical views?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>B?lthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="575" to="599" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SE3-nets: Learning rigid body motion using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ICRA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SE3-Pose-Nets: Structured deep dynamics models for visuomotor planning and control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meier</forename><forename type="middle">F</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ICRA</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robustperception age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">K</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural networks and neuroscienceinspired computer vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="921" to="R929" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Episode-based active learning with Bayesian neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07473</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Deep Learning for Robotic Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recovering 6D object pose and predicting next-bestview in the crowd</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3583" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Item Response Theory for Psychologists</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Embretson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Reise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02910</idno>
		<title level="m">Deep Bayesian active learning with image data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<idno>CoRR abs/1505.07818</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sensation and perception</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brockmole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Cengage Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural expectation maximization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6694" to="6704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Continuous deep Q-learning with model-based acceleration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04599</idno>
		<title level="m">On calibration of modern neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Backprop KF: learning discriminative deterministic state estimators</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<idno>CoRR abs/1605.07148</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXivarXiv:1704.00710</idno>
		<title level="m">Hierarchical surface prediction for 3D object reconstruction</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02819</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning continuous control policies by stochastic value gradients</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5796-learning-continuous-control-policies-by-stochastic-value-gradients" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-12">2015. 2015, 7-12 December 2015. March 2018</date>
			<biblScope unit="page" from="2944" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Occlusion is hard: Comparing predictive reaching for visible and hidden objects in infants and adults</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hespos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gredeba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Hofsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1483" to="1502" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of state representations for multiple tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>H?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at the Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">3D simulation for robot arm control with deep Q-learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning state representations with robotic priors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="428" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end learnable histogram filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning for Action and Interaction at the Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pves: Position-velocity encoders for unsupervised learning of structured state representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09805</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Contextual learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>H?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<idno>CoRR abs/1511.06429</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">iSAM2: Incremental smoothing and mapping using the Bayes tree</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johannsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ila</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">J</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="235" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object perception as Bayesian inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mamassian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="271" to="304" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optimizationbased locomotion planning, estimation, and control design for the Atlas humanoid robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kuindersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="455" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A general framework for graph optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>K?mmerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Robotics and Automation (ICRA)</title>
		<meeting>International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3607" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Envisioning the qualitative effects of robot manipulation actions using simulation-based projections</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page" from="352" to="380" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6393" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning neural network policies with guided policy search under unknown dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1071" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">T</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1334" to="1373" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<idno>CoRR abs/1509.02971</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fidler</forename><forename type="middle">S</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A lesson of Tesla crashes? Computer vision can&apos;t do it all yet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lohr</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2016/09/20/science/computer-vision-tesla-driverless-cars.html" />
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2016-09-19">2016. 19 September. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Core50: a new dataset and benchmark for continuous object recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03550</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML 2015)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11">2015. 6-11 July 2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno>CoRR abs/1511.03643</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep active object recognition by joint label and action prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malmir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="128" to="137" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Intuitive physics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">248</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="122" to="130" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ECCV 2012)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="488" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Dropout sampling for robust object detection in open-set conditions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3D models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV 2015)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-13">2015. 2015. 7-13 December 2015</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">The Construction of Reality in the Child</title>
		<author>
			<persName><forename type="first">J</forename><surname>Piaget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Routledge</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Monocular slam supported object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paul</surname></persName>
		</author>
		<ptr target="http://sites.google.com/view/nips17robotlearning/home" />
		<title level="m">Neural Information Processing Systems (NIPS) Workshop on Acting and Interacting in the Real World: Challenges in Robot Learning</title>
		<imprint>
			<date type="published" when="2016-03">2016. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Folk Physics for Apes: The Chimpanzee&apos;s Theory of How the World Works</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Povinelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental classifier and representation learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Available at</title>
		<imprint>
			<date type="published" when="2017-03">2017. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Psyphy: A psychophysics driven evaluation framework for visual recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Richardwebster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Se</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<idno>CoRR abs/1611.06448</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">The Logic of Perception</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Rumsfeld</surname></persName>
		</author>
		<ptr target="http://archive.defense.gov/Transcripts/Transcript.aspx?TranscriptID=2636" />
		<title level="m">DoD News Briefing addressing unknown unknowns</title>
		<imprint>
			<date type="published" when="2002-03">2002. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Progressive neural networks. preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Cad2rl: Real single-image flight without a single real image</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04201</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">SLAM++: Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1352" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Perceptual annotation: Measuring human vision to improve computer vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1679" to="1686" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Probability models for open set recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2317" to="2324" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">On reverse engineering in the brain and cognitive sciences</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="150" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Robust real-time tracking with visual and physical constraints for robot manipulation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hertkorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ICRA</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">M</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Neural implementation of hierarchical bayesian inference by importance sampling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference (NIPS)</title>
		<meeting>the Neural Information Processing Systems Conference (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">The Sciences of the Artificial</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Render for CNN: viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV 2015)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">2015. December 2015</date>
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Place categorization and semantic mapping on a mobile robot</title>
		<author>
			<persName><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcmahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5729" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<ptr target="http://juxi.net/workshop/deep-learning-rss-2017/" />
		<title level="m">Robotics: Science and Systems (RSS) Workshop on New Frontiers for Deep Learning in Robotics</title>
		<imprint>
			<date type="published" when="2017-03">2017. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Robotics: Science and Systems (RSS) Workshop Are the Sceptics Right? Limits and Potentials of Deep Learning in Robotics</title>
		<author>
			<persName><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<ptr target="http://juxi.net/workshop/deep-learning-rss-2016/" />
		<imprint>
			<date type="published" when="2016-03">2016. March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Meaningful maps -object-oriented semantic mapping</title>
		<author>
			<persName><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milford</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR abs/1312.6199</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Value iteration networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>CoRR abs/1602.02867</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Probabilistic Robotics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems IROS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">a) Towards adapting deep visuomotor representations from simulated to real environments</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><forename type="middle">C</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno>CoRR abs/1511.07111</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">T</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV 2015)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-13">2015b. 7-13 December 2015</date>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrell</forename><forename type="middle">T</forename></persName>
		</author>
		<idno>CoRR abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Handbuch der physiologischen Optik</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Helmholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1867">1867</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2728" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A neural implementation of the Kalman Filter</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference (NIPS)</title>
		<meeting>the Neural Information Processing Systems Conference (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Galileo: Perceiving physical object properties by integrating a physics engine with deep learning</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yildirim</forename><forename type="middle">I</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Freeman</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3D object reconstruction without 3D supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08212</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR abs/1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Towards vision-based deep reinforcement learning for robotic motion control</title>
		<author>
			<persName><forename type="first">F</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Sparse representation-based open set recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence PP</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Deepcontext: context-encoding neural pathways for 3D holistic scene understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Rethinking reprojection: Closing the loop for pose-aware shape reconstruction from a single image</title>
		<author>
			<persName><forename type="first">R</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">C</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<idno>CoRR abs/1609.05143</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
