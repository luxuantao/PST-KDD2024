<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-11">11 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
							<email>jinheon.baek@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><forename type="middle">Bok</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<email>sjhwang82@kaist.ac.kr</email>
						</author>
						<author>
							<persName><surname>Kaist</surname></persName>
						</author>
						<author>
							<persName><surname>Aitrics</surname></persName>
						</author>
						<author>
							<persName><forename type="first">South</forename><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-11">11 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.06648v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many practical graph problems, such as knowledge graph construction and drugto-drug interaction, require to handle multi-relational graphs. However, handling real-world multi-label graphs with Graph Neural Networks (GNNs) is often challenging due to their evolving nature, where new entities (nodes) can emerge over time. Moreover, newly emerged entities often have few links, which makes the learning even more difficult. Motivated by this challenge, we introduce a realistic problem of few-shot out-of-graph link prediction, where we not only predict the links between the seen and unseen nodes as in a conventional out-of-knowledge link prediction but also between the unseen nodes, with only few edges per node. We tackle this problem with a novel transductive meta-learning framework which we refer to as Graph Extrapolation Networks (GEN). GEN meta-learns both the node embedding network for inductive inference (seen-to-unseen) and the link prediction network for transductive inference (unseen-to-unseen). For transductive link prediction, we further propose a stochastic embedding layer to model uncertainty in the link prediction between unseen entities. We validate our model on multiple benchmark datasets for knowledge graph link prediction and drug-to-drug interaction prediction. The results show that our model significantly outperforms relevant baselines for out-of-graph link prediction tasks.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs have strong expressive power to represent structured data, as it can model data into a set of nodes (objects) and edges (relations). To exploit the graph structured data which works on a non-Euclidean domain, several recent works propose graph-based neural architectures, referred to as Graph Neural Networks (GNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. While early work mostly deals with simple graphs with unlabeled edges, recently proposed relation-aware GNNs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> consider multi-relational graphs with labels and directions on the edges. These multi-relational graphs expand the application of GNNs to more real-world domains such as social networks modeling <ref type="bibr" target="#b17">[18]</ref>, natural language understanding <ref type="bibr" target="#b24">[25]</ref>, modeling protein structure <ref type="bibr" target="#b12">[13]</ref>, and drug-to-drug interaction prediction <ref type="bibr" target="#b59">[60]</ref>.</p><p>Among multi-relational graphs, Knowledge Graphs (KGs), which represent knowledge bases (KBs) such as Freebase <ref type="bibr" target="#b2">[3]</ref> and WordNet <ref type="bibr" target="#b26">[27]</ref>, get the most attention. They represent entities as nodes and relations among the entities as edges, in the form of a triplet: (head entity, relation, tail entity) (e.g. (Louvre museum, is located in, Paris)). Although knowledge graphs in general contain a huge amount of triplets, they are well known to be highly incomplete <ref type="bibr" target="#b27">[28]</ref>. Therefore, automatically completing knowledge graphs, which is known as the link prediction task, is a practically important problem for KGs. Prior work tackles this problem, i.e. inferring missing triplets, by learning embedding of entities and relations from existing triplets, achieving impressive performances <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>LAN <ref type="bibr">[Wang et al. 19]</ref> Seen Unseen Ours  Despite this success, the link prediction for KGs in real-world scenarios remains to be challenging for a couple of reasons. First, knowledge graphs dynamically evolve over time, rather than staying static. Shi and Weninger <ref type="bibr" target="#b39">[40]</ref> report that around 200 new entities emerge every day. Predicting links on these emerging entities pose a new challenge, especially when predicting the links between the emerging entities themselves. Moreover, real-world KGs generally exhibit long-tail distribution, where a large portion of the entities have only a few triplets to train (See Figure <ref type="figure" target="#fig_1">2</ref>). The embedding-based methods, however, usually assume that a sufficient amount of associative triplets exist for training, and cannot embed unseen entities. Thus they are highly suboptimal for learning and inference on evolving real-world graphs.</p><p>Motivated by the limitations of existing approaches, we introduce a realistic problem of few-Shot Out-Of-Graph (OOG) link prediction for emerging entities. In this task, we not only predict the links between seen and unseen entities but also between the unseen entities themselves (Figure <ref type="figure" target="#fig_0">1</ref>, left). To this end, we propose a novel meta-learning framework for OOG link prediction, which we refer to as Graph Extrapolation Networks (GENs). GENs are meta-learned to extrapolate the knowledge from seen to unseen entities and transfer knowledge from entities with many links to few links.</p><p>Specifically, given embeddings of the seen entities for a multi-relational graph, we meta-train two GNNs to predict the links between seen-to-unseen, and unseen-to-unseen entities. The first GNN, inductive GEN, learns to predict the embeddings of unseen entities that are not observed, and predicts the links between seen and unseen entities. The second GNN, transductive GEN, learns to predict the links between the unseen entities. This transductive inference is possible since our meta-learning framework can simulate the unseen entities during meta-training, while they are unobservable in conventional learning frameworks. Also, since link prediction for unseen entities is inherently unreliable, which gets worse when few triplets are available for each entity, we learn the distribution of unseen representations for stochastic embedding to account for uncertainty. Moreover, we apply transfer learning strategy to model the long-tail distribution. These lead GEN to learn embeddings for unseen entities to be well aligned with seen entities (See Figure <ref type="figure" target="#fig_0">1</ref>, right).</p><p>We validate GENs for their OOG link prediction performance on two benchmark knowledge graph completion datasets, namely FB15K-237 <ref type="bibr" target="#b2">[3]</ref> and NELL-995 <ref type="bibr" target="#b54">[55]</ref>. We also validate GENs for OOG drug-to-drug interaction prediction task on DeepDDI <ref type="bibr" target="#b35">[36]</ref> and BIOSNAP-sub <ref type="bibr" target="#b25">[26]</ref>. The experimental results on four datasets show that our model obtains significantly superior performance over the relevant baselines. Further analysis of each component shows that both inductive and transductive layer of GEN help with the accurate link prediction for out-of-graph entities. In sum, our main contributions are as follows:</p><p>• We tackle a realistic problem setting of few-shot out-of-graph link prediction, aiming to perform link prediction between unseen (emerging) entities for multi-relational graphs that exhibit long-tail distributions, where each entity has only few associative triplets. • To tackle this problem, we propose a novel meta-learning framework, Graph Extrapolation Network (GEN), which meta-learns the node embeddings for unseen entities, to obtain low error on link prediction for both seen-to-unseen (inductive) and unseen-to-unseen (transductive) cases. • We validate GEN for few-shot OOG link prediction tasks on four benchmark datasets for knowledge graph completion and drug-to-drug interaction prediction, on which it significantly outperforms relevant baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Neural Network Existing GNNs encode the nodes by aggregating the features from the neighboring nodes, that use recurrent neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref>, mean pooling with layer-wise propagation rule <ref type="bibr" target="#b21">[22]</ref>, learnable attention-weighted combination of the features <ref type="bibr" target="#b46">[47]</ref>, to name a few. While most of the existing models work with simple undirected graphs, some recent work tackles multi-relational graphs for their practical importance. Directed-GCN <ref type="bibr" target="#b24">[25]</ref> and Weighted-GCN <ref type="bibr" target="#b38">[39]</ref> consider direction and relation types, respectively. Also, R-GCN <ref type="bibr" target="#b37">[38]</ref> considers direction and relation types simultaneously. Recently, Vashishth et al. <ref type="bibr" target="#b45">[46]</ref> propose to jointly embed nodes and relations in a multi-relational graph. Since our GEN is a general framework for OOG link prediction rather than a specific GNN, it is compatible with any GNN implementations for multi-relational graphs.</p><p>Meta Learning Meta-learning, whose objective is to generalize over a distribution of tasks, is an essential approach for our few-shot OOG link prediction framework, where we simulate the unseen test nodes with a subset of training nodes. To mention a few, metric-based approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b40">41]</ref> learn a shared metric space to minimize the distance between correct and instance embeddings. Also, gradient-based approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> learn shared parameters for an initialization, to generalize over diverse tasks in a bi-level optimization framework. Relatively few works consider meta-learning with GNNs. Meta-GNN <ref type="bibr" target="#b58">[59]</ref> uses meta-learning for few-shot node classification, and Meta-Graph <ref type="bibr" target="#b5">[6]</ref> proposes to construct graphs over seen nodes, with only a small sample of known edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-relational Graphs</head><p>A popular application of multi-relation graphs is knowledge graph completion. Previous methods for this problem can be broadly classified as translational distance based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b50">51]</ref>, semantic matching based <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45]</ref> and deep neural network based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. While they require a large amount of training instances, many real-world graphs exhibit long-tail distribution. Few-shot relational learning methods tackle this issue by learning few relations of seen entities <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b6">7]</ref>. Nonetheless, the problem becomes more difficult as knowledge graphs have an evolving nature with new emerging entities. Several models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b51">52]</ref> tackle this problem by utilizing extra information about the entities, such as their textual description. Some recent methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b0">1]</ref> propose to handle unseen entities in an inductive manner. However, since they can not simulate the unseen entities in the training phase, there are some fundamental limitations on the generalization for handling actual unseen entities. On the other hand, our method entirely tackles both of seen-to-unseen and unseen-to-unseen link prediction, under the transductive meta-learning framework. Drug-to-drug interaction (DDI) prediction is another important real-world application of multi-relational graphs, where the problem is to predict interactions between drugs. Recently, Zitnik et al. <ref type="bibr" target="#b59">[60]</ref> and Ma et al. <ref type="bibr" target="#b23">[24]</ref> propose end-to-end GNNs to tackle this problem, which demonstrate comparatively better performance over non-GNN methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>3 Few-Shot Out-Of-Graph Link Prediction</p><p>Our goal is to perform link prediction for emerging entities of a multi-relational graph, in which a large portion of the entities have only few triplets associated with them. We begin with the formal definition of the multi-relational graph and the link prediction task. Link prediction for multi-relational graphs Link prediction is essentially the problem of assigning high scores to the true triplets, and therefore, many existing methods use score function s(e h , r, e t ) to measure the score of a given triplet, where the inputs depend on their respective embeddings (see Table <ref type="table" target="#tab_0">1</ref>). As a result, the objective of the link prediction is to find the representation of triplet elements and the function parameters in a parametric model case, which maximize the score of the true triplets. Which embedding methods to use depends on their specific application domain. However, existing work mostly tackles the link prediction between seen entities that already exist in the given multi-relational graph. In this work, we tackle a task of the few-shot Out-Of-Graph (OOG) link prediction defined as follows:</p><formula xml:id="formula_0">Definition 3.3. (Few-Shot Out-Of-Graph Link Prediction) Given a graph G ⊆ E × R × E,</formula><p>an unseen entity is an entity e ∈ E , where E ∩ E = ∅. Then, an out-of-graph link prediction is the problem of performing link prediction on (e , r, ?), (?, r, e ), (e , ?, e), or (e, ?, e ), where e ∈ (E ∪ E ). We further assume that each unseen entity e is associated with K triplets:</p><p>|{(e , r, e) or (e, r, e )}| ≤ K and e ∈ (E ∪ E ) , where K is a small number (e.g., 1 or 3).</p><p>While few existing works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref> tackle the entity prediction between seen and unseen entities, in real-world settings, unseen entities do not emerge one by one but may emerge simultaneously as a set, with only few triplets available for each entity. Thus, they are highly suboptimal for handling such real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning to Extrapolate Knowledge with Graph Extrapolation Networks</head><p>We now introduce Graph Extrapolation Networks (GENs) for the out-of-graph link prediction task. Since most of the previous methods assume that every entity in the test set is seen during training, they cannot handle emerging entities, which are unobserved during training. While few existing works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref> train for seen-to-seen link prediction with the hope that the models generalize on seen-to-unseen cases, they are suboptimal in handling unseen entities. Therefore, we use metalearning framework to handle the OOG link prediction problem, whose goal is to train a model over a distribution of tasks such that the model generalizes well on unseen tasks. Figure <ref type="figure" target="#fig_0">1</ref> illustrates our learning framework. Basically, we meta-train GEN which performs both inductive and transductive inference on various simulated test sets of OOG entities, such that it extrapolates the knowledge of existing graphs to any unseen entities. We describe the framework in details in next few paragraphs.</p><p>Learning Objective Suppose that we are given a multi-relational graph G ⊆ E × R × E, which consists of seen entities e ∈ E and relations r ∈ R. Then, we aim to represent the unseen entities e ∈ E over a distribution p(E ), by extrapolating the knowledge on a given graph G, to predict the link between seen e and unseen e entities: (e, r, e ) or (e , r, e), or even between unseen entities themselves: (e , r, e ). Toward this goal, we have to maximize the score of a triplet s(e h , r, e t ) that contains any unseen entities e , with embedding and score function parameters θ:</p><formula xml:id="formula_1">max θ E e ∼p(E ) [s(e h , r, e t ; θ)]</formula><p>, where e h ∈ E or e t ∈ E .</p><p>(</p><formula xml:id="formula_2">)<label>1</label></formula><p>While this is a seemingly impossible goal as it involves generalization to unseen entities, we can tackle it with meta-learning, which we describe next.</p><p>Meta-Learning Framework While conventional learning frameworks can not handle unseen entities in the training phase, with meta-learning, we can formulate a set of tasks such that the model learns to generalize over unseen entities, which are simulated using seen entities. To formulate the OOG link prediction problem into a meta-learning problem, we first randomly split the entities in a given graph into the meta-training and meta-test set. Then, we generate a task by sampling the set of (simulated) unseen entities for meta-training (See Figure <ref type="figure" target="#fig_0">1</ref>). Formally, each task T corresponds to a set of unseen entities E T ⊂ E , with a predefined number of instances |E T | = N . Then we divide the triplets associative with each entity e i ∈ E T into the support set S i and the query set</p><formula xml:id="formula_3">Q i : T = N i=1 S i ∪ Q i , where S i , Q i correspond to e i : S i = {(e i , r j , e j )</formula><p>or (e j , r j , e i )} K j=1 and Q i = {(e i , r j , e j ) or (e j , r j , e i )} Mi j=K+1 ; e j ∈ (E ∪ E ). K is the few-shot size, and M i is the number of triplets associated with each unseen entity e i . Our meta-objective is then learning to represent the unseen entities as φ using a support set S, to maximize the triplet score on a query set Q:</p><formula xml:id="formula_4">max θ E T ∼p(T )   1 N N i=1 1 |Q i | Mi j=K+1</formula><p>s(e i , r j , e j ; φ i , θ) or s(e j , r j , e i ; φ i , θ)</p><formula xml:id="formula_5">  , φ i = f θ (S i ). (2)</formula><p>We refer to this specific setting as K-shot OOG link prediction throughout this paper. Once the model is trained with the meta-training tasks T train , we can apply it to unseen meta-test tasks T test , whose set of entities is disjoint from T train , as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Meta-Learning of GEN</head><p>Require: Distribution over train tasks p (Ttrain), Require: Learning rate for meta-update α 1: Initialize parameters Θ = {θ, θµ, θσ} 2: while not done do 3: Sample a task T ∼ p (Ttrain) 4: for all e i ∈ T do 5: Sample support and query set {Si, Qi} correspond to e i 6:</p><p>Inductively generate using (3): φi = f θ (Si) 7: end for 8: for all e i ∈ T do 9:</p><p>Transductively generate using (4): µi = g θµ (Si, φ) and σi = g θσ (Si, φ) 10:</p><p>Sample</p><formula xml:id="formula_6">φ i ∼ N µi, diag σ 2 i 11:</formula><p>end for 12: Update Θ ← Θ − α∇Θ i L (Qi; φ i ) using ( <ref type="formula" target="#formula_12">6</ref>) 13: end while Graph Extrapolation Networks In order to extrapolate knowledge of a given graph G to an unseen entity e i through a support set S i , we propose a GNN-based meta-learner that outputs the representation of unseen entities. We formulate our meta-learner f θ (•) as follows (Figure <ref type="figure" target="#fig_2">3</ref>-</p><formula xml:id="formula_7">Inductive): f θ (S i ) = σ 1 K (r,e)∈n(Si) W r C r,e (3)</formula><p>, where we denote the set of neighboring entity and relation as n (S i ) = {(r, e) | (e i , r, e) or (e, r, e i ) ∈ S i }. Further, K is the size of n(S i ), W r ∈ R d×2d is a relation-specific transformation matrix that is meta-learned, and C r,e ∈ R 2d is a concatenation of feature representation of the relation-entity pair and σ is the ReLU function. Since GEN is essentially a framework for OOG link prediction, it is compatible with any GNNs.</p><p>Transductive Meta-Learning of GENs The previously described inductive GEN constructs the representation of each unseen entity e i through a support set S i , as described in (3), and perform link prediction on a query set Q i , independently. A major drawback of this inductive scheme is that it does not consider the relationships between unseen entities. However, to tackle unseen entities simultaneously as a set, one should consider not only the relationships between seen and unseen entities as with inductive GEN, but also among unseen entities themselves. To tackle this issue, we extend the inductive GEN to further perform a transductive inference, which will allow knowledge to propagate between unseen entities.</p><p>More specifically, we add one more GEN layer g θ (•), which is similar to the inductive meta-learner f θ (•), to consider inter-relationships between unseen entities (Figure <ref type="figure" target="#fig_2">3</ref>-Transductive): g θ (S i , φ) = 1 K (r,e)∈n(Si) W r C r,e +W 0 φ i , (4), where W 0 ∈ R d×d is a weight matrix for the self-connection to consider embedding φ i , which is updated by the previous inductive layer f θ (S i ). To leverage the knowledge of neighboring unseen entities, our transductive layer g θ (•) aggregates the representations across all the associative neighbors with transductive weight matrix W r ∈ R d×2d , where the neighbors can include the representations of unseen entities φ, rather than treating them as noises.</p><p>Stochastic Inference A naive transductive GEN generalizes to unseen entities by simulating them with seen entities during the meta-training. However, due to the intrinsic unreliability of few-shot OOG link prediction with each entity having only few triplets, there could be high uncertainties on the representation of unseen entities. To model such uncertainties, we stochastically embed the unseen entities by learning the distribution over an unseen entity embedding φ i . To this end, we first assume that the true posterior distribution has a following form: p(φ i | S i , φ). Since computation of the true posterior distribution is intractable, we approximate the posterior using</p><formula xml:id="formula_8">q (φ i | S i , φ) = N φ i | µ i , diag σ 2 i</formula><p>, and then compute the mean and variance via two individual transductive GEN layers µ i = g θµ (S i , φ) and σ i = g θσ (S i , φ), which modifies GraphVAE <ref type="bibr" target="#b20">[21]</ref> to our setting. The form to maximize the score function is then defined as follows:</p><formula xml:id="formula_9">s (e h , r, e t ) = 1 L L l=1</formula><p>s e h , r, e t ; φ (l) , θ , φ (l) ∼ q(φ | S, φ).</p><p>(</p><formula xml:id="formula_10">)<label>5</label></formula><p>where we set the MC sample size to L = 1 during meta-training for computational efficiency. Also, we perform MC approximation with sufficiently large sample size (e.g. L = 10) at meta-testing. We let the approximate posterior same as the prior to make the consistent pipeline at training and testing (see Sohn et al. <ref type="bibr" target="#b41">[42]</ref>). We also model the source of uncertainty on output embedding of an unseen entity from the transductive GEN layer via Monte Carlo dropout <ref type="bibr" target="#b13">[14]</ref>. Our final GEN is trained for both the inductive and transductive steps, as described in Algorithm 1.</p><p>Loss Function Each task T consists of a support set and a query set: T = {S, Q}. During training, we represent the embedding of unseen entities e i ∈ E T using the support set S i with GENs. After that, at the test time, we use the true labeled query set Q i to optimize our GENs. Since every query set contains only positive triplets, we perform negative sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">57]</ref> to update meta-learner by allowing it to distinguish positive from negative triplets. Specifically, we replace the entity of each triplet in the query set:</p><formula xml:id="formula_11">Q − i = {(e i , r, e − ) or (e − , r, e i )}</formula><p>, where e − is the corrupted entity and Q − i holds negative samples for an unseen entity e i . We use hinge loss to optimize our model:</p><formula xml:id="formula_12">L (Q i ) = (e h ,r,et)∈Qi (e h ,r,et) − ∈Q − i max γ − s + (e h , r, e t ) + s − (e h , r, e t ) − , 0 ,<label>(6)</label></formula><p>where γ &gt; 0 is a margin hyper-parameter and s is the result of each score function in the Table <ref type="table" target="#tab_0">1</ref>. s + and s − denote the scores of positive and negative triplets, respectively. For Drug-to-Drug interaction predict task, we follow Ryu et al. <ref type="bibr" target="#b35">[36]</ref> to optimize our model, where binary cross-entropy loss is calculated for each label with a sigmoid output of the score function in the Table <ref type="table" target="#tab_0">1</ref>.</p><p>Meta-Learning for Long-Tail Tasks Since many real-world graphs follow the long-tail distribution (See Figure <ref type="figure" target="#fig_1">2</ref>), it would be beneficial to transfer the knowledge from entities with large links to entities with only few links. To this end, we follow a scheme similar to Wang et al. <ref type="bibr" target="#b49">[50]</ref> and start to learn the model with many shot cases, then gradually decrease the number of shots to few shot cases in a logarithmic scale. Further implementation details are given in the Section B of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We validate GEN on few-shot OOG link prediction tasks for two different applications of multirelational graphs: knowledge graph (KG) completion and drug-to-drug interaction (DDI) prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Knowledge Graph Completion</head><p>Datasets For knowledge graph completion datasets, we consider out-of-graph entity prediction, whose goal is to predict the other entity given an unseen entity and a relation. 1) FB15k-237. This dataset <ref type="bibr" target="#b43">[44]</ref> consists of 310, 116 triplets from 14, 541 entities and 237 relations, which is collected via crowdsourcing. 2) NELL-995. This dataset <ref type="bibr" target="#b54">[55]</ref> consists of 154, 213 triplets from 75, 492 entities and 200 relations, which is collected by a lifelong learning system <ref type="bibr" target="#b28">[29]</ref>. Since existing benchmark datasets do not target OOG link prediction, they assume that all entities given at the test time are seen during training. Therefore, we modify the dataset such that the triplets used for link prediction at test time contain at least one unseen entity (see Appendix A.1 for the detailed setup).</p><p>Baselines and our models 1) TransE. 2) RotatE. Translation distance based embedding methods for multi-relational graph <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref>. 3) DistMult. 4) ComplEx. Semantic matching based embedding methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b44">45]</ref>. 5) R-GCN. This is a GNN-based method for modeling relational data <ref type="bibr" target="#b37">[38]</ref>. 6) MEAN. 7) LAN. These are GNN models for a out-of-knowledge base task, which tackle unseen entities without meta-learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>. 8) GMatching. This model tackles the link prediction on unseen relations of seen entities, and we extend it to our meta-learning framework <ref type="bibr" target="#b55">[56]</ref>. 9) I-GEN.</p><p>An inductive version of our GEN which is meta-learned to embed an unseen entity. 10) T-GEN. A transductive version of GEN, with additional stochastic transductive GNN layers to predict the link between unseen entities. We report detailed description in the Appendix A.2.</p><p>Implementation Details. For both I-GEN and T-GEN, we use DistMult for the initial embedding of entities and relations, and the score function. However, the models that use TransE for embedding and score function perform similarly, which we report in the Appendix C. Following Xiong et al. <ref type="bibr" target="#b55">[56]</ref>, we train seen-to-seen link prediction baselines including support triplets of meta-valid and meta-test sets, since baselines assume that only seen entities will appear at the test time and thus unable to solve the problem if the entity in triplets is completely unseen. However, for our methods, we train them only with the meta-training set, where we generate OOG entities using episodic training. Detailed experimental setups used for both datasets are described in the Appendix A.3. Model MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen TransE <ref type="bibr" target="#b4">[5]</ref> .   Evaluation Metrics For evaluation, we use the ranking procedure by Bordes et al. <ref type="bibr" target="#b3">[4]</ref>. For a triplet with an unseen head entity, we replace its corresponding tail entity with candidate entities from the dictionary to construct corrupted triplets. Then, we rank all the triplets, including the correct and corrupted ones by a scoring measure, to obtain the rank of the correct triplet. We provide the results using mean reciprocal rank (MRR) and Hits at n (H@n); MRR is the average of the multiplicative inverse of the rank of the correct triplets and H@n is the ratio of the correct triplets ranked smaller than or equal to n. Moreover, as done in previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>, we measure the ranks in a filtered setting where we do not consider triplets that appeared in either training, validation, or test sets.</p><p>Results Table <ref type="table" target="#tab_1">2</ref> shows that our I-GEN and T-GEN outperform all baselines by impressive margins in all evaluation metrics with 1-shot and 3-shot settings. Baseline models work poorly on emerging entities that come with only a few triplets to train, even when they have seen the entities during training (with Support Set in Table <ref type="table" target="#tab_1">2</ref>). However, in our meta-learning framework, our GENs show superior performance over the baselines, with even one training triplet for each unseen entity. Moreover, while GMatching <ref type="bibr" target="#b55">[56]</ref>, which handles unseen entities by searching for the closest entity pair, with our meta-learning framework achieves decent performance, it significantly underperforms ours. We also observe that T-GEN outperforms I-GEN on both datasets by all evaluation metrics.</p><p>To see where the performance improvement comes from, we further examine the link prediction results for seen-to-unseen and unseen-to-unseen cases. Figure <ref type="figure" target="#fig_3">4</ref> shows that T-GEN obtains significant performance gain on the unseen-to-unseen link prediction problems, whereas I-GEN mostly cannot handle the case as it does not consider the relationships between unseen nodes. Also, while we mostly target a long-tail graph with the majority of the entities having few links, our method works well on many-shot cases as well (Figure <ref type="figure" target="#fig_4">5</ref>), on which I-and T-GENs still largely outperform the baselines, even though R-GCN sees the unseen entities during training.</p><p>Table <ref type="table">3</ref>: Cross-shot learning results of T-GEN on KG completion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Shot 3-Shot</head><p>Test MRR H@1 H@10 MRR H@1 H@10 We further experiment our GEN with varying the number of triplets by considering 1-, 3-, 5-, and random-shot (between 1 and 5) during meta-training and meta-test. Table <ref type="table">3</ref> shows that the difference in the number of shots used for training and test does not significantly affect the performance, which demonstrates the robustness of GENs on varying number of triplets at test time. Moreover, our model trained on 1-shot setting obtains even better performance on 3-shot setting. Furthermore, we conduct an ablation study of the T-GEN on seen-to-unseen (S/U) and unseen-to-unseen (U/U) cases. Table <ref type="table" target="#tab_4">4</ref> shows that using stochastic modeling on the transductive inference layer helps significantly improve the unseen-to-unseen link prediction performance. Moreover, the meta-learning strategy of learning on entities with many links and then progressing to entities with few links performs well. Finally, we observe that using pre-trained embedding of a seen graph leads to better performance. We visualize the output representations of unseen entities with seen entities. Figure <ref type="figure" target="#fig_0">1</ref> (Right) shows that the embeddings of unseen entities are well aligned with the seen entities. Regarding concrete examples of link prediction on NELL-995, see the Section D of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Drug-to-Drug Interaction</head><p>Datasets We further validate our GENs on the OOG relation prediction task using two public Drug-to-Drug Interaction (DDI) datasets. 1) DeepDDI. This dataset <ref type="bibr" target="#b35">[36]</ref>  Baselines and our models 1) MLP. Feed-forward neural networks used in DDI task <ref type="bibr" target="#b35">[36]</ref>. 2) MPNN. Graph Neural Networks that use edge-conditioned convolution <ref type="bibr" target="#b14">[15]</ref>. 3) R-GCN. The same model used in the entity prediction on KG completion task <ref type="bibr" target="#b37">[38]</ref>. 4) I-GEN. Inductive GEN, which only uses feature representation of an entity e k , instead of a relation-entity pair (r k , e k ). This is because the relation is the prediction target for the DDI tasks. 5) T-GEN. Transductive GEN with an additional transductive stochastic layers for unseen-to-unseen relation prediction.</p><p>Implementation Details and Evaluation Metrics For both I-GEN and T-GEN, we use MPNN for the initial embedding of entities with linear score function in Table <ref type="table" target="#tab_0">1</ref>. To train baselines, we use the same training scheme as the KG completion task, where support triplets of meta-valid and meta-test sets are included in the training set. Detailed experimental settings are described in the Appendix A.3. For evaluation, we use the area under the receiver operating characteristic curve (ROC), the area under the precision-recall curve (PR), and the classification accuracy (Acc). Results Table <ref type="table" target="#tab_6">5</ref> shows the DDI prediction performance of the baselines and GENs. Note that the performances on BIOSNAP-sub are comparatively lower in comparison to DeepDDI, due to the use of the preprocessed input features, as suggested by Ryu et al. <ref type="bibr" target="#b35">[36]</ref>. Similarly with the KG completion tasks, both I-and T-GEN outperform all baselines by impressive margins in all evaluation metrics. These results demonstrate that our GENs can be extended to OOG link prediction for other real-world applications of multi-relational graphs. We also compare the link prediction performance for both seen-to-unseen and unseen-to-unseen cases on two DDI datasets. The rightmost two columns of Figure <ref type="figure" target="#fig_3">4</ref> show that T-GEN obtains superior performance over I-GEN on unseen-to-unseen link prediction, especially on stochastic modeling cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a realistic problem of the few-shot out-of-graph (OOG) link prediction, which considers link prediction between unseen (or emerging) entities for multi-relational graphs, where each entity comes with only few associative triplets. To this end, we proposed a novel meta-learning framework for OOG link prediction, which we refer to as Graph Extrapolation Network. Under the defined K-shot learning setting, GENs learn to extrapolate the knowledge of a given graph to unseen entities, with a stochastic transductive layer to further propagate the knowledge between the unseen entities and model uncertainty in the link prediction. We validated the OOG link prediction performance of GENs on four benchmark datasets, on which it largely outperformed the relevant baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Constructing knowledge bases which accurately reflect up-to-date knowledge about the entities and the links between them is crucial for its application in real-world scenarios. However, conventional link prediction methods for knowledge base systems mostly consider static knowledge graph that does not change over time. Yet, as new entities emerge every day <ref type="bibr" target="#b39">[40]</ref> (e.g., COVID-19), the ability to dynamically incorporating them into the existing knowledge graph is becoming a significantly important problem, which we mainly tackle in this paper.</p><p>As a specific example of our approach, the novel coronavirus, COVID-19, is threatening our lives around the globe. To eradicate the novel coronavirus, we may want to best utilize the accumulated knowledge about existing coronavirus variants <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b7">8]</ref> by identifying the links between the seen (SARS and MERS) and unseen entities (COVID- <ref type="bibr" target="#b18">19)</ref>, or the links between unseen entities that have newly emerged (COVID-19 and novel vaccine under study). The following are more use cases of our proposed out-of-graph link prediction system:</p><p>• The proposed meta-learning based few-shot out-of-graph link prediction method can infer and inform the relationship between the entities that describe past coronavirus outbreaks and the current COVID-19 situation. • Our transductive inference, with stochastic transductive GENs, can lead to finding the relationships among novel entities regarding COVID-19 which rapidly emerge over time, that may allow us to discover meaningful links among them. • Regarding drug-to-drug interaction prediction, our method can be further utilized to analyze the side-effects of simultaneously taking a novel antiviral drugs for COVID-19 and existing drugs, before the clinical trials.</p><p>While we describe the impact of our method on a specific, but significantly important topic, our method can be broadly applied to any real-world applications that require to predict the links which involve unseen entities. While our method obtains significantly better performance over existing methods on out-of-graph link prediction, its prediction performance is yet far from perfect. Thus, the model should be used more as a candidates selection tool (Hits@N) when inferring critical information (e.g. drug-to-drug interaction prediction for COVID-19), and more efforts should be made to develop a reliable system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Datasets</head><p>Since existing benchmark datasets assume that all entities given at the test time are seen during training, we modify the datasets to formulate the Out-of-Graph (OOG) link prediction task, where completely unseen entities appear at the test time. Datasets modification processes are as follows:</p><p>• First, we randomly sample the unseen entities, which have a relatively small amount of triplets on each dataset. We then divide the sampled unseen entities into meta-training/validation/test sets. • Second, we select the triplets which are used for constructing an In-Graph, where the head and tail entities of every triplet in the In-Graph do not contain any unseen entity. • Finally, we match the unseen entities in the meta-sets with their triplets. Each triplet in meta-sets contains at least one unseen entity. Also, every triplet in meta-sets is not included in the In-Graph. 1) FB15k-237. This dataset <ref type="bibr" target="#b43">[44]</ref> consists of 14,541 entities, which is used for the knowledge graph completion task. We randomly sample the 5,000 entities from 10,938 entities, which have associated triplets between 10 and 100. Also, we split the entities such that we have 2,500/1,000/1,500 unseen (Out-of-Graph) entities and 72,065/6,246/9,867 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph. As shown in the Figure <ref type="figure" target="#fig_5">6</ref>, this dataset follows a highly long-tailed distribution.</p><p>2) NELL-995. This dataset <ref type="bibr" target="#b54">[55]</ref> consists of 75,492 entities, which is used for the knowledge graph completion task. We randomly sample the 3,000 entities from 5,694 entities, which have associated triplets between 7 and 100. Also, we split the entities such that we have 1,500/600/900 unseen (Out-of-Graph) entities and 22,345/3,676/5,852 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph. As shown in the Figure <ref type="figure" target="#fig_5">6</ref>, this dataset follows a highly long-tailed distribution.</p><p>3) DeepDDI. This dataset <ref type="bibr" target="#b35">[36]</ref> consists of 1,861 entities, which is used for the drug-to-drug interaction prediction task. We randomly sample the 500 entities from 1,039 entities, which have associated triplets between 7 and 300. Also, we split the entities such that we have 250/100/150 unseen (Out-of-Graph) entities and 27,726/1,171/2,160 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph.</p><p>4) BIOSNAP-sub. This dataset <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref> consists of 637 entities, which is used for the drug-todrug interaction prediction task. We randomly sample the 150 entities from 507 entities, which have associated triplets between 7 and 300. Also, we split the entities such that we have 75/30/45 unseen (Out-of-Graph) entities and 7,140/333/643 associated triplets containing unseen entities for meta-training/validation/test. The remaining triplets that do not hold an unseen entity are used for constructing In-Graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baselines and Our Models</head><p>Knowledge Graph Completion We describe the baseline models and our graph extrapolation networks for few-shot out-of-graph entity prediction on the knowledge graph (KG) completion task.</p><p>1) TransE. The translation embedding model for a multi-relational data by Bordes et al. <ref type="bibr" target="#b4">[5]</ref>. It represents both entities and relations as vectors in the same space, where the relation in a triplet is used as a translation operation between the head and the tail entity.</p><p>2) RotatE. This model represents entities as complex vectors and relations as rotations in a complex vector space <ref type="bibr" target="#b42">[43]</ref>, which extends TransE with a complex operation.</p><p>3) DistMult. This model represents the relationship between the head and the tail entity in a bi-linear formulation, which captures pairwise interaction between entities <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>ComplEx. This model extends the DistMult by introducing embeddings on a complex space to consider asymmetric relations, where scores are differently measured based on the order of the entities <ref type="bibr" target="#b44">[45]</ref>.</p><p>5) R-GCN. This is a GNN-based method for modeling relational data, which extends the graph convolutional network to consider multi-relational structure, by Schlichtkrull et al. <ref type="bibr" target="#b37">[38]</ref>.</p><p>6) MEAN. This model computes the embedding of entities by GNN based neighboring aggregation scheme, where they only train for seen-to-seen link prediction, with the hope that the model generalizes on seen-to-unseen cases <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>) LAN. This model extends the MEAN to consider relation and neighbor-level information by utilizing attention mechanisms <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8)</head><p>GMatching. This model tackles the link prediction on unseen relations of seen entities by searching for the closest entity pair, and we extend it in our meta-learning framework such that it can handle unseen entities <ref type="bibr" target="#b55">[56]</ref>.</p><p>9) I-GEN. An inductive version of our Graph Extrapolation Network (GEN), that is meta-learned to embed an unseen entity into the embedding space to infer hidden links between seen and unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10) T-GEN.</head><p>A transductive version of GEN, with additional stochastic transductive GNN layers on top of the I-GEN, that is meta-learned to predict the links not only between unseen entities but also between seen and unseen entities.</p><p>Drug-to-Drug Interaction We describe the baseline models and our graph extrapolation networks for few-shot out-of-graph relation prediction on the drug-to-drug interaction (DDI) task.</p><p>1) MLP. The feed-forward neural network used in DeepDDI <ref type="bibr" target="#b35">[36]</ref>. It classifies the relation of two drugs using their pairwise features.</p><p>2) MPNN. The GNN-based model which uses features about relation types with edge-conditioned convolution operations <ref type="bibr" target="#b14">[15]</ref>.</p><p>3) R-GCN. The same model used in the entity prediction on KG completion tasks, applied to DDI tasks.</p><p>4) I-GEN. An inductive GEN, which only uses the feature representation of the entity e k , instead of using the concatenated representation of the relation-entity pair (r k , e k ), when aggregating neighboring information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) T-GEN.</head><p>A transductive GEN, with additional transductive stochastic layers for unseen-to-unseen relation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details</head><p>For every dataset, we set the embedding dimension of entity and relation as 100. Also, we set the initial embedding of unseen entities as zero vector. Furthermore, since we consider a highly multi-relational graph, we use the basis decomposition on weight matrices W r and W r to prevent the excessive increase in the model size, proposed in Schlichtkrull et al. <ref type="bibr" target="#b37">[38]</ref>: W r = B b=1 a r b V b , where B is the number of basis, a r b is a coefficient of each relation r ∈ R and V b ∈ R d×2d is a shared representation of various relations. For all the experiment, we use PyTorch <ref type="bibr" target="#b34">[35]</ref> and PyTorch geometric <ref type="bibr" target="#b10">[11]</ref> frameworks on a single Titan XP or a single GeForce RTX 2080 Ti GPU. We optimize the proposed GENs using Adam <ref type="bibr" target="#b19">[20]</ref>.</p><p>Table <ref type="table">6</ref>: The naive and meta-learning strategy results of 1-and 3-shot OOG link prediction on FB15k-237 and NELL-995. Bold numbers denote the best results on I-GEN and T-GEN, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB15k-237 NELL-995</head><p>Model MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3 Knowledge Graph Completion For both I-GEN and T-GEN, we search for the learning rate α in the range of 3 × 10 −4 , 1 × 10 −3 , 3 × 10 −3 , margin γ in the range of {0.25, 0.5, 1}, and dropout ratio at every GEN layer in the range of {0.1, 0.2, 0.3}. To select the best model, we use the mean reciprocal rank (MRR) as an evaluation metric. For FB15k-237 dataset, we set the α = 1 × 10 −3 and γ = 1 with dropout rate 0.3. Also, we set the number of basis units B = 100 for the basis decomposition on each GEN layer, and sample 32 negative triplets for each positive triplet in both I-GEN and T-GEN. At every episodic training, we randomly sample 500 unseen entities in the meta-training set. For NELL-995 dataset, we use the same parameter settings with FB15k-237, except that we sample 64 negative triplets for each positive triplet. For both datasets, we consider the inverse relation as suggested by several recent works on multi-relational graphs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>, where directed relation information flows along with both directions.</p><p>Drug-to-Drug Interaction For both I-GEN and T-GEN, we search for the learning rate α in the range of 5 × 10 −4 , 1 × 10 −3 , 5 × 10 −3 , and dropout ratio at every GEN layer in the range of {0.1, 0.2, 0.3}. As a score function, we use two linear layers with ReLU activation function at the end of the first layer. To select the best model, we use the area under the receiver operating characteristic curve (ROC) as an evaluation metric. For DeepDDI dataset, we set the α = 1 × 10 −3 with dropout rate 0.3. Also, we set the number of basis units B = 200 for the basis decomposition. At every episodic training, we randomly sample 80 unseen entities in the meta-training set. For BIOSNAP-sub dataset, we set the α = 1 × 10 −3 with dropout rate 0.1 for I-GEN and 0.2 for T-GEN, respectively. Also, we set the number of basis units B = 200 for the basis decomposition. At every episodic training, we randomly sample 50 unseen entities in the meta-training set. For both datasets, we consider the inverse relation as in the case of knowledge graph completion task, where directed relation information flows along with both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Meta-learning for Long-tail Task</head><p>Implementation Details Many real-world graphs follow the long-tail distribution, where few entities have many links while the majority have few (See Figure <ref type="figure" target="#fig_5">6</ref>). For such an imbalanced graph, it would be beneficial to transfer the knowledge from entities with many links to entities with few links. To this end, we transfer the meta-knowledge on data-rich entities to data-poor entities by simulating the data-rich circumstance under the meta-learning framework, motivated by Wang et al. <ref type="bibr" target="#b49">[50]</ref>. Specifically, we firstly meta-train our GENs with many shot cases (e.g., K = 10), and then gradually decrease the number of shots to few shots cases (e.g., K = 1 or 3) in logarithmic scale: K i = log 2 (max-iteration/i) + K, where K i is the training shot size at the current iteration number i, and K is the test shot size. In this way, GENs learn to represent the unseen entities using data-rich instances, and entities with few links regimes may experience like data-rich instances, with the model parameters trained on the entities with many links and fine-tuned on the entities with few links.</p><p>More Ablation Studies Since knowledge graphs follow a highly long-tailed distribution (See Figure <ref type="figure" target="#fig_5">6</ref>), we provide the more experimental results about transfer strategies on knowledge graph completion tasks, to demonstrate the effectiveness of the proposed meta-learning scheme on a long-tail task. Table <ref type="table">6</ref> shows that the transfer strategy outperforms naive I-GEN and T-GEN on all evaluation metrics, except for two H@1 cases of T-GEN on 3-shot OOG link prediction settings. We conjecture that the effectiveness of the meta-learning scheme is especially larger on 1-shot cases, where data is extremely poor, rather than the 3-shot cases. Table <ref type="table">7</ref>: Total, seen-to-unseen and unseen-to-unseen results of 1-and 3-shot OOG link prediction on FB15k-237.</p><p>* means training a model within our meta-learning framework. Bold numbers denote the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total</head><p>Seen to Unseen Unseen to Unseen Model MRR H@1 H@3 H@10 MRR H@10 MRR H@10 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S 1-S 3-S Seen to Seen TransE <ref type="bibr" target="#b4">[5]</ref> . Effect of Score Function We also evaluate proposed GENs on the few-shot OOG link prediction task with another popular score function, namely TransE <ref type="bibr" target="#b4">[5]</ref>. We use the same settings with DistMult <ref type="bibr" target="#b56">[57]</ref> score function, except that we use TransE for the initial embedding and the score measurement. Table <ref type="table">7</ref> shows that our I-GEN and T-GEN with TransE score function also outperform all baselines by impressive margins, where they perform comparably to DistMult. These results suggest that our model works regardless of the score function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Initialization</head><p>We further demonstrate the meta-training effectiveness of our meta-learner, by randomly initializing In-Graph, in which GEN extrapolates knowledge for an unseen entity without using the pre-trained embedding of entity and relation. Table <ref type="table">7</ref> shows that, while results with the random initialization are lower than pre-trained models, GENs are still powerful on the unseen entity, compared to the baseline. These results suggest that GENs trained under the meta-learning framework can be applied to more difficult situations, as pre-trained In-Graph might not be available for the few-shot OOG link prediction in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Examples</head><p>Table <ref type="table" target="#tab_9">8</ref> shows some concrete examples of the OOG link prediction result from NELL-995 dataset, where the 7 to 9 rows show that our T-GEN correctly performs link prediction for two unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Discussion on Inductive and Transductive</head><p>In this section, we describe in detail about task-level transductive inference and meta-level inductive inference for the proposed transductive GEN (T-GEN) model. Since transductive GEN requires to predict links between two unseen test entities which is impossible to handle using conventional link prediction approaches, the problem is indeed transductive. Furthermore, the inference of unseento-unseen links could be also considered as inductive at meta-level, where we inductively learn the parameters of GEN across the batch of tasks. Thus, we are tackling transductive inference problems by considering them as meta-level inductive problems, but the intrinsic unseen-to-unseen link prediction is still transductive. To illustrate more concretely, different sets of unseen entities make mutually inconsistent predictions, which is caused by transduction. Other transductive meta-learning approaches such as TPN <ref type="bibr" target="#b22">[23]</ref> and EGNN <ref type="bibr" target="#b18">[19]</ref> tackle the problem with similar high-level ideas, where they classify unseen classes by leveraging both of the information on labeled and unlabeled nodes. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Concept (Left): An illustration of out-of-graph link prediction for emerging entities. Blue dotted arrows denote inferred relationships between seen and unseen entities, and red dotted arrows denote inferred relationships between unseen entities. (Center): An illustration of our meta-learning framework for Out-Of-Graph link prediction task. Orange arrows denote the support (training) set S and green dotted arrows denote the query (test) set Q. Visualizations of the learned embeddings (Right): Our transductive GEN embeds the unseen entities on the manifold of seen entities, while the baseline [49] embeds them off-manifold.</figDesc><graphic url="image-1.png" coords="2,111.71,72.00,289.58,82.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution for entity frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall framework of our model for each task. We extrapolate knowledge by using a support set S with inductive and transductive learning, and then predict links with output of the embedding φ .</figDesc><graphic url="image-2.png" coords="5,353.54,72.00,142.96,119.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The results of seen to unseen (S/U), unseen to unseen (U/U) and total link prediction of I-and T-GEN with deterministic (D) and stochastic (S) modeling on KG completion and DDI prediction tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Diverse shots link prediction results with baselines and GENs on KG completion tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution for entity occurrences on four datasets.</figDesc><graphic url="image-3.png" coords="15,109.19,251.00,92.66,69.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Definition 3.1. (Multi-relational Graph) Let E and R be two sets of entities and relations respectively. Then a relation is defined as a triplet (e h , r, e t ), where e h , e t ∈ E are the head and the tail entity, and r ∈ R is a specific type of relation between them. A multi-relational graph G is represented as a collection of triplets. That is, G = {(e h , r, e t )} ⊆ E × R × E. Score functions for multi-relational graph, where ⊕ denotes concatenation.</figDesc><table><row><cell cols="3">Definition 3.2. (Link Prediction) Link prediction refers to the task of predicting an unknown item of</cell></row><row><cell cols="3">a triplet, when given two other items. We consider both of the entity prediction and relation prediction</cell></row><row><cell cols="3">tasks. Entity prediction refers to the problem of predicting e ⊆ E, given the entity and the relation:</cell></row><row><cell cols="3">(e h , r, ?) or (?, r, e t ). Relation prediction refers to the problem of predicting r ⊆ R, given the head</cell></row><row><cell>and tail entities: (e h , ?, e t ).</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Score Function</cell><cell>Domain</cell></row><row><cell>TransE [5]</cell><cell cols="2">− eh + r − et 2 Knowledge Graph</cell></row><row><cell>DistMult [57]</cell><cell>eh, r, et</cell><cell>Knowledge Graph</cell></row><row><cell>Linear [15]</cell><cell>r(eh ⊕ et)</cell><cell>Drug Interaction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results of 1-and 3-shot OOG link prediction on FB15k-237 and NELL-995. * means training a model within our meta-learning framework. Bold numbers denote the best results.</figDesc><table><row><cell>FB15k-237</cell><cell>NELL-995</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of T-GEN on FB15k-237.SI means whether to apply stochastic inference.</figDesc><table><row><cell></cell><cell>S / U</cell><cell>U / U</cell></row><row><cell>Model</cell><cell cols="2">SI MRR H@3 MRR H@3</cell></row><row><cell>T-GEN</cell><cell cols="2">O .379 .424 .185 .187</cell></row><row><cell>w/o transfer strategy</cell><cell cols="2">O .374 .414 .183 .175</cell></row><row><cell>w/o pretrain</cell><cell cols="2">O .361 .400 .168 .164</cell></row><row><cell cols="3">w/o stochastic inference X .384 .425 .153 .158</cell></row><row><cell cols="3">w/o transductive scheme X .366 .403 .000 .000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>consists of 1,861 drugs (entities) and 222,127 drug-drug pairs (triplets) from DrugBank, where 113 different relation types are used as labels. 2) BIOSNAP-sub. This dataset<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref> consists of 645 drugs (entities) and 46,221 drug-drug pairs (triplets), where 200 different relation types are used as labels. Similar to the experiments on OOG knowledge graph completion tasks, we modify drug-to-drug interaction dataset for OOG link prediction task (see Appendix A.1 for the detailed setup).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The results of 3-shot relation prediction on DeepDDI and BIOSNAP-sub.</figDesc><table><row><cell></cell><cell>DeepDDI</cell><cell>BIOSNAP-sub</cell></row><row><cell>Model</cell><cell cols="2">ROC PR Acc ROC PR Acc</cell></row><row><cell>MLP</cell><cell cols="2">.928 .476 .528 .597 .034 .049</cell></row><row><cell cols="3">MPNN [15] .939 .478 .681 .597 .026 .067</cell></row><row><cell cols="3">R-GCN [38] .928 .397 .640 .594 .041 .051</cell></row><row><cell>I-GEN</cell><cell cols="2">.946 .681 .807 .608 .062 .073</cell></row><row><cell>T-GEN</cell><cell cols="2">.954 .708 .815 .625 .067 .089</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.052 .058 .109 .119 .207 .217 .112 .121 .221 .231 .000 .000 .000 .000 LAN [49] .112 .112 .057 .055 .118 .119 .214 .218 .119 .119 .228 .232 .000 .000 .000 .000 GMatching* [56] .224 .238 .157 .168 .249 .263 .352 .372 .239 .254 .375 .400 .000 .000 .000 .000 Ours I-GEN (Random) .309 .319 .236 240 .337 .352 .455 .477 .329 .339 .485 .508 .000 .000 .000 .000 I-GEN (DistMult) .348 .367 .270 .281 .382 .407 .504 .537 .371 .391 .537 .571 .000 .000 .000 .000 I-GEN (TransE) .345 .371 .259 .275 .385 .416 .515 .559 .367 .395 .548 .594 .000 .000 .000 .000 T-GEN (Random) .349 .360 .268 .273 .385 .398 .508 .532 .361 .373 .529 .554 .168 .164 .185 .192 T-GEN (DistMult) .367 .382 .282 .289 .410 .430 .530 .565 .379 .396 .550 .588 .185 .175 .220 .201 T-GEN (TransE) .356 .374 .267 .282 .403 .425 .531 .552 .368 .387 .552 .572 .175 .175 .205 .235</figDesc><table><row><cell></cell><cell>053 .048 .034 .026 .050 .050 .082 .077 .055 .050 .086 .081 .016 .014 .029 .025</cell></row><row><cell>DistMult [57]</cell><cell>.017 .014 .010 .009 .019 .014 .029 .022 .018 .015 .029 .022 .011 .007 .025 .015</cell></row><row><cell>R-GCN [38]</cell><cell>.008 .006 .004 .003 .007 .005 .011 .010 .003 .003 .005 .006 .076 .050 .101 .070</cell></row><row><cell>MEAN [17]</cell><cell>.105 .114</cell></row><row><cell>Seen to Unseen</cell><cell></cell></row></table><note>C More Experimental Results</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Examples of the OOG link prediction on NELL-995. S: seen, U: unseen, O: correct prediction, X: incorrect prediction, (H): head entity, (R): relation, (T): tail entity, and : unseen entity.</figDesc><table><row><cell cols="4">Type I-GEN T-GEN Triplet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) musician_vivaldi,</cell></row><row><cell>S-U</cell><cell>O</cell><cell>O</cell><cell>(R) musician_plays_instrument,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) music_instrument_string</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) city_hawthorne,</cell></row><row><cell>S-U</cell><cell>O</cell><cell>O</cell><cell>(R) city_located_in_state,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) state_or_province_california</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) journalist_maureen_dowd,</cell></row><row><cell>S-U</cell><cell>O</cell><cell>O</cell><cell>(R) works_for,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) company_york_times</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) person_monroe,</cell></row><row><cell>S-U</cell><cell>O</cell><cell>O</cell><cell>(R) person_born_in_location,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) county_york_city</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) ceo_stan_o_neal,</cell></row><row><cell>S-U</cell><cell>O</cell><cell>O</cell><cell>(R) works_for,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) retailstore_merrill</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) insect_insects,</cell></row><row><cell>S-U</cell><cell>O</cell><cell>O</cell><cell>(R) invertebrate_feed_on_food ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) agricultural_product_wood</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) person_katsuaki_watanabe,</cell></row><row><cell>U-U</cell><cell>X</cell><cell>O</cell><cell>(R) person_leads_organization,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) automobilemaker_toyota</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) mlauthor_web_search,</cell></row><row><cell>U-U</cell><cell>X</cell><cell>O</cell><cell>(R) agent_competes_with_agent,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) website_altavista_com</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) chemical_chromium,</cell></row><row><cell>U-U</cell><cell>X</cell><cell>O</cell><cell>(R) chemical_is_type_of_chemical,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) chemical_heavy_metals</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(H) food_meals,</cell></row><row><cell>U-U</cell><cell>X</cell><cell>X</cell><cell>(R) food_decreases_the_risk_of_disease,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(T) disease_heart_disease</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Out-of-sample representation learning for multi-relational graphs</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Albooyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13230</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity network fusion for aggregating data types on a genomic scale</title>
		<author>
			<persName><forename type="first">F</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brudno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mezlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Nature Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="333" to="337" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008</title>
				<meeting>the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">June 10-12, 2008. 2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011</title>
				<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">August 7-11, 2011, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
				<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Meta-graph: Few shot link prediction via meta learning</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09867</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="4216" to="4225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficacy of hydroxychloroquine in patients with covid-19: results of a randomized clinical trial. medRxiv</title>
		<author>
			<persName><forename type="first">Zhaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruhong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.03.22.20040758</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017. 2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017. 2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
				<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005-07">2005. July 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach</title>
		<author>
			<persName><forename type="first">Takuo</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidekazu</forename><surname>Oiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">August 19-25, 2017. 2017</date>
			<biblScope unit="page" from="1802" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 16-20, 2019. 2019</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GENN: predicting correlated drug-drug interactions with graph energy neural networks</title>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02107</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09">2017. September 9-11, 2017. 2017</date>
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rok</forename><surname>Sosič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/biodata" />
		<title level="m">BioSNAP Datasets: Stanford biomedical network dataset collection</title>
				<imprint>
			<date type="published" when="2018-08">August 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
				<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Westin Peachtree Plaza Hotel</publisher>
			<date type="published" when="2013">June 9-14, 2013. 2013</date>
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neverending learning</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pratim Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thahir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><forename type="middle">A</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abulhair</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">January 25-30, 2015. 2015</date>
			<biblScope unit="page" from="2302" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">June 1-6, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML 2011</title>
				<meeting>the 28th International Conference on Machine Learning, ICML 2011<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-02">June 28 -July 2, 2011. 2011</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Label propagation prediction of drug-drug interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sorrentino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific reports</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning improves prediction of drug-drug and drug-food interactions</title>
		<author>
			<persName><forename type="first">Jae</forename><surname>Yong Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Uk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><forename type="middle">Yup</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
				<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2018">18. 2018</date>
			<biblScope unit="page" from="E4304" to="E4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
				<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03">2018. June 3-7, 2018. 2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-01">January 27 -February 1, 2019. 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open-world knowledge graph completion</title>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17">2015. September 17-21, 2015. 2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03082</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Logic attention based neighborhood aggregation for inductive knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-01">January 27 -February 1, 2019. 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="7152" to="7159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Québec City, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">July 27 -31, 2014. 2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tackling long-tailed relations and uncommon entities in knowledge graph completion</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Kwun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="250" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Saliva is more sensitive for sars-cov-2 detection in covid-19 patients than nasopharyngeal swabs. medRxiv</title>
		<author>
			<persName><forename type="first">Anne Louise</forename><surname>Wyllie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnau</forename><surname>Casanovas-Massana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tokuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavithra</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Catherine</forename><surname>Muenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Chantal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">E</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">M</forename><surname>Petrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiwen</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lu-Culligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Earnest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupak</forename><surname>Simonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nida</forename><surname>Handoko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><forename type="middle">R</forename><surname>Naushad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Sewanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">B</forename><surname>Valdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><surname>Lapidus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Kalinich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eriko</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Linehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miyu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Eun</forename><surname>Moriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annsea</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takehiro</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><surname>Taura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Orr-El Weizman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yexin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santos</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camila</forename><surname>Bermejo</surname></persName>
		</author>
		<author>
			<persName><surname>Odio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">S Dela</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelli</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Farhadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Martinello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">I</forename><surname>Grubaugh</surname></persName>
		</author>
		<author>
			<persName><surname>Ko</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.04.16.20067835</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">February 12-17, 2016. 2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09">2017. September 9-11, 2017. 2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11-04">October 31 -November 4. 2018. 2018</date>
			<biblScope unit="page" from="1980" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Predicting potential drug-drug interactions by integrating chemical, biological, phenotypic and network data</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Metagnn: On few-shot node classification in graph meta-learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goce</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="2357" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bioinformatics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
