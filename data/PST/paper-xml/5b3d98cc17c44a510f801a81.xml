<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christy</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xiaodan1@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<email>zhitingh@cs.cmu.edu</email>
						</author>
						<author>
							<persName><roleName>Inc</roleName><forename type="first">Eric</forename><forename type="middle">P Xing</forename><surname>Petuum</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">32nd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2018)</postCode>
									<settlement>Montr√©al</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A78BA385C56BD8D3FDB3D20DCCABB361</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decisionmaking procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection precision of medical abnormality terminologies, and improved human evaluation performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Beyond the traditional visual captioning task <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b16">18]</ref> that produces one single sentence, generating long and topic-coherent stories or reports to describe visual contents (images or videos) has recently attracted increasing research interests <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b20">22]</ref>, posed as a more challenging and realistic goal towards bridging visual patterns with human linguistic descriptions. Particularly, report generation has several challenges to be resolved: 1) The generated report is a long narrative consisting of multiple sentences or paragraphs, which must have a plausible logic and consistent topics; 2) There is a presumed content coverage and specific terminology/phrases, depending on the task at hand. For example, a sports game report should describe competing teams, wining points, and outstanding players <ref type="bibr" target="#b36">[38]</ref>. 3) The content ordering is very crucial. For example, a sports game report usually talks about the competition results before describing teams and players in detail.</p><p>As one of the most representative and practical report generation task, the desired medical image report generation must satisfy more critical protocols and ensure the correctness of medical term usage. As shown in Figure <ref type="figure">1</ref>, a medical report consists of a findings section describing medical observations in details of both normal and abnormal features, an impression or conclusion sentence Figure <ref type="figure">1</ref>: An example of medical image report generation. The middle column is a report written by radiologists for the chest x-ray image on the left column. The right column contains three reports generated by a retrieval-based system (R), a generation-based model (G) and our proposed model (HRGR-Agent) respectively. The retrieval-based model correctly detects effusion while the generative model fails to. Our HRGR-Agent detects effusion and also describes supporting evidence.</p><p>indicating the most prominent medical observation or conclusion, and comparison and indication sections that list patient's peripheral information. Among these sections, the findings section posed as the most important component, ought to cover contents of various aspects such as heart size, lung opacity, bone structure; any abnormality appearing at lungs, aortic and hilum; and potential diseases such as effusion, pneumothorax and consolidation. And, in terms of content ordering, the narrative of findings section usually follows a presumptive order, e.g. heart size, mediastinum contour followed by lung opacity, remarkable abnormalities followed by mild or potential abnormalities.</p><p>State-of-the-art caption generation models <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b32">34]</ref> tend to perform poorly on medical report generation with specific content requirements due to several reasons. First, medical reports are usually dominated by normal findings, that is, a small portion of majority sentences usually forms a template database. For these normal cases, a retrieval-based system (e.g. directly perform classification among a list of majority sentences given image features) can perform surprisingly well due to the low variance of language. For instance, in Figure <ref type="figure">1</ref>, a retrieval-based system correctly detects effusion from a chest x-ray image, while a generative model that generates word-by-word given image features, fails to detect effusion. On the other hand, abnormal findings which are relatively rare and remarkably diverse, however, are of higher importance. Current text generation approaches <ref type="bibr" target="#b14">[16]</ref> often fail to capture the diversity of such small portion of descriptions, and pure generation pipelines are biased towards generating plausible sentences that look natural by the language model but poor at finding visual groundings <ref type="bibr" target="#b15">[17]</ref>. On the contrary, a desirable medical report usually has to not only describe normal and abnormal findings, but also support itself by visual evidences such as location and attributes of the detected findings appearing in the image.</p><p>Inspired by the fact that radiologists often follow templates for writing reports and modify them accordingly for each individual case <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b8">10]</ref>, we propose a Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which is the first attempt to incorporate human prior knowledge with learningbased generation for medical reports. HRGR-Agent employs a retrieval policy module to decide between automatically generating sentences by a generation module and retrieving specific sentences from the template database, and then sequentially generates multiple sentences via a hierarchical decision-making. The template database is built based on human prior knowledge collected from available medical reports. To enable effective and robust report generation, we jointly train the retrieval policy module and generation module via reinforcement learning (RL) <ref type="bibr" target="#b28">[30]</ref> guided by sentence-level and word-level rewards, respectively. Figure <ref type="figure">1</ref> shows an example generated report by our HRGR-Agent which correctly describes "a small effusion" from the chest x-ray image, and successfully supports its finding by providing the appearance ("blunting") and location ("costophrenic sulcus") of the evidence.</p><p>Our main contribution is to bridge rule-based (retrieval) and learning-based generation via reinforcement learning, which can achieve plausible, correct and diverse medical report generation. Moreover, our HRGR-Agenet has several technical merits compared to existing retrieval-generation-based models: 1) our retrieval and generation modules are updated and benefit from each other via policy learning; 2) the retrieval actions are regarded as a part of the generation whose selection of templates directly influences the final generated result. 3) the generation module is encouraged to learn diverse and complicated sentences while the retrieval policy module learns template-like sentences, driven by distinct word-level and sentence-level rewards, respectively. Other work such as <ref type="bibr" target="#b22">[24]</ref> still enforces the generative model to predict template-like sentences.</p><p>We conduct extensive experiments on two medical image report dataset <ref type="bibr" target="#b6">[8]</ref>. Our HRGR-Agent achieves the state-of-the-art performance on both datasets under three kinds of evaluation metrics: automatic metrics such as CIDEr <ref type="bibr" target="#b31">[33]</ref>, BLEU <ref type="bibr" target="#b23">[25]</ref> and ROUGE <ref type="bibr" target="#b18">[20]</ref>, human evaluation, and detection precision of medical terminologies. Experiments show that the generated sentences by HRGR-Agent shares a descent balance between concise template sentences, and complicated and diverse sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual Captioning and Report Generation. Visual captioning aims at generating a descriptive sentence for images or videos. State-of-the-art approaches use CNN-RNN architectures and attention mechanisms <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b26">28]</ref>. The generated sequence is usually short, describing only the dominating visual event, and is primarily rewarded by language fluency in practice. Generating reports that are informative and have multiple sentences <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b14">16]</ref> poses higher requirements on content selection, relation generation, and content ordering. The task differs from image captioning <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b21">23]</ref> and sentence generation <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b4">6]</ref> where usually single or few sentences are required, or summarization <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b42">44]</ref> where summaries tend to be more diverse without clear template sentences. State-of-the-art methods on report generation <ref type="bibr" target="#b14">[16]</ref> are still remarkably cloning expert behaviour, and incapable of diversifying language and depicting rare but prominent findings. Our approach prevents from mimicking teacher behaviour by sparing the burden of automatic generative model with a template selection and retrieval mechanism, which by design promotes language diversity and better content selection.</p><p>Template Based Sequence Generation. Some of the recent approaches bridged generative language approaches and traditional template-based methods. However, state-of-the-art approaches either treat a retrieval mechanism as latent guidance <ref type="bibr" target="#b42">[44]</ref>, the impact of which to text generation is limited, or still encourage the generation network to mimic template-like sequences <ref type="bibr" target="#b22">[24]</ref>. Our method is close to previous copy mechanism work such as pointer-generator <ref type="bibr" target="#b0">[2]</ref>, however, we are different in that: 1) our retrieval module aims to retrieve from an external common template base, which is particularly effective to the task, as opposed to copying from a specific source article; 2) we formulate the retrieval-generation choices as discrete actions (as opposed to soft weights as in previous work) and learn with hierarchical reinforcement learning for optimizing both short-and long-term goals.</p><p>Reinforcement Learning for Sequence Generation. Recently, reinforcement learning (RL) has been receiving increasing popularity in sequence generation <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b11">13]</ref> such as visual captioning <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b16">18</ref>], text summarization <ref type="bibr" target="#b24">[26]</ref>, and machine translation <ref type="bibr" target="#b37">[39]</ref>. Traditional methods use cross entropy loss which is prone to exposure bias <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b29">31]</ref> and do not necessarily optimize evaluation metrics such as CIDEr <ref type="bibr" target="#b31">[33]</ref>, ROUGE <ref type="bibr" target="#b18">[20]</ref>, BLEU <ref type="bibr" target="#b23">[25]</ref> and METEOR <ref type="bibr" target="#b2">[4]</ref>. In contrast, reinforcement learning can directly use the evaluation metrics as reward and update model parameters via policy gradient. There has been some recent efforts <ref type="bibr" target="#b40">[42]</ref> devoted in applying hierarchical reinforcement learning (HRL) <ref type="bibr" target="#b5">[7]</ref> where sequence generation is broken down into several sub-tasks each of which targets at a chunk of words. However, HRL for long report generation is still under-explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Medical image report generation aims at generating a report consisting of a sequence of sentences Y = (y 1 , y 2 , . . . , y M ) given a set of medical images I = {I j } K j=1 of a patient case. Each sentence comprises a sequence of words y i = (y i,1 , y i,2 , . . . , y i,N ), y i,j ‚àà V where i is the index of sentences, j the index of words, and V the vocabulary of all output tokens. In order to generate long and topic-coherent reports, we formulate the decoding process in a hierarchical framework that first produces a sequence of hidden sentence topics, and then predicts words of each sentence conditioning on each topic.</p><p>It is observed that doctors writing a report tend to follow certain patterns and reuse templates, while adjusting statements for each individual case when necessary. To mimic the procedure, we propose to combine retrieval and generation for automatic report generation. In particular, we first compile an off-the-shelf template database T that consists of a set of sentences that occur frequently in the training corpus. Such sentences typically describe general observations, and are often inserted into medical reports, e.g., "the heart size is normal" and "there is no pleural effusion or pneumothorax".</p><p>(Table <ref type="table" target="#tab_2">1</ref> provides more examples.)  As described in Figure <ref type="figure" target="#fig_0">2</ref>, a set of images for each sample is first fed into a CNN to extract visual features which is then transformed into a context vector by an image encoder. Then a sentence decoder recurrently generates a sequence of hidden states q = (q 1 , q 2 , . . . , q M ) which represent sentence topics. Given each topic state q i , a retrieval policy module decides to either automatically generate a new sentence by invoking a generation module, or retrieve an existing template from the template database. Both the retrieval policy module (that determines between automatic generation or template retrieval) and the generation module (that generates words) are making discrete decisions and be updated via the REINFORCE algorithm <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b28">30]</ref>. We devise sentence-level and word-level rewards accordingly for the two modules, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hybrid Retrieval-Generation Reinforced Agent</head><p>Image Encoder. Given a set of images {I j } K j=1 , we first extract their features {v j } K j=1 with a pretrained CNN, and then average {v j } K j=1 to obtain v. The image encoder converts v into a context vector h v ‚àà R D which is used as the visual input for all subsequent modules. Specifically, the image encoder is parameterized as a fully-connected layer, and the visual features are extracted from the last convolution layer of a DenseNet <ref type="bibr" target="#b13">[15]</ref> or VGG-19 <ref type="bibr" target="#b27">[29]</ref>.</p><p>Sentence Decoder. Sentence decoder comprises stacked RNN layers which generates a sequence of topic states q. We equip the stacked RNNs with attention mechanism to enhance text generation, inspired by <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b21">23]</ref>. Each stacked RNN first generates an attentive context vector c s i , where i indicates time steps, given the image context vector h v and previous hidden state h s i-1 . It then generates a hidden state h s i based on c s i and h s i-1 . The generated hidden state h s i is further projected into a topic space as q i and a stop control probability z i ‚àà [0, 1] through non-linear functions respectively. Formally, the sentence decoder can be written as:</p><formula xml:id="formula_0">c s i = F s attn (h v , h s i-1 ) (1) h s i = F s RNN (c s i , h s i-1 )<label>(2)</label></formula><formula xml:id="formula_1">q i = œÉ(W q h s i + b q ) (3) z i = Sigmoid(W z h s i + b z ),<label>(4)</label></formula><p>where F s attn denotes a function of the attention mechanism <ref type="bibr" target="#b26">[28]</ref>, F s RNN denotes the non-linear functions of Stacked RNN, W q and b q are parameters which project hidden states into the topic space while W z and b z are parameters for stop control, and œÉ is a non-linear activation function. The stop control probability z i greater than or equal to a predefined threshold (e.g. 0.5) indicates stopping generating topic states, and thus the hierarchical report generation process.</p><p>Retrieval Policy Module. Given each topic state q i , the retrieval policy module takes two steps. First, it predicts a probability distribution u i ‚àà R 1+|T| over actions of generating a new sentence and retrieving from |T| candidate template sentences. Based on the prediction of the first step, it triggers different actions. If automatic generation obtains the highest probability, the generation module is activated to generate a sequence of words conditioned on current topic state (the second row on the right side of Figure <ref type="figure" target="#fig_0">2</ref>). If a template in T obtains the highest probability, it is retrieved from the off-the-shelf template database and serves as the generation result of current sentence topic (the first row on the right side of Figure <ref type="figure" target="#fig_0">2</ref>). We reserve 0 index to indicate the probability of selecting automatic generation and positive integers in {1, |T|} to index the probability of selecting templates in T. The first step is parameterized as a fully-connected layer with Softmax activation:</p><formula xml:id="formula_2">u i = Softmax(W u q i + b u ) (5) m i = argmax(u i ),<label>(6)</label></formula><p>where W u and b u are network parameters, and the resulting m i is the index of highest probability in u i .</p><p>Generation Module. Generation module generates a sequence of words conditioned on current topic state q i and image context vector h v for each sentence. It comprises RNNs which take environment parameters and previous hidden state h g i,t-1 as input, and generate a new hidden state h g i,t which is further transformed to a probability distribution a i,t over all words in V, where t indicates t-th word. We define environment parameters as a concatenation of current topic state q i , context vector c g i,t</p><p>encoded by following the same attention paradigm in sentence decoder, and embedding of previous word e i,t-1 . The procedure of generating each word is written as follows, which is an attentional decoding step:</p><formula xml:id="formula_3">c g i,t = F g attn (h v , [e i,t-1 ; q i ], h g i,t-1 )<label>(7)</label></formula><formula xml:id="formula_4">h g i,t = F g RNN ([c g i,t ; e i,t-1 ; q i ], h g i,t-1 )<label>(8)</label></formula><formula xml:id="formula_5">a t = Softmax(W y h g i,t + b y )<label>(9)</label></formula><formula xml:id="formula_6">y t = argmax(a t ) (10) e i,t = W e O(y i,t ),<label>(11)</label></formula><p>where F g attn denotes the attention mechanism of generation module, F g RNN denotes non-linear functions of RNNs, W y and b y are parameters for generating word probability distribution, y i,t is index of the maximum probable word, W e is a learnable word embedding matrix initialized uniformly, and O denotes one hot vector.</p><p>Reward Module. We use automatic metrics CIDEr for computing rewards since recent work on image captioning <ref type="bibr" target="#b26">[28]</ref> has shown that CIDEr performs better than many traditional automatic metrics such as BLEU, METEOR and ROUGE. We consider two kinds of reward functions: sentence-level reward and word-level reward. For the i-th generated sentence y i = (y i,1 , y i,2 , . . . , y i,N ) either from retrieval or generation outputs, we compute a delta CIDEr score at sentence level, which is</p><formula xml:id="formula_7">R sent (y i ) = f ({y k } i k=1 , gt) -f ({y k } i-1 k=1 , gt)</formula><p>, where f denotes CIDEr evaluation, and gt denotes ground truth report. This assesses the advantages the generated sentence brings in to the existing sentences when evaluating the quality of the whole report. For a single word input, we use reward as delta CIDEr score which is R word (y t ) = f ({y k } t k=1 , gt s ) -f ({y k } t-1 k=1 , gt s ) where gt s denotes the ground truth sentence. The sentence-level and word-level rewards are used for computing discounted reward for retrieval policy module and generation module respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Reinforcement Learning</head><p>Our objective is to maximize the reward of generated report Y compared to ground truth report Y * . Omitting the condition on image features for simplicity, the loss function can be written as:</p><formula xml:id="formula_8">L(Œ∏) = -E z,m,y [R(Y, Y * )] (12) ‚àá Œ∏ L(Œ∏) = -E z,m,y [‚àá Œ∏ log p(z, m, y)R(Y, Y * )]<label>(13)</label></formula><p>= -E z,m,y i=1</p><formula xml:id="formula_9">1(z i &lt; 1 2 |z i-1 ) ‚àá Œ∏r L(Œ∏ r ) + 1(m i = 0|m i-1 )‚àá Œ∏g L(Œ∏ g ) ,<label>(14)</label></formula><p>where Œ∏, Œ∏ r ,and Œ∏ g denote parameters of the whole network, retrieval policy module, and generation module respectively; 1(‚Ä¢) is binary indicator; z i is the probability of topic stop control in Equation <ref type="formula" target="#formula_1">4</ref>;</p><p>m i is the action chosen by retrieval policy module among automatic generation (m i = 0) and all templates (m i ‚àà [1, |T|]) in the template database. The loss of HRGR-Agent comes from two parts: retrieval policy module L(Œ∏ r ) and generation module L(Œ∏ g ) as defined below.</p><p>Policy Update for Retrieval Policy Module. We define the reward for retrieval policy module R r at sentence level. The generated sentence or retrieved template sentence is used for computing the reward. The discounted sentence-level reward and its corresponding policy update according to REINFORCE algorithm <ref type="bibr" target="#b28">[30]</ref> can be written as:</p><formula xml:id="formula_10">R r (y i ) = ‚àû j=0 Œ≥ j R sent (y i+j )<label>(15)</label></formula><formula xml:id="formula_11">L(Œ∏ r ) = -E mi [R r (m i , m * i )] (16) ‚àá Œ∏r L(Œ∏ r ) = -E mi [‚àá Œ∏r log p(m i |m i-1 )R r (m i , m * i )],<label>(17)</label></formula><p>where Œ≥ is a discount factor; y i is the i-th generated sequence; and Œ∏ r represents parameters of retrieval policy module which are W u and b u in Equation <ref type="formula">5</ref>.</p><p>Policy Update for Generation Module. We define the word-level reward R g (y t ) for each word generated by generation module as discounted reward of all generated words after the considered word. The discounted reward function and its policy update for generation module can be written as:</p><formula xml:id="formula_12">R g (y t ) = ‚àû j=0 Œ≥ j R word (y t+j )<label>(18)</label></formula><formula xml:id="formula_13">L(Œ∏ g ) = -E yt [R g (y t , y * t )]<label>(19)</label></formula><formula xml:id="formula_14">‚àá Œ∏g L(Œ∏ g ) = -E yt [ t=1 ‚àá Œ∏g log p(y t |y t-1 )R g (y t , y * t )],<label>(20)</label></formula><p>where Œ≥ is a discount factor, and Œ∏ g represents the parameters of generation module such as W y , b y , W e in Equation <ref type="formula" target="#formula_5">9</ref>-11 and parameters of attention functions in Equation <ref type="formula" target="#formula_3">7</ref>and RNNs in Equation <ref type="formula" target="#formula_4">8</ref>. Detailed policy update algorithm is provides in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>Datasets. We conduct experiments on two medical image report datasets. First, Indiana University Chest X-Ray Collection (IU X-Ray) <ref type="bibr" target="#b6">[8]</ref> is a public dataset consists of 7,470 frontal and lateral-view chest x-ray images paired with their corresponding diagnostic reports. Each patient has 2 images and a report which includes impression, findings, comparison and indication sections. We preprocess the reports by tokenizing, converting to lower-cases, and filtering tokens of frequency no less than 3 as vocabulary, which results in 1185 unique tokens covering over 99.0% word occurrences in the corpus.</p><p>CX-CHR is a proprietary internal dataset of chest X-ray images with Chinese reports collected from a professional medical institution for health checking. The dataset consists of 35,500 patients. Each patient has one or multiple chest x-ray images in different views such as posteroanterior and lateral, and a corresponding Chinese report. We select patients with no more than 2 images and obtained 33,236 patient samples in total which covers over 93% of the dataset. We preprocess the reports through tokenizing by Jieba [1] and filtering tokens of frequency no less than 3 as vocabulary, which results in 1282 unique tokens.</p><p>On both datasets, we randomly split the data by patients into training, validation and testing by a ratio of 7:1:2. There is no overlap between patients in different sets. We predict the 'findings' section as it is the most important component of reports. On CX-CHR dataset, we pretrain a DenseNet with public available ChestX-ray8 dataset <ref type="bibr" target="#b34">[36]</ref> on classification, and fine-tune it on CX-CHR dataset on 20 common thorax disease labels. As IU X-Ray dataset is relatively small, we do not directly fine-tune the pretrained DenseNet on it, and instead extract visual features from a DenseNet pretrained jointly on ChestX-ray8 dataset <ref type="bibr" target="#b34">[36]</ref> and CX-CHR datasets. Please see Supplementary Material for more details.</p><p>Template Database. We select sentences in the training set whose document frequencies (the number of occurrence of a sentence in training documents) are no less than a threshold as template candidates.</p><p>We further group candidates that express the same meaning but have a little linguistic variations. For example, "no pleural effusion or pneumothorax" and "there is no pleural effusion or pneumonthorax" are grouped as one template. This results in 97 templates with greater than 500 document frequency for CX-CHR and 28 templates with greater than 100 document frequency for IU X-Ray. Upon retrieval, only the most frequent sentence of a template group will be retrieved for HRGR-Agent or any rule-based models that we compare with. Although this introduces minor but inevitable error in the generated results, our experiments show that the error is negligible compared to the advantages that a hybrid of retrieval-based and generation-based approaches brings in. Besides, separating templates of the same meaning into different categories diminishes the capability of retrieval policy module to predict the most suitable template for a given visual input, as multiple templates share the exact same meaning. Cardiomediastin silhouett is within normal limit. 5.12 The cardiomediastin silhouett is within normal limit. The cardiomediastin silhouett is within normal limit for size and contour. Evaluation Metrics. We use three kinds of evaluation metrics: 1) automatic metrics including CIDEr, ROUGE, and BLEU; 2) medical abnormality terminology detection accuracy: we select 10 most frequent medical abnormality terminologies in medical reports and evaluate average precision and average false positive (AFP) of compared models; 3) human evaluation: we randomly select 100 samples from testing set for each method and conduct surveys through Amazon Mechanical Turk. Each survey question gives a ground truth report, and ask candidate to choose among reports generated by different models that matches with the ground truth report the best in terms of language fluency, content selection, and correctness of medical abnormal finding. A default choice is provided in case of no or both reports are preferred. We collect results from 20 participants and compute the average preference percentage for each model excluding default choices.</p><p>Training Details. We implement our model on PyTorch and train on a GeForce GTX TITAN GPU. We first train all models with cross entropy loss for 30 epochs with an initial learning rate of 5e-4, and then fine-tune the retrieval policy module and generation module of HRGR-Agent via RL with a fixed learning rate 5e-5 for another 30 epochs. We use 512 as dimension of all hidden states and word embeddings, and batch size 16. We set the maximum number of sentences of a report and maximum number of tokens in a sentence as 18 and 44 for CX-CHR and 7 and 15 for IU X-Ray. Besides, as observed from baseline models which overly predict most popular and normal reports for all testing samples and the fact that most medical reports describe normal cases, we add postprocessing to increase the length and comprehensiveness of the generated reports for both datasets while maintaining the design of HRGR-Agent to better predict abnormalities. The post-processing we use is that we first select 4 most commonly predicted key words with normal descriptions by other baselines, then for each key word, if the generated report does not describe any abnormality nor normality of these key words, we add the a corresponding sentence of these key words that describe their normal cases respectively. The key words for IU X-Ray are 'heart size and mediastinal contours', 'pleural effusion or pneumothorax', 'consolidation', and 'lungs are clear'. As observed in our experiments, this step maintains the same medical abnormality term detection results, and improves the automatic report generation metrics, especially on BLEU-n metrics.</p><p>Baselines. On both datasets, we compare with four state-of-the-art image captioning models: CNN-RNN <ref type="bibr" target="#b32">[34]</ref>, LRCN <ref type="bibr" target="#b7">[9]</ref>, AdaAtt <ref type="bibr" target="#b21">[23]</ref>, and Att2in <ref type="bibr" target="#b26">[28]</ref>. Visual features for all models are extracted from the last convolutional layer of pretrained densetNets respectively as mentioned in 4, yielding 16 √ó 16 √ó 256 feature maps for both datasets. We use greedy search and argmax sampling for HRGR-Agent and the baselines on both datasets. On IU X-Ray dataset, we also compare with CoAtt <ref type="bibr" target="#b14">[16]</ref> which uses different visual features extracted from a pretrained ResNet <ref type="bibr" target="#b9">[11]</ref>. The authors of CoAtt <ref type="bibr" target="#b14">[16]</ref> re-trained their model using our train/test split, and provided evaluation results for Table <ref type="table">3</ref>: Average precision (Prec.) and average false positive (AFP) of medical abnormality terminology detection, and human evaluation (Hit). The higher Prec. and the lower AFP, the better.</p><p>automatic report generation metrics using greedy search and sampling temperature 0.5 at test time. We further evaluated their prediction to obtain medical abnormality terminology detection precision and AFP. Due to the relatively large size of CX-CHR, we conduct additional experiments on it to compare HRGR-Agent with its different variants by removing individual components (Retrieval, Generation, RL). We train a hierarchical generative model (Generation) without any template retrieval or RL fine-tuning, and our model without RL fine-tuning (HRG). To exam the quality of our pre-defined templates, we separately evaluate the retrieval policy module of HRGR-Agent by masking out the generation part and only use the retrieved templates as prediction (Retrieval). Note that Retrieval uses the same model as HRG-Agent whose training involves automatic generation of sentences, thus the results of which may be higher than a general retrieval-based system (e.g. directly perform classification among a list of majority sentences given image features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and Analyses</head><p>Automatic Evaluation. Table <ref type="table" target="#tab_3">2</ref> shows automatic evaluation comparison of state-of-the-art methods and our model variants. Most importantly, HRGR-Agent outperforms all baseline models that have no retrieval mechanism or hierarchical structure on both datasets by great margins, demonstrating its effectiveness and robustness. On IU X-Ray dataset, HRGR-Agent achieves slightly lower BLEU-1,4 and ROUGE score than that of CoAtt <ref type="bibr" target="#b14">[16]</ref>. However, CoAtt uses different pre-processing of reports and visual features, jointly predicts 'impression' and 'findings', and uses single-image input while our method focuses on 'findings' and use combined frontal and lateral view of patients. On CX-CHR, HRGR-Agent increases CIDEr score by 0.73 compared to HRG, demonstrating that reinforcement fine-tuning is crucial to performance increase since it directly optimizes the evaluation metric. Besides, Retrieval surpasses Generation by relatively large margins, showing that retrievalbased method is beneficial to generating structured reports, which leads to boosted performance of HRGR-Agent when combined with neural generation approaches (generation module). To better understand HRGR-Agent's performance, each generated report at testing has on average 7.2 and 4.8 sentences for CX-CHR and IU X-Ray dataset, respectively. The percentages of retrieval vs generation are 83.5 vs 16.5 on the CX-CHR data, and 82.0 vs 18.0 on IU X-Ray, respectively.</p><p>Medical Abnormality Terminology Evaluation. Table <ref type="table">3</ref> shows evaluation results of average precision and average false positive of medical abnormality terminology detection. HGRG-Agent achieves the highest precision, and is only slightly lower AFP than CoAtt, demonstrating that its robustness on detecting rare abnormal findings which are among the most important components of medical reports.  Retrieval vs. Generation. It's worth knowing that on CX-CHR, Retrieval achieves higher automatic evaluation scores (Table <ref type="table" target="#tab_3">2</ref> the 7 th row) but lower medical term detection precision (Table <ref type="table">3</ref> the 2 nd column) than Generation. Note that Retrieval evaluates retrieval policy module of HRGR-Agent by masking out the generation results of generation module. The result shows that simply composing templates that mostly describe normal medical findings can lead to high automatic evaluation scores since the majority reports describe normal cases. However, this kind retrieval-based approaches lack of the capability of detecting significant but rare abnormal findings. On the other hand, the high medical abnormality term detection precision and low average false positive of HRGR-Agent verifies that its generation module learns to describe abnormal findings. The win-win combination of retrieval policy module and generation module leads to state-of-the-art performance of HRGE-Agent, surpassing a generative model (Generation) that is purely trained without any retrieval mechanism.</p><p>Human Evaluation. Table <ref type="table">3</ref> (last row) shows average human preference percentage of HRGR-Agent compared with Generation and CoAtt <ref type="bibr" target="#b14">[16]</ref> on CX-CHR and IU X-Ray respectively, evaluated in terms of content coverage, specific terminology accuracy and language fluency. HRGR-Agent achieves much higher human preference than baseline models, showing that it is able to generate natural and plausible reports that are human preferable.</p><p>Qualitative Analysis. Figure <ref type="figure" target="#fig_2">3</ref> demonstrate qualitative results of HRGR-Agent and baseline models on IU X-Ray dataset. The reports of HRGR-Agent are generally longer than that of the baseline models, and share a well balance of templates and generated sentences. And, among the generated sentences, HRGR-Agent has higher rate of detecting abnormal findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) to perform robust medical image report generation. Our approach is the first attempt to bridge human prior knowledge and generative neural network via reinforcement learning. Experiments show that HRGR-Agent does not only achieve state-of-the-art performance on two medical image report datasets, but also generates robust reports that has high precision on medical abnormal findings detection and best human preference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hybrid Retrieval-Generation Reinforced Agent. Visual features are encoded by a CNN and image encoder, and fed to a sentence decoder to recurrently generate hidden topic states. A retrieval policy module decides for each topic state to either automatic generate a sentence, or retrieve a specific template from a template database. Dashed black lines indicate hierarchical policy learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The cardiomediastinal silhouette is within normal limits. Calcified right lower lobe granuloma. No focal airspace consolidation. No visualized pneumothorax or large pleural effusion. No acute bony abnormalities. The heart is normal in size. The mediastinum is unremarkable. The lungs are clear. The cardiomediastinal silhouette is normal size and configuration. Pulmonary vasculature within normal limits. There is right middle lobe airspace disease, may reflect granuloma or pneumonia. No pleural effusion. No pneumothorax. No acute bony abnormalities. Exam limited by patient rotation. Mild rightward deviation of the trachea. Stable cardiomegaly. Unfolding of the thoracic aorta. Persistent right pleural effusion with adjacent atelectasis. Low lung volumes. No focal airspace consolidation. There is severe degenerative changes of the right shoulder. The heart size and pulmonary vascularity appear within normal limits. The lungs are free of focal airspace disease. No pleural effusion or pneumothorax. No acute bony abnormality. The heart is enlarged. Possible cardiomegaly. There is pulmonary vascular congestion with diffusely increased interstitial and mild patchy airspace opacities. Suspicious pleural effusion. There is no pneumothorax. There are no acute bony findings. Frontal and lateral views of the chest with overlying external cardiac monitor leads show reduced lung volumes with bronchovascular crowding of basilar atelectasis. No definite focal airspace consolidation or pleural effusion. The cardiac silhouette appears mildly enlarged. The heart size and pulmonary vascularity appear within normal limits. The lungs are free of focal airspace disease. No pleural effusion or pneumothorax. no acute bony abnormality. The heart is mildly enlarged. The aorta is atherosclerotic and ectatic. Chronic parenchymal changes are noted with mild scarring and/or subsegmental atelectasis in the right lung base. No focal consolidation or significant pleural effusion identified. Costophrenic UNK are blunted. Apparent cardiomegaly partially accentuated by low lung volumes. No focal consolidation, pneumothorax or large pleural effusion. Right base calcified granuloma. Stable right infrahilar nodular density (lateral view). Negative for acute bone abnormality. The heart is normal in size. The mediastinum is unremarkable. The lungs are clear. The heart size and pulmonary vascularity appear within normal limits. Low lung volumes. Suspicious calcified granuloma. No pleural effusion or pneumothorax. No acute bony abnormality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of ground truth report and generated reports by CoAtt [16] and HRGR-Agent. Highlighted phrases are medical abnormality terms. Italicized text is retrieved from template database.</figDesc><graphic coords="9,113.98,278.29,59.53,62.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Generation Module Template Database</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sentence Decoder</cell><cell>Retrieve template</cell><cell>Template sentence</cell><cell></cell></row><row><cell>CNN</cell><cell>Visual features</cell><cell>Encoder</cell><cell>Image</cell><cell>Context vector</cell><cell>Topic state Topic state</cell><cell>Retrieval Policy Module</cell><cell>Reward Module</cell><cell>Reward of sentence Reward</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of words</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Topic state</cell><cell>Automatic</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>generation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Generated sentence</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1 shows examples of templates for IU X-Ray dataset. More template examples are provided in supplementary materials.</figDesc><table><row><cell>Template</cell><cell>df(%)</cell></row><row><cell>No pneumothorax or pleural effusion.</cell><cell></cell></row><row><cell>No pleural effusion or pneumothorax.</cell><cell>18.36</cell></row><row><cell>There is no pleural effusion or pneumothorax.</cell><cell></cell></row><row><cell>The lungs are clear</cell><cell></cell></row><row><cell>Lungs are clear.</cell><cell>23.60</cell></row><row><cell>The lung are clear bilaterally.</cell><cell></cell></row><row><cell>No evidence of focal consolidation, pneumothorax, or pleural effusion.</cell><cell></cell></row><row><cell>no focal consolidation, pneumothorax or large pleural effusion.</cell><cell>6.55</cell></row><row><cell>No focal consolidation, pleural effusion, or pneumothorax identified.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Examples of template database of IU X-Ray dataset. Each template is constructed by a group of sentences of the same meaning but slightly different linguistic variations. Top 3 most frequent sentences for a template are displayed in the first and third column. The second column shows document frequency (in percentage of training corpus) of each template.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Automatic evaluation results on CX-CHR (upper part) and IU X-Ray Datasets (lower part). BLEU-n denotes BLEU score uses up to n-grams.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell></cell><cell cols="6">CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE</cell></row><row><cell></cell><cell cols="2">CNN-RNN [34]</cell><cell>1.580</cell><cell>0.590</cell><cell>0.506</cell><cell>0.450</cell><cell>0.411</cell><cell>0.577</cell></row><row><cell></cell><cell cols="2">LRCN [9]</cell><cell>1.588</cell><cell>0.593</cell><cell>0.508</cell><cell>0.452</cell><cell>0.413</cell><cell>0.577</cell></row><row><cell></cell><cell cols="2">AdaAtt [23]</cell><cell>1.568</cell><cell>0.588</cell><cell>0.503</cell><cell>0.446</cell><cell>0.409</cell><cell>0.575</cell></row><row><cell>CX-CHR</cell><cell cols="2">Att2in [28] Generation</cell><cell>1.566 0.361</cell><cell>0.587 0.307</cell><cell>0.503 0.216</cell><cell>0.446 0.160</cell><cell>0.408 0.121</cell><cell>0.576 0.322</cell></row><row><cell></cell><cell>Retrieval</cell><cell></cell><cell>2.565</cell><cell>0.535</cell><cell>0.475</cell><cell>0.437</cell><cell>0.409</cell><cell>0.536</cell></row><row><cell></cell><cell>HRG</cell><cell></cell><cell>2.800</cell><cell>0.629</cell><cell>0.547</cell><cell>0.497</cell><cell>0.463</cell><cell>0.588</cell></row><row><cell></cell><cell cols="2">HRGR-Agent</cell><cell>2.895</cell><cell>0.673</cell><cell>0.587</cell><cell>0.530</cell><cell>0.486</cell><cell>0.612</cell></row><row><cell></cell><cell cols="2">CNN-RNN [34]</cell><cell>0.294</cell><cell>0.216</cell><cell>0.124</cell><cell>0.087</cell><cell>0.066</cell><cell>0.306</cell></row><row><cell></cell><cell cols="2">LRCN [9]</cell><cell>0.284</cell><cell>0.223</cell><cell>0.128</cell><cell>0.089</cell><cell>0.067</cell><cell>0.305</cell></row><row><cell>IU X-Ray</cell><cell cols="2">AdaAtt [23] Att2in [28]</cell><cell>0.295 0.297</cell><cell>0.220 0.224</cell><cell>0.127 0.129</cell><cell>0.089 0.089</cell><cell>0.068 0.068</cell><cell>0.308 0.308</cell></row><row><cell></cell><cell cols="2">CoAtt* [16]</cell><cell>0.277</cell><cell>0.455</cell><cell>0.288</cell><cell>0.205</cell><cell>0.154</cell><cell>0.369</cell></row><row><cell></cell><cell cols="2">HRGR-Agent</cell><cell>0.343</cell><cell>0.438</cell><cell>0.298</cell><cell>0.208</cell><cell>0.151</cell><cell>0.322</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">CX-CHR</cell><cell></cell><cell></cell><cell></cell><cell>IU X-Ray</cell></row><row><cell>Models</cell><cell cols="8">Retrieval Generation HRGR-Agent CNN-RNN [34] CoAtt [16] HRGR-Agent</cell></row><row><cell>Prec. (%)</cell><cell>14.13</cell><cell>27.50</cell><cell></cell><cell>29.19</cell><cell>0.00</cell><cell></cell><cell>5.01</cell><cell>12.14</cell></row><row><cell>AFP</cell><cell>0.133</cell><cell>0.064</cell><cell></cell><cell>0.059</cell><cell>0.000</cell><cell></cell><cell>0.019</cell><cell>0.043</cell></row><row><cell>Hit (%)</cell><cell>-</cell><cell>23.42</cell><cell></cell><cell>52.32</cell><cell>-</cell><cell></cell><cell>28.00</cell><cell>48.00</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> *   <p>This work was conducted when Christy Y. Li was at Petuum, Inc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D M</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bosmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Weyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>De Schepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Parizel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The radiology report as seen by radiologists and referring clinicians: results of the cover and rover surveys</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoNLL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feudal reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evidence-based guideline for the written radiology report: Methods, recommendations and implementation challenges</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Goergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Fahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical imaging and radiation oncology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Content analysis of reporting templates and free-text radiology reports</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00794</idno>
		<title level="m">A modularized, versatile, and extensible toolkit for text generation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the automatic generation of medical imaging reports</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end video captioning with multitask reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent topic-transition gan for visual paragraph generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved image captioning via policy gradient optimization of spider</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Let your photos talk: Generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Connecting the dots between mle and rl for text generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">No metrics are perfect: Adversarial reward learning for visual storytelling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Encode, review, and decode: Reviewer module for caption generation</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y Y Y Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S W W</forename><surname>Cohen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical text generation and planning for strategic dialogue</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Ziqiang Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
