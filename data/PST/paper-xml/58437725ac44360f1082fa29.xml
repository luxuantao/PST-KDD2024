<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unbiased Learning-to-Rank with Biased Feedback *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unbiased Learning-to-Rank with Biased Feedback *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of data in human-interactive systems. While implicit feedback has many advantages (e.g., it is inexpensive to collect, usercentric, and timely), its inherent biases are a key obstacle to its effective use. For example, position bias in search rankings strongly influences how many clicks a result receives, so that directly using click data as a training signal in Learning-to-Rank (LTR) methods yields sub-optimal results. To overcome this bias problem, we present a counterfactual inference framework that provides the theoretical basis for unbiased LTR via Empirical Risk Minimization despite biased data. Using this framework, we derive a propensity-weighted ranking SVM for discriminative learning from implicit feedback, where click models take the role of the propensity estimator. Beyond the theoretical support, we show empirically that the proposed learning method is highly effective in dealing with biases, that it is robust to noise and propensity model mis-specification, and that it scales efficiently. We also demonstrate the real-world applicability of our approach on an operational search engine, where it substantially improves retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Batch training of retrieval systems requires annotated test collections that take substantial effort and cost to amass. While economically feasible for web search, eliciting relevance annotations from experts is infeasible or impossible for most other ranking applications (e.g., personal collection search, intranet search). For these applications, implicit feedback from user behavior is an attractive source of data. Unfortunately, existing approaches for Learning-to-Rank (LTR) from implicit feedback -and clicks on search results in particular -have several limitations or drawbacks.</p><p>First, the naïve approach of treating a click/no-click as a positive/negative relevance judgment is severely biased. In particular, the order of presentation has a strong influence on where users click <ref type="bibr">[Joachims et al., 2007]</ref>. This presentation bias leads to an incomplete and skewed sample of relevance judgments that is far from uniform, thus leading to biased learning-to-rank.</p><p>Second, treating clicks as preferences between clicked and skipped documents has been found to be accurate <ref type="bibr">[Joachims, 2002;</ref><ref type="bibr">Joachims et al., 2007]</ref>, but it can only infer preferences that oppose the presented order. This again leads to severely biased data, and learning algorithms trained with these preferences tend to reverse the presented order unless additional heuristics are used <ref type="bibr">[Joachims, 2002]</ref>.</p><p>Third, probabilistic click models (see <ref type="bibr" target="#b0">Chuklin et al. [2015]</ref>) have been used to model how users produce clicks. By estimating latent parameters of these generative click models, one can infer the relevance of a given document for a given query. However, inferring reliable relevance judgments typically requires that the same query is seen multiple times, which is unrealistic in many retrieval settings (e.g., personal collection search) and for tail queries.</p><p>Fourth, allowing the LTR algorithm to randomize what is presented to the user, like in online learning algorithms <ref type="bibr" target="#b3">[Raman et al., 2013;</ref><ref type="bibr" target="#b0">Hofmann et al., 2013]</ref> and batch learning from bandit feedback (BLBF) [Swaminathan and <ref type="bibr" target="#b0">Joachims, 2015]</ref> can overcome the problem of bias in click data in a principled manner. However, requiring that rankings be actively perturbed during system operation whenever we collect training data decreases ranking quality and, therefore, incurs a cost compared to observational data collection.</p><p>In this paper we present a theoretically principled and empirically effective approach for learning from observational implicit feedback that can overcome the limitations outlined above. By drawing on counterfactual estimation techniques from causal inference <ref type="bibr">[Imbens and Rubin, 2015]</ref> and work on correcting sampling bias at the query level <ref type="bibr" target="#b4">[Wang et al., 2016]</ref>, we first develop a provably unbiased estimator for evaluating ranking performance using biased feedback data. Based on this estimator, we propose a Propensity-Weighted Empirical Risk Minimization (ERM) approach to LTR, which we implement efficiently in a new learning method we call Propensity SVM-Rank. While our approach uses a click model, the click model is merely used to assign propensities to clicked results in hindsight, not to extract aggregate relevance judgments. This means that our Propensity SVM-Rank does not require queries to repeat, making it applicable to a large range of ranking scenarios. Finally, our methods can use observational data and we do not require that the system randomizes rankings during data collection, except for a small pilot experiment to estimate the propensity model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Full-Info Learning to Rank</head><p>Before we derive our approach for LTR from biased implicit feedback, we first review the conventional problem of LTR from editorial judgments. In conventional LTR, we are given a sample X of i.i.d. queries x i ∼ P(x) for which we assume the relevances rel(x, y) of all documents y are known. Since all relevances are assumed to be known, we call this the Full-Information Setting. The relevances can be used to compute the loss ∆(y|x) (e.g., negative DCG) of any ranking y for query x. Aggregating the losses of individual rankings by taking the expectation over the query distribution, we can define the overall risk of a ranking system S that returns rankings S(x) as</p><formula xml:id="formula_0">R(S) = ∆(S(x)|x) d P(x).<label>(1)</label></formula><p>The goal of learning is to find a ranking function S ∈ S that minimizes R(S) for the query distribution P(x). Since R(S) cannot be computed directly, it is typically estimated via the empirical risk</p><formula xml:id="formula_1">R(S) = 1 |X| xi∈X ∆(S(x i )|x i ).</formula><p>A common learning strategy is Empirical Risk Minimization (ERM) <ref type="bibr" target="#b4">[Vapnik, 1998]</ref>, which corresponds to picking the system Ŝ ∈ S that optimizes the empirical risk Ŝ = argmin S∈S R(S) , possibly subject to some regularization in order to control overfitting. There are several LTR algorithms that follow this approach (see <ref type="bibr" target="#b3">Liu [2009]</ref>), and we use SVM-Rank <ref type="bibr">[Joachims, 2002]</ref> as a representative algorithm in this paper.</p><p>The relevances rel(x, y) are typically elicited via expert judgments. Apart from being expensive and often infeasible (e.g., in personal collection search), expert judgments come with at least two other limitations. First, it is clearly impossible to get explicit judgments for all documents, and pooling techniques <ref type="bibr">[Sparck-Jones and van Rijsbergen, 1975]</ref> often introduce bias. The second limitation is that expert judgments rel(x, y) have to be aggregated over all intents that underlie the same query string, and it can be challenging for a judge to properly conjecture the distribution of query intents to assign an appropriate rel(x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Partial-Info Learning to Rank</head><p>Learning from implicit feedback has the potential to overcome the above-mentioned limitations of full-information LTR. By drawing the training signal directly from the user, it naturally reflects the user's intent, since each user acts upon their own relevance judgement subject to their specific context and information need. It is therefore more appropriate to talk about query instances x i that include contextual information about the user, instead of query strings x. For a given query instance x i , we denote with r i (y) the user-specific relevance of result y for query instance x i . One may argue that what expert assessors try to capture with rel(x, y) is the mean of the relevances r i (y) over all query instances that share the query string. Relying on implicit feedback instead for learning allows us to remove a lot of guesswork about what the distribution of users meant by a query.</p><p>However, when using implicit feedback as a relevance signal, unobserved feedback is an even greater problem than missing judgments in the pooling setting. In particular, implicit feedback is distorted by presentation bias, and it is not missing completely at random <ref type="bibr">[Little and Rubin, 2002]</ref>. To nevertheless derive well-founded learning algorithms, we adopt the following counterfactual model. It closely follows <ref type="bibr" target="#b3">[Schnabel et al., 2016]</ref>, which unifies several prior works on evaluating information retrieval systems.</p><p>For concreteness and simplicity, assume that relevances are binary, r i (y) ∈ {0, 1}, and our performance measure of interest is the sum of the ranks of the relevant results</p><formula xml:id="formula_2">∆(y|x i , r i ) = y∈y rank(y|y) • r i (y).</formula><p>(2)</p><p>Analogous to (1), we can define the risk of a system as</p><formula xml:id="formula_3">R(S) = ∆(S(x)|x, r) d P(x, r).<label>(3)</label></formula><p>In our counterfactual model, there exists a true vector of relevances r i for each incoming query instance (x i , r i ) ∼ P(x, r). However, only a part of these relevances is observed for each query instance, while typically most remain unobserved. In particular, given a presented ranking ȳi we are more likely to observe the relevance signals (e.g., clicks) for the top-ranked results than for results ranked lower in the list.</p><p>Let o i denote the 0/1 vector indicating which relevance values were revealed, o i ∼ P(o|x i , ȳi , r i ). For each element of o i , denote with Q(o i (y) = 1|x i , ȳi , r i ) the marginal probability of observing the relevance r i (y) of result y for query x i , if the user was presented the ranking ȳi . We refer to this probability value as the propensity of the observation. We discuss how o i and Q can be obtained more in Section 4. Using this counterfactual modeling setup, we can get an unbiased estimate of ∆(y|x i , r i ) for any new ranking y (typically different from the presented ranking ȳi ) via the inverse propensity scoring (IPS) estimator <ref type="bibr" target="#b0">[Horvitz and Thompson, 1952;</ref><ref type="bibr" target="#b3">Rosenbaum and Rubin, 1983;</ref><ref type="bibr">Imbens and Rubin, 2015]</ref> </p><formula xml:id="formula_4">∆IP S (y|x i , ȳi , o i ) = y:oi(y)=1 rank(y|y)•r i (y) Q(o i (y) = 1|x i , ȳi , r i ) = y:oi(y)=1 ri(y)=1 rank(y|y) Q(o i (y) = 1|x i , ȳi , r i )</formula><p>.</p><p>This is an unbiased estimate of ∆(y|x i , r i ) for any y, if Q(o i (y) = 1|x i , ȳi , r i ) &gt; 0 for all y that are relevant r i (y) = 1 (but not necessarily for the irrelevant y). The proof for this is quite straightforward and can be found in the original paper <ref type="bibr" target="#b0">[Joachims et al., 2017]</ref>.</p><p>An interesting property of ∆IP S (y|x i , ȳi , o i ) is that only those results y with [o i (y) = 1 ∧ r i (y) = 1] (i.e. clicked results, as we will see later) contribute to the estimate. We therefore only need the propensities Q(o i (y) = 1|x i , ȳi , r i ) for relevant results. Since we will eventually need to estimate the propensities Q(o i (y) = 1|x i , ȳi , r i ), an additional requirement for making ∆IP S (y|x i , ȳi , o i ) computable while remaining unbiased is that the propensities only depend on observable information (i.e., unconfoundedness, see <ref type="bibr">Imbens and Rubin [2015]</ref>).</p><p>Having a sample of N query instances x i , recording the partially-revealed relevances r i as indicated by o i , and the propensities Q(o i (y) = 1|x i , ȳi , r i ), the empirical risk of a system is simply the IPS estimates averaged over query instances:</p><formula xml:id="formula_5">RIP S (S) = 1 N N i=1 y:oi(y)=1 ri(y)=1 rank(y|S(x i )) Q(o i (y) = 1|x i , ȳi , r i ) .<label>(4)</label></formula><p>Since ∆IP S (y|x i , ȳi , o i ) is unbiased for each query instance, the aggregate RIP S (S) is also unbiased for R(S) from ( <ref type="formula" target="#formula_3">3</ref>),</p><formula xml:id="formula_6">E[ RIP S (S)] = R(S).</formula><p>Furthermore, it is easy to verify that RIP S (S) converges to the true R(S) under mild additional conditions (i.e., propensities bounded away from 0) as we increase the sample size N of query instances. So, we can perform ERM using this propensity-weighted empirical risk, Ŝ = argmin S∈S RIP S (S) .</p><p>Finally, using standard results from statistical learning theory <ref type="bibr" target="#b4">[Vapnik, 1998]</ref>, consistency of the empirical risk paired with capacity control implies consistency also for ERM. In intuitive terms, this means that given enough training data, the learning algorithm is guaranteed to find the best system in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Position-Based Propensity Model</head><p>The previous section showed that the propensities of the observations Q(o i (y) = 1|x i , ȳi , r i ) are the key component for unbiased LTR from biased observational feedback. To derive propensities of observed clicks, we consider a straightforward examination model analogous to <ref type="bibr" target="#b3">Richardson et al. [2007]</ref>, where a click on a search result depends on the probability that a user examines a result (i.e., e i (y)) and then decides to click on it (i.e., c i (y)) in the following way:</p><p>P (e i (y) = 1| rank(y| ȳ)) • P (c i (y) = 1| r i (y), e i (y) = 1).</p><p>In this model, examination depends only on the rank of y in ȳ. So, P (e i (y) = 1| rank(y| ȳi )) can be represented by a vector of examination probabilities p r , one for each rank r, which are precisely the propensities Q(o i (y) = 1|x i , ȳi , r i ).</p><p>These examination probabilities can model presentation bias found in eye-tracking studies <ref type="bibr">[Joachims et al., 2007]</ref>, where users are more likely to see results at the top of the ranking than those further down.</p><p>Under this propensity model, we can simplify the IPS estimator from (4) by substituting p r as the propensities and by using</p><formula xml:id="formula_7">c i (y) = 1 ↔ [o i (y) = 1 ∧ r i (y) = 1] RIP S (S) = 1 n n i=1 y:ci(y)=1 rank(y|S(x i ))</formula><p>p rank(y| ȳi) .</p><p>(5)</p><p>RIP S (S) is an unbiased estimate of R(S) under the positionbased propensity model if p r &gt; 0 for all ranks. While absence of a click does not imply that the result is not relevant (i.e., c i (y) = 0 → r i (y) = 0), the IPS estimator has the nice property that such explicit negative judgments are not needed to compute an unbiased estimate of R(S) for the loss in (2). Similarly, while absence of a click leaves us unsure about whether the result was examined (i.e., e i (y) = ?), the IPS estimator only needs to know the indicators o i (y) = 1 for results that are also relevant (i.e., clicked results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Propensity SVM-Rank</head><p>We now derive a concrete learning method that implements propensity-weighted LTR. It is based on SVM-Rank <ref type="bibr">[Joachims, 2002;</ref><ref type="bibr" target="#b2">Joachims, 2006</ref>], but we conjecture that propensity-weighted versions of other LTR methods can be derived as well. Consider a dataset of n examples of the following form. For each query-result pair (x j , y j ) that is clicked, we compute the propensity q i = Q(o i (y) = 1|x i , ȳi , r i ) of the click according to our click propensity model. We also record the candidate set Y j of all results for query x j . Typically, Y j contains a few hundred documents -selected by a stage-one ranker <ref type="bibr" target="#b4">[Wang et al., 2011]</ref> -that we aim to rerank. Note that each click generates a separate training example, even if multiple clicks occur for the same query.</p><p>Given this propensity-scored click data, we define Propensity SVM-Rank as a generalization of conventional SVM-Rank. Propensity SVM-Rank learns a linear scoring function f (x, y) = w • φ(x, y) that can be used for ranking results, where w is a weight vector and φ(x, y) is a feature vector that describes the match between query x and result y.</p><p>Propensity SVM-Rank optimizes the following objective,</p><formula xml:id="formula_8">ŵ = argmin w,ξ 1 2 w • w + C n n j=1 1 q j y∈Yj ξ jy s.t. ∀y ∈ Y 1 \{y 1 } : w•[φ(x 1 , y 1 ) − φ(x 1 , y)] ≥ 1−ξ 1y . . . ∀y ∈ Y n \{y n } : w•[φ(x n , y n ) − φ(x n , y)] ≥ 1−ξ ny ∀j∀y : ξ jy ≥ 0.</formula><p>C is a regularization parameter that is typically selected via cross-validation. The training objective optimizes an upper bound on the regularized IPS estimated empirical risk of (5), since each line of constraints corresponds to the rank of a relevant document (minus 1).</p><p>In particular, for any feasible (w, ξ)</p><formula xml:id="formula_9">rank(y i |y)−1 = y =yi 1 w•[φ(xi,y)−φ(xi,yi)]&gt;0 ≤ y =yi max(1−w • [φ(x i , y i ) − φ(x i , y)], 0) ≤ y =yi ξ iy .</formula><p>We can solve this type of Quadratic Program efficiently via a one-slack formulation <ref type="bibr" target="#b2">[Joachims, 2006]</ref>, and we are using SVM-Rank with appropriate modifications to include IPS weights 1/q j . The code is available online 1 .</p><p>In the empirical evaluation, we compare against the naive application of SVM-Rank, which minimizes the rank of the clicked documents while ignoring presentation bias. In particular, Naive SVM-Rank sets all the q i uniformly to the same constant (e.g., 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>The original paper <ref type="bibr" target="#b0">[Joachims et al., 2017]</ref> takes a twopronged approach to evaluation. First, it uses synthetically generated click data to explore the behavior of our methods over the whole spectrum of presentation bias severity, click noise, and propensity mis-specification. Due to space constraints, we do not include these experiments here, but focus on a real-world experiment that evaluates our approach on an operational search engine using real click-logs from live traffic. In particular, we examine the performance of Propensity SVM-Rank when learning a new ranking function for the Arxiv Full-Text Search 2 based on real-world click logs from this system. The search engine uses a linear scoring function f (x, y) = w • φ(x, y). The Query-document features φ(x, y) are represented by a 1000−dimensional vector, and the production ranker used for collecting training clicks employs a hand-crafted weight vector w (denoted Prod). Observed clicks on rankings served by this ranker over a period of 21 days provide implicit feedback data for LTR as outlined in Section 3.2.</p><p>To estimate the propensity model, we consider the simple position-based model of Section 3.1 and we collect new click data via randomized interventions for 7 days as detailed in <ref type="bibr" target="#b0">[Joachims et al., 2017]</ref> with landmark rank k = 1. In short, before presenting the ranking, we take the top-ranked document and swap it with the document at a uniformly-atrandom chosen rank j ∈ {1, . . . 21}. The ratio of observed click-through rates (CTR) on the formerly top-ranked document now at position j vs. its CTR at position 1 gives a noisy estimate of p j /p 1 in the position-based click model. We additionally smooth these estimates by interpolating with the overall observed CTR at position j (normalized so that CT R@1 = 1). This yields p r that approximately decay with rank r, and the smallest p r 0.12. For r &gt; 21, we impute p r = p 21 . Since the original paper appeared, another technique for propensity estimation from observational data has been proposed that could also be used <ref type="bibr" target="#b4">[Wang et al., 2018</ref> We partition the click-logs into a train-validation split: the first 16 days are the train set and provide 5437 click-events, while the remaining 5 days are the validation set with 1755 click events. The hyper-parameter C is picked via cross validation. We use the IPS estimator for Propensity SVM-Rank, and the naive estimator with Q(o(y) = 1|x, ȳ, r) = 1 for Naive SVM-Rank. With the best hyper-parameter settings, we re-train on all 21 days worth of data to derive the final weight vectors for either method.</p><p>We fielded these learnt weight vectors in two online interleaving experiments <ref type="bibr" target="#b0">[Chapelle et al., 2012]</ref>, the first comparing Propensity SVM-Rank against Prod and the second comparing Propensity SVM-Rank against Naive SVM-Rank. The results are summarized in Table <ref type="table" target="#tab_0">1</ref>. We find that Propensity SVM-Rank significantly outperforms the hand-crafted production ranker that was used to collect the click data for training (two-tailed binomial sign test p = 0.001 with relative risk 0.71 compared to null hypothesis). Furthermore, Propensity SVM-Rank similarly outperforms Naive SVM-Rank, demonstrating that even a simple propensity model provides benefits on real-world data (two-tailed binomial sign test p = 0.006 with relative risk 0.77 compared to null hypothesis). Note that Propensity SVM-Rank not only significantly, but also substantially outperforms both other rankers in terms of effect size -and the synthetic data experiments suggest that additional training data will further increase its advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future</head><p>This paper introduced a principled approach for learning-torank under biased feedback data. Drawing on counterfactual modeling techniques from causal inference, we present a theoretically sound Empirical Risk Minimization framework for LTR. We instantiate this framework with a propensityweighted ranking SVM. Real-world experiments on a live search engine show that the approach leads to substantial retrieval improvements.</p><p>Beyond the specific learning methods and propensity models we propose, this paper may have even bigger impact for its theoretical contribution of developing the general counterfactual model for LTR, thus articulating the key components necessary for LTR under biased feedback. First, the insight that propensity estimates are crucial for ERM learning opens a wide area of research on designing better propensity models. Second, the theory demonstrates that LTR methods should optimize propensity-weighted ERM objectives, raising the question of which other learning methods can be adapted. Third, we conjecture that propensity-weighted ERM approaches can be developed also for pointwise and listwise LTR methods using techniques from <ref type="bibr" target="#b3">Schnabel et al. [2016]</ref>.</p><p>Beyond learning from implicit feedback, propensityweighted ERM techniques may prove useful even for optimizing offline IR metrics on manually annotated test collections. First, they can eliminate pooling bias, since the use of sampling during judgment elicitation puts us in a controlled setting where propensities are known (and can be optimized <ref type="bibr" target="#b3">[Schnabel et al., 2016]</ref>) by design. Second, propensities estimated via click models can enable click-based IR metrics like click-DCG to better correlate with test set DCG.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>]. Per-query balanced interleaving results for detecting relative performance between the hand-crafted production ranker used for click data collection (Prod), Naive SVM-Rank and Propensity SVM-Rank.</figDesc><table><row><cell>1 http://www.joachims.org/svm light/svm proprank.html</cell></row><row><cell>2 http://search.arxiv.org:8081/</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part through NSF Awards IIS-1247637, IIS-1513692, IIS-1615706, and a gift from Bloomberg. We thank Maarten de Rijke, Alexey Borisov, Artem Grotov, and Yuning Mao for valuable feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shimon Whiteson, and Maarten de Rijke. Reusing historical interaction data for faster online learning to rank for IR</title>
		<author>
			<persName><surname>Chapelle</surname></persName>
		</author>
		<idno>6:1-6:41</idno>
	</analytic>
	<monogr>
		<title level="m">Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. Click Models for Web Search. Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
				<editor>
			<persName><forename type="first">Laura</forename><surname>Granka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bing</forename><surname>Pan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Helene</forename><surname>Hembrooke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Geri</forename><surname>Gay</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1952">2012. 2012. 2015. 2015. 2013. 2013. 1952. 2015. April 2007. 2017. 2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
	<note>ACM Conference on Web Search and Data Mining (WSDM)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training linear SVMs in linear time</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (KDD)</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Roderick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Little</surname></persName>
		</editor>
		<editor>
			<persName><surname>Rubin</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="2002">2006. 2006. 2002. 2002</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
	<note>Statistical Analysis with Missing Data</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu ; Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on the Theory of Information Retrieval (ICTIR)</title>
				<imprint>
			<publisher>Swaminathan and Joachims</publisher>
			<date type="published" when="1975">2009. March 2009. 2013. 2013. 2007. 2007. 1983. 1983. 2016. 2016. 1975. 1975. 2015. Sep 2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1731" to="1755" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Machine Learning Research (JMLR). Special Issue in Memory of Alexey Chervonenkis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Position bias estimation for unbiased learning to rank in personal search</title>
		<author>
			<persName><surname>Vapnik ; Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Chichester</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998. 1998. 2011. 2011. 2016. 2016. 2018. 2018</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
	<note>Conference on Web Search and Data Mining (WSDM)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
