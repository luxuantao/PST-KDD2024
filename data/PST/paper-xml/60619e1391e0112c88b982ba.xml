<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Equivariant Point Network for 3D Point Cloud Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-25">25 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiwei</forename><surname>Chen</surname></persName>
							<email>chenh@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shichen</forename><surname>Liu</surname></persName>
							<email>lshichen@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weikai</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<address>
									<country>Tencent America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Pinscreen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Randall</forename><surname>Hill</surname></persName>
							<email>hill@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Equivariant Point Network for 3D Point Cloud Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-25">25 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.14147v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Features that are equivariant to a larger group of symmetries have been shown to be more discriminative and powerful in recent studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b5">6]</ref>. However, higher-order equivariant features often come with an exponentiallygrowing computational cost. Furthermore, it remains relatively less explored how rotation-equivariant features can be leveraged to tackle 3D shape alignment tasks. While many past approaches have been based on either nonequivariant or invariant descriptors to align 3D shapes, we argue that such tasks may benefit greatly from an equivariant framework. In this paper, we propose an effective and practical SE(3) (3D translation and rotation) equivariant network for point cloud analysis that addresses both problems. First, we present SE(3) separable point convolution, a novel framework that breaks down the 6D convolution into two separable convolutional operators alternatively performed in the 3D Euclidean and SO(3) spaces respectively. This significantly reduces the computational cost without compromising the performance. Second, we introduce an attention layer to effectively harness the expressiveness of the equivariant features. While jointly trained with the network, the attention layer implicitly derives the intrinsic local frame in the feature space and generates attention vectors that can be integrated with different alignment tasks. We evaluate our approach through extensive studies and visual interpretations. The empirical results demonstrate that our proposed model outperforms strong baselines in a variety of benchmarks. Code is available at https://github.com/nintendops/EPN PointCloud.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of 2D CNNs stems in large part from the ability of exploiting the translational symmetries via weight sharing and translation equivariance. Recent trends strive Figure <ref type="figure">1</ref>: The core of our network is a convolution operator on point clouds, termed SE(3) separable point convolution (SPConv), that consumes features defined in the SE(3) space and outputs per-point features that are SE(3) equivariant. When the output feature is spatially pooled over the Euclidean space, it becomes SO(3) equivariant, as visualized above by projecting onto the spherical domain. Our method also supports a faithful conversion from the equivariant feature to its invariant counterpart by using a novel attentive fusing mechanism. Thereby, we offer a general framework that can generate equivariant or invariant point feature depending on the nature of downstream applications.</p><p>to duplicate this success to 3D domain in order to shed new light on the 3D learning tasks. With the 3D scanning technology being the mainstream manner of measuring the real world, point cloud arises naturally as one of the most prominent 3D representations. Yet, despite its simple and unified structure, it remains a nuisance to extend the CNN architecture to analyzing point clouds. In addition, the group of transformations in 3D data is more complex compared to 2D images, as 3D entities are often transformed by arbitrary 3D translations and 3D rotations when observed. Although group-invariant operators could render identical features even under different group transforms, it fails to distinguish distinct instances with internal symmetries (e.g. the counterparts of "6" and "9" in 3D scenarios regarding rotational symmetry). In contrast, equivariant features are much more expressive thanks to their ability to retain information about the input group transform on the feature maps throughout the neural layers. As a result, it could be very beneficial for point cloud features to be equivariant to the SE(3) group of transformations while being invariant to point permutations.</p><p>Despite the importance of deriving SE(3)-equivariant features for point clouds, progress in this regard remains highly sparse. The main obstacles arise in two aspects. First, the cost of computing convolutions between 6dimensional functions over the entire SE(3) spaces is prohibitive especially in the presence of bulky 3D raw scans. Second, it remains challenging to fully harness the expressiveness of equivariant features without losing important structural information at a low computational cost. In particular, matching any two group-equivariant features is the prerequisite of many applications like correspondence computation, pose estimation, etc. One common practice is to compute the best relative group transformation that maximizes the similarity of the input features when the transformation is applied. This typically requires solving a PnP optimization which is quite costly considering the high dimensionality of the features. Another option is to fuse the equivariant features into invariant ones via pooling operation and directly compare the invariant features to obtain similarity. However, we argue that the naive pooling operations will inevitably discard useful features and damage the equivariant structure of the feature.</p><p>In this paper, we strive to address both of the problems by introducing an effective and practical framework for learning SE(3)-equivariant features of point clouds. In particular, inspired by the spirit of "going wider" in the Inception module <ref type="bibr" target="#b40">[42]</ref>, we first propose SE(3) separable convolution, a novel paradigm that breaks down the naive 6D convolution into two separable convolutional operators alternatively performed in the 3D Euclidean and SO(3) spaces. Due to the non-commutative and non-compact nature of SE(3) group, it is non-trivial to factorize SE(3) convolution into two separable sub-operators. We achieve this goal by first lifting the input points to the homogeneous space. We then take advantage of the finite rotation groups such as the icosahedral and aggregate spatially-convoluted features as functions on the rotation groups that are processed via group convolution. The proposed SE(3) separable convolution significantly reduces the computational cost of a SE(3) convolution and leads to practical solutions that can be deployed in the commodity hardware.</p><p>Second, we present an attention mechanism specially tailored for fusing SE(3)-equivariant features. We observe that while the commonly used pooling operations, such as max or mean pooling, work well in translation equivariant networks like 2D CNNs, they are not best suited for fusing equivariant features in SO(3) groups. This is mostly due to the highly sparse and non-linear structure of SO(3) features which poses additional challenges for max/mean pooling to maintain its unique pattern without losing too much information. We introduce group attentive pooling (GA pooling) to adaptively fuse rotation-equivariant features into their invariant counterparts. Trained together with the network, the GA pooling layer implicitly learns an intrinsic local frame of the feature space and generates attention weights to guide the pooling of rotation-equivariant features.</p><p>Third, compared to invariant features, equivariant features preserves, rather than discards, spatial structure and therefore can be seen as a more discriminative representation. It is for this reason that translational equivariance has been the premise for convolutional approaches for detection and instance segmentation <ref type="bibr" target="#b15">[16]</ref>. Similarly, through the attention mechanism, the equivariant framework can be utilized for inferring 3D rotations. We demonstrate in the experiments that this structure significantly outperforms a non-equivariant framework in a shape alignment task.</p><p>We validate our proposed framework on a variety of tasks. Experimental results show that our approach consistently outperforms strong baselines. We also perform ablation analysis and qualitative visualization to evaluate the effectiveness of each algorithmic component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human-engineered Point Features. There is rich literature <ref type="bibr" target="#b18">[19]</ref> on investigating local geometric descriptors. One mainstream strategy resorts to local shape context encoded by geometry histogram and its variants <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b24">25]</ref>. The other line of research strives to achieve rotation-invariance in designing the 3D feature. In particular, local reference frame (LRF) <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b51">53]</ref> is widely employed to transform the local neighborhood of the point to a canonical space where the point features are analyzed and compared. In contrast, several approaches <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">39</ref>] leverage intrinsically invariant features, e.g. point pair features, without requiring LRF estimation. Though significant progress has been made, the hand-crafted features often fail to deal with noisy and incomplete data.</p><p>Learning-based Point Descriptor. The seminal work on handling irregular structure of point cloud places the main emphasis on permutation-invariant functions <ref type="bibr" target="#b35">[36]</ref>. Later works proposes shift equivariant hierarchical architectures with localized filters to align with the regular grid CNNs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. Explicit convolution kernels have also received tremendous attention in recent years. In particular, various kernel forms have been proposed, including voxel bins <ref type="bibr" target="#b21">[22]</ref>, polynomial functions <ref type="bibr" target="#b50">[52]</ref> or linear func-tions <ref type="bibr" target="#b17">[18]</ref>. Other works consider different representations of point clouds, noticeably image projection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> and voxels <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">51]</ref>. We point interested readers to <ref type="bibr" target="#b20">[21]</ref> for a comprehensive survey on point cloud convolution.</p><p>Rotation invariant point descriptors have been an active research area due to its importance to correspondence matching. While the features extracted by most of the above approaches are permutation-invariant, very few of them can achieve rotation-invariance. The Perfect Match <ref type="bibr" target="#b16">[17]</ref> incorporates a local reference frame (LRF) to extract rotationinvariant features from the voxelized point cloud. Similarly, <ref type="bibr" target="#b53">[55]</ref> proposes a capsule network that consumes a point cloud along with the estimated LRF to disentangle shape and pose information. By only taking point pair as input, PPF-FoldNet <ref type="bibr" target="#b8">[9]</ref> can learn rotation-invariant descriptors using folding-based encoding of point pair features. However, invariant features may be limited in expressiveness as spatial information is discarded a priori.</p><p>Learning Rotation-equivariant Features. Since CNNs are sensitive to rotations, a rapidly growing body of work focus on investigating rotation-equivariant variants. Starting from the 2D domain, various approaches have been proposed to achieve rotation equivariance by applying multiple oriented filters <ref type="bibr" target="#b33">[34]</ref>, performing a log-polar transform of the input <ref type="bibr" target="#b12">[13]</ref>, replacing filters with circular harmonics <ref type="bibr" target="#b48">[50]</ref> or rotating the filters <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">48]</ref>. Cohen and Welling later extend the domain of 2D CNNs from translation to finite groups <ref type="bibr" target="#b4">[5]</ref> and further to arbitrary compact groups <ref type="bibr" target="#b7">[8]</ref>.</p><p>When it comes to the domain of 3D rotation, previous efforts can be divided into spectral and non-spectral methods. In the spectral branch, generalized Fourier transform for S 2 and SO(3) underlies designs for rotation equivariant CNN. We would like to highlight two seminal works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref> that define convolution operators respectively by spherical (S 2 ) correlation, and SO(3) correlation with circularly symmetric kernels. The works most relevant to our setting are extensions of the two spectral paradigms to the 3D spatial domain. A number of works extend spherical CNNs to 3D voxels grids <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24</ref>]. Yet, the research work on exploring the potential on point clouds remains sparse, with the exception of a concurrent work Tensor field network (TFN) <ref type="bibr" target="#b42">[44]</ref>, which achieves SE(3) equivariance on irregular point clouds. While <ref type="bibr" target="#b42">[44]</ref> shares with us in the use of tensor-field representation, their proposed filters are products of radial function and spherical harmonics. We instead focus on a non-spectral, computationally efficient separable framework.</p><p>Our work finds inspiration from the non-spectral group equivariant approaches that have seen recent progress, extending from mathematical framework derived in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. Specifically, <ref type="bibr" target="#b6">[7]</ref> provides a general framework for the practical implementation of convolution on discretized rotation group, with icosahedral convolution as an examplar. Dis-crete group convolution characterizes many recent works on images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>, spherical signal <ref type="bibr" target="#b39">[41]</ref>, voxel grid <ref type="bibr" target="#b47">[49]</ref> and point cloud <ref type="bibr" target="#b28">[29]</ref>. Most of these works focus only on rotational equivariance. We are the first in this branch to provide a unified, hierarchical framework for point cloud convolution that is equivariant to the space of SE(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview. In this section, we first start with the preliminaries of SE(3) convolutions. We will then provide the detailed mathematical formulation of our approach: (1) the SE(3) separable convolution; and (2) attention mechanism for the equivariant features. The Lie group SE(3) is the group of rigid body transformations in three-dimensions:</p><formula xml:id="formula_0">SE(3) = {A|A = R t 0 1 , R ∈ SO(3), t ∈ R 3 }. SE(3) is homeomorphic to R 3 × SO(3)</formula><p>. Therefore, a function that is equivariant to SE(3) must be equivariant to both 3D translation t ∈ R 3 and 3D rotation g ∈ SO(3). Given a spatial point x and a rotation g, let us first define the continuous feature representation in SE(3) as a function</p><formula xml:id="formula_1">F(x i , g j ) : R 3 × SO(3) → R D . Equivariance to SE(3) is expressed as satisfying ∀A ∈ SE(3), A(F * h)(x, g) = (AF * h)(x, g).</formula><p>The SE(3) equivariant continuous convolutional operator can be defined as</p><formula xml:id="formula_2">(F * h)(x, g) = xi∈R 3 gj ∈SO(3) F(x i , g j )h(g −1 (x − x i ), g −1 j g),<label>(1)</label></formula><p>where h is a kernel h(x, g) : R 3 × SO(3) → R D . The convolution is computed by translating and rotating the kernel and then computing a dot product with the input function F. We prove that this convolution is equivariant to SE(3) in the supplementals.</p><p>Discretization. To discretize Eq. 1, we starts with discretizing the SE(3) space into a composition of a finite set of 3D spatial point P : {x|x ∈ R 3 } and a finite rotation group G ⊂ SO(3). This leads to a discrete SE(3) feature mapping function F(x i , g j ) : P × G → R D . The discrete convolutional operator in SE(3) is therefore:</p><formula xml:id="formula_3">(F * h)(x, g) = xi∈P gj ∈G F(x i , g j )h(g −1 (x − x i ), g −1 j g).</formula><p>(2) We note that such discretization serves as a good approximation of the continuous formulation in Eq. 1, where the approximation error can be further mitigated by the rotation augmentation <ref type="bibr" target="#b0">[1]</ref>. If we interpret P as a set of 3D displace-  ments, this leads to an equivalent definition:</p><formula xml:id="formula_4">(F * h)(x, g) = xi∈P gj ∈G F(g −1 (x − x i ), g −1 j g)h(x i , g j ) = x i ∈Pg gj ∈G F(x − x i , g −1 j g)h(gx i , g j ).</formula><p>(</p><formula xml:id="formula_5">)<label>3</label></formula><p>Without loss of generality, we assume the coordinate is expressed in the local frame of x and therefore g −1 x = x. In the second row of Eq. 3, the summation over the set P becomes a summation over a rotated set P g : {g −1 x|x ∈ P}. When written this way, we can see that the kernel is parameterized by a set of translation offsets and rotation offsets under the reference frame given by g. We call the discrete set P × G the domain of the kernel with a kernel size of |P| × |G|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SE(3) Separable Convolution.</head><p>A key issue with Eq. 3 is that the convolution is computed over a 6-dimensional space -a naive implementation would be computational prohibitive. Inspired by the core idea of separable convolution <ref type="bibr" target="#b3">[4]</ref>, we observe that the kernel h with a kernel size |P| × |G| can be separated into two smaller kernels, denoting h 1 with a kernel size of |P| × 1 and h 2 with a kernel size of 1 × |G|. This divides the domain of the kernel to two smaller domains: P × {I} for h 1 , and {0} × G for h 2 , where I is the identity matrix, and 0 is a zero displacement vector. From here, we are ready to separate Eq. 3 into two convolutions:</p><formula xml:id="formula_6">(F * h 1 )(x, g) = x i ∈Pg F(x − x i , g)h 1 (gx i , I) (4) (F * h 2 )(x, g) = gj ∈G F(x, g −1 j g)h 2 (0, g j )<label>(5)</label></formula><p>We can see that h 1 is a kernel only varied by translation in the reference frame of g, and h 2 is a kernel only varied by the rotation g j . In the following text, we simplify them to h 1 (gx i ) and h 2 (g j ). The division here matches with the observation that the space SE(3) can be factorized into two spaces R 3 and SO <ref type="bibr" target="#b2">(3)</ref>. Sequentially applying the two convolutions in Eq. (4-5) approximates the 6D convolution in Eq. 3 (Fig. <ref type="figure" target="#fig_1">2(d)</ref>) while maintaining equivariance to SE(3) (proofs provided in the supplementary materials). The working principle here is similar to that of the Inception module <ref type="bibr" target="#b40">[42]</ref> and its follow-up works <ref type="bibr" target="#b3">[4]</ref>, which have shown the promising property of separable convolutions in improving the network performance with reduced cost. We name the two consecutive convolutions as SE(3) point convolution and SE(3) group convolution, respectively, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We refer the combined convolutions as SE(3) separable point convolution (SPConv). Formally, the original 6D convolution is approximated by: F * h ≈ (F * h 1 ) * h 2 .</p><p>A SE(3) equivariant convolutional network can be realized by consecutive blocks of SPConv. The network consumes the input P and produces a SE(3) equivariant feature for the point set. Since SPConv only takes functions defined on SE(3) as input, for each point in the input point set, we set F(x, g) = 1 for each g ∈ G. In the following sections, we discuss in details the form of kernel and how it can be localized for each convolution module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SE(3) point convolution</head><p>Our SE(3) point convolution layer aims at aggregating point spatial correlations locally under a rotation group element g. Let N x = {x i ∈ P x − x i r} be the set of neighbor points for point x, with a radius r, the SE(3) point convolution with localized kernel is: <ref type="bibr" target="#b5">(6)</ref> where</p><formula xml:id="formula_7">(F * h 1 )(x, g) = x i ∈Ngx F(x − x i , g)h 1 (gx i ),</formula><formula xml:id="formula_8">N gx = {g −1 (x − x i )|x i ∈ N x }</formula><p>is the set of displacements to the neighbor points under a rotation g. h 1 is a kernel defined in a canonical neighbor space B 3 r . Given that the convolution is computed as a spatial correlation under a rotation g, the form of the kernel can be naturally extended from any spatial kernel function. While our framework is general to support various spatial kernel definitions, we introduce two kernel formulations that are used in our implementation.</p><p>Explicit kernels. Given kernel size K, we can define a set of kernel points {ỹ k } K evenly distributed in B 3 r . Each kernel point is associated with a kernel weight matrix W k ∈ R Din×Dout , where D in and D out are the input and output channel, respectively. Let κ(•, •) be the correlation function between two points, we have</p><formula xml:id="formula_9">h 1 (x i ) = K k κ(x i , ỹk )W k .<label>(7)</label></formula><p>The correlation function κ(y, ỹ) can be either linear or Gaussian. For example, in the linear case described in <ref type="bibr" target="#b42">[44]</ref>,</p><formula xml:id="formula_10">κ(y, ỹ) = max(0, 1 − y−ỹ σ</formula><p>), where σ adjusts the bandwidth. Implicit kernels. The implicit formulation gives a function on point set that does not utilize parameterized kernels and is generally not considered a convolutional operation. Rather, spatial correlation is computed implicitly by concatenating the local frame coordinates of points to their corresponding features. In the SE(3) equivariant extension, the local coordinates are also composed by a corresponding rotation g. The implicit filter for the input signal F is:</p><formula xml:id="formula_11">h 1 (F(x, g)) = xi∈Nx h 1 (F(x i , g), g −1 x i ) = xi∈Nx F(x i , g) g −1 x i W.<label>(8)</label></formula><p>We believe that other choices of kernel functions can be naturally extended from these two examples. In our implementation of the network, we use the explicit kernel formulation in all convolutional layers. The last layer before the output block of our network filters point features globally and therefore utilizes the implicit formulation, as it scales better to process a larger set of point features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SE(3) group convolution</head><p>Given a discrete rotation group G, the SE(3) group convolution computes SO(3) correlation between the input signal and a kernel defined on the group domain.</p><p>We define a set of kernel rotation and their associate kernel weight matrices as N g = {g j ∈ G} K and {W j ∈ R Din×Dout } K , with the kernel size K = |N g |. Thus the kernel is simply h 2 (g j ) = W j . Our SE(3) group convolution layer aggregates information from neighboring rotation signals within the group, which is given by</p><formula xml:id="formula_12">(F * h 2 )(x, g) = gj ∈Ng F(x, g −1 j g)h 2 (g j ).<label>(9)</label></formula><p>In our implementation, the icosahedron group can be used as the discrete rotation group. The K neighbor rotations are a subset of the group that is smallest in the corresponding angle. The computation can be accelerated by pre-computing the permutation index and only performing constant-time query with an index layer at run time.</p><p>Complexity analysis. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, by combining the two equivariance-preserving convolutions, we can achieve a similar effect with Eq. 2 at a significantly lower computational cost. In particular, suppose we divide the original number of kernels K into K p and K g , the number of kernels in the point and group convolution; C = C i C o where C i and C o are the number of input and output channels, N = N p N a is the product of the number of points and the number of SO(3) element in a rotation group. The naive 6D convolution requires a computational complexity of O(K p K g CN ). In contrast, the complexity of our approach is reduced to O((K p + K g )CN ), which could achieve orders-lower complexity compared to the naive solution especially with large K p and K g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Shape Matching with Attention mechanism</head><p>In this section, we demonstrate how attention mechanism can be utilized to harness the power of equivariant feature. Given spatially pooled features that are equivariant to SO(3):</p><formula xml:id="formula_13">F(g) : G → R D , we define a rotation-based attention A : G → R, A(g) = {a g | g∈G a g = 1}.</formula><p>SO(3) Detection. Suppose a task requires the network to predict the pose R ∈ SO(3) of an input shape. When the attention weight is used as a probability score, the equivariant network turns the pose estimation task into a SO(3) detection task, which is analogous to bounding box detection. Intuitively, each element from the discrete rotation group can be interpreted as an anchor. A two-branch network is used to classify whether the anchor is the "dominant rotation". Every anchor regresses a small rotational offset from its corresponding rotation. The multi-task loss for rotational regression is then given by:</p><formula xml:id="formula_14">L(a, u, R, R u ) = L cls (a, u) + λ[u = 1]L 2 (R u R T ) (10)</formula><p>where a = {a g |g ∈ G} are the predicted probabilities and R are the predicted relative rotations. u = {u g |g ∈ G} is the ground-truth label with u g = 1 if g is the nearest rotation to the target ground truth rotation</p><formula xml:id="formula_15">R GT . R u = {R u g |∀g ∈ G, R u g g = R GT } is the ground truth relative rotation.</formula><p>Group Attentive Pooling. Global pooling layers are integrated as part of the network for spatial reduction of the representation. As many common tasks, such as classification, benefit from rotation invariance of the learned feature, global pooling is utilized by most rotation-equivariant architectures to aggregate information into an invariant representation. To integrate attention mechanism with global pooling, we propose group attentive pooling (GA pooling), which is given by</p><formula xml:id="formula_16">F inv = g exp(a g /T )F G (g) g exp(a g /T ) ,<label>(11)</label></formula><p>where F G (g) and a g are the input rotation-equivariant feature and attention weight on rotation g. T is a temperature score to control the sharpness of the function response.</p><p>As visualized in Fig. <ref type="figure">1</ref>, the output feature is invariant given a rotated input point cloud. The confidence weight a can be learned by minimizing the loss L = L task + λL sa , where L task is a task-specific loss (e.g. cross-entropy loss for classification and triplet loss for correspondence matching); L sa is a optional cross-entropy loss that encourages the network to learn the canonical axis from the candidate orientations when ground truth canonical pose is available for supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>The core element of our network is the SPConv block as shown in Fig. <ref type="figure" target="#fig_1">2(d)</ref>. It consists of one SE(3) point convolution and one SE(3) group convolution operator, with a batch normalization and a leaky ReLU activation inserted in between and after. We employ a 5-layer hierarchical convolutional network. Each layer contains two SPConv blocks, with the first one being strided by a factor of 2. The network outputs spatially pooled features that are equivariant to the rotation group G. It can be then pooled into an invariant feature through a GA pooling layer. For the classification network, the feature is fed into a fully connected layer and a softmax layer. For the task of metric learning, the feature is processed with an L2 normalization. We provide detailed network parameters and downsampling strategy in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We hypothesize that our approach is most suitable for tasks where the objects of interest are rotated arbitrarily. To this end, we evaluate our approach on two rotationrelated datasets: the rotated Modelnet40 dataset <ref type="bibr" target="#b49">[51]</ref> and the 3DMatch dataset <ref type="bibr" target="#b52">[54]</ref>. To ensure a fair comparison to previous works, in all experiments, we use the implementation provided by the authors or the reported statistics if no source code is available. We provide the training details of the experiments in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Rotated ModelNet40</head><p>Dataset. The official aligned Modelnet40 dataset provides a setting where canonical poses are known, and therefore it allows us to evaluate the effectiveness of pose supervision. We create the rotated ModelNet40 dataset based on the train/test split of the aligned ModelNet40 dataset <ref type="bibr" target="#b49">[51]</ref>. We mainly focus on a more challenging "rotated" setting where each object is randomly rotated. For each object, we randomly subsample 1,024 points from the surface of the 3D object and perform random rotation augmentation before feeding it into the network.  Pose Estimation. The pose estimation task predicts the rotation R ∈ SO(3) that aligns a rotated input shape to its canonical pose. To avoid ambiguities indued by rotationally symmetric objects, we only use the airplane category from the dataset. We train the network with N=1252 airplane point clouds and test it with N=101 held-out point cloud, each augmented with random rotations. The evaluation compares equivariant models with KPConv <ref type="bibr" target="#b41">[43]</ref>, a network that has similar kernel function to our implementation of point convolution, while not equivariant to 3D rotation. The equivariant models (Ours-N) are varied by the size of rotation group (N), similar to the setting in <ref type="bibr" target="#b13">[14]</ref>, and use the multitask detection loss described in Sec. 3.3. KPConv directly regresses the output rotation. Each model is trained for 80k iterations. The regressors in all models produce a rotation in the quaternion representation. We evaluate the performance by measuring angular errors between the predicted rotations and the ground-truth rotations. Tab. 1 shows the mean, median and max angular errors in each setting, and Fig. <ref type="figure" target="#fig_2">3</ref> plots the error percentile curves. As shown in the results, the equivariant networks significantly outperform the baseline network, with Ours-60 having the lowest errors. The equivariant networks also perform significantly more stable (max angular errors are kept within 9 degrees), while KPConv could produce unstable results for a certain inputs. This experiment showcases that a hierarchical rotation model can be much more effective in task that requires direct prediction of 3D rotation.</p><p>Classification and Retrieval. The classification and retrieval tasks on Modelnet40 follow evaluation metric from <ref type="bibr" target="#b49">[51]</ref>. In addition, our network is trained with GA pooling and pose supervision introduced in Sec. 3.3. In Tab. 2, we show the results comparing with the state-of-theart methods in the setting where models are both trained and tested with rotation augmentation. We categorize the base- Representation Methods Acc (%) Retrieval (mAP) 3D Surface RotationNet <ref type="bibr" target="#b25">[26]</ref> 80.0 74.2 Sph. CNN <ref type="bibr" target="#b11">[12]</ref> 86.9 -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud</head><p>QENet <ref type="bibr" target="#b53">[55]</ref> 74.4 -PointNet <ref type="bibr" target="#b35">[36]</ref> 83.6 -PointNet++ <ref type="bibr" target="#b37">[38]</ref> 85.0 70.3 DGCNN <ref type="bibr" target="#b35">[36]</ref> 81.1 -PointCNN <ref type="bibr" target="#b37">[38]</ref> 84.5 -KPConv <ref type="bibr" target="#b41">[43]</ref> 86.7 77.5 Ours 88.3 79.7</p><p>Table <ref type="table">2</ref>: Results on shape classification and retrieval on randomly rotated objects of ModelNet40. line approaches based on the input 3D representations: 3D surface and point cloud.</p><p>In the classification and retrieval task, our models also achieve the best performance, as shown in Tab. 2. This indicates that our proposed framework can learn more effective and discriminative features even in the challenging cases that all the objects are randomly rotated.</p><p>Ablation Analysis. We further conduct an ablation study to validate the effectiveness of each algorithmic component. In particular, we experiment with five variants of our model by altering key designs in our network under the same architecture as shown in Tab. 3. By using the supervised attentive pooling, we can improve the classification accuracy with the same number of parameters compared to the max and mean pooling. However, the unsupervised attentive pooling does not outperform max pooling. This may be partly due to the difficulty of learning canonical pose in an unsupervised manner. In addition, only using point convolution will lead to a decline in performance, indicating the effectiveness of group convolution.</p><p>How well does the attention layer learn? It is possible that the performance of GA pooling in distinguishing canonical poses could be compromised by the rotational symmetry of the object. If a shape is circularly symmetric, and the canonical poses prescribed by the rotational la-  bel is aligned with an axis of symmetry, the attention layer would naturally fail to provide a deterministic prediction. We summarize the classification accuracy based on the attention confidence for each category of ModelNet objects, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. The results indeed support our intuition: the attention layer is ambiguous on objects with circular symmetry (e.g. cone and flower pot) and very confident on categories that have distinctive canonical orientation. On one hand, this shows that when the object of interest is asymmetric in rotation, the GA pooling does help improve classification performance by establishing a local reference frame. On the other hand, the GA pooling only fails at symmetric object that benefits relatively less from a equivariant representation. In the extreme case, the attention layer could be reduced to an average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Shape Alignment on 3DMatch</head><p>Dataset. The 3DMatch dataset is a real-scan dataset consisting of 433 overlapping fragments from 8 indoor scenes for evaluation, and RGB-D data set from 62 indoor scenes for training. The pose of each fragment is determined by the camera angle during capturing, and two fragments at most overlap partially. Evaluating our model on this dataset is meaningful as shape registration in such setting would benefit from descriptors that are invariant to rigid camera motion. Each test fragment is a densely sampled point cloud with 150,000 to 600,000 points. To be consistent with our baselines, we use an evaluation metric based on the aver-  age recall of keypoints correspondence without performing RANSAC, following <ref type="bibr" target="#b9">[10]</ref>. We also follow previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> to set the matching threshold τ 1 = 0.1m and the inlier ratio τ 2 = 0.05.</p><p>Comparison with baselines. We designed a Siamese network for this task and trained our model with the batchhard triplet loss proposed in <ref type="bibr" target="#b16">[17]</ref>. The input to the network is 1024-point patches extracted locally from keypoints in a fragment. The output is 64-dim invariant descriptors. Since a canonical ground truth pose is not known in this setting, the attentive pooling module in our model is trained in an unsupervised manner. Our results are shown in Tab. 4. To provide a comprehensive comparison, we select the stateof-the-art baselines using a variety of approaches: 1) convolutional network without rotational invariance, e.g. <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b9">10]</ref> 2) handcrafted invariant features w/ and w/o deep learning, e.g. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b8">9]</ref>, 3) features learned from LRF aligned input <ref type="bibr" target="#b16">[17]</ref>, and 4) multi-view network <ref type="bibr" target="#b31">[32]</ref>. We report the 64-dim results of <ref type="bibr" target="#b16">[17]</ref> to match the feature dimension of our model. Since the official 3DMatch test dataset does not contain point normal information, we report two results of <ref type="bibr" target="#b31">[32]</ref>: a result of their model trained and tested without normal information <ref type="bibr">(Li [32]</ref> in Tab. 4) and one that is trained and tested with the authors' provided point normals <ref type="bibr">(Li [32]</ref> in Tab. 4). We evaluate our model with the interest points provided by the authors of the dataset, which is consistent with the reported results of our baselines. Overall, our model outperforms all of the baselines in average recall, without the need to precompute an invariant representation or a local reference frame. Compare to some baselines (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>) that requires dense point input, our model can learn discriminative features from very sparsely sampled sets of 1024 point. Our result is also better than the state-of-the-art method <ref type="bibr" target="#b31">[32]</ref>, even without needing normal information as input. In the official setting where point normal information is not available, the performance of our model marks a great leap forward.</p><p>Qualitative analysis. We provide a T-SNE visualization of the features learned by our network in Fig. <ref type="figure" target="#fig_4">5</ref>. As differ- ent features are labeled with distinct colors, we can observe that the features learned by our network can robustly generate correct geometry correspondences even when the point cloud is incomplete, partially aligned, or significantly rotated. For instance, in the third column, the bottom scene is only partially aligned with the top one and is viewed at an entirely different angle, our network can still reliably label the corresponded points with similar features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussions</head><p>We have presented a novel framework that efficiently computes and leverages SE(3)-equivariant features for point cloud analysis. First, we introduce a novel formulation named SE(3) separable convolution that factorizes the naive SE(3) convolution into two concatenated operators performed in two subspaces. Second, we propose the incorporation of attention mechanism that can appreciate and maintain the expressiveness of SE(3)-equivariant features, which provides a novel way for 3D alignment tasks and can be used as a pooling layer that fuses the equivariant features into their more ready-to-use invariant counterparts. Such paradigm has led to leaps of performance in a variety of challenging tasks. Our approach is one of the earliest attempts of investigating SE(3)-equivariant features for point cloud analysis. We believe there are still ample opportunities for more efficient methods and extension of the equivariant features to a broader range of applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Naïve SE(3) convolution (b) SE(3) point convolution (c) SE(3) group convolution g j SPConv block O(K p K g CN ) O(K p CN ) O(K g CN )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of SPConv. Each arrow represents an element in the group and each edge represents a correlation needed to compute in the convolution operator. We propose to use two separable convolutions (b)(c) to achieve SE(3) equivariance. The computational cost is much lower than the naive 6D convolution (a). (d) shows the structure of a basic SPConv block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Percentile of errors comparing KPConv [43] and two equivariant models (Ours-N) varied in number of SO(3) elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification accuracy based on the attention confidence for each object category. The attention layer is trained on rotated dataset to learn a canonical orientation for the given object.</figDesc><graphic url="image-6.png" coords="7,196.27,72.86,196.26,130.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: T-SNE visualization of features learned by our network. Each column contains a pair of fragments from the same scene. Regions in correspondence are automatically labeled with similar features.</figDesc><graphic url="image-29.png" coords="8,411.33,261.90,95.34,76.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Angular errors in point cloud pose estimation.</figDesc><table><row><cell></cell><cell>Mean ( • )</cell><cell>Median ( • )</cell><cell>Max( • )</cell></row><row><cell>KPConv [43]</cell><cell>11.46</cell><cell>8.06</cell><cell>82.32</cell></row><row><cell>Ours-20</cell><cell>1.36</cell><cell>1.16</cell><cell>8.30</cell></row><row><cell>Ours-60</cell><cell>1.25</cell><cell>1.11</cell><cell>6.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of ablation studies on ModelNet40 dataset. The conv column denotes the configuration of convolution layers. The global pool column denotes the type of global pooling method. Loss configuration follows notation from Sec. 3.3.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of average recall of keypoint correspondences on 3DMatch. Li<ref type="bibr" target="#b31">[32]</ref> denotes results tested with point normal information provided by the authors. All other results are tested on the official 3DMatch evaluation set without point normals.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was sponsored by the Army Research Office and was accomplished under Cooperative Agreement Number W911NF-20-2-0053, and sponsored by the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005, the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and in part by the ONR YIP grant N00014-17-S-FO14. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Why do deep convolutional networks generalize so poorly to small image transformations?</title>
		<author>
			<persName><forename type="first">Aharon</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12177</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point pair features based object detection and pose estimation revisited</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on 3D Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="527" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
	<note>Franc ¸ois Chollet</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spherical cnns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
				<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkay</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04615</idno>
		<title level="m">Gauge equivariant convolutional networks and the icosahedral cnn</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Steerable cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors</title>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="602" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ppfnet: Global context aware local features for robust 3d point matching</title>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d point cloud registration for localization using a deep neural network auto-encoder</title>
		<author>
			<persName><forename type="first">Gil</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamar</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01889</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Polar transformer networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Equivariant multi-view networks</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinshuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing objects in range data using regional point descriptors</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bülow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="224" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The perfect match: 3d point cloud matching with smoothed densities</title>
		<author>
			<persName><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5545" to="5554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flex-convolution</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><forename type="middle">Pa</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A comprehensive performance evaluation of 3d local feature descriptors. International Journal of Computer Vision</title>
		<author>
			<persName><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="66" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rotational projection statistics for 3d local surface description and object recognition. International journal of computer vision</title>
		<author>
			<persName><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="63" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning local shape descriptors from part correspondences with multiview convolutional networks</title>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Chiyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02039</idno>
		<title level="m">Philip Marcus, Matthias Niessner, et al. Spherical cnns on unstructured grids</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="433" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Group equivariant capsule networks</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Libuschewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8844" to="8853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Discrete rotation equivariance for point cloud recognition</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcai</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gim</forename><surname>Hee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00319</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point analysis</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gim</forename><surname>Hee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep rotation equivariant network</title>
		<author>
			<persName><forename type="first">Junying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end learning local multi-view descriptors for 3d point clouds</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1919">1919-1928, 2020. 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Ruizhongtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2017. 2009. 2009</date>
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
	<note>Fast point feature histograms (fpfh) for 3d registration</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Csaba Marton</surname></persName>
		</author>
		<author>
			<persName><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning an effective equivariant 3d descriptor without supervision</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6401" to="6410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Franc ¸ois Goulette</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08889</idno>
		<title level="m">Kpconv: Flexible and deformable convolution for point clouds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unique shape context for 3d data description</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM workshop on 3D object retrieval</title>
				<meeting>the ACM workshop on 3D object retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2010. 2, 8</date>
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10381" to="10392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant cnns</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cubenet: Equivariance to 3d rotation and translation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="567" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Daniel E Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Toldi: An effective and robust approach for 3d local shape description</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Yongheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12098</idno>
		<title level="m">Quaternion equivariant capsule networks for 3d point clouds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
