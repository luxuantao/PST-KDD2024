<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPT Understands, Too</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-18">18 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>yang@rcrai.com&gt;</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Recurrent AI, Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>&lt;jietang@tsinghua.edu.cn&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPT Understands, Too</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-18">18 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.10385v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuningwhich employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Language model pre-training has been a successful approach for many natural language processing tasks <ref type="bibr" target="#b1">(Brown et al., 2020)</ref>. Evidences suggest that during the pre-training, not only do language models learn contextualized text representations, but also grammar <ref type="bibr" target="#b32">(Vig, 2019;</ref><ref type="bibr" target="#b3">Clark et al., 2019b)</ref>, syntactic <ref type="bibr" target="#b11">(Hewitt &amp; Manning, 2019)</ref>, commonsense <ref type="bibr" target="#b6">(Davison et al., 2019)</ref> and even world knowledge <ref type="bibr" target="#b22">(Petroni et al., 2019;</ref><ref type="bibr" target="#b35">Wang et al., 2020)</ref>.</p><p>According to the training objectives, pre-trained language models can be divided into three categories: unidirectional language models (e.g., GPT <ref type="bibr" target="#b25">(Radford et al., 2019)</ref>) for natural language generation (NLG), bidirectional language models (e.g., BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>) for natural lan- guage understanding (NLU) and hybrid language models (e.g., XLNet <ref type="bibr" target="#b36">(Yang et al., 2019)</ref>, UniLM <ref type="bibr" target="#b9">(Dong et al., 2019)</ref>) for combining the first two paradigms. For long, researchers have observed that GPT-style models perform poorly for NLU tasks with fine-tuning, and thus assumed that they are not suitable for language understanding in nature.</p><p>The emerging GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref> and its particular performance on few-shot and zero-shot learning with handcrafted prompts has swept the machine learning community. Its success suggests that giant unidirectional language models together with appropriate manual prompt may work for natural language understanding. However, handcrafting a best-performing prompt is like finding a needle in a haystack, which often requires impractically large validation sets. In many cases, prompt engineering effectively means overfitting the test set. Besides, it is easy to create adversarial prompts that result in a substantial performance decrease. In light of these problems, recent works have focused on automatically searching discrete prompts <ref type="bibr" target="#b13">(Jiang et al., 2020b;</ref><ref type="bibr" target="#b31">Shin et al., 2020;</ref><ref type="bibr" target="#b28">Reynolds &amp; McDonell, 2021;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref> and demonstrated their effectiveness. However, since neural networks are inherently continuous, discrete prompts can be sub-optimal.</p><p>In this work, we propose a novel method-P-tuning-to automatically search prompts in the continuous space to bridge the gap between GPTs and NLU applications.<ref type="foot" target="#foot_0">1</ref> Ptuning leverages few continuous free parameters to serve as prompts fed as the input to the pre-trained language models. We then optimize the continuous prompts using gradient descent as an alternative to discrete prompt searching.</p><p>The simple P-tuning method brings substantial improvements to GPTs. We examine the P-tuning based GPTs on two NLU benchmarks: the LAMA <ref type="bibr" target="#b22">(Petroni et al., 2019)</ref> knowledge probing and SuperGLUE <ref type="bibr" target="#b34">(Wang et al., 2019b)</ref>. In LAMA knowledge probing where model parameters are fixed, compared to original handcraft prompts, GPTs based on P-tuning show absolute gains of 26.2%-41.1% in Precision@1. The best one achieves 64.2% in LAMA, which significantly surpasses the state-of-the-art 45.2% prompt searching approach. In another NLU benchmark, Super-Glue, we jointly apply the P-tuning and fine-tuning in both few-shot and fully supervised scenarios. As a result, GPTs present a competitive performance to BERT models with the same scales, and for some datasets, GPTs even outperform BERTs. Further experiments demonstrate that BERT-style models can also benefit from P-tuning to some extent. We show that ALBERT with P-tuning substantially outperforms previous approaches and achieves new state-of-the-art results on the few-shot SuperGLUE benchmark.</p><p>Our discovery breaks the stereotype that GPTs can only generate but do not understand. It also suggests that language models contain much more world knowledge and prior task knowledge than we previously assumed. P-tuning also serves as a general method to tune pre-trained language models for the best downstream task performance. To sum up, we make the following contributions:</p><p>• We show that GPTs can be as competitive as BERTs in natural language understanding (sometimes even better) with P-tuning, which can boost pre-trained language models' performance. This reveals that the potential of GPT-style architectures for natural language understanding has been under-estimated.</p><p>• We show that P-tuning is a general method to improve GPTs and BERTs in both few-shot and fullysupervised settings. Particularly, with P-tuning, our method outperforms state-of-the-art methods on LAMA knowledge probing and few-shot SuperGlue, which indicates that language models have grasped more world knowledge and prior-task knowledge during pre-training than we previously thought.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>The miracle of GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref> and DALL-E <ref type="bibr" target="#b27">(Ramesh et al., 2021)</ref> seem to suggest that giant models are always nothing short of a panacea for boosting machine intelligence. However, behind the prosperity, there are unignorable challenges.</p><p>A fatal one is that giant models suffer from poor transferability. Fine-tuning on downstream tasks hardly works for those trillion-scale models. Even for the many-shot finetuning setting, these models are still too large to memorize the fine-tuning samples <ref type="bibr" target="#b37">(Yue et al., 2020)</ref> quickly.</p><p>As a substitution, GPT-3 and DALL-E have been reported to leverage handcrafted prompts to steer the model for downstream applications. However, handcraft prompt searching heavily relies on impractically large validations sets, but its performance is also volatile. We show a similar case in LAMA <ref type="bibr" target="#b22">(Petroni et al., 2019)</ref> knowledge probing (Table <ref type="table">1</ref>), where a single-word's change can cause a drastic difference.</p><p>Prompt P@1 [X] is located in <ref type="bibr">[Y]</ref>. <ref type="bibr">(original)</ref> 31.29 [X] is located in which country or state? <ref type="bibr">[Y]</ref>. <ref type="bibr">19.78 [X]</ref> is located in which country? <ref type="bibr">[Y]</ref>.</p><p>31.40 [X] is located in which country? In <ref type="bibr">[Y]</ref>.</p><p>51.08</p><p>Table <ref type="table">1</ref>. Case study on LAMA-TREx P17 with bert-base-cased. A single-word change in prompts could yield a drastic difference.</p><p>In light of the challenge, while some recent works have concentrated on automating the search of discrete prompts by mining training corpus <ref type="bibr" target="#b13">(Jiang et al., 2020b)</ref>, gradient searching <ref type="bibr" target="#b31">(Shin et al., 2020</ref>) and using separate model <ref type="bibr" target="#b10">(Gao et al., 2020)</ref>, we delve into the problem of finding continuous prompts that can be differentially optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method: P-tuning</head><p>In this section, we present the implementation of P-tuning. Similar to discrete prompts, the P-tuning only applies noninvasive modification to the input. Nevertheless, the Ptuning replaces the input embeddings of pre-trained language models with its differential output embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>Given a pre-trained language model M, a sequence of discrete input tokens x 1:n = {x 0 , x 1 , ..., x n } will be mapped to input embeddings {e(x 0 ), e(x 1 ), ..., e(x n )} by the pretrained embedding layer e ∈ M. In a specific scenario, condition on the context x, we often use the output embeddings of a set of target tokens y for downstream processing. For instance, in the pre-training, x refers to the unmasked tokens while y refers to the <ref type="bibr">[MASK]</ref> ones; and in the sentence classification, x refers to the sentence tokens while y often refers to the <ref type="bibr">[CLS]</ref>.</p><p>The function of a prompt p is to organize context x, target y and itself into a template T . For example, in the task of predicting a country's capital (LAMA-TREx P36), a template may be "The capital of Britain is <ref type="bibr">[MASK]</ref>." (see Figure <ref type="figure" target="#fig_5">2</ref>), in which "The capital of ... is ... ." is prompt, "Britain" is the context and "[MASK]" is the target. Prompts can be so flexible that we may even insert them into the context or target.</p><p>Let V refers to the vocabulary of a language model M and [P i ] refers to the i th prompt token in a template T . For sim-Pre-trained Language Model (GPT, BERT, … Prompt Encoder</p><formula xml:id="formula_0">[P 0 ]</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 D X R f n p o 5 4 6 J x X E W z 0 B M I Z p w Z / g = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t O u 1 e k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D U j 6 U + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 D X R f n p o 5 4 6 J x X E W z 0 B M I Z p w Z / g = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9</p><formula xml:id="formula_1">C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t O u 1 e k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D U j 6 U + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 D X R f n p o 5 4 6 J x X E W z 0 B M I Z p w Z / g = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t O u 1 e k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D U j 6 U + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 D X R f n p o 5 4 6 J x X E W z 0 B M I Z p w Z / g = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t O u 1 e k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D U j 6 U + w = = &lt; / l a t e x i t &gt; [P i ] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 4 H d 8 4 / X Z 1 U c n o y 1 l r a c O F T z C n 0 = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t M u 7 3 S L J a t s 6 W X O A z s D J W S r G h V f 0 E Y P E T y k C M A Q Q h L 2 4 S C h p w U b F m L i O p g Q J w h x H W e Y o k D a l L I Y Z T j E j u g 7 o F 0 r Y 0 P a K 8 9 E q z 0 6 x a d X k N L E A W k i y h O E 1 W m m j q f a W b G / e U + 0 p 7 r b m P 5 u 5 h U Q K z E k 9 i / d L P O / O l W L R B + n u g Z O N c W a U d V 5 m U u q u 6 J u b n 6 p S p J D T J z C P Y o L w p 5 W z v p s a k 2 i a 1 e 9 d X T 8 T W c q V u 2 9 L D f F u 7 o l D d j + O c 5 5 U D 8 q 2 1 b Z v j w u V c 6</formula><p>y U e e x h 3 0 c 0 j x P U M E F q q i R t 8 A j n v B s X B l j 4 8 6 4 / 0 w 1 c p l m F 9 + W 8 f A B 2 d e V N A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 4 H</p><formula xml:id="formula_2">d 8 4 / X Z 1 U c n o y 1 l r a c O F T z C n 0 = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t M u 7 3 S L J a t s 6 W X O A z s D J W S r G h V f 0 E Y P E T y k C M A Q Q h L 2 4 S C h p w U b F m L i O p g Q J w h x H W e Y o k D a l L I Y Z T j E j u g 7 o F 0 r Y 0 P a K 8 9 E q z 0 6 x a d X k N L E A W k i y h O E 1 W m m j q f a W b G / e U + 0 p 7 r b m P 5 u 5 h U Q K z E k 9 i / d L P O / O l W L R B + n u g Z O N c W a U d V 5 m U u q u 6 J u b n 6 p S p J D T J z C P Y o L w p 5 W z v p s a k 2 i a 1 e 9 d X T 8 T W c q V u 2 9 L D f F u 7 o l D d j + O c 5 5 U D 8 q 2 1 b Z v j w u V c 6</formula><p>y U e e x h 3 0 c 0 j x P U M E F q q i R t 8 A j n v B s X B l j 4 8 6 4 / 0 w 1 c p l m F 9 + W 8 f A B 2 d e V N A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 4 H</p><formula xml:id="formula_3">d 8 4 / X Z 1 U c n o y 1 l r a c O F T z C n 0 = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t M u 7 3 S L J a t s 6 W X O A z s D J W S r G h V f 0 E Y P E T y k C M A Q Q h L 2 4 S C h p w U b F m L i O p g Q J w h x H W e Y o k D a l L I Y Z T j E j u g 7 o F 0 r Y 0 P a K 8 9 E q z 0 6 x a d X k N L E A W k i y h O E 1 W m m j q f a W b G / e U + 0 p 7 r b m P 5 u 5 h U Q K z E k 9 i / d L P O / O l W L R B + n u g Z O N c W a U d V 5 m U u q u 6 J u b n 6 p S p J D T J z C P Y o L w p 5 W z v p s a k 2 i a 1 e 9 d X T 8 T W c q V u 2 9 L D f F u 7 o l D d j + O c 5 5 U D 8 q 2 1 b Z v j w u V c 6</formula><p>y U e e x h 3 0 c 0 j x P U M E F q q i R t 8 A j n v B s X B l j 4 8 6 4 / 0 w 1 c p l m F 9 + W 8 f A B 2 d e V N A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 4 H</p><formula xml:id="formula_4">d 8 4 / X Z 1 U c n o y 1 l r a c O F T z C n 0 = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t M u 7 3 S L J a t s 6 W X O A z s D J W S r G h V f 0 E Y P E T y k C M A Q Q h L 2 4 S C h p w U b F m L i O p g Q J w h x H W e Y o k D a l L I Y Z T j E j u g 7 o F 0 r Y 0 P a K 8 9 E q z 0 6 x a d X k N L E A W k i y h O E 1 W m m j q f a W b G / e U + 0 p 7 r b m P 5 u 5 h U Q K z E k 9 i / d L P O / O l W L R B + n u g Z O N c W a U d V 5 m U u q u 6 J u b n 6 p S p J D T J z C P Y o L w p 5 W z v p s a k 2 i a 1 e 9 d X T 8 T W c q V u 2 9 L D f F u 7 o l D d j + O c 5 5 U D 8 q 2 1 b Z v j w u V c 6</formula><p>y U e e x h 3 0 c 0 j x P U M E F q q i R t 8 A j n v B s X B l j 4 8 6 4 / 0 w 1 c p l m F 9 + W 8 f A B 2 d e V N A = = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Pi+1]</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 4 H k T b S j 3</p><formula xml:id="formula_5">P F Q 1 G f A l S r l X j y h L + Y = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r 1 p f s S 7 d B I s g C C U R Q Z d F N y 4 r 2 I f U U p I 4 1 c G 8 m E z E U o o 7 c e s P u N U / E v 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + X B D y V t v 1 a M K a m Z 2 b n i v O l h c W l 5 R V z t d x M 4 0 z 4 r O H H Q S z a n p u y g E e s I b k M W D s R z A 2 9 g L W 8 q 0 M V b 1 0 z k f I 4 O p G D h H V D 9 y L i f e 6 7 k q i e W e 6 c h a 6 8 F O G w P u o N + b Y z 6 v b M i l 2 1 9 b I m g Z O D C v J V j 8 0 X n O E c M X x k C M E Q Q R I O 4 C K l p w M H N h L i u h g S J w h x H W c Y o U T a j L I Y Z b j E X t H 3 g n a d n I 1 o r z x T r f b p l I B e Q U o L m 6 S J K U 8 Q V q d Z O p 5 p Z 8 X + 5 j 3 U n u p u A / p 7 u V d I r M Q l s X / p x p n / 1 a l a J P r Y 1 z V w q i n R j K r O z 1 0 y 3 R V 1 c + t L V Z I c E u I U P q e 4 I O x r 5 b j P l t a k u n b V W 1 f H 3 3 S m Y t X e z 3 M z v K t b 0 o C d n + O c B M 2 d q m N X n e P d S u 0 g H 3 U R 6 9 j A F s 1 z D z U c o Y 4 G e d / g E U 9 4 N k 6 N W + P O u P 9 M N Q q 5 Z g 3 f l v H w A U R q l u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 4 H k T b S j 3 P F Q 1 G f A l S r l X j y h L + Y = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r 1 p f s S 7 d B I s g C C U R Q Z d F N y 4 r 2 I f U U p I 4 1 c G 8 m E z E U o o 7 c e s P u N U / E v 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + X B D y V t v 1 a M K a m Z 2 b n i v O l h c W l 5 R V z t d x M 4 0 z 4 r O H H Q S z a n p u y g E e s I b k M W D s R z A 2 9 g L W 8 q 0 M V b 1 0 z k f I 4 O p G D h H V D 9 y L i f e 6 7 k q i e W e 6 c h a 6 8 F O G w P u o N + b Y z 6 v b M i l 2 1 9 b I m g Z O D C v J V j 8 0 X n O E c M X x k C M E Q Q R I O 4 C K l p w M H N h L i u h g S J w h x H W c Y o U T a j L I Y Z b j E X t H 3 g n a d n I 1 o r z x T r f b p l I B e Q U o L m 6 S J K U 8 Q V q d Z O p 5 p Z 8 X + 5 j 3 U n u p u A / p 7 u V d I r M Q l s X / p x p n / 1 a l a J P r Y 1 z V w q i n R j K r O z 1 0 y 3 R V 1 c + t L V Z I c E u I U P q e 4 I O x r 5 b j P l t a k u n b V W 1 f H 3 3 S m Y t X e z 3 M z v K t b 0 o C d n + O c B M 2 d q m N X n e P d S u 0 g H 3 U R 6 9 j A F s 1 z D z U c o Y 4 G e d / g E U 9 4 N k 6 N W + P O u P 9 M N Q q 5 Z g 3 f l v H w A U R q l u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 4 H k T b S j 3 P F Q 1 G f A l S r l X j y h L + Y = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r 1 p f s S 7 d B I s g C C U R Q Z d F N y 4 r 2 I f U U p I 4 1 c G 8 m E z E U o o 7 c e s P u N U / E v 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + X B D y V t v 1 a M K a m Z 2 b n i v O l h c W l 5 R V z t d x M 4 0 z 4 r O H H Q S z a n p u y g E e s I b k M W D s R z A 2 9 g L W 8 q 0 M V b 1 0 z k f I 4 O p G D h H V D 9 y L i f e 6 7 k q i e W e 6 c h a 6 8 F O G w P u o N + b Y z 6 v b M i l 2 1 9 b I m g Z O D C v J V j 8 0 X n O E c M X x k C M E Q Q R I O 4 C K l p w M H N h L i u h g S J w h x H W c Y o U T a j L I Y Z b j E X t H 3 g n a d n I 1 o r z x T r f b p l I B e Q U o L m 6 S J K U 8 Q V q d Z O p 5 p Z 8 X + 5 j 3 U n u p u A / p 7 u V d I r M Q l s X / p x p n / 1 a l a J P r Y 1 z V w q i n R j K r O z 1 0 y 3 R V 1 c + t L V Z I c E u I U P q e 4 I O x r 5 b j P l t a k u n b V W 1 f H 3 3 S m Y t X e z 3 M z v K t b 0 o C d n + O c B M 2 d q m N X n e P d S u 0 g H 3 U R 6 9 j A F s 1 z D z U c o Y 4 G e d / g E U 9 4 N k 6 N W + P O u P 9 M N Q q 5 Z g 3 f l v H w A U R q l u E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 4 H k T b S j 3 P F Q 1 G f A l S r l X j y h L + Y = " &gt; A A A C 1 3 i c j V H L S s N A F D 2 N r 1 p f s S 7 d B I s g C C U R Q Z d F N y 4 r 2 I f U U p I 4 1 c G 8 m E z E U o o 7 c e s P u N U / E v 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 9 5 y Z e 6 + X B D y V t v 1 a M K a m Z 2 b n i v O l h c W l 5 R V z t d x M 4 0 z 4 r O H H Q S z a n p u y g E e s I b k M W D s R z A 2 9 g L W 8 q 0 M V b 1 0 z k f I 4 O p G D h H V D 9 y L i f e 6 7 k q i e W e 6 c h a 6 8 F O G w P u o N + b Y z 6 v b M i l 2 1 9 b I m g Z O D C v J V j 8 0 X n O E c M X x k C M E Q Q R I O 4 C K l p w M H N h L i u h g S J w h x H W c Y o U T a j L I Y Z b j E X t H 3 g n a d n I 1 o r z x T r f b p l I B e Q U o L m 6 S J K U 8 Q V q d Z O p 5 p Z 8 X + 5 j 3 U n u p u A / p 7 u V d I r M Q l s X / p x p n / 1 a l a J P r Y 1 z V w q i n R j K r O z 1 0 y 3 R V 1 c + t L V Z I c E u I U P q e 4 I O x r 5 b j P l t a k u n b V W 1 f H 3 3 S m Y t X e z 3 M z v K t b 0 o C d n + O c B M 2 d q m N X n e P d S u 0 g H 3 U R 6 9 j A F s 1 z D z U c o Y 4 G e d / g E U 9 4 N k 6 N W + P O u P 9 M N Q q 5 Z g 3 f l v H w A U R q l u E = &lt; / l a t e x i t &gt; [P m ]</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j</p><formula xml:id="formula_6">i K p T O v d f 5 v 8 j 2 2 g L q r 1 m b M F + o = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t N u 0 O k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e</formula><p>x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D 4 1 u V O A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j</p><formula xml:id="formula_7">i K p T O v d f 5 v 8 j 2 2 g L q r 1 m b M F + o = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t N u 0 O k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e</formula><p>x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D 4 1 u V O A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j</p><formula xml:id="formula_8">i K p T O v d f 5 v 8 j 2 2 g L q r 1 m b M F + o = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t N u 0 O k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e</formula><p>x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D 4 1 u V O A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 </p><formula xml:id="formula_9">i K p T O v d f 5 v 8 j 2 2 g L q r 1 m b M F + o = " &gt; A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 5 a 7 c C R Q x F M q t N u 0 O k W S 1 b Z 0 s u c B 3 Y G S s h W N S q + o I 0 e I n h I E Y A h h C T s w 0 F C T w s 2 L M T E d T A h T h D i O s 4 w R Y G 0 K W U x y n C I H d F 3 Q L t W x o a 0 V 5 6 J V n t 0 i k + v I K W J A 9 J E l C c I q 9 N M H U + 1 s 2 J / 8 5 5 o T 3 W 3 M f 3 d z C s g V m J I 7 F + 6 W e Z / d a o W i T 5 O d Q 2 c a o o 1 o 6 r z M p d U d 0 X d 3 P x S l S S H m D i F e x Q X h D 2 t n P X Z 1 J p E 1 6 5 6 6 + j 4 m 8 5 U r N p 7 W W 6 K d 3 V L G r D 9 c 5 z z o H 5 U t q 2 y f X l c q p x l o 8 5 j D / s 4 p H m e o I I L V F E j b 4 F H P O H Z u D L G x p 1 x / 5 l q 5 D L N L r 4 t 4 + E D 4 1 u V O A = = &lt; / l a t e x i t &gt; … … h 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X I h p n 8 O x N U v h Z R M 6 e 2 E n U b W V 7 t U = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 H P 6 Z U r T t X R y 5 4 H r g E V m N V I y i + 4 Q R 8 J A u S I w B B D E g 7 h I a O n A x c O U u K 6 m B I n C H E d Z 7 h H i b Q 5 Z T H K 8 I g d 0 3 d I u 4 5 h Y 9 o r z 0 y r A z o l p F e Q 0 s Y B a R L K E 4 T V a b a O 5 9 p Z s b 9 5 T 7 W n u t u E / r 7 x i o i V G B H 7 l 2 6 W + V + d q k V i g F N d A 6 e a U s 2 o 6 g L j k u u u q J v b X 6 q S 5 J A S p 3 C f 4 o J w o J W z P t t a k + n a V W 8 9 H X / T m Y p V + 8 D k 5 n h X t 6 Q B u z / H O Q 9 a R 1 X X q b o X x 5 X a m R l 1 E X v Y x y H N 8 w Q 1 1 N F A k 7 y H e M Q T n q 2 6 F V u 5 d f e Z a h W M Z h f f l v X w A e V 8 k B A = &lt; / l a t e x i t &gt; h i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n W n m f x j 2 c + W F Y 4 V a m C 6 w C 0 3 + U x 4 = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l C S d t k P z Y j J R S h H 8 A b f 6 a e I f 6 F 9 4 Z 5 y C W k Q n J D l z 7 j 1 n 5 t 7 r p y H P p O O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T y p J c B K w Z J G E i r n 0 v Y y G P W V N y G b L r V D A v 8 k P W 9 s f n K t 6 + Z S L j S X w l J y n r R t 4 w 5 g M e e J K o y 1 G P 9 8 o V p + r o Z c 8 D 1 4 A K z G o k 5 R f c o I 8 E A X J E Y I g h C Y f w k N H T g Q s H K X F d T I k T h L i O M 9 y j R N q c s h h l e M S O 6 T u k X c e w M e 2 V Z 6 b V A Z 0 S 0 i t I a e O A N A n l C c L q N F v H c + 2 s 2 N + 8 p 9 p T 3 W 1 C f 9 9 4 R c R K j I j 9 S z f L / K 9 O 1 S I x w K m u g V N N q W Z U d Y F x y X V X 1 M 3 t L 1 V J c k i J U 7 h P c U E 4 0 M p Z n 2 2 t y X T t q r e e j r / p T M W q f W B y c 7 y r W 9 K A 3 Z / j n A e t o 6 r r V N 2 L 4 0 r t z I y 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q 3 c u v t M t Q p G s 4 t v y 3 r 4 A G z r k E k = &lt; / l a t e x i t &gt; … h i+1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v 7 s H j M 7 5 Z j q 7 Y 0 1 g 8 5 o p t k E E A s = " &gt; A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k U Q h J K I o M u i G x c u K t g H 1 F K S 6 b Q d T J M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s B Y W l 5 Z X i q u l t f W N z a 3 y 9 k 4 z i V L J e I N F Q S T b v p f w Q I S 8 o Y Q K e D u W 3 B v 7 A W / 5 d x c 6 3 r r n M h F R e K M m M e + O v W E o B o J 5 i q j W q J e J I 3 f a K 1 e c q m O W P Q / c H F S Q r 3 p U f s E t + o j A k G I M j h C K c A A P C T 0 d u H A Q E 9 d F R p w k J E y c Y 4 o S e V N S c V J 4 x N 7 R d 0 i 7 T s 6 G t N c 5 E + N m d E p A r y S n j Q P y R K S T h P V p t o m n J r N m f 8 u d m Z z 6 b h P 6 + 3 m u M b E K I 2 L / 8 s 2 U / / X p W h Q G O D M 1 C K o p N o y u j u V Z U t M V f X P 7 S 1 W K M s T E a d y n u C T M j H P W Z 9 t 4 E l O 7 7 q 1 n 4 m 9 G q V m 9 Z 7 k 2 x b u + J Q 3 Y / T n O e d A 8 r r p O 1 b 0 + q d T O 8 1 E X s Y d 9 H N I 8 T 1 H D J e p o m C o f 8 Y R n 6 8 q S 1 s T K P q V W I f f s 4 t u y H j 4 A M o 2 R x Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v 7 s H j M 7 5 Z j q 7 Y 0 1 g 8 5 o p t k E E A s = " &gt; A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k U Q h J K I o M u i G x c u K t g H 1 F K S 6 b Q d T J M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s B Y W l 5 Z X i q u l t f W N z a 3 y 9 k 4 z i V L J e I N F Q S T b v p f w Q I S 8 o Y Q K e D u W 3 B v 7 A W / 5 d x c 6 3 r r n M h F R e K M m M e + O v W E o B o J 5 i q j W q J e J I 3 f a K 1 e c q m O W P Q / c H F S Q r 3 p U f s E t + o j A k G I M j h C K c A A P C T 0 d u H A Q E 9 d F R p w k J E y c Y 4 o S e V N S c V J 4 x N 7 R d 0 i 7 T s 6 G t N c 5 E + N m d E p A r y S n j Q P y R K S T h P V p t o m n J r N m f 8 u d m Z z 6 b h P 6 + 3 m u M b E K I 2 L / 8 s 2 U / / X p W h Q G O D M 1 C K o p N o y u j u V Z U t M V f X P 7 S 1 W K M s T E a d y n u C T M j H P W Z 9 t 4 E l O 7 7 q 1 n 4 m 9 G q V m 9 Z 7 k 2 x b u + J Q 3 Y / T n O e d A 8 r r p O 1 b 0 + q d T O 8 1 E X s Y d 9 H N I 8 T 1 H D J e p o m C o f 8 Y R n 6 8 q S 1 s T K P q V W I f f s 4 t u y H j 4 A M o 2 R x Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v 7 s H j M 7 5 Z j q 7 Y 0 1 g 8 5 o p t k E E A s = " &gt; A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k U Q h J K I o M u i G x c u K t g H 1 F K S 6 b Q d T J M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s B Y W l 5 Z X i q u l t f W N z a 3 y 9 k 4 z i V L J e I N F Q S T b v p f w Q I S 8 o Y Q K e D u W 3 B v 7 A W / 5 d x c 6 3 r r n M h F R e K M m M e + O v W E o B o J 5 i q j W q J e J I 3 f a K 1 e c q m O W P Q / c H F S Q r 3 p U f s E t + o j A k G I M j h C K c A A P C T 0 d u H A Q E 9 d F R p w k J E y c Y 4 o S e V N S c V J 4 x N 7 R d 0 i 7 T s 6 G t N c 5 E + N m d E p A r y S n j Q P y R K S T h P V p t o m n J r N m f 8 u d m Z z 6 b h P 6 + 3 m u M b E K I 2 L / 8 s 2 U / / X p W h Q G O D M 1 C K o p N o y u j u V Z U t M V f X P 7 S 1 W K M s T E a d y n u C T M j H P W Z 9 t 4 E l O 7 7 q 1 n 4 m 9 G q V m 9 Z 7 k 2 x b u + J Q 3 Y / T n O e d A 8 r r p O 1 b 0 + q d T O 8 1 E X s Y d 9 H N I 8 T 1 H D J e p o m C o f 8 Y R n 6 8 q S 1 s T K P q V W I f f s 4 t u y H j 4 A M o 2 R x Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u v 7 s H j M 7 5 Z j q 7 Y 0 1 g 8 5 o p t k E E A s = " &gt; A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k U Q h J K I o M u i G x c u K t g H 1 F K S 6 b Q d T J M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s B Y W l 5 Z X i q u l t f W N z a 3 y 9 k 4 z i V L J e I N F Q S T b v p f w Q I S 8 o Y Q K e D u W 3 B v 7 A W / 5 d x c 6 3 r r n M h F R e K M m M e + O v W E o B o J 5 i q j W q J e J I 3 f a K 1 e c q m O W P Q / c H F S Q r 3 p U f s E t + o j A k G I M j h C K c A A P C T 0 d u H A Q E 9 d F R p w k J E y c Y 4 o S e V N S c V J 4 x N 7 R d 0 i 7 T s 6 G t N c 5 E + N m d E p A r y S n j Q P y R K S T h P V p t o m n J r N m f 8 u d m Z z 6 b h P 6 + 3 m u M b E K I 2 L / 8 s 2 U / / X p W h Q G O D M 1 C K o p N o y u j u V Z U t M V f X P 7 S 1 W K M s T E a d y n u C T M j H P W Z 9 t 4 E l O 7 7 q 1 n 4 m 9 G q V m 9 Z 7 k 2 x b u + J Q 3 Y / T n O e d A 8 r r p O 1 b 0 + q d T O 8 1 E X s Y d 9 H N I 8 T 1 H D J e p o m C o f 8 Y R n 6 8 q S 1 s T K P q V W I f f s 4 t u y H j 4 A M o 2 R x Q = = &lt; / l a t e x i t &gt; h m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M V 7 v t 7 1 u o 3 H w F t G I d D y h A d p O c g w = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l G Q 6 b Y f m x W S i l C L 4 A 2 7 1 0 8 Q / 0 L / w z p i C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 F I l e O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T S u N M M t 5 k c R D L a 9 9 L e S A i 3 l R C B f w 6 k d w L / Y C 3 / f G 5 j r d v u U x F H F 2 p S c K 7 o T e M x E A w T x F 1 O e q F v X L F q T p m 2 f P A z U E F + W r E 5 R f c o I 8 Y D B l C c E R Q h A N 4 S O n p w I W D h L g u p s R J Q s L E O e 5 R I m 1 G W Z w y P G L H 9 B 3 S r p O z E e 2 1 Z 2 r U j E 4 J 6 J W k t H F A m p j y J G F 9 m m 3 i m X H W 7 G / e U + O p 7 z a h v 5 9 7 h c Q q j I j 9 S z f L / K 9 O 1 6 I w w K m p Q V B N i W F 0 d S x 3 y U x X 9 M 3 t L 1 U p c k i I 0 7 h P c U m Y G e W s z 7 b R p K Z 2 3 V v P x N 9 M p m b 1 n u W 5 G d 7 1 L W n A 7 s 9 x z o P W U d V 1 q u 7 F c a V 2 l o + 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q z M u v t M t Q q 5 Z h f f l v X w A X Z r k E 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M V 7 v t 7 1 u o 3 H w F t G I d D y h A d p O c g w = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l G Q 6 b Y f m x W S i l C L 4 A 2 7 1 0 8 Q / 0 L / w z p i C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 F I l e O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T S u N M M t 5 k c R D L a 9 9 L e S A i 3 l R C B f w 6 k d w L / Y C 3 / f G 5 j r d v u U x F H F 2 p S c K 7 o T e M x E A w T x F 1 O e q F v X L F q T p m 2 f P A z U E F + W r E 5 R f c o I 8 Y D B l C c E R Q h A N 4 S O n p w I W D h L g u p s R J Q s L E O e 5 R I m 1 G W Z w y P G L H 9 B 3 S r p O z E e 2 1 Z 2 r U j E 4 J 6 J W k t H F A m p j y J G F 9 m m 3 i m X H W 7 G / e U + O p 7 z a h v 5 9 7 h c Q q j I j 9 S z f L / K 9 O 1 6 I w w K m p Q V B N i W F 0 d S x 3 y U x X 9 M 3 t L 1 U p c k i I 0 7 h P c U m Y G e W s z 7 b R p K Z 2 3 V v P x N 9 M p m b 1 n u W 5 G d 7 1 L W n A 7 s 9 x z o P W U d V 1 q u 7 F c a V 2 l o + 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q z M u v t M t Q q 5 Z h f f l v X w A X Z r k E 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M V 7 v t 7 1 u o 3 H w F t G I d D y h A d p O c g w = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l G Q 6 b Y f m x W S i l C L 4 A 2 7 1 0 8 Q / 0 L / w z p i C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 F I l e O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T S u N M M t 5 k c R D L a 9 9 L e S A i 3 l R C B f w 6 k d w L / Y C 3 / f G 5 j r d v u U x F H F 2 p S c K 7 o T e M x E A w T x F 1 O e q F v X L F q T p m 2 f P A z U E F + W r E 5 R f c o I 8 Y D B l C c E R Q h A N 4 S O n p w I W D h L g u p s R J Q s L E O e 5 R I m 1 G W Z w y P G L H 9 B 3 S r p O z E e 2 1 Z 2 r U j E 4 J 6 J W k t H F A m p j y J G F 9 m m 3 i m X H W 7 G / e U + O p 7 z a h v 5 9 7 h c Q q j I j 9 S z f L / K 9 O 1 6 I w w K m p Q V B N i W F 0 d S x 3 y U x X 9 M 3 t L 1 U p c k i I 0 7 h P c U m Y G e W s z 7 b R p K Z 2 3 V v P x N 9 M p m b 1 n u W 5 G d 7 1 L W n A 7 s 9 x z o P W U d V 1 q u 7 F c a V 2 l o + 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q z M u v t M t Q q 5 Z h f f l v X w A X Z r k E 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M V 7 v t 7 1 u o 3 H w F t G I d D y h A d p O c g w = " &gt; A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F 2 w q 1 l G Q 6 b Y f m x W S i l C L 4 A 2 7 1 0 8 Q / 0 L / w z p i C W k Q n J D l z 7 j 1 n 5 t 7 r J 4 F I l e O 8 F q y F x a X l l e J q a W 1 9 Y 3 O r v L 3 T S u N M M t 5 k c R D L a 9 9 L e S A i 3 l R C B f w 6 k d w L / Y C 3 / f G 5 j r d v u U x F H F 2 p S c K 7 o T e M x E A w T x F 1 O e q F v X L F q T p m 2 f P A z U E F + W r E 5 R f c o I 8 Y D B l C c E R Q h A N 4 S O n p w I W D h L g u p s R J Q s L E O e 5 R I m 1 G W Z w y P G L H 9 B 3 S r p O z E e 2 1 Z 2 r U j E 4 J 6 J W k t H F A m p j y J G F 9 m m 3 i m X H W 7 G / e U + O p 7 z a h v 5 9 7 h c Q q j I j 9 S z f L / K 9 O 1 6 I w w K m p Q V B N i W F 0 d S x 3 y U x X 9 M 3 t L 1 U p c k i I 0 7 h P c U m Y G e W s z 7 b R p K Z 2 3 V v P x N 9 M p m b 1 n u W 5 G d 7 1 L W n A 7 s 9 x z o P W U d V 1 q u 7 F c a V 2 l o + 6 i D 3 s 4 5 D m e Y I a 6 m i g S d 5 D P O I J z 1 b d i q z M u v t M t Q q 5 Z h f f l v X w A X Z r k E 0 = &lt; / l a t</formula><formula xml:id="formula_10">g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D</formula><formula xml:id="formula_11">E c N W k 2 x x h Y 8 = " &gt; A A A C 2 3 i c j V H L S s N A F D 2 N r 1 p f U c G N m 2 A R 6 q Y k I u i y 6 M Z l h b 6 g L S V</formula><p>J p 2 0 w L 5 K J W G J X 7 s S t P + B W / 0 f 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u g 6 M d f 1 1 5 w y N 7 + w u J R f L q y s r q 1 v q J t b j T h</p><formula xml:id="formula_12">I I p v V 7 c A N o p Z l x s x 1 f F b n D n d Z K 4 y Y 6 V k u a 1 q X Z y L e v G J R 7 A R + j Y 9 D 1 v X M o e 8 M H N v k R P X U n Y 5 n 8 p E 1 S N m k 1 O H s m q e 1 E Z s c 9 N S i X t b l 0 m a B k Y E i s l U N 1 B d 0 0 E c A G w k 8 M P j g h F 2 Y i O l p w 4 C O k L g u U u I i Q o 6 M M 0 x Q I G 1 C W Y w y T G I v 6 T u k X T t j f d o L z 1 i q b T r F p T c i p Y Z 9 0 g S U F x E W p 2 k y n k h n w f 7 m n U p P c b c x / a 3 M y y O W Y 0 T s X 7 p p 5 n 9 1 o h a O A U 5 k D Q 7 V F E p G V G d n L o n s i r i 5 9 q U q T g 4 h c Q L 3 K R 4 R t q V y 2 m d N a m J Z u + i t K e N v M l O w Y m 9 n u Q n e x S 1 p w M b P c c 6 C x m H Z 0 M v G x V G x c p q N O o 9 d 7 K F E 8 z x G B e e o o k 7 e N 3 j E E 5 6 V r n K r 3 C n 3 n 6 l K L t N s 4 9 t S H j 4 A C k C Y v g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X / Y I D I x g / H s e 8 W N 1 E c N W k 2 x x h Y 8 = " &gt; A A A C 2 3 i c j V H L S s N A F D 2 N r 1 p f U c G N m 2 A R 6 q Y k I u i y 6 M Z l h b 6 g L S V</formula><p>J p 2 0 w L 5 K J W G J X 7 s S t P + B W / 0 f 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u g 6 M d f 1 1 5 w y N 7 + w u J R f L q y s r q 1 v q J t b j T h</p><formula xml:id="formula_13">I I p v V 7 c A N o p Z l x s x 1 f F b n D n d Z K 4 y Y 6 V k u a 1 q X Z y L e v G J R 7 A R + j Y 9 D 1 v X M o e 8 M H N v k R P X U n Y 5 n 8 p E 1 S N m k 1 O H s m q e 1 E Z s c 9 N S i X t b l 0 m a B k Y E i s l U N 1 B d 0 0 E c A G w k 8 M P j g h F 2 Y i O l p w 4 C O k L g u U u I i Q o 6 M M 0 x Q I G 1 C W Y w y T G I v 6 T u k X T t j f d o L z 1 i q b T r F p T c i p Y Z 9 0 g S U F x E W p 2 k y n k h n w f 7 m n U p P c b c x / a 3 M y y O W Y 0 T s X 7 p p 5 n 9 1 o h a O A U 5 k D Q 7 V F E p G V G d n L o n s i r i 5 9 q U q T g 4 h c Q L 3 K R 4 R t q V y 2 m d N a m J Z u + i t K e N v M l O w Y m 9 n u Q n e x S 1 p w M b P c c 6 C x m H Z 0 M v G x V G x c p q N O o 9 d 7 K F E 8 z x G B e e o o k 7 e N 3 j E E 5 6 V r n K r 3 C n 3 n 6 l K L t N s 4 9 t S H j 4 A C k C Y v g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X / Y I D I x g / H s e 8 W N 1 E c N W k 2 x x h Y 8 = " &gt; A A A C 2 3 i c j V H L S s N A F D 2 N r 1 p f U c G N m 2 A R 6 q Y k I u i y 6 M Z l h b 6 g L S V</formula><p>J p 2 0 w L 5 K J W G J X 7 s S t P + B W / 0 f 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u g 6 M d f 1 1 5 w y N 7 + w u J R f L q y s r q 1 v q J t b j T h</p><formula xml:id="formula_14">I I p v V 7 c A N o p Z l x s x 1 f F b n D n d Z K 4 y Y 6 V k u a 1 q X Z y L e v G J R 7 A R + j Y 9 D 1 v X M o e 8 M H N v k R P X U n Y 5 n 8 p E 1 S N m k 1 O H s m q e 1 E Z s c 9 N S i X t b l 0 m a B k Y E i s l U N 1 B d 0 0 E c A G w k 8 M P j g h F 2 Y i O l p w 4 C O k L g u U u I i Q o 6 M M 0 x Q I G 1 C W Y w y T G I v 6 T u k X T t j f d o L z 1 i q b T r F p T c i p Y Z 9 0 g S U F x E W p 2 k y n k h n w f 7 m n U p P c b c x / a 3 M y y O W Y 0 T s X 7 p p 5 n 9 1 o h a O A U 5 k D Q 7 V F E p G V G d n L o n s i r i 5 9 q U q T g 4 h c Q L 3 K R 4 R t q V y 2 m d N a m J Z u + i t K e N v M l O w Y m 9 n u Q n e x S 1 p w M b P c c 6 C x m H Z 0 M v G x V G x c p q N O o 9 d 7 K F E 8 z x G B e e o o k 7 e N 3 j E E 5 6 V r n K r 3 C n 3 n 6 l K L t N s 4 9 t S H j 4 A C k C Y v g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X / Y I D I x g / H s e 8 W N 1 E c N W k 2 x x h Y 8 = " &gt; A A A C 2 3 i c j V H L S s N A F D 2 N r 1 p f U c G N m 2 A R 6 q Y k I u i y 6 M Z l h b 6 g L S V</formula><p>J p 2 0 w L 5 K J W G J X 7 s S t P + B W / 0 f 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u g 6 M d f 1 1 5 w y N 7 + w u J R f L q y s r q 1 v q J t b j T h</p><formula xml:id="formula_15">I I p v V 7 c A N o p Z l x s x 1 f F b n D n d Z K 4 y Y 6 V k u a 1 q X Z y L e v G J R 7 A R + j Y 9 D 1 v X M o e 8 M H N v k R P X U n Y 5 n 8 p E 1 S N m k 1 O H s m q e 1 E Z s c 9 N S i X t b l 0 m a B k Y E i s l U N 1 B d 0 0 E c A G w k 8 M P j g h F 2 Y i O l p w 4 C O k L g u U u I i Q o 6 M M 0 x Q I G 1 C W Y w y T G I v 6 T u k X T t j f d o L z 1 i q b T r F p T c i p Y Z 9 0 g S U F x E W p 2 k y n k h n w f 7 m n U p P c b c x / a 3 M y y O W Y 0 T s X 7 p p 5 n 9 1 o h a O A U 5 k D Q 7 V F E p G V G d n L o n s i r i 5 9 q U q T g 4 h c Q L 3 K R 4 R t q V y 2 m d N a m J Z u + i t K e N v M l O w Y m 9 n u Q n e x S 1 p w M b P c c 6 C x m H Z 0 M v G x V G x c p q N O o 9 d 7 K F E 8 z x G B e e o o k 7 e N 3 j E E 5 6 V r n K r 3 C n 3 n 6 l K L t N s 4 9 t S H j 4 A C k C Y v g = = &lt; / l a t e x i t &gt; e(capital)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o</p><formula xml:id="formula_16">N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g</p><formula xml:id="formula_17">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g 5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h g t i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u + J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g</p><formula xml:id="formula_18">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g</formula><p>5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h g t i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u</p><formula xml:id="formula_19">+ J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g</p><formula xml:id="formula_20">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g</formula><p>5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h g t i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u</p><formula xml:id="formula_21">+ J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g</p><formula xml:id="formula_22">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g</formula><p>5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h g t i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u</p><formula xml:id="formula_23">+ J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M = &lt; / l a t e x i t &gt; e(of)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f l e b G 6 e 5 L B 7 r i m f i p Q 3 v J z I F X B g = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g K S V J J 2 0 w L 5 K J W E I 3 7 s S t P + B W P 0 j 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u S 5 C d e 0 1 4 I y N 7 + w u F R c L q 2 s r q 1 v l D e 3 W k m Y x j Z r 2 q E X x h 3 L T J j n B q z J X e 6 x T h Q z 0 7 c 8 1 r a u z k S 8 f c 3 i x A 2 D S z 6 O W M 8 3 h 4 H r u L b J i e q X d w z f 5 C P L y d i k a n B 2 w 7 P Q m R z 0 y x W t p s m l z g</p><formula xml:id="formula_24">I 9 B x X k q x G W X 2 B g g B A 2 U v h g C M A J e z C R 0 N O F D g 0 R c T 1 k x M W E X B l n m K B E 2 p S y G G W Y x F 7 R d 0 i 7 b s 4 G t B e e i V T b d I p H b 0 x K F f u k C S k v J i x O U 2 U 8 l c 6 C / c 0 7 k 5 7 i b m P 6 W 7 m X T y z H i N i / d N P M / + p E L R w O T m Q N L t U U S U Z U Z + c u q e y K u L n 6 p S p O D h F x A g 8 o H h O 2 p X L a Z 1 V q E l m 7 6 K 0 p 4 2 8 y U 7 B i b + e 5 K d 7 F L W n A + s 9 x z o L W Y U 3 X a v r F U a V + m o + 6 i F 3 s o U r z P E Y d 5 2 i g S d 4 Z H v G E Z 8 V Q b p U 7 5 f 4 z V S n k m m 1 8 W 8 r D</formula><p>B y e C m G g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f l e b G 6 e 5 L B 7 r i m f i p Q 3 v J z I F X B g = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g K S V J J 2 0 w L 5 K J W E I 3 7 s S t P + B W P 0 j 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u S 5 C d e 0 1 4 I y N 7 + w u F R c L q 2 s r q 1 v l D e 3 W k m Y x j Z r 2 q E X x h 3 L T J j n B q z J X e 6 x T h Q z 0 7 c 8 1 r a u z k S 8 f c 3 i x A 2 D S z 6 O W M 8 3 h 4 H r u L b J i e q X d w z f 5 C P L y d i k a n B 2 w 7 P Q m R z 0 y x W t p s m l z g</p><formula xml:id="formula_25">I 9 B x X k q x G W X 2 B g g B A 2 U v h g C M A J e z C R 0 N O F D g 0 R c T 1 k x M W E X B l n m K B E 2 p S y G G W Y x F 7 R d 0 i 7 b s 4 G t B e e i V T b d I p H b 0 x K F f u k C S k v J i x O U 2 U 8 l c 6 C / c 0 7 k 5 7 i b m P 6 W 7 m X T y z H i N i / d N P M / + p E L R w O T m Q N L t U U S U Z U Z + c u q e y K u L n 6 p S p O D h F x A g 8 o H h O 2 p X L a Z 1 V q E l m 7 6 K 0 p 4 2 8 y U 7 B i b + e 5 K d 7 F L W n A + s 9 x z o L W Y U 3 X a v r F U a V + m o + 6 i F 3 s o U r z P E Y d 5 2 i g S d 4 Z H v G E Z 8 V Q b p U 7 5 f 4 z V S n k m m 1 8 W 8 r D</formula><p>B y e C m G g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f l e b G 6 e 5 L B 7 r i m f i p Q 3 v J z I F X B g = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g K S V J J 2 0 w L 5 K J W E I 3 7 s S t P + B W P 0 j 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u S 5 C d e 0 1 4 I y N 7 + w u F R c L q 2 s r q 1 v l D e 3 W k m Y x j Z r 2 q E X x h 3 L T J j n B q z J X e 6 x T h Q z 0 7 c 8 1 r a u z k S 8 f c 3 i x A 2 D S z 6 O W M 8 3 h 4 H r u L b J i e q X d w z f 5 C P L y d i k a n B 2 w 7 P Q m R z 0 y x W t p s m l z g</p><formula xml:id="formula_26">I 9 B x X k q x G W X 2 B g g B A 2 U v h g C M A J e z C R 0 N O F D g 0 R c T 1 k x M W E X B l n m K B E 2 p S y G G W Y x F 7 R d 0 i 7 b s 4 G t B e e i V T b d I p H b 0 x K F f u k C S k v J i x O U 2 U 8 l c 6 C / c 0 7 k 5 7 i b m P 6 W 7 m X T y z H i N i / d N P M / + p E L R w O T m Q N L t U U S U Z U Z + c u q e y K u L n 6 p S p O D h F x A g 8 o H h O 2 p X L a Z 1 V q E l m 7 6 K 0 p 4 2 8 y U 7 B i b + e 5 K d 7 F L W n A + s 9 x z o L W Y U 3 X a v r F U a V + m o + 6 i F 3 s o U r z P E Y d 5 2 i g S d 4 Z H v G E Z 8 V Q b p U 7 5 f 4 z V S n k m m 1 8 W 8 r D</formula><p>B y e C m G g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f l e b G 6 e 5 L B 7 r i m f i p Q 3 v J z I F X B g = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g K S V J J 2 0 w L 5 K J W E I 3 7 s S t P + B W P 0 j 8 A / 0 L 7 4 w p q E V 0 Q p I z 5 9 5 z Z u 6 9 V u S 5 C d e 0 1 4 I y N 7 + w u F R c L q 2 s r q 1 v l D e 3 W k m Y x j Z r 2 q E X x h 3 L T J j n B q z J X e 6 x T h Q z 0 7 c 8 1 r a u z k S 8 f c 3 i x A 2 D S z 6 O W M 8 3 h 4 H r u L b J i e q X d w z f 5 C P L y d i k a n B 2 w 7 P Q m R z 0 y x W t p s m l z g</p><formula xml:id="formula_27">I 9 B x X k q x G W X 2 B g g B A 2 U v h g C M A J e z C R 0 N O F D g 0 R c T 1 k x M W E X B l n m K B E 2 p S y G G W Y x F 7 R d 0 i 7 b s 4 G t B e e i V T b d I p H b 0 x K F f u k C S k v J i x O U 2 U 8 l c 6 C / c 0 7 k 5 7 i b m P 6 W 7 m X T y z H i N i / d N P M / + p E L R w O T m Q N L t U U S U Z U Z + c u q e y K u L n 6 p S p O D h F x A g 8 o H h O 2 p X L a Z 1 V q E l m 7 6 K 0 p 4 2 8 y U 7 B i b + e 5 K d 7 F L W n A + s 9 x z o L W Y U 3 X a v r F U a V + m o + 6 i F 3 s o U r z P E Y d 5 2 i g S d 4 Z H v G E Z 8 V Q b p U 7 5 f 4 z V S n k m m 1 8 W 8 r D B y e C m G g = &lt; / l a t e x i t &gt; e(is)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3</p><formula xml:id="formula_28">K T S y i v Q R k 0 8 + u 9 1 k K Y R w D o H q 0 = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g L S V J p + 1 g X i Q T s Y R u 3 I l b f 8 C t f p D 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 7 d D l s T C M 1 5 w 2 N 7 + w u J R f L q y s r q 1 v F D e 3 G n G Q R A 6 r O 4 E b R C 3 b i p n L f V Y X X L i s F U b M 8 m y X N e 2 r M x l v X r M o 5 o F / K c Y h 6 3 r W 0 O c D 7 l i C q F 5 x p + N Z Y m Q P U j Y p d w S 7 E S m P J w e 9 Y s m o G G r p s 8 D M Q A n Z q g X F F 3 T Q R w A H C T w w + B C E X V i I 6 W n D h I G Q u C 5 S 4 i J C X M U Z J i i Q N q E s R h k W s V f 0 H d K u n b E + 7 a V n r N Q O n e L S G 5 F S x z 5 p A s q L C M v T d B V P l L N k f / N O l a e 8 2 5 j + d u b l E S s w I v Y v 3 T T z v z p Z i 8 A A J 6 o G T j W F i p H V O Z l L o r o i b 6 5 / q U q Q Q 0 i c x H 2 K R 4 Q d p Z z 2 W V e a W N U u e 2 u p + J v K l K z c O 1 l u g n d 5 S x q w + X O c s 6 B x W D G N i n l x V K q e Z q P O Y x d 7 K N M 8 j 1 H F O W q o k 3 e K R z</formula><p>z h W e t o t 9 q d d v + Z q u U y z T a + L e 3 h A z g q m G 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3</p><formula xml:id="formula_29">K T S y i v Q R k 0 8 + u 9 1 k K Y R w D o H q 0 = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g L S V J p + 1 g X i Q T s Y R u 3 I l b f 8 C t f p D 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 7 d D l s T C M 1 5 w 2 N 7 + w u J R f L q y s r q 1 v F D e 3 G n G Q R A 6 r O 4 E b R C 3 b i p n L f V Y X X L i s F U b M 8 m y X N e 2 r M x l v X r M o 5 o F / K c Y h 6 3 r W 0 O c D 7 l i C q F 5 x p + N Z Y m Q P U j Y p d w S 7 E S m P J w e 9 Y s m o G G r p s 8 D M Q A n Z q g X F F 3 T Q R w A H C T w w + B C E X V i I 6 W n D h I G Q u C 5 S 4 i J C X M U Z J i i Q N q E s R h k W s V f 0 H d K u n b E + 7 a V n r N Q O n e L S G 5 F S x z 5 p A s q L C M v T d B V P l L N k f / N O l a e 8 2 5 j + d u b l E S s w I v Y v 3 T T z v z p Z i 8 A A J 6 o G T j W F i p H V O Z l L o r o i b 6 5 / q U q Q Q 0 i c x H 2 K R 4 Q d p Z z 2 W V e a W N U u e 2 u p + J v K l K z c O 1 l u g n d 5 S x q w + X O c s 6 B x W D G N i n l x V K q e Z q P O Y x d 7 K N M 8 j 1 H F O W q o k 3 e K R z</formula><p>z h W e t o t 9 q d d v + Z q u U y z T a + L e 3 h A z g q m G 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3</p><formula xml:id="formula_30">K T S y i v Q R k 0 8 + u 9 1 k K Y R w D o H q 0 = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g L S V J p + 1 g X i Q T s Y R u 3 I l b f 8 C t f p D 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 7 d D l s T C M 1 5 w 2 N 7 + w u J R f L q y s r q 1 v F D e 3 G n G Q R A 6 r O 4 E b R C 3 b i p n L f V Y X X L i s F U b M 8 m y X N e 2 r M x l v X r M o 5 o F / K c Y h 6 3 r W 0 O c D 7 l i C q F 5 x p + N Z Y m Q P U j Y p d w S 7 E S m P J w e 9 Y s m o G G r p s 8 D M Q A n Z q g X F F 3 T Q R w A H C T w w + B C E X V i I 6 W n D h I G Q u C 5 S 4 i J C X M U Z J i i Q N q E s R h k W s V f 0 H d K u n b E + 7 a V n r N Q O n e L S G 5 F S x z 5 p A s q L C M v T d B V P l L N k f / N O l a e 8 2 5 j + d u b l E S s w I v Y v 3 T T z v z p Z i 8 A A J 6 o G T j W F i p H V O Z l L o r o i b 6 5 / q U q Q Q 0 i c x H 2 K R 4 Q d p Z z 2 W V e a W N U u e 2 u p + J v K l K z c O 1 l u g n d 5 S x q w + X O c s 6 B x W D G N i n l x V K q e Z q P O Y x d 7 K N M 8 j 1 H F O W q o k 3 e K R z</formula><p>z h W e t o t 9 q d d v + Z q u U y z T a + L e 3 h A z g q m G 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 3</p><formula xml:id="formula_31">K T S y i v Q R k 0 8 + u 9 1 k K Y R w D o H q 0 = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l i E u i m J C L o s u n F Z w T 6 g L S V J p + 1 g X i Q T s Y R u 3 I l b f 8 C t f p D 4 B / o X 3 h l T U I v o h C R n z r 3 n z N x 7 7 d D l s T C M 1 5 w 2 N 7 + w u J R f L q y s r q 1 v F D e 3 G n G Q R A 6 r O 4 E b R C 3 b i p n L f V Y X X L i s F U b M 8 m y X N e 2 r M x l v X r M o 5 o F / K c Y h 6 3 r W 0 O c D 7 l i C q F 5 x p + N Z Y m Q P U j Y p d w S 7 E S m P J w e 9 Y s m o G G r p s 8 D M Q A n Z q g X F F 3 T Q R w A H C T w w + B C E X V i I 6 W n D h I G Q u C 5 S 4 i J C X M U Z J i i Q N q E s R h k W s V f 0 H d K u n b E + 7 a V n r N Q O n e L S G 5 F S x z 5 p A s q L C M v T d B V P l L N k f / N O l a e 8 2 5 j + d u b l E S s w I v Y v 3 T T z v z p Z i 8 A A J 6 o G T j W F i p H V O Z l L o r o i b 6 5 / q U q Q Q 0 i c x H 2 K R 4 Q d p Z z 2 W V e a W N U u e 2 u p + J v K l K z c O 1 l u g n d 5 S x q w + X O c s 6 B x W D G N i n l x V K q e Z q P O Y x d 7 K N M 8 j 1 H F O W q o k 3 e K R z</formula><p>z h W e t o t 9 q d d v + Z q u U y z T a + L e 3 h A z g q m G 8 = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e([MASK])</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s</p><formula xml:id="formula_32">= " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s</p><formula xml:id="formula_33">= " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s</p><formula xml:id="formula_34">= " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; /</formula><p>l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s</p><formula xml:id="formula_35">= " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; / l a t e x i t &gt; Britain [MASK] e(Britain)</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0</p><formula xml:id="formula_36">g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a p a 0 n k 0 g E V H o A / T Q Z e d T M I 5 F q m Y = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G 9 7 v q T j f B I t R N S U T Q p e j G p Y J t h b a U S T p t B / N i M h F L K b h z J 2 7 9 A b f 6 N + I f 6 F 9 4 Z 4 y g F t E J S c 6 c e 8 + Z u f d 6 S S B S 5 T g v Y 9 b 4 x O T U 9 M z s 3 P z C 4 t J y Y W W 1 m s a Z 9 H n F j 4 N Y n n s s 5 Y G I e E U J F f D z R H I W e g G v e R d H O l 6 7 5 D I V c X S m + g l v h q w b i Y 7 w m S K q V V h v h E z 1 v M 6 A D 0 s N x a / U 4 F A K x U Q 0 3 G 4 V i k 7 Z M c s e B W 4 O i s j X S V x 4 R g N t x P C R I Q R H B E U 4 A E N K T x 0 u H C T E N T E g T h I S J s 4 x x B x p M 8 r i l M G I v a B v l 3 b 1 n I 1 o r z 1 T o / b p l I B e S U o b W 6 S J K U 8 S 1 q f Z J p 4 Z Z 8 3 + 5 j 0 w n v p u f f p 7 u V d I r E K P 2 L 9 0 n 5 n / 1 e l a F D r Y N z U I q i k x j K 7 O z 1 0 y 0 x V 9 c / t L V Y o c E u I 0 b l N c E v a N 8 r P P t t G k p n b d W 2 b i r y Z T s 3 r v 5 7 k Z 3 v Q t a c D u z 3 G O g u p O 2 X X K 7 u l u 8 e A w H / U M N r C J E s 1 z D w c 4 x g k q 5 H 2 N B z z i y W L W j X V r 3 X 2 k W m O 5 Z g 3 f l n X / D r e Z m o 4 = &lt; / l a t e x i t &gt; e([MASK])</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s</p><formula xml:id="formula_37">= " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s = " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s = " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m V m O I H t B w X A q q 1 j P q B s I 6 S C I 4 s = " &gt; A A A C 3 n i c j V H L S s N A F D 2 N r 1 p f V V f i J l i E u i m J C L q s u h F E q G g f 0 J a a p N M a m h f J R C y l u H M n b v 0 B t / o 5 4 h / o X 3 h n T E E t o h O S n D n 3 n j N z 7 z U D x 4 6 4 p r 2 m l I n J q e m Z 9 G x m b n 5 h c S m 7 v F K J / D i 0 W N n y H T + s m U b E H N t j Z W 5 z h 9 W C k B m u 6 b C q 2 T s U 8 e o V C y P b 9 8 5 5 P 2 B N 1 + h 6 d s e 2 D E 5 U K 7 v W c A 1 + a X Y G b J h v c H b N B / W T / b P j 5 n C r l c 1 p B U 0 u d R z o C c g h W S U / + 4 I G 2 v B h I Y Y L B g + c s A M D E T 1 1 6 N A Q E N f E g L i Q k C 3 j D E N k S B t T F q M M g 9 g e f b u 0 q y e s R 3 v h G U m 1 R a c 4 9 I a k V L F J G p / y Q s L i N F X G Y + k s 2 N + 8 B 9 J T 3 K 1 P f z P x c o n l u C T 2 L 9 0 o 8 7 8 6 U Q t H B 3 u y B p t q C i Q j q r M S l 1 h 2 R d x c / V I V J 4 e A O I H b F A 8 J W 1 I 5 6 r M q N Z G s X f T W k P E 3 m S l Y s b e S 3 B j v 4 p Y 0 Y P 3 n O M d B Z b u g a w X 9 d C d X P E h G n c Y 6 N p C n e e 6 i i C O U U C b v G z z i C c / K h X K r 3 C n 3 n 6 l K K t G s 4 t t S H j 4 A Z Z G Z n w = = &lt; / l a t e x i t &gt; (a) Discrete Prompt Search (b) P-tuning capital e(capital) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3 i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g </p><formula xml:id="formula_38">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u</formula><formula xml:id="formula_39">i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g </p><formula xml:id="formula_40">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u</formula><formula xml:id="formula_41">i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g </p><formula xml:id="formula_42">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u</formula><formula xml:id="formula_43">i c j V H L S s N A F D 3 G V 3 1 X 3 e k m W A T d l E Q E X R b d u K x g q 9 C W M h m n d T A v k o l Y S s G d O 3 H r D 7 j V v x H / Q P / C O 2 M K a h G d k O T M u f e c m X u v F / s y V Y 7 z O m a N T 0 x O T R d m Z u f m F</formula><p>x a X i s s r 9 T T K E i 5 q P P K j 5 M x j q f B l K G p K K l + c x Y l g g e e L U + / y U M d P r 0 S S y i g </p><formula xml:id="formula_44">8 U b 1 Y t A L W D W V H c q a I a h f X m g F T F 1 6 n L w Z b T S W u V Z + z W C r m D 7 b b x Z J T d s y y R 4 G b g x L y V Y 2 K L 2 j i H B E 4 M g Q Q C K E I + 2 B I 6 W n A h Y O Y u</formula><p>where h i (0 ≤ i &lt; m) are trainable embedding tensors. This enables us to find a better continuous prompts beyond the original vocabulary V of M could express. Finally, with the downstream loss function L, we can differentially optimize the continuous prompt h i (0 ≤ i &lt; m) by ĥ0:m = arg min h L(M(x, y))</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization</head><p>Although the idea of training continuous prompts is straightforward, in practice, it faces two optimization challenges: 1) Discreteness: the original word embedding e of M has already become highly discrete after pre-training. If h is initialized with random distribution and then optimized with stochastic gradient descent (SGD), which has been proved to only change the parameters in a small neighborhood <ref type="bibr" target="#b0">(Allen-Zhu et al., 2019)</ref>, the optimizer would easily fall into local minima. 2) Association: another concern would be, intuitively, we believe the values of prompt embeddings h i should be dependent on each other rather than independent. We need some mechanism to associate prompt embeddings with each other.</p><p>In light of the challenges, in the P-tuning we propose to also model the h i as a sequence using a prompt encoder consists of a very lite neural network that can solve the discreteness and association problems. And in practice, we choose a bidirectional long-short term memory networks (LSTM), with a ReLU activated two-layer multilayer perceptron (MLP) to encourage discreteness. Formally speaking, the real input embeddings h i to the language model M is derived from</p><formula xml:id="formula_46">h i = MLP([ − → h i : ← − h i ]) = MLP([LSTM(h 0:i ) : LSTM(h i:m )])<label>(4)</label></formula><p>Though the LSTM head's use indeed adds some parameters to the training of continuous prompts, the LSTM head is several magnitude orders smaller than the pre-trained model. Moreover, in the inference, we only need the output embedding h and can discard the LSTM head.</p><p>Besides, we also find that adding few anchor tokens helps some NLU tasks in the SuperGLUE benchmark. For instance, for RTE task, the token "?" within prompt template "[PRE][prompt tokens][HYP]?[prompt tokens][MASK]" is specially added as an anchor token and affects the performance a lot. Usually such anchor words characterize each component, where in this case "?" indicate that "[HYP]" acts as an interrogation part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on two widely acknowledged natural language understanding benchmarks: LAMA <ref type="bibr" target="#b22">(Petroni et al., 2019)</ref> knowledge probing and SuperGlue <ref type="bibr" target="#b34">(Wang et al., 2019b)</ref>. The encouraging results show that P-tuning can substantially boost GPTs' performance on natural language understanding, and BERTstyle models can also be improved with a smaller gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Knowledge Probing</head><p>Knowledge probing, or referred to as fact retrieval, evaluates how much real-world knowledge has language models gained from pre-training. The LAMA <ref type="bibr" target="#b22">(Petroni et al., 2019)</ref> dataset evaluates it with cloze tests created from triples selected in the knowledge bases. For example, we will transform the triple (Dante, born-in, Florence) into a cloze sentence with the handcraft prompt "Dante was born in <ref type="bibr">[MASK]</ref>.", and then we ask language models to inference the target. Because we want to evaluate knowledge gained from pre-training, pre-trained language models' parameters are fixed (i.e., not fine-tuned).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">DATASETS AND FORMULATION</head><p>Datasets. LAMA enforces all answers in single-token format. We first adopt the original LAMA-TREx dataset, consisting of 41 Wikidata relations and altogether 34,039 testing triples (namely LAMA-34k, which covers all BERT vocabularies). Because GPT and BERT's vocabularies are different, we set up another version of LAMA, which covers the intersection of GPT and BERT's vocabulary. This subset adds up to about 29,000 testing triples, and we name it the LAMA-29k.</p><p>As for training, all prompt searching approaches need some additional data to train or find the prompts. We follow the setting in AutoPrompt <ref type="bibr" target="#b31">(Shin et al., 2020)</ref>, where the authors construct a training set from the original TRE-x dataset. This training set is similar to the test set but with a slightly different answer distribution.</p><p>Evaluation. Originally, LAMA has provided a handcraft prompt for each relation such as Table <ref type="table">1</ref>, which are effective but sub-optimal. For bidirectional masked language models, we only need to replace the "[X]" with the subject entity and "[Y]" with the [MASK] token; for unidirectional language models such as GPT, following LAMA's original setting on Transformer-XL <ref type="bibr" target="#b5">(Dai et al., 2019)</ref>, we use the network output just before the target position. In terms of conducting P-tuning , we use a (3, sub,3, obj,3) template for bidirectional models and (3, sub,3, obj) for unidirectional models, where the number indicates the number of prompt tokens. In this knowledge probing task, we do not use any anchor tokens. During the training, we set the learning rate to 1e-5 and use the Adam optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">RESULTS</head><p>General performance. The results are presented in Table 2. The P-tuning significantly pushes the boundary of knowledge probing from 43.3% to 50.6% in LAMA-34k and 45.2% to a maximum of 64.2% in LAMA-29k. This result strongly suggests that language models capture far more knowledge than people previously believed by merely finding a better prompt and without fine-tuning. When Ptuning is compared with previous discrete prompt searching approaches such as AutoPrompt <ref type="bibr" target="#b31">(Shin et al., 2020)</ref> and LPAQA <ref type="bibr" target="#b13">(Jiang et al., 2020b)</ref> on the same-size models, Ptuning still outperforms them.</p><p>P-tuning v.s. Fine-tuning. It is not allowed to change the pre-trained model's parameters by fine-tuning in traditional knowledge probing. We seek to evaluate how much knowledge has language models learned during pre-training. However, this work's essential aspect is to compare P-tuning and fine-tuning, particularly on unidirectional language models like GPT. We are especially interested in the following question: Are unidirectional and bidirectional language models gaining similar improvement from P-tuning ?</p><p>To make a comprehensive review on existing tuning methods, we include the following approaches: 1) Manual Prompt (MP): use original handcraft prompts from LAMA.</p><p>2) Fine-tuning (FT): only to present the subject and finetune the model to predict the object. 3) Manual Prompt with Fine-tuning (MP+FT): fine-tuning the language model with the handcraft prompts. 4) P-tuning : use continuous prompts (while fixing language models' parameters).</p><p>We implement the four strategies in the LAMA-29k (see Table <ref type="table" target="#tab_1">2</ref>, right), and we find that P-tuning is comparable to or better than fine-tuning-based methods, which is surprising but reasonable. The surprising thing is that fine-tuning should have been more potent since it tunes all language models' parameters, while P-tuning not. However, it is also reasonable because, in terms of knowledge probing, many facts can only be hard-coded rather than inferenced by language models. The fine-tuning of parameters might result in catastrophic forgetting. On the contrary, P-tuning does not change the pre-trained models' parameters but evoke the stored knowledge by finding a better continuous prompt.</p><p>Besides, it is interesting to see a clear gap between BERT and GPT's improvement to the P-tuning . Fine-tuning with high-quality manual prompts (MP+FT) <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref> has been proved to be quite effective, which is also observed in our experiments. However, it is surprising that GPTs do not benefit from MP+FT as much as from P-tuning as BERTs do. In other words, P-tuning shows a better affinity with unidirectional language models. In terms of much larger models such as MegatronLM<ref type="foot" target="#foot_1">2</ref> with 11 billion parameters, while fine-tuning hardly works, Ptuning is still applicable and achieve the state-of-the-art on LAMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SuperGLUE</head><p>To evaluate P-tuning , we perform experiments on the Super-GLUE <ref type="bibr" target="#b33">(Wang et al., 2019a)</ref> benchmark. There are 8 natural language understanding (NLU) tasks in total, and we focus on 7 of them as <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref>, since the other ReCoRD <ref type="bibr" target="#b38">(Zhang et al., 2018)</ref>  ). Since it has been proven that larger development sets confer additional significant advantages <ref type="bibr" target="#b10">(Gao et al., 2020)</ref>, D dev32 is built by randomly selecting samples from unused training data and is strictly restricted to be no larger than the size of few-shot train sets. We adopt the same evaluation metrics as <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref>.</p><p>We reformulate NLU tasks into blank filling tasks. Unlike <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020</ref>) that use patterns with human hand-crafted prompts, P-tuning puts initial prompt embeddings in different positions within patterns and then finetunes the prompt embeddings together with the pretrained models. For fully-supervised settings, we use the AdamW optimizer with a linearly decayed learning rate. We perform grid search of hyper-parameters and take the best combination on D dev or D dev32 . Specifically, we take learning rates from 1e-5, 2e-5, 3e-5 and batch sizes from 16, 32.</p><p>For small datasets (COPA, WSC, CB, RTE), we fine-tune pretrained models for 20 epochs. For larger datasets (WiC, BoolQ, MultiRC), we reduce the number of training epochs to be 10 as the model converges earlier. We evaluate the performance of every epoch. We use early stopping to avoid overfitting to the training data. For few-shot learning, we use the same hyperparameters as <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref> except extending fine-tuning steps to be 3500 (instead of 250 as <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref>), since the fine-tuning of prompt embeddings requires even more steps.</p><p>P-tuning can be used on all unidirectional and bidirectional models. We choose models with similar scales of total computes for a fair comparison, where we choose to compare BERT-base<ref type="foot" target="#foot_2">3</ref> with GPT2-base and compare BERT-large with GPT2-medium. Models like RoBERTa have a similar model size but was trained with much larger compute, which should be compared with larger-scale GPTs. This is left to future work. Besides, for few-shot learning, we also experiment with albert-xxlarge-v2, which is proved to be the best-performed pretrained models for the few-shot setting in <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref>. For each pretrained model, we report the performance of standard finetuning (i.e., classification using [CLS] embeddings), PET finetuning <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref>, PET zero-shot, and P-tuning .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">FULLY-SUPERVISED LEARNING</head><p>The main results are shown in Table <ref type="table">3 and Table 4</ref>. Firstly, for both bert-base-cased and bert-large-cased models, Ptuning outperforms all the other bert-based models on 5 out of 7 tasks. Exceptions are WiC and MultiRC, where P-tuning performs a little bit worse than standard fine-tune. Since both WiC and MultiRC have relatively large train sets, we conjecture this can be attributed to the fact that standard fine-tune could take even more advantages from a larger dataset than P-tuning . On the contrary, P-tuning appears to be more beneficial in low-resource settings. Similar obser- complicated than we thought. Moreover, Table <ref type="table" target="#tab_4">6</ref> also proves that it is impossible to find the best-performed manual prompts using D dev32 . This indicates it is also challenging to pick the best manual prompts out in the few-shot setting.</p><p>In contrast, P-tuning appears promising in automatically searching better prompts with far fewer hand-crafts.</p><p>Updated SOTA for SuperGLUE Few-shot Learning. Table 5 presents the latest state-of-the-art results for Super-GLUE few-shot learning, achieved by P-tuning . We compared it with several baselines, including PET <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref> and GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref>, which achieves previous SuperGLUE Few-shot SOTA.</p><p>It is worth noting that, aside from manual prompt finetuning, the original PET <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref> adopts multiple additional technologies, including data augmentation, ensemble, and distillation, to boost the performance.</p><p>Besides, it performs model selection and hyper-parameter tuning by over-fitting test sets. To ensure a fair comparison, PET is re-experimented under our D dev32 setting, removing all auxiliary technologies (data augmentation, ensemble, and distillation). Considering PET provides multiple manual prompts, both averaged performance, and the bestperformed-prompt performance are reported.</p><p>Table <ref type="table" target="#tab_3">5</ref> illustrates that P-tuning consistently outperforms PET (D dev32 ) and PET-best (D dev32 ) with manual prompts on all tasks. The improvements of solution over PET (D dev32 ) are larger than the standard deviations over multiple patterns on 5 out of 7 tasks, proving that P-tuning can search far better prompts than manual ones and significantly improve few-shot task performance. On tasks including CB,WiC,RTE and WSC, P-tuning even outperforms PET/iPET (D dev ), which adopt auxiliary technologies (data augmentation, ensemble and distillation). Compared with GPT-3, with much larger scale than P-tuning (albert-xxlarge-v2), P-tuning outperforms on 6 out of 7 tasks. Results demonstrate the advantages of P-tuning in few-shot NLU tasks.</p><p>4.2.3. FINETUNING V.S. MP FINETUNING V.S. P-TUNING Table <ref type="table">3</ref> and Table <ref type="table">4</ref> present results of three tuning-based paradigms for improving NLU performance. We are particularly interested in how these tuning-based paradigms perform differently. Overall, P-tuning outperforms finetuning and MP fine-tuning on average by around 2 points on BERT-based models and more than 5 points on GPT2based models. Specifically, though P-tuning achieves the best results on most of the tasks, fine-tuning can outperform on tasks (e.g., WiC) that are hard to formulate as cloze questions. Comparing P-tuning and MP fine-tuning, P-tuning generally shows more advantages than MP finetuning on average, since it is tricky for MP fine-tuning to find good manual prompts. In contrast, P-tuning can always automatically search for better prompts.</p><p>As a new paradigm of tuning pretrained models, Ptuning could search over a vast prompt space while tuning pretrained models' parameters. Results demonstrate its competitive potential in prompting larger-scale pre-trained models that are hard to fine-tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pre-trained Language Models</head><p>The recent breakthrough in self-supervised <ref type="bibr" target="#b20">(Liu et al., 2020)</ref> pre-trained language models has boosted the development of natural language processing. GPT <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> first leverages the transformer architecture to pre-train on large-scale web texts. BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> proposes the masked language modeling and creates the pre-train/finetuning paradigm. Later on, various kinds of language models grown up, including XLNet <ref type="bibr" target="#b36">(Yang et al., 2019)</ref> which innovates the permutation language modeling. RoBERTa <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> conducts detailed experiments to demonstrate useful techniques related to pre-training. BART <ref type="bibr" target="#b17">(Lewis et al., 2019)</ref>, T5 <ref type="bibr" target="#b26">(Raffel et al., 2019)</ref> and UniLM <ref type="bibr" target="#b9">(Dong et al., 2019)</ref> which try to unify the language understanding and generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Language Models as Knowledge Base</head><p>Since the birth of language models, researchers have observed that they not only learn contextualized text representations but also various types and amounts of knowledge, including linguistic and world knowledge. <ref type="bibr" target="#b11">(Hewitt &amp; Manning, 2019)</ref> demonstrates that contextualized representation produced by language models can form a parsing tree in the embedding space. <ref type="bibr" target="#b32">(Vig, 2019;</ref><ref type="bibr" target="#b3">Clark et al., 2019b</ref>) look into the multi-head attention internal transformers and discover that certain attention heads may correspond to some grammatical functions, including co-reference and noun modifiers.</p><p>Another important stream is about how much world knowledge or factual knowledge has language models learned. LAMA <ref type="bibr" target="#b22">(Petroni et al., 2019;</ref><ref type="bibr">2020)</ref> propose to leverage cloze test transformed from fact triples in knowledge bases to examine language model's ability in memorizing facts with answers in the single-token format. In <ref type="bibr" target="#b35">(Wang et al., 2020)</ref>, the authors investigate the attention matrices to find that the attentions would also indicate knowledge triples contained in the context and thus develop an open knowledge graph construction framework. <ref type="bibr" target="#b12">(Jiang et al., 2020a)</ref> based on LAMA develops a multi-token fact retrieval dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Language Model Prompting</head><p>The birth of GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref> has blown people's minds with its outstanding performance in multi-task and few-shot learning. However, GPT-3 is not designed for fine-tuning, and it heavily relies on handcraft prompts (or the in-context learning <ref type="bibr" target="#b19">(Liu et al., 2021;</ref><ref type="bibr" target="#b39">Zhao et al., 2021)</ref>) to transfer to downstream tasks. To better apply large language models to natural language understanding (NLU), recent works have concentrated on automating the search of discrete prompts by mining training corpus <ref type="bibr" target="#b13">(Jiang et al., 2020b)</ref>, token-based gradient searching <ref type="bibr" target="#b31">(Shin et al., 2020)</ref> and using separate model <ref type="bibr" target="#b10">(Gao et al., 2020)</ref> such as T5 to generate prompts. However, the search over discrete space is challenging to optimize and sub-optimal due to the continuous nature of neural networks.</p><p>Recently, <ref type="bibr" target="#b18">(Li &amp; Liang, 2021)</ref> propose prefix-tuning for natural language generation (NLG) tasks, which adopts a similar strategy to our P-tuning to train continuous prompt. Nevertheless, they are different in several aspects. First, prefixtuning is designed for NLG and GPTs, while P-tuning targets NLU and all types of language models. Second, prefixtuning only allows adding prompt tokens at the beginning of the input sequence, while P-tuning can insert the tokens anywhere. Third, prefix-tuning invasively concatenates continuous prompt tokens in every layer of the transformer because the authors find mere prompting in the input does not take effect; on the contrary, P-tuning non-invasively adds continuous prompts only in the input to work well. Finally, P-tuning also introduces how to use anchor prompts for further improvement. Despite the differences, we believe that both our P-tuning and prefix-tuning point out that learning continuous prompts is useful and superior to discrete prompt searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present-P-tuning-which augments pretrained model's ability in natural language understanding by automatically searching better prompts in the continuous space. Our P-tuning method relies less on a large validation set, suffers less from adversarial prompts, and alleviates over-fitting. We show that our P-tuning method can recover 64% (P@1) of world knowledge from a pre-trained language model without any additional text provided during test time. On the SuperGLUE benchmark, P-tuning endows GPT-style models to show competitive performance with similar-size BERTs in natural language understanding, which is assumed impossible in the past. P-tuning also helps on bidirectional models and consequently outperforms stateof-the-art methods in the few-shot SuperGlue benchmark. It</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Average scores on 7 dev datasets of SuperGlue. GPTs can be better than similar-sized BERTs on NLU with P-tuning.</figDesc><graphic url="image-1.png" coords="1,307.44,174.91,233.99,73.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r e Z m o 4 = &lt; / l a t e x i t &gt; e(The) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X / Y I D I x g / H s e 8 W N 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g 5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h gt i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u + J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M =&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g 5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h gt i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u + J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M =&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>B b 6 x C W E p I k L D D B L 2 o y y B G U w Y i / p 2 6 V d I 2 d D 2 m v P 1 K g 5 n e L T m 5 D S x i Z p I s p L C O v T b B P P j L N m f / P u G 0 9 9 t x 7 9 v d w r I F b h gt i / d M P M / + p 0 L Q o d 7 J s a J N U U G 0 Z X x 3 O X z H R F 3 9 z + U p U i h 5 g 4 j c 8 p n h D m R j n s s 2 0 0 q a l d 9 5 a Z + J v J 1 K z e 8 z w 3 w 7 u + J Q 3 Y / T n O U V D f K b t O 2 T 3 e L V U O 8 l E X s I 4 N b N E 8 9 1 D B E a q o k f c N H v G E Z 4 t Z t 9 a d d f + Z a o 3 l m l V 8 W 9 b D B + o t m q M =&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r w o N 1 C Z f w D A A g s 8 2 3 D m s s B j r T S U = " &gt; A A A C 3 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of prompt search for "The capital of Britain is [MASK]". Given the context (blue zone, "Britain") and target (red zone, "[MASK]"), the orange zone refer to the prompt tokens. In (a), the prompt generator only receives discrete rewards; on the contrary, in (b) the pseudo prompts and prompt encoder can be optimized in a differentiable way. Sometimes, adding few task-related anchor tokens (such as "capital" in (b)) will bring further improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Knowledge probing Precision@1 on LAMA-34k (left) and LAMA-29k (right). P-tuning outperforms all the discrete prompt searching baselines. And interestingly, despite fixed pre-trained model parameters, P-tuning overwhelms the fine-tuning GPTs in LAMA-29k. (MP: Manual prompt; FT: Fine-tuning; MP+FT: Manual prompt augmented fine-tuning; PT: P-tuning ).</figDesc><table><row><cell>Prompt type Original (MP) Discrete P-tuning</cell><cell>Model BERT-base BERT-large E-BERT LPAQA (BERT-base) LPAQA (BERT-large) AutoPrompt (BERT-base) BERT-base BERT-large</cell><cell>P@1 31.1 32.3 36.2 34.1 39.4 43.3 48.3 50.6</cell><cell>Model BERT-base (109M) -AutoPrompt (Shin et al., 2020) BERT-large (335M) RoBERTa-base (125M) -AutoPrompt (Shin et al., 2020) RoBERTa-large (355M) GPT2-medium (345M) GPT2-xl (1.5B) MegatronLM (11B)</cell><cell>MP 31.7 -33.5 18.4 -22.1 20.3 22.8 23.1 OOM  *  FT 51.6 -54.0 49.2 -52.3 41.9 44.9</cell><cell>MP+FT P-tuning 52.1 52.3 (+20.6) -45.2 55.0 54.6 (+21.1) 50.0 49.3 (+30.9) -40.0 52.4 53.5 (+31.4) 38.2 46.5 (+26.2) 46.5 54.4 (+31.6) OOM  *  64.2 (+41.1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">* MegatronLM (11B) is too large for effective fine-tuning.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Few-shot learning (32 train samples) on SuperGLUE dev. Previous few-shot learning approaches use the original full dev set (D dev ) for validation, which does not make sense. We construct a new dev set (D dev32 ) with 32 unused samples from original training set. Under fair comparison, P-tuning significantly outperforms PET (D dev32 ) and PET best (D dev32 ) on all tasks. More interestingly, P-tuning even outperforms GPT-3, PET (D dev ) and iPET (D dev ) on 4 out of 7 tasks. Subscripts in red represents the improvements of P-tuning over PET(D dev32 ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Prompt D dev Acc. D dev32 Acc. Does [PRE] agree with [HYP]? [MASK]. 57.16 53.12 Does [HYP] agree with [PRE]? [MASK]. 51.38 50.00 Premise: [PRE] Hypothesis: [HYP] Answer: [MASK]. 68.59 55.20 [PRE] question: [HYP]. true or false? answer: [MASK]. Few-shot performance comparison of different manual prompts and tuned prompts on RTE tasks using albert-xxlarge-v2.Experiments use D dev32 for model selection and hyper-parameter tuning and evaluate on D dev . There's no obvious correlations between manual prompts and performance. Besides, D dev32 is not able to select the best manual prompts.</figDesc><table><row><cell>70.15</cell><cell>53.12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Codes will be at https://github.com/THUDM/ P-tuning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Provided in fairseq: https://github.com/pytorch/ fairseq/tree/master/examples/megatron_11b</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We use the cased version of BERT for comparison since the vocabulary of GPT2 is also cased.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* We report the same results taken from SuperGLUE <ref type="bibr" target="#b34">(Wang et al., 2019b)</ref>.</p><p>Table <ref type="table">4</ref>. Fully-supervised learning on SuperGLUE dev with large-scale models. MP refers to manual prompt. For fair comparison, MP zero-shot and MP fine-tuning report results of a single pattern, while anchors for P-tuning are selected from the same prompt. Subscripts in red represents improvements of GPT with P-tuning over the best results of BERT.</p><p>vations are also shown in Section 4.2.2. Secondly, for both gpt2-base and gpt2-medium models, P-tuning achieves the most promising results among all gpt2-base models. Above all, we can conclude that P-tuning can effectively boost the NLU performance of both bert-based and gpt-based models.</p><p>Besides, under the scale of base models, gpt2-base with Ptuning outperforms the best results of BERT-based models on 6 out of 7 tasks while achieving comparable results on WiC. Comparing with BERT-large models, GPT2-medium with P-tuning shows advantages on 4 out of 7 tasks while being comparable on RTE and WSC tasks. The only exception is the WiC task. It is noticed that on the WiC task, standard fine-tune shows the best results on different models with different settings. We speculate that it is because the word sense disambiguation task is not appropriate for prompt-based MLM prediction. Above all, we conclude that with P-tuning , GPT2 achieves comparable and even better performance as BERT-based models. The discovery subverts our common belief that bidirectional models (such as BERT) are always better at NLU tasks than unidirectional models (such as GPT2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">FEW-SHOT LEARNING</head><p>Sub-optimal and Sensitive Manual Prompts. Initially, PET/iPET <ref type="bibr" target="#b30">(Schick &amp; Schütze, 2020)</ref> has achieved the stateof-the-arts on SuperGLUE few-shot learning tasks with several manually-written prompts, which are effective, but sub-optimal and labor-intensive. For a comprehensive understanding of manual prompts, comparative experiments are first conducted. Table <ref type="table">6</ref> shows the results of using different manual prompts and P-tuning . First, results show that the few-shot performance has no obvious correlations with prompts' semantics, format, grammar. Prompts that humans consider reasonable is not necessarily effective for language models. Second, minor changes in manual prompts would cause substantial performance differences. Pretrained language models are pretty sensitive to the choice of prompts. We can conclude that manual handwriting prompts are more also proves that language models effectively capture more world knowledge and prior-task knowledge than we thought during pre-training.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Boolq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of Sinn und Bedeutung</title>
				<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">X-factr: Multilingual factual knowledge retrieval from pretrained language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="5943" to="5959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning (ICML 2000)</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Machine Learning (ICML 2000)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06804</idno>
		<title level="m">What makes good in-context examples for gpt-3?</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04611</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wic: 10, 000 example pairs for evaluating context-sensitive representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno>CoRR, abs/1808.09121</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Zero-shot textto-image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prompt programming for large language models: Beyond the few-shot paradigm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07350</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Autoprompt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05714</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Su-perGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019a</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language models are open knowledge graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11967</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13000</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">X.-S. Interventional few-shot learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09690</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
