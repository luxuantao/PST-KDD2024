<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Training of Mixture Models via Coresets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dan</forename><surname>Feldman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Faulkner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<title level="a" type="main">Scalable Training of Mixture Models via Coresets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D7E12E64B23F7112DDF5F16C1068ABEC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset will also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk 3 /ε 2 ) data points suffices for computing a (1 + ε)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efficiently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the problem of training statistical mixture models, in particular mixtures of Gaussians and some natural generalizations, on massive data sets. Such data sets may be distributed across a cluster, or arrive in a data stream, and have to be processed with limited memory. In contrast to parameter estimation for models with compact sufficient statistics, mixture models generally require inference over latent variables, which in turn depends on the full data set. In this paper, we show that Gaussian mixture models (GMMs), and some generalizations, admit small coresets: A coreset is a weighted subset of the data which guarantees that models fitting the coreset will also provide a good fit for the original data set. Perhaps surprisingly, we show that Gaussian mixtures admit coresets of size independent of the size of the data set.</p><p>We focus on ε-semi-spherical Gaussians, where the covariance matrix Σ i of each component i has eigenvalues bounded in [ε, 1/ε], but some of our results generalize even to the semi-definite case. In particular, we show that given a data set D of n points in R d , ε &gt; 0 and k ∈ N, how one can efficiently construct a weighted set C of O(dk 3 /ε 2 ) points, such that for any mixture of k ε-semispherical Gaussians θ = [(w 1 , µ 1 , Σ 1 ), . . . , (w k , µ k , Σ k )] it holds that the log-likelihood ln P (D | θ) of D under θ is approximated by the (properly weighted) log-likelihood ln P (C | θ) of C under θ to arbitrary accuracy as ε → 0. Thus solving the estimation problem on the coreset C (e.g., using weighted variants of the EM algorithm, see Section 3.3) is almost as good as solving the estimation problem on large data set D. Our algorithm for constructing C is based on adaptively sampling points from D and is simple to implement. Moreover, coresets can be efficiently constructed in a map-reduce style computation, as well as in a streaming setting (using space and update time per point of poly(dkε -1 log n log(1/δ))).</p><p>Existence and construction of coresets have been investigated for a number of problems in computational geometry (such as k-means and k-median) in many recent papers (cf., surveys in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>). In this paper, we demonstrate how these techniques from computational geometry can be lifted to the realm of statistical estimation. As a by-product of our analysis, we also close an open question on the VC dimension of arbitrary mixtures of Gaussians. We evaluate our algorithms on several synthetic and real data sets. In particular, we use our approach for density estimation for acceleration data, motivated by an application in earthquake detection using mobile phones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Problem Statement</head><p>Fitting mixture models by MLE. Suppose we are given a data set D = {x 1 , . . . , x n } ⊆ R d . We consider fitting a mixture of Gaussians θ = [(w 1 , µ 1 , Σ 1 ), . . . , (w k , µ k , Σ k )], i.e., the distribution</p><formula xml:id="formula_0">P (x | θ) = k i=1 w i N (x; µ i , Σ i ),</formula><p>where w 1 , . . . , w k ≥ 0 are the mixture weights, i w i = 1, and µ i and Σ i are mean and covariance of the i-th mixture component, which is modeled as a multivariate normal distribution N (x, µ i ,</p><formula xml:id="formula_1">Σ i ) = 1 √ |2πΣi| exp -1 2 (x -µ i ) T Σ -1 i (x -µ i ) .</formula><p>In Section 4, we will discuss extensions to more general mixture models. Assuming the data was generated i.i.d., the negative log likelihood of the data is L(D | θ) =j ln P (x j | θ), and we wish to obtain the maximum likelihood estimate (MLE) of the parameters θ * = argmin θ∈C L(D | θ), where C is a set of constraints ensuring that degenerate solutions are avoided <ref type="foot" target="#foot_0">1</ref> . Hereby, for a symmetric matrix A, spec A is the set of all eigenvalues of A. We define</p><formula xml:id="formula_2">C = C ε = {θ = [(w 1 , µ 1 , Σ 1 ), . . . , (w k , µ k , Σ k )] | ∀ i : spec(Σ i ) ⊆ [ε, 1/ε]</formula><p>} to be the set of all mixtures of k Gaussians θ, such that all the eigenvalues of the covariance matrices of θ are bounded between ε and 1/ε for some small ε &gt; 0.</p><p>Approximating the log-likelihood. Our goal is to approximate the data set D by a weighted set</p><formula xml:id="formula_3">C = {(γ 1 , x 1 ), . . . , (γ m , x m )} ⊆ R × R d , such that L(D | θ) ≈ L(C | θ) for all θ, where we define L(C | θ) = -i γ i ln P (x i | θ).</formula><p>What kind of approximation accuracy may we hope to expect? Notice that there is a nontrivial issue of scale: Suppose we have a MLE θ * for D, and let α &gt; 0. Then straightforward linear algebra shows that we can obtain an MLE θ * α for a scaled data set αD = {αx : x ∈ D} by simply scaling all means by α, and covariance matrices by α 2 . For the log-likelihood, however, it holds that</p><formula xml:id="formula_4">L(αD | θ * α ) = d ln α + L(D | θ * )</formula><p>. Therefore, optimal solutions on one scale can be efficiently transformed to optimal solutions at a different scale, while maintaining the same additive error. This means, that any algorithm which achieves absolute error ε at any scale could be used to achieve parameter estimates (for means, covariances) with arbitrarily small error, simply by applying the algorithm to a scaled data set and transforming back the obtained solution. An alternative, scaleinvariant approach may be to strive towards approximating L(D | θ) up to multiplicative error (1 + ε). Unfortunately, this goal is also hard to achieve: Choosing a scaling parameter α such that d ln α + L(D | θ * ) = 0 would require any algorithm that achieves any bounded multiplicative error to essentially incur no error at all when evaluating L(αD | θ * ). The above observations hold even for the case k = 1 and Σ = I, where the mixture θ consists of a single Gaussian, and the log-likelihood is the sum of squared distances to a point µ and an additive term.</p><p>Motivated by the scaling issues discussed above, we use the following error bound that was suggested in <ref type="bibr" target="#b2">[3]</ref> (who studied the case where all Gaussians are identical spheres). We decompose the negative log-likelihood L(D | θ) of a data set D as</p><formula xml:id="formula_5">L(D | θ) = - n j=1 ln k i=1 w i |2πΣ i | exp - 1 2 (x j -µ i ) T Σ -1 i (x j -µ i ) = -n ln Z(θ) + φ(D | θ)</formula><p>where</p><formula xml:id="formula_6">Z(θ) = i wi √ |2πΣi|</formula><p>is a normalizer, and the function φ is defined as</p><formula xml:id="formula_7">φ(D | θ) = - n j=1 ln k i=1 w i Z(θ) |2πΣ i | exp - 1 2 (x j -µ i ) T Σ -1 i (x j -µ i ) .</formula><p>Hereby, Z(θ) plays the role of a normalizer, which can be computed exactly, independently of the set D. φ(D | θ) captures all dependencies of L(D | θ) on D, and via Jensen's inequality, it can be seen that φ(D | θ) is always nonnegative.</p><p>We can now use this term φ(D | θ) as a reference for our error bounds. In particular, we call θ a</p><formula xml:id="formula_8">(1 + ε)-approximation for θ if (1 -ε)φ(D | θ) ≤ φ(D | θ) ≤ φ(D | θ)(1 + ε).</formula><p>Coresets. We call a weighted data set C a (k, ε)-coreset for another (possibly weighted) set  Hereby φ(C | θ) is generalized to weighted data sets C in the natural way (weighing the contribution of each summand x j ∈ C by γ j ). Thus, as ε → 0, for a sequence of (k, ε)-coresets C ε we have that</p><formula xml:id="formula_9">D ⊆ R d , if for all mixtures θ ∈ C of k Gaussians it holds that (1 -ε)φ(D | θ) ≤ φ(C | θ) ≤ φ(D | θ)(1 + ε).</formula><formula xml:id="formula_10">sup θ∈C |L(C ε | θ) -L(D | θ)| → 0, i.e., L(C ε | θ) uniformly (over θ ∈ C) approximates L(D | θ).</formula><p>Further, under the additional condition that all variances are sufficiently large (formally λ∈spec(Σi) λ ≥ 1 (2π) d for all components i), the log-normalizer ln Z(θ) is negative, and consequently the coreset in fact provides a multiplicative (1 + ε) approximation to the log-likelihood, i.e.,</p><p>(</p><formula xml:id="formula_11">1 -ε)L(D | θ) ≤ L(C | θ) ≤ L(D | θ)(1 + ε).</formula><p>More details can be found in the supplemental material. Note that if we had access to a (k, ε)-coreset C, then we could reduce the problem of fitting a mixture model on D to one of fitting a model on C, since the optimal solution θ C is a good approximation (in terms of log-likelihood) of θ * . While finding the optimal θ C is a difficult problem, one can use a (weighted) variant of the EM algorithm to find a good solution. Moreover, if |C| |D|, running EM on C may be orders of magnitude faster than solving it on D. In Section 3.3, we give more details about solving the density estimation problem on the coreset.</p><p>The key question is whether small (k, ε)-coresets exist, and whether they can be efficiently constructed. In the following, we answer this question affirmatively. We show that, perhaps surprisingly, one can efficiently find coresets C of size independent of the size n of D, and with polynomial dependence on 1 ε , d and k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Coreset Construction via Adaptive Sampling</head><p>Naive approach: uniform sampling. A naive approach towards approximating D would be to just pick a subset C uniformly at random. In particular, suppose the data set is generated from a mixture of two spherical Gaussians (Σ i = I) with weights</p><formula xml:id="formula_12">w 1 = 1 √ n and w 2 = 1 -1 √ n . Unless m = Ω( √ n)</formula><p>points are sampled, with constant probability no data point generated from Gaussian 2 is selected. By moving the means of the Gaussians arbitrarily far apart, L(D | θ C ) can be made arbitrarily worse than L(D | θ D ), where θ C and θ D are MLEs on C and D respectively. Thus, even for two well-separated Gaussians, uniform sampling can perform arbitrarily poorly. This example already suggests that, intuitively, in order to achieve small multiplicative error, we must devise a sampling scheme that adaptively selects representative points from all "clusters" present in the data set. However, this suggests that obtaining a coreset requires solving a chicken-and-egg problem, where we need to understand the density of the data to obtain the coreset, but simultaneously would like to use the coreset for density estimation.</p><p>Better approximation via adaptive sampling. The key idea behind the coreset construction is that we can break the chicken-and-egg problem by first obtaining a rough approximation B of the clustering solution (using more than k components, but far fewer than n), and then to use this solution to bias the random sampling. Surprisingly, a simple procedure which iteratively samples a small number β of points, and removes half of the data set closest to the sampled points, provides a sufficiently accurate first approximation B for this purpose. This initial clustering is then used to sample the data points comprising coreset C according to probabilities which are roughly proportional to the squared distance to the set B. This non-uniform random sampling can be understood as an importance-weighted estimate of the log-likelihood L(D | θ), where the weights are optimized in order to reduce the variance. The same general idea has been found successful in constructing coresets for geometric clustering problems such as k-means and k-median <ref type="bibr" target="#b3">[4]</ref>. The pseudocode for obtaining the approximation B, and for using it to obtain coreset C is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Coreset construction</head><formula xml:id="formula_13">Input: Data set D, ε, δ, k Output: Coreset C = (γ(x1), x1), . . . , (γ(x |C| ), x |C| ) D ← D; B ← ∅; while |D | &gt; 10dk ln(1/δ) do Sample set S of β = 10dk ln(1/δ) points uniformly at random from D ; Remove |D |/2 points x ∈ D closest to S (i.e., minimizing dist(x, S)) from D ; Set B ← B ∪ S; Set B ← B ∪ D ; for each b ∈ B do D b ← the points in D whose closest point in B is b. Ties broken arbitrarily; for each b ∈ B and x ∈ D b do m(x) ← 5 |D b | + dist(x,B) 2</formula><p>x ∈D dist(x ,B) 2 ; Pick a non-uniform random sample C of 10 dk|B| 2 ln(1/δ)/ε 2 points from D, where for every x ∈ C and x ∈ D, we have x = x with probability m(x)/ x ∈D m(x );</p><formula xml:id="formula_14">for each x ∈ C do γ(x ) ← x∈D m(x) |C|•m(x ) ;</formula><p>We have the following result, proved in the supplemental material: Theorem 3.1. Suppose C is sampled from D using Algorithm 1 for parameters ε, δ and k. Then, with probability at least 1 -δ it holds that for all</p><formula xml:id="formula_15">θ ∈ C ε , φ(D | θ)(1 -ε) ≤ φ(C | θ) ≤ φ(D | θ)(1 + ε).</formula><p>In our experiments, we compare the performance of clustering on coresets constructed via adaptive sampling, vs. clustering on a uniform sample. The size of C in Algorithm 1 depends on |B| 2 = log 2 n. By replacing B in the algorithm with a constant factor approximation B , |B | = l for the k-means problem, we can get a coreset C of size independent of n. Such a set B can be computed in O(ndk) time either by applying exhaustive search on the output C of the original Algorithm 1 or by using one of the existing constant-factor approximation algorithms for k-means (say, <ref type="bibr" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sketch of Analysis: Reduction to Euclidean Spaces</head><p>For space limitations, the proof of Theorem 3.1 is included in the supplemental material, we only provide a sketch of the analysis, carrying the main intuition. The key insight in the proof is that the contribution log P (x | θ) to the likelihood L(D | θ) can be expressed in the following way: Lemma 3.2. There exist functions φ, ψ, and f such that, for any point x ∈ R d and mixture model θ, ln</p><formula xml:id="formula_16">P (x | θ) = -f φ(x) (ψ(θ)) + Z(θ), where f x(y) = -ln i wi exp -W i dist(x -μi , s i ) 2 .</formula><p>Hereby, φ is a function that maps a point x ∈ R d into x = φ(x) ∈ R 2d , and ψ is a function that maps a mixture model θ into a tuple y = (s, w, μ, W ) where w is a k-tuple of nonnegative weights w1 , . . . , wk summing to 1, s = s 1 , . . . , s k ⊆ R 2d is a set of k d-dimensional subspaces that are weighted by weights</p><formula xml:id="formula_17">W 1 , • • • , W k &gt; 0, and μ = μ1 , • • • , μk ∈ R 2d is a set of k means.</formula><p>The main idea behind Lemma 3.2 is that level sets of distances between points and subspaces are quadratic forms, and can thus represent level sets of the Gaussian probability density function (see Figure <ref type="figure" target="#fig_3">2</ref>(a) for an illustration). We recognize the "soft-min" function ∧ w (η) ≡  -ln i w i exp (-η i ) as an approximation upper-bounding the minimum min(η) = min i η i for</p><formula xml:id="formula_18">η i = W i dist(x -μi , s i ) 2 and η = [η 1 , . . . , η k ].</formula><p>The motivation behind this transformation is that it allows expressing the likelihood P (x | θ) of a data point x given a model θ in a purely geometric manner as soft-min over distances between points and subspaces in a transformed space. Notice that if we use the minimum min() instead of the soft-min ∧ w(), we recover the problem of approximating the data set D (transformed via φ) by k-subspaces. For semi-spherical Gaussians, it can be shown that the subspaces can be chosen as points while incurring a multiplicative error of at most 1/ε, and thus we recover the well-known k-means problem in the transformed space. This insight suggests using a known coreset construction for k-means, adapted to the transformation employed.</p><p>The remaining challenge in the proof is to bound the additional error incurred by using the soft-min function ∧ w(•) instead of the minimum min(•). We tackle this challenge by proving a generalized triangle inequality adapted to the exponential transformation, and employing the framework described in <ref type="bibr" target="#b3">[4]</ref>, which provides a general method for constructing coresets for clustering problems of the form min s i f x(s).</p><p>As proved in <ref type="bibr" target="#b3">[4]</ref>, the key quantity that controls the size of a coreset is the pseudo-dimension of the functions F d = {f x for x ∈ R 2d }. This notion of dimension is closely related to the VC dimension of the (sub-level sets of the) functions F d and therefore represents the complexity of this set of functions. The final ingredient in the proof of Theorem 3.1 is a new bound on the complexity of mixtures of k Gaussians in d dimensions proved in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Streaming and Parallel Computation</head><p>One major advantage of coresets is that they can be constructed in parallel, as well as in a streaming setting where data points arrive one by one, and it is impossible to remember the entire data set due to memory constraints. The key insight is that coresets satisfy certain composition properties, which have previously been used by <ref type="bibr" target="#b5">[6]</ref> for streaming and parallel construction of coresets for geometric clustering problems such as k-median and k-means.</p><formula xml:id="formula_19">1. Suppose C 1 is a (k, ε)-coreset for D 1 , and C 2 is a (k, ε)-coreset for D 2 . Then C 1 ∪ C 2 is a (k, ε)-coreset for D 1 ∪ D 2 . 2. Suppose C is a (k, ε)-coreset for D, and C is a (k, δ)-coreset for C. Then C is a (k, (1 + ε)(1 + δ) -1)-coreset for D.</formula><p>In the following, we review how to exploit these properties for parallel and streaming computation.</p><p>Streaming. In the streaming setting, we assume that points arrive one-by-one, but we do not have enough memory to remember the entire data set. Thus, we wish to maintain a coreset over time, while keeping only a small subset of O(log n) coresets in memory. There is a general reduction that shows that a small coreset scheme to a given problem suffices to solve the corresponding problem on a streaming input <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. The idea is to construct and save in memory a coreset for every block of poly(dk/ε) consecutive points arriving in a stream. When we have two coresets in memory, we can merge them (resulting in a (k, ε)-coreset via property (1)), and compress by computing a single coreset from the merged coresets (via property (2)) to avoid increase in the coreset size. An important subtlety arises: While merging two coresets (via property (1)) does not increase the approximation error, compressing a coreset (via property (2)) does increase the error. A naive approach that merges and compresses immediately as soon as two coresets have been constructed, can incur an exponential increase in approximation error. Fortunately, it is possible to organize the merge-and-compress operations in a binary tree of height O(log n), where we need to store in memory a single coreset for each level on the tree (thus requiring only poly(dkε -1 log n) memory). Figure <ref type="figure" target="#fig_3">2</ref>(b) illustrates this tree computation. In order to construct a coreset for the union of two (weighted) coresets, we use a weighted version of Algorithm 1, where we consider a weighted point as duplicate copies of a non-weighted point (possibly with fractional weight). A more formal description can be found in <ref type="bibr" target="#b7">[8]</ref>. We summarize our streaming result in the following theorem. Theorem 3.3. A (k, ε)-coreset for a stream of n points in R d can be computed for the εsemi-spherical GMM problem with probability at least 1 -δ using space and update time poly(dkε -1 log n log(1/δ)).</p><p>Parallel/Distributed computations. Using the same ideas from the streaming model, a (nonparallel) coreset construction can be transformed into a parallel one. We partition the data into sets, and compute coresets for each set, independently, on different computers in a cluster. We then (in parallel) merge (via property ( <ref type="formula">1</ref>)) two coresets, and compute a single coreset for every pair of such coresets (via property ( <ref type="formula">2</ref>)). Continuing in this manner yields a process that takes O(log n) iterations of parallel computation. This computation is also naturally suited for map-reduce <ref type="bibr" target="#b8">[9]</ref> style computations, where the map tasks compute coresets for disjoint parts of D, and the reduce tasks perform the merge-and-compress operations. Figure <ref type="figure" target="#fig_3">2</ref>(b) illustrates this parallel construction. Theorem 3.4. A (k, ε)-coreset for a set of n points in R d can be computed for the ε-semispherical GMM problem with probability at least 1 -δ using m machines in time (n/m) • poly(dkε -1 log(1/δ) log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fitting a GMM on the Coreset using Weighted EM</head><p>One approach, which we employ in our experiments, is to use a natural generalization of the EM algorithm, which takes the coreset weights into account. We here describe the algorithm for the case of GMMs. For other mixture distributions, the E and M steps are modified appropriately. </p><formula xml:id="formula_20">w i N (x j ;µ i ,Σ i )</formula><p>w N (x j ;µ ,Σ ) ; for i = 1 to k do wi ← wi/ wi; µi ← j ηi,jx j / j ηi,j; Σi ← j ηi,j x j -µi x j -µi T / j ηi,j;</p><formula xml:id="formula_21">until L(C | θ) ≥ L old -T OL ;</formula><p>Using a similar analysis as for the standard EM algorithm, Algorithm 2 is guaranteed to converge, but only to a local optimum. However, since it is applied on a much smaller set, it can be initialized using multiple random restarts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extensions and Generalizations</head><p>We now show how the connection between estimating the parameters for mixture models and problems in computational geometry can be leveraged further. Our observations are based on the link between mixture of Gaussians and projective clustering (multiple subspace approximation) as shown in Lemma 3.2.</p><p>Generalizations to non-semi-spherical GMMs. For simplicity, we generalized the coreset construction for the k-means problem, which required assumptions that the Gaussians are ε-semispherical. However, several more complex coresets for projective clustering were suggested recently (cf., <ref type="bibr" target="#b3">[4]</ref>). Using the tools developed in this article, each such coreset implies a corresponding coreset for GMMs and generalizations. As an example, the coresets for approximating points by lines <ref type="bibr" target="#b9">[10]</ref> implies that we can construct small coresets for GMMs even if the smallest singular value of one of the corresponding covariance matrices is zero.</p><p>Generalizations to q distances and other norms. Our analysis is based on combinatorics (such as the complexity of sub-levelsets of GMMs) and probabilistic methods (non-uniform random sampling). Therefore, generalizations to other non-Euclidean distance functions, or error functions such as (non-squared) distances (mixture of Laplace distributions) is straightforward. The main property     that we need is a generalization of the triangle inequality, as proved in the supplemental material. For example, replacing the squared distances by non-squared distances yields a coreset for mixture of Laplace distributions. The double triangle inequality a -c</p><formula xml:id="formula_22">2 ≤ 2( a -b + b -c 2 ) that we used in this paper is replaced by Hölder's inequality, a -c 2 ≤ 2 O(q) a -b + 2 b -c 2 .</formula><p>Such a result is straight-forward from our analysis, and we summarize it in the following theorem. Theorem 4.1. Let q ≥ 1 be an integer. Consider Algorithm 1, where dist(•, •) 2 is replaced by dist(•, •) q and ε 2 is replaced by ε O(q) . Suppose C is sampled from D using this updated version of Algorithm 1 for parameters ε, δ and k. Then, with prob. at least 1 -δ it holds that for all θ ∈ C ε ,</p><formula xml:id="formula_23">φ(D | θ)(1 -ε) ≤ φ(C | θ) ≤ φ(D | θ)(1 + ε), where Z(θ) = i wi g(θi) and φ(D | θ) = -x∈D ln k i=1 wi Z(θ)g(θi) exp -1 2 Σ -1/2 i (x -µ i ) q using the normalizer g(θ i ) = exp -1 2 Σ -1/2 i (x -µ i ) q dx.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experimentally evaluate the effectiveness of using coresets of different sizes for training mixture models. We compare against running EM on the full set, as well as on an unweighted, uniform sample from D. Results are presented for three real datasets.</p><p>MNIST handwritten digits. The MNIST dataset contains 60,000 training and 10,000 testing grayscale images of handwritten digits. As in <ref type="bibr" target="#b10">[11]</ref>, we normalize each component of the data to have zero mean and unit variance, and then reduce each 784-pixel (28x28) image using PCA, retaining only the top d = 100 principal components as a feature vector. From the training set, we produce coresets and uniformly sampled subsets of sizes between 30 and 5000, using the parameters k = 10 (a cluster for each digit), β = 20 and δ = 0.1 (see Algorithm 1), and fit GMMs using EM with 3 random restarts. The log likelihood (LLH) of each model on the testing data is shown in Figure <ref type="figure" target="#fig_9">3(a)</ref>. Notice that coresets significantly outperform uniform samples of the same size, and even a coreset of 30 points performs very well. Further note how the test-log likelihood begins to flatten out for |C| = 1000. Constructing the coreset and running EM on this size takes 7.9 seconds (Intel Xeon 2.6 GHz), over 100 times faster than running EM on the full set (15 minutes).</p><p>Neural tetrode recordings. We also compare coresets and uniform sampling on a large dataset containing 319,209 records of rat hippocampal action potentials, measured by four co-located electrodes. As done by <ref type="bibr" target="#b10">[11]</ref>, we concatenate the 38-sample waveforms produced by each electrode to obtain a 152-dimensional vector. The vectors are normalized so each component has zero mean and unit variance. The 319,209 records are divided in half to obtain training and testing sets. From the training set, we produce coresets and uniformly sampled subsets of sizes between 70 and 1000, using the parameters k = 33 (as in <ref type="bibr" target="#b10">[11]</ref>), β = 66, and δ = 0.1, and fit GMMs. The log likelihood of each model on the held-out testing data is shown in Figure <ref type="figure" target="#fig_9">3</ref>(b). Coreset GMMs obtain consistently higher LLH than uniform sample GMMs for sets of the same size, and even a coreset of 100 points performs very well. Overall, training on coresets achieves approximately the same likelihood as training on the full set about 95 times faster (1.2 minutes vs. 1.9 hours).</p><p>CSN cell phone accelerometer data. Smart phones with accelerometers are being used by the Community Seismic Network (CSN) as inexpensive seismometers for earthquake detection. In <ref type="bibr" target="#b11">[12]</ref>, 7 GB of acceleration data were recorded from volunteers while carrying and operating their phone in normal conditions (walking, talking, on desk, etc.). From this data, 17-dimensional feature vectors were computed (containing frequency information, moments, etc.). The goal is to train, in an online fashion, GMMs based on normal data, which then can be used to perform anomaly detection to detect possible seismic activity. Motivated by the limited storage on smart phones, we evaluate coresets on a data set of 40,000 accelerometer feature vectors, using the parameters k = 6, β = 12, and δ = 0.1. Figure <ref type="figure" target="#fig_9">3</ref>(c) presents the results of this experiment. Notice that on this data set, coresets show an even larger improvement over uniform sampling. We hypothesize that this is due to the fact that the recorded accelerometer data is imbalanced, and contains clusters of vastly varying size, so uniform sampling does not represent smaller clusters well. Overall, the coresets obtain a speedup of approximately 35 compared to training on the full set. We also evaluate how GMMs trained on the coreset compare with the baseline GMMs in terms of anomaly detection performance. For each GMM, we compute ROC curves measuring the performance of detecting earthquake recordings from the Southern California Seismic Network (cf., <ref type="bibr" target="#b11">[12]</ref>). Note that even very small coresets lead to performance comparable to training on the full set, drastically outperforming uniform sampling (Fig. <ref type="figure" target="#fig_9">3(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Theoretical results on mixtures of Gaussians. There has been a significant amount of work on learning and applying GMMs (and more general distributions). Perhaps the most commonly used technique in practice is the EM algorithm <ref type="bibr" target="#b12">[13]</ref>, which is however only guaranteed to converge to a local optimum of the likelihood. Dasgupta <ref type="bibr" target="#b13">[14]</ref> is the first to show that parameters of an unknown GMM P can be estimated in polynomial time, with arbitrary accuracy ε, given i.i.d. samples from P . However, his algorithm assumes a common covariance, bounded excentricity, a (known) bound on the smallest component weight, as well as a separation (distance of the means), that scales as Ω( √ d). Subsequent works relax the assumption on separation to d 1/4 [15] and k 1/4 <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr" target="#b2">[3]</ref> is the first to learn general GMMs, with separation d 1/4 . <ref type="bibr" target="#b16">[17]</ref> provides the first result that does not require any separation, but assumes that the Gaussians are axis-aligned. Recently, <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref> provide algorithms with polynomial running time (except exponential dependence on k) and sample complexity for arbitrary GMMs. However, in contrast to our results, all the results described above crucially rely on the fact that the data set D is actually generated by a mixture of Gaussians. The problem of fitting a mixture model with near-optimal log-likelihood for arbitrary data is studied by <ref type="bibr" target="#b2">[3]</ref>, who provides a PTAS for this problem. However, their result requires that the Gaussians are identical spheres, in which case the maximum likelihood problem is identical to the k-means problem. In contrast, our results make only mild assumptions about the Gaussian components. Furthermore, none of the algorithms described above applies to the streaming or parallel setting.</p><p>Coresets. Approximation algorithms in computational geometry often make use of random sampling, feature extraction, and -samples <ref type="bibr" target="#b19">[20]</ref>. Coresets can be viewed as a general concept that includes all of the above, and more. See a comprehensive survey on this topic in <ref type="bibr" target="#b3">[4]</ref>. It is not clear that there is any commonly agreed-upon definition of a coreset, despite several inconsistent attempts to do so <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Coresets have been the subject of many recent papers and several surveys <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. They have been used to great effect for a host of geometric and graph problems, including k-median <ref type="bibr" target="#b5">[6]</ref>, k-mean <ref type="bibr" target="#b7">[8]</ref>, k-center <ref type="bibr" target="#b20">[21]</ref>, k-line median <ref type="bibr" target="#b9">[10]</ref> subspace approximation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>, etc. Coresets also imply streaming algorithms for many of these problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>. A framework that generalizes and improves several of these results has recently appeared in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have shown how to construct coresets for estimating parameters of GMMs and natural generalizations. Our construction hinges on a natural connection between statistical estimation and clustering problems in computational geometry. To our knowledge, our results provide the first rigorous guarantees for obtaining compressed ε-approximations of the log-likelihood of mixture models for large data sets. The coreset construction relies on an intuitive adaptive sampling scheme, and can be easily implemented. By exploiting certain closure properties of coresets, it is possible to construct them in parallel, or in a single pass through a stream of data, using only poly(dkε -1 log n log(1/δ)) space and update time. Unlike most of the related work, our coresets provide guarantees for any given (possibly unstructured) data, without assumptions on the distribution or model that generated it. Lastly, we apply our construction on three real data sets, demonstrating significant gains over no or naive subsampling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Final approximation B (e) Sampling distribution (f) Coreset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the coreset construction for example data set (a). (b,c) show two iterations of constructing the set B. Solid squares are points sampled uniformly from remaining points, hollow squares are points selected in previous iterations. Red color indicates half the points furthest away from B, which are kept for next iteration. (d) final approximate clustering B on top of original data set. (e) Induced non-uniform sampling distribution: radius of circles indicates probability; color indicates weight, ranging from red (high weight) to yellow (low weight). (f) Coreset sampled from distribution in (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Gaussian pdf as Euclidean distancesx 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 x 10 x 11 x 12 x 13 x 14 x 15 x 16 Tree for coreset construction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Level sets of the distances between points on a plane (green) and (disjoint) k-dimensional subspaces are ellipses, and thus can represent contour lines of the multivariate Gaussian. (b) Tree construction for generating coresets in parallel or from data streams. Black arrows indicate "merge-and-compress" operations. The (intermediate) coresets C1, .. . , C7 are enumerated in the order in which they would be generated in the streaming case. In the parallel case, C1, C2, C4 and C5 would be constructed in parallel, followed by parallel construction of C3 and C6, finally resulting in C7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Weighted EM for Gaussian mixtures Input: Coreset C, k, TOL Output: Mixture model θC L old = ∞; Initialize means µ1, . . . , µ k by sampling k points from C with probability proportional to their weight. Initialize Σi = I and wi = 1 k for all i; repeat L old = L(C | θ); for j = 1 to n do for i = 1 to k do Compute ηi,j = γi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results for three real data sets. We compare likelihood of the best model obtained on subsets C constructed by uniform sampling, and by the adaptive coreset sampling procedure.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>equivalently, C can be interpreted as prior thresholding.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research was partially supported by ONR grant N00014-09-1-1044, NSF grants CNS-0932392, IIS-0953413 and DARPA MSEE grant FA8650-11-1-7156.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric approximations via coresets</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorial and Computational Geometry -MSRI Publications</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sublinear-time approximation algorithms for clustering via random sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Czumaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Struct. Algorithms (RSA)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="226" to="256" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning mixtures of separated nonspherical gaussians</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1A</biblScope>
			<biblScope unit="page" from="69" to="92" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified framework for approximating and clustering data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Langberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 41th Annu. ACM Symp. on Theory of Computing (STOC)</title>
		<meeting>41th Annu. ACM Symp. on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smaller coresets for k-median and k-means clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kushal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On coresets for k-means and k-median clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mazumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Annu. ACM Symp. on Theory of Computing (STOC)</title>
		<meeting>36th Annu. ACM Symp. on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decomposable searching problems i: Static-to-dynamic transformation</title>
		<author>
			<persName><forename type="first">Jon</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="358" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A PTAS for k-means clustering based on weak coresets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd ACM Symp. on Computational Geometry (SoCG)</title>
		<meeting>23rd ACM Symp. on Computational Geometry (SoCG)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;04: Sixth Symposium on Operating System Design and Implementation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coresets for weighted facilities and their applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 47th IEEE Annu. Symp. on Foundations of Computer Science (FOCS)</title>
		<meeting>47th IEEE Annu. Symp. on Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The next big one: Detecting earthquakes and other rare events from community-based sensors</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mani</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)</title>
		<meeting>ACM/IEEE International Conference on Information essing in Sensor Networks (IPSN)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning mixtures of gaussians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fortieth Annual IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A two-round variant of em for gaussian mixtures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 43rd Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pac learning axis-aligned mixtures of gaussians with no separation assumption</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>O'donnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Settling the polynomial learnability of mixtures of gaussians</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Foundations of Computer Science (FOCS)</title>
		<meeting>Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polynomial learning of distribution families</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Foundations of Computer Science (FOCS)</title>
		<meeting>Foundations of Computer Science (FOCS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decision theoretic generalizations of the PAC model for neural net and other learning applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="150" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-dimensional shape fitting in linear time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="288" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CUR matrix decompositions for improved data analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">697</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coresets in dynamic geometric data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Frahling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Annu. ACM Symp. on Theory of Computing (STOC)</title>
		<meeting>37th Annu. ACM Symp. on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
