<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Algorithms for Cooperative Multisensor Surveillance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Hironobu</forename><surname>Fujiyoshi</surname></persName>
						</author>
						<author>
							<persName><roleName>FELLOW, IEEE</roleName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Diamondback Vision, Inc</orgName>
								<address>
									<postCode>20191</postCode>
									<settlement>Reston</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Chubu University</orgName>
								<address>
									<postCode>487-8501</postCode>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Algorithms for Cooperative Multisensor Surveillance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53A37817F720A48F5F6EFA6C541368A3</idno>
					<note type="submission">received October 3, 2000; revised February 25, 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active vision</term>
					<term>cooperative systems</term>
					<term>geolocation</term>
					<term>multisensor systems</term>
					<term>site security monitoring</term>
					<term>user interfaces</term>
					<term>video surveillance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CMU) has developed an end-to-end, multicamera surveillance system that allows a single human operator to monitor activities in a cluttered environment using a distributed network of active video sensors. Video understanding algorithms have been developed to automatically detect people and vehicles, seamlessly track them using a network of cooperating active sensors, determine their three-dimensional locations with respect to a geospatial site model, and present this information to a human operator who controls the system through a graphical user interface. The goal is to automatically collect and disseminate real-time information to improve the situational awareness of security providers and decision makers. The feasibility of real-time video surveillance has been demonstrated within a multicamera testbed system developed on the campus of CMU. This paper presents an overview of the issues and algorithms involved in creating this semiautonomous, multicamera surveillance system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There are immediate needs for automated surveillance systems in commercial, law enforcement, and military applications. Mounting video cameras is cheap, but finding available human resources to observe the output is expensive. Although surveillance cameras are already prevalent in banks, stores, and parking lots, video data currently is used only "after the fact" as a forensic tool, thus losing its primary benefit as an active, real-time medium. What is needed is continuous 24-hour monitoring of surveillance video to alert se-curity officers to a burglary in progress, or a suspicious individual loitering in the parking lot, while there is still time to prevent the crime.</p><p>There is growing interest in developing automated video understanding algorithms to provide this constant vigilance. Automated video surveillance addresses real-time observation of people and vehicles within a busy environment, leading to a description of their actions and interactions (e.g., <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b11">[12]</ref>). Large research projects devoted to video surveillance research have been conducted in the United States (e.g., DARPA's Video Surveillance and Monitoring (VSAM) project <ref type="bibr" target="#b12">[13]</ref>), Europe (the ESPRIT PASSWORDS <ref type="bibr" target="#b13">[14]</ref>, AVS-PV <ref type="bibr" target="#b14">[15]</ref> and VIEWS <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> projects) and Japan (the Cooperative Distributed Vision project <ref type="bibr" target="#b17">[18]</ref>). Automated surveillance has also been the topic of recent international workshops <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref> and special sections in journals <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In addition to the obvious security and traffic monitoring applications, other diverse uses are possible, including compiling consumer demographics in shopping malls, logging routine maintenance tasks at nuclear facilities, monitoring livestock, and segmenting moving objects from commercials to provide hooks for user interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multisensor Surveillance</head><p>In realistic surveillance scenarios, it is impossible for a single sensor to see all areas at once, or to visually track a moving object for a long period of time. Objects become occluded by trees and buildings and sensors themselves have limited fields of view. A promising solution to this problem is to use a network of video sensors to cooperatively monitor all objects within an extended area and seamlessly track individual objects that cannot be viewed continuously by a single sensor alone. Some of the technical challenges within this approach are to: 1) actively control sensors to cooperatively track multiple moving objects; 2) fuse information from multiple sensors into scene-level object representations; 3) mon- itor the scene for events and activities that should "trigger" further processing or operator involvement; and 4) provide human users with a high-level interface for dynamic scene visualization and system tasking.</p><p>Within the context of the DARPA VSAM project, we have developed an end-to-end, multicamera surveillance system that allows a single human operator to monitor activities in a cluttered environment using a distributed network of active video sensors. Central to our design philosophy is the notion of "smart" sensors that are independently capable of performing real-time, autonomous detection of objects and events (see Fig. <ref type="figure" target="#fig_0">1</ref>). Sensors are modular units that can be added or removed without affecting the other sensors in the network. Each sensor performs real-time video processing to digest incoming video streams into symbolic descriptions of objects and events. The current suite of video understanding algorithms running on each sensor includes moving object detection, object tracking (including active tracking by sensors having active pan/tilt/zoom control), classification of detected moving blobs into semantic categories such as human and vehicle, and identifying simple human motions such as walking and running (see Section II).</p><p>Surveillance research typically employs a single sensor to locate and track objects in the field of view, with detections indicated to the observer through graphical annotations (e.g., bounding boxes) on the video stream. However, when multiple, active sensors are used in a cooperative mode, more sophisticated surveillance capabilities and user interaction are both possible and necessary. One of the goals of the VSAM project is to alter user interaction with a surveillance system from image-space to scene-space and from sensor-specific commands such as "pan sensor A to position B" toward higher level task commands such as "alert me when a delivery vehicle enters the East parking lot." To achieve interactivity at this level requires system-level algorithms that fuse sensor data, task sensors to perform autonomous cooperative behaviors, and display results to the operator in a comprehensible form (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Data Fusion: Data from disparate sensors is integrated within a central three-dimensional (3-D) scene coordinate system to provide a complete representation of the union of all objects seen from all cameras. Every object observation from each sensor is mapped from the camera-centric image-space of the sensor into 3-D geodetic coordinates (latitude, longitude and elevation) through a process called geolocation. Geolocated object observations are compared to current 3-D object hypotheses maintained by the system using viewpoint-independent features and objects that match are conjoined to form an updated hypothesis (see Section III).</p><p>Sensor Tasking: An outdoor surveillance system must optimize the use of its limited sensor assets. Sensors must be allocated to perform all user-specified tasks and, if enough sensors are present, to gather redundant observations. An arbitration function determines the cost of assigning each sensor to each of the tasks, based on task priority, the load on each sensor, and visibility of the objects from each sensor. The system performs a greedy optimization of the cost to determine which pairing of sensors and tasks maximizes overall system performance requirements. Through this mechanism, objects can be tracked long distances by handing off between cameras situated along the object's trajectory (see Section IV).</p><p>Scene Visualization: A single human operator cannot effectively monitor a large area by looking at dozens of monitors showing raw video output. That amount of sensory overload virtually guarantees that information will be ignored and requires a prohibitive amount of transmission bandwidth. Our approach is to provide an interactive, graphical user interface (GUI) showing a synthetic view of the environment, upon which the system displays dynamic agents representing people and vehicles. This approach has the benefit that visualization of scene events is no longer tied to the original resolution and viewpoint of a single video sensor and the operator can therefore infer proper spatial relationships between multiple objects and scene features (see Section V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Surveillance Testbed</head><p>We have built a testbed system to demonstrate how multiple sensors using automated video understanding to the user on a map-based GUI. The data is also accessible through a set of distributed visualization nodes (VIS). Each component of the testbed system architecture is described briefly below.</p><p>Each SPU consists of a camera [Fig. <ref type="figure" target="#fig_1">2</ref>(c)] paired with a processor to form a smart sensor that acts as an intelligent filter between a video signal and the VSAM network. The SPU analyzes raw video imagery to extract objects and events and transmits that detected information symbolically to the OCU. Performing as much video processing as possible on the SPU greatly reduces the bandwidth requirements of the network. This arrangement also allows for many different sensor modalities to be seamlessly integrated into the system. For example, we have integrated color sensors with active pan/tilt/zoom control, fixed field of view monochrome sensors, a Cyclovision omni-directional sensor <ref type="bibr" target="#b25">[26]</ref>, thermal sensors, a van-mounted relocatable sensor system, an indoor video event detection system developed by Texas Instruments <ref type="bibr" target="#b26">[27]</ref> and an airborne sensor platform <ref type="bibr" target="#b27">[28]</ref>, all using the same communication protocol. We have even prototyped man-portable SPUs that can be placed, calibrated and connected to the system in only a few minutes.</p><p>The OCU [Fig. <ref type="figure" target="#fig_2">2(d)</ref>] integrates symbolic object information from the SPUs with a 3-D site model to determine object locations. The OCU supports one GUI [Fig. <ref type="figure" target="#fig_2">2(e)</ref>] through which all user-related command and control information is passed. The GUI contains a map of the site, overlaid with all object locations, sensor platform locations and sensor fields of view. In addition, a low-bandwidth, compressed video stream from one of the sensors can be selected for real-time display. The OCU schedules sensor tasks to perform cooperative multicamera surveillance. The operator can task individual sensor units through the GUI, as well as instructing the entire testbed sensor suite to perform surveillance operations such as generating a quick summary of all object activities in the area.</p><p>VIS nodes [Fig. <ref type="figure" target="#fig_1">2</ref>(f)] are designed to distribute surveillance results to remote users by providing graphical representations of detected activities overlaid on maps or 3-D synthetic views. We have developed a Java-based visualization client that can be played on any laptop connected to the VSAM system network. This two-dimensional (2-D) map display maintains much of the character of the operator GUI, but without the ability to control the system. We have also interfaced to ModSAF and ModStealth, which are 2-D and 3-D scene viewers developed within the context of Synthetic Training Environments <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. See Section V.</p><p>Prior knowledge of the terrain and important scene features is represented within a 3-D site model. Some of the surveillance tasks supported by scene-specific knowledge provided by the site model are: 1) computation of object location by intersecting viewing rays with the terrain <ref type="bibr" target="#b30">[31]</ref>; 2) landmark-based calibration of camera exterior orientation <ref type="bibr" target="#b31">[32]</ref>; 3) visibility analysis to predict what portions of the scene are visible from which cameras, thereby improving tracking <ref type="bibr" target="#b32">[33]</ref> and allowing more effective sensor tasking; 4) geometric focus of attention, for example to task a sensor to monitor the door of a building, or specify that vehicles should appear on roads; 5) visualization of the scene to enable quick comprehension of geometric relationships between sensors, objects and scene features and 6) simulation for planning best sensor placement and for debugging algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VIDEO UNDERSTANDING TECHNOLOGIES</head><p>A multicamera surveillance system is built upon the basic capabilities provided by each sensor. At a minimum, each sensor must be able to detect moving objects from raw video at nearly frame-rate. This section provides an overview of the video understanding algorithms implemented within the VSAM testbed to detect moving objects, track them through a video sequence, classify them into semantic categories (e.g., human and vehicle) and analyze human motions such as walking and running. These descriptions are very brief in order to devote more space to multisensor aspects of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Moving Object Detection</head><p>Detection of moving objects in video streams is known to be a significant and difficult research problem <ref type="bibr" target="#b33">[34]</ref>. Conventional approaches to moving object detection include temporal differencing <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>; background subtraction <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> and optical flow <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>. One of the most successful approaches to date is adaptive background subtraction <ref type="bibr" target="#b36">[37]</ref>. The basic idea is to maintain a running statistical average of the intensity at each pixel. When the value of a pixel in a new image differs significantly from average, the pixel is flagged as potentially containing a moving object. One problem of this approach, along with the other conventional approaches to motion detection, is that objects that cease moving within the image simply disappear from the representation. A robust detection system should continue to "see" objects that have stopped and disambiguate between overlapping objects in the image. For example, a car that comes into the scene and parks should not be considered as part of the scene background, however its stationary pixels should play the role of background for detecting motion of a person getting out of the car.</p><p>We have developed a novel approach to object detection based on layered adaptive background subtraction. Layered detection is based on two processes: pixel analysis and region analysis. Pixel analysis determines whether a pixel is stationary or transient by observing its intensity value over time. The technique is derived from the observation that legitimately moving objects in a scene cause much faster intensity transitions than changes due to lighting or weather. Fig. <ref type="figure" target="#fig_3">3</ref>(a) graphically depicts the process. To capture the nature of changes in pixel intensity profiles, two factors are important: the existence of a significant step change in intensity and the intensity value to which the profile stabilizes after passing through a period of instability. An object moving through the pixel displays a profile that exhibits a step change in intensity, followed by a period of instability, then another step back to the original background intensity. An object moving to the pixel and stopping displays a profile that exhibits a step change in intensity, followed by a period of instability, then a step to a new intensity as the object stops. Lighting and weather effects tend to cause smooth changes with no large steps. Therefore, by observing the intensity transitions at each pixel, different intensity layers connected by transient periods can be postulated.</p><p>Region analysis collects groups of labeled pixels into moving regions and stopped regions and assigns them to spatio-temporal layers [Fig. <ref type="figure" target="#fig_3">3(b)</ref>]. Regions that consist of stationary pixels are added as a layer over the background, or over a previously determined layer. Regions consisting of moving pixels are represented as transient objects that occlude all layers. A layer management process that operates much like the window manager on a modern workstation is responsible for creating and deleting new regions, updating intensity information and keeping track of depth ordering between overlapping regions. Pixel values in stationary layered regions and the scene background are updated by an Infinite Impulse Response (running average) filter to accommodate slow lighting changes and noise in the imagery, as well as to compute statistically significant step-change thresholds.</p><p>An additional mechanism is built into this algorithm to detect sharp changes in the overall scene, caused by motion of the camera (which is mounted on a pan/tilt head) or occasional sharp lighting changes (e.g., the sun comes out from behind cloud cover). If a majority of the image pixels are found to be changing, the detection algorithm tem-porarily shuts down until the view stabilizes, as determined by a simple two-frame differencing algorithm. At this point, all pixel statistics are reinitialized and the detection algorithm resumes.</p><p>This detection algorithm has been evaluated on 4 h of video tape for which ground-truth labeling of moving objects (people and vehicles) was manually determined. 2 h of data were taken on a sunny day and 2 h on a cloudy day. Probability of detection was determined as the percentage of human-detected moving objects that were also detected by the system. The detection rate was 89.6% for sunny day data and 94.5% for cloudy day data. The main reason for failure to detect was low image contrast between the moving object and the background. Sunny day detection rates are lower because of the additional loss of image contrast in areas of deep shadow. False positive detection rates were not recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Tracking</head><p>To begin building a temporal model of activity, individual object blobs generated by motion detection are tracked over time by matching them between frames of the video sequence. Among the many approaches to tracking are model-based matching <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, image contour matching <ref type="bibr" target="#b43">[44]</ref> and image region matching <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Multiple potential matches typically arise, which can be disambiguated using statistical data association techniques <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> or by imposing smooth trajectory motion models using Kalman filters <ref type="bibr" target="#b48">[49]</ref>.</p><p>Our approach lies squarely in the image region matching camp. Given a moving object region in a current frame, we determine the best match in the next frame by performing image correlation matching, computed as the normalized cross correlation of the object's intensity template over candidate regions in the new image <ref type="bibr" target="#b49">[50]</ref>. Due to real-time processing constraints in the VSAM testbed system, this basic correlation matching algorithm is only computed for "moving" pixels, regions are culled that are inconsistent with current estimates of object position and velocity and imagery is dynamically subsampled to ensure a constant computation time per match.</p><p>In the spirit of <ref type="bibr" target="#b43">[44]</ref> the tracker maintains multiple match hypotheses with varying degrees of matching confidence and can merge and split hypotheses as appropriate to describe objects that temporarily occlude each other as they pass. Any object that has not been matched maintains its position/velocity estimates and current image template, but its confidence is reduced. If the confidence of any object drops below a given threshold, it is considered lost and is dropped from the list. High confidence objects (ones that have been tracked for a reasonable period of time) will persist for several frames, so if an object is momentarily occluded but then reappears, the tracker will reacquire it. More details can be found in <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Some sample trajectories resulting from this approach are shown in Fig. <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Type Classification</head><p>Bottom-up motion detection and tracking algorithms (which do not try to fit a priori models to image data) view objects in the scene as moving blobs of pixels. Object classification routines begin to add semantics to these observations by providing class labels for each blob <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. We have developed two algorithms for view-dependent visual object classification. The first is a neural network classifier, trained for each sensor view <ref type="bibr" target="#b54">[55]</ref> (see also <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> for additional neural network approaches to object classification). The neural network is a standard three-layer network, trained using the backpropagation algorithm. Input features to the network are measured directly from the image blob and camera settings: blob dispersedness (perimeter area); blob area; blob aspect ratio and camera zoom value. There are four output classes: single human; human group; vehicle and clutter. This neural network classification approach is fairly effective for single image frames; however, one of the advantages of video is its temporal component. To exploit this, classification is performed on each blob as it is tracked through the sequence of frames. The classification results for each frame are kept in a histogram and at each time step, the most likely class label for the blob is chosen based on all classifications that have been made for it. Sample results are shown in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>A second method of view-dependent object classification uses linear discriminant analysis to provide a finer distinction between vehicle types (e.g., van, truck, sedan). This method has also been successfully trained to recognize specific types of vehicles, such as UPS trucks and campus police cars. The method has two submodules: one for classifying object shape and the other for determining color. The latter is needed because the color of an object is difficult to determine under varying outdoor lighting. Each submodule computes an independent discriminant classification space using linear discriminant analysis (LDA) and calculates the most likely class in that space using a weighted k-class nearest-neighbor (k-NN) method <ref type="bibr" target="#b57">[58]</ref>. In LDA, feature vectors computed on training examples of different object classes are considered to be labeled points in a high-dimensional feature space. Training examples are mapped into shape space as an 11-dimensional feature vector computed from the motion blob: area; center of gravity; width; height and first, second, and third moments taken along the row and column pixel axes. Color space is three dimensional, with features R G B ; R B and G R B computed from RGB values of pixels within the motion blob. Given training points in these feature spaces, LDA computes a set of discriminant functions, formed as linear combinations of feature values, that best separate the clusters of points that correspond to different object labels and color labels. See <ref type="bibr" target="#b58">[59]</ref> for more details. Some sample results are shown in Fig. <ref type="figure" target="#fig_6">5</ref>.</p><p>Table <ref type="table">1</ref> shows cross validation between targets (columns) and classified results (rows) gathered from 4 h of hand-labeled video data. The recognition rate was roughly 90% across both sunny and cloudy weather conditions. (Note: a "Mule" is a golf-cart-like vehicle used by campus mainte-nance workers). Currently, the system does not work well when it is raining or snowing, because the raindrops and snowflakes interfere with the measured RGB values in the images. For the same reason, the system does not work well in early mornings and late evenings, due to the nonrepresentativeness of the lighting conditions. The algorithm is also foiled by backlighting and specular reflection from vehicle bodies and windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Human Motion Analysis</head><p>Classifying moving objects enables a surveillance system to subsequently invoke object-specific motion analysis methods to generate more detailed descriptions of object behavior. There has been considerable interest in the area of human motion tracking in recent years <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b66">[67]</ref>. More references can be found in <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Many human motion tracking algorithms assume that the size of the person in the image is large enough to track individual limbs. On the other hand, many surveillance applications involve more distant observations and a subsequent smaller number of "pixels on target."</p><p>We have developed a "star" skeletonization procedure for analyzing human gaits in these situations <ref type="bibr" target="#b69">[70]</ref>. The key idea is that simple, fast extraction of the broad internal motion features of an object can be employed to analyze its motion. The star skeleton consists of the centroid of a motion blob and all of the local extremal points that are recovered when traversing the boundary [see Fig. <ref type="figure" target="#fig_7">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Experimental Evaluation of LDA Classification the posture of the running person can easily be distinguished from that of the walking person (people lean forward when they run). Also, the frequency of cyclic motion of the leg segments provides cues to whether this is a walking or running gait.</p><p>Gait classification using star skeleton features has been tested on a set of video sequences of adults and children walking and running. There are approximately 20 video sequences in each category, with pixels on target ranging from 50 to 400. Star skeletons and values for and were extracted from the video at a frame rate of 8 Hz. The average walking frequency was found to be 1.75 (Hz) and for running 2.875 (Hz). A threshold frequency of 2.0 (Hz) correctly classifies 97.5% of the gaits. Note that these frequencies are twice the actual footstep frequency because only the leftmost leg segment is considered. For each video sequence, the average inclination of the torso showed that the forward leaning torso of a running figure can be clearly distinguished from the more vertical torso of a walking one. A threshold value of 0.15 rad correctly classifies 90% of the gaits. More details can be found in <ref type="bibr" target="#b69">[70]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTISENSOR DATA FUSION</head><p>A multisensor surveillance system is more than a collection of sensors acting independently to detect and track objects. At some point, all observations must be brought into a common frame of reference to form a coherent, dynamic scene representation. This scene representation should be complete, in that it contains the union of all observations made by all sensors. Multiple observations of the same object from different cameras should be identified and merged into a single, more accurate object description. The representation should make explicit the spatial relationships between sensors, objects, and scene features, to aid sensor tasking and visualization of the scene by the human operator.</p><p>We believe that bringing all observations into a common 3-D coordinate system is the key to coherently representing a large number of object hypotheses from multiple, widely spaced sensors. We choose geodetic coordinates as this common coordinate system. In contrast to all other surveillance systems that we know of, which work in an arbitrary local scene coordinate system, we compute the latitude, longitude, and elevation with respect to the WGS84 datum (so-called "GPS coordinates" <ref type="bibr" target="#b70">[71]</ref>), of each person and vehicle we detect and track. Since geometric computations can be difficult in the spherical geodetic coordinate system, our internal computations are carried out within a set of local Cartesian frames defined throughout the site; however, all of these Cartesian frames have known transforms to and from geodetic coordinates. We believe that having precise knowledge of where objects are in the world, represented in a commonly agreed upon global (in the literal sense of the word) coordinate system, is a significant advantage. For example, this choice has allowed us to easily merge our ground-base surveillance results with hypotheses generated independently by an airborne sensor operated by The Sarnoff Corporation and the U.S. Army's Night Vision and Electronic Sensors Directorate <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b71">[72]</ref>. It also has allowed us to use third-party cartographic software and datasets such as United States Geological Survey maps, orthophotos, DEMS, and road network graphs, in the development of our site model <ref type="bibr" target="#b71">[72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Camera Calibration</head><p>A mapping between sensor coordinates and scene coordinates is determined by calibrating each sensor with respect to the geodetic coordinate system. We have developed methods for fitting a projection model consisting of intrinsic (lens) and extrinsic (pose) parameters of a camera with active pan, tilt and zoom control. Intrinsic parameters are calibrated by fitting parametric models to the optic flow induced by rotating and zooming the camera. This procedure is fully automatic and does not require precise knowledge of 3-D scene structure. Extrinsic parameters are calculated by sighting a sparse set of scene landmarks that have been surveyed using differential GPS <ref type="bibr" target="#b70">[71]</ref>. Actively rotating the camera to measure landmarks over a virtual hemispherical field of view leads to a well-conditioned exterior orientation estimation problem. Details of the calibration procedures are presented in <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Geolocation</head><p>In regions where multiple sensor viewpoints overlap, object locations can be determined by wide-baseline triangulation <ref type="bibr" target="#b72">[73]</ref>. However, regions of the scene that can be simultaneously viewed by multiple sensors are likely to be a small percentage of the total area of regard in real outdoor surveillance applications. Determining object locations from a single sensor requires domain constraints, in this case the assumption that the object is in contact with the terrain. This contact location is estimated by passing a viewing ray through the bottom of the object in the image and intersecting it with a model representing the terrain [see Fig. <ref type="figure" target="#fig_9">7(a)</ref>].</p><p>Previous uses of the ray intersection technique for object localization in surveillance research have been restricted to small areas of planar terrain, where the relation between image pixels and terrain locations is a simple 2-D homography <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>. This has the benefit that no camera calibration is required to determine the backprojection of an image point onto the scene plane, provided the mappings of at least four coplanar scene points are known beforehand. However, large outdoor scene areas may contain significantly varied terrain. To handle this situation, we perform geolocation using ray intersection with a full terrain model provided by a georeferenced digital elevation map (DEM). A simple geometric traversal technique based on the well-known Bresenham algorithm <ref type="bibr" target="#b75">[76]</ref> for drawing rasterized line segments is used. Consider the vertical projection of the viewing ray ) containing the sensor, each cell ( ) that the ray passes through is examined in turn, progressing outward, until the elevation stored in that DEM cell exceeds the component of the 3-D viewing ray at that location. See <ref type="bibr" target="#b30">[31]</ref> for more details.</p><p>Since geolocation estimates are computed by backprojecting the center of the lowest side of the bounding box enclosing a moving blob, the surveillance system maintains a running estimate of the variance of this point. An internal estimate of horizontal variance of the geolocated point is formed by propagating this variance from the image, through the inverse projection equations, onto a horizontal plane with an elevation corresponding to the value of the 3-D geolocation estimate. This in general yields a covariance matrix with elliptical contours. A simplified uncertainty representation consisting of a single variance value is formed by taking the trace of this covariance matrix and dividing by 2. The horizontal ( ) location of a point, along with its approximate variance as computed above, is called the "map-plane" coordinate of a point in Section III-C.</p><p>We have evaluated geolocation accuracy for two cameras on the CMU campus using a Leica laser-tracking theodolite to generate ground truth (see Fig. <ref type="figure" target="#fig_10">8</ref>). The experiment was run by having a person carry the theodolite prism for two loops around the parking lot, while the system logged time-stamped horizontal ( ) locations measured by the theodolite. The system also simultaneously tracked the person using each camera, while logging time-stamped geolocation estimates. Standard deviations of errors between ground truth locations and geolocation estimates from each camera are roughly on the order of .6 m along the axis of maximum spread and roughly .25 m at minimum, over an average camera-object distance of 65 m. The axis of maximum error for each camera is oriented along the direction vector from the camera to the object being observed. For more details, see <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Fusion</head><p>Sensor data fusion is a classic subject in engineering <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>. In our case, the central idea of the data fusion process is to make associations between image-based sensor observations and scene-based object hypotheses. The central operator control unit (OCU) receives a continual stream of time-stamped symbolic object observations from each remote sensor processing unit (SPU). Each observation is geolocated, as described previously and compared to a list of known object hypotheses to see if it is an observation of an object already being tracked by the system. However, because the SPU sensors are scattered widely throughout the scene, one may be viewing the front of an object, one the side, and another the top. Therefore, comparison of SPU observations to 3-D hypotheses needs to use features that are insensitive to viewpoint <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b78">[79]</ref>. We use three features: 1) object geolocation (Section III-B); 2) object class (Section II-C); and 3) color. Color is represented by three coarse color histograms (red, green, and blue), concatenated into a single vector and normalized so that the sum over all counts is 1. This has the effect of normalizing the resulting color histogram, so that the representation is less sensitive to color variations due to viewpoint, illumination and sensor color response <ref type="bibr" target="#b79">[80]</ref>.</p><p>Observations received by the OCU are processed in timestamped order. Features of a new observation are compared against the features stored for each existing object hypothesis using a match score function. Two situations may arise.</p><p>• The observation does not match any known object (match score is below a threshold for all hypotheses). In this case, a new object is hypothesized and its location, class and color information are initialized to those provided by the observation. A confidence value stored with the hypothesis is set to a nominal low value. • The observation matches at least one object hypothesis (match score exceeds a threshold for one or more hypotheses). The object hypothesis with the highest match score value is chosen as the best match. The feature values of the SPU observation are used to update the features of this object hypothesis and the confidence of the object is increased. A separate mechanism culls objects that leave the field of regard of the entire system. Any 3-D object hypothesis that is not matched and updated for 2 s is flagged as inactive. After ten more seconds pass with no activity, the hypothesis is declared dead and removed from the list of known hypotheses.</p><p>Computing the Match Score: Match score between an incoming observation and a known object hypothesis is computed by comparing location, class, and color information match location class color</p><p>To compare location information, the geolocation technique described previously is used to transform the location measurement of the sensor observation into a "map plane" 2-D location measurement and an associated variance , which corresponds to a circular Gaussian covariance matrix of the form . In the following equations, all covariances are approximated as circular Gaussians of the form and the uncertainty propagation equations are simplified by using a single variance weight . The underlying mechanism, however, is propagation of 2-D Gaussian covariances, as in <ref type="bibr" target="#b80">[81]</ref>, with any resulting 2-D covariance matrix being approximated as Trace . For a given object hypothesis with a sequence of previous map plane trajectory points (</p><p>) and variances ( ), the OCU predicts a new 2-D location and variance for the hypothesis assuming a constant velocity linear trajectory and where is a function of the timestamps (</p><p>) on the sequence of processed video frames from which the hypotheses were generated.</p><p>The prediction and the current observation are then tentatively merged into a joint sample point with variance This is treated as a statistical distribution on and the score location of matching the SPU observation with object hypothesis is taken to be proportional to the joint probability of two independent samples and drawn from the distribution for location Note that this is not a proper probability since the bivariate Gaussian normalization constant has been dropped.</p><p>To compute class , a simple heuristic function is used. The current classifications supported by the match score function are: vehicle, human, human group, and unclassified (if the classification algorithm does not have enough data to make a suggestion). Given the classification of an object hypothesis and the classification of the new observation , the heuristic score class is computed as follows: If , the value is 1.0; if either or is unclassified, the value is 0.75; if is human and is human group, or vice versa, the value is 0.6; otherwise, the value is 0. Color comparison between a new observation and an existing object hypothesis is based on their color histogram vectors. The value of color is taken as med where is the set of differences between the color vectors of the two objects.</p><p>Feature Updating: When a new observation matches an object hypothesis already known to the system, the features of the hypothesis are updated by merging the new information from the observation with the previously stored values in the hypothesis. The new location and variance estimates for the matched hypothesis become and . The hypothesis class is updated to be the most frequent classification given to that hypothesis so far, as determined by a histogram of associated classifications (this is the same mechanism used to improve temporal classification performance at the SPU). The color vector of the object hypothesis is simply replaced with the color vector from the new observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MULTISENSOR TASKING AND CONTROL</head><p>An important goal of VSAM is to enable a single human operator to task a multisensor surveillance system at a relatively high level of abstraction. Traditional camera-centric commands such as "pan sensor A to position B" become cumbersome when many sensors are available, and it is nearly impossible for a person to orchestrate control strategies by commanding multiple sensors in a specific temporal order. We seek instead to issue high-level requests such as "track this car" or "report any red sedans that enter the gate," and have the system (specifically the OCU) decompose them into a sequence of low-level commands issued to the appropriate sensors at the correct times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sensor Tasking</head><p>High-Level Tasking: Through the GUI, the operator can specify objects to be actively tracked and geographic locations to monitor for events. The operator can choose to operate in either an image-centric or scene-centric coordinate system. For example, the user can choose to see video imagery from one of the sensors and can click on an image feature to task the system to control the sensor pan and tilt to bring that feature into the center of the image. If the user clicks on a moving object within the video display, the system is automatically tasked to actively track that object.</p><p>More interesting and novel, is the ability of the operator to specify operations in scene space using a map overlaid with sensor locations and moving icons representing tracked objects. For example, the user can click on a sensor location and drag the mouse to a point on the map, which tasks the sensor to look at that geographic location in the scene. The user can also click on any of the moving object icons displayed on the GUI and the system will allocate resources to continually track that object. Another high-level tasking command is to specify a region of interest (ROI) event trigger. To specify a ROI, the user traces the outline of a polygonal region on the map and the OCU then determines which sensors have the best view of that area and assigns them to observe it. Any object entering the ROI triggers an alert to the operator. The operator can also specialize a ROI trigger to particular classes of objects (e.g., human or vehicle) and the system will provide an alert only when that type of object enters. For example, it is easy to task the system to "report all pedestrians entering this restricted area" by creating an appropriate ROI trigger. Given suitably sophisticated video understanding algorithms at each sensor, ROI triggers could also be specialized to specific events or activities occurring in a geographic location.</p><p>Sensor Arbitration: The GUI sends high-level user requests to the OCU, which keeps a list of all tasks currently being handled by the system. Each SPU has only a few basic capabilities, such as and . The OCU must therefore "compile" high-level user requests into a sequence of sensor specifications and commands. This compilation mainly involves choosing a sequence of sensors to carry out the task and commanding them to look at the appropriate scene locations at the right times. Sensor-specific pan, tilt, and zoom commands associated with a particular object or ROI are computed by the OCU using the known calibration parameters of the sensor and the known geometry of the object and scene.</p><p>The hard part of automated sensor tasking is allocating sensors to perform all of the tasks required by the system and requestedbytheoperator.Whentherearenotenoughresources to complete all required tasks, the system performance should degrade gracefully. When there are relatively few tasks, the system should automatically exploit redundant sensors to provide as much information as possible. Automated sensor selection for each system task is performed according to a cost matrix. At every iteration of system time, each sensor is assigned a cost of performing each task and a greedy strategy is used for assigning tasks to sensors. The factors that contribute to the cost function are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Visibility</head><p>: This is a binary measure (0 or 1) indicating whether sensor can view the geographic location associated with task (1 indicates visible). Visibility is determined using the geometric site model to determine if there are any significant occluding objects between the camera and the point of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Distance</head><p>: This is the distance between sensor and the location of task .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Tasking</head><p>: This is a binary measure (0 or 1) indicating whether sensor is currently executing operations involved in task (0 indicates already tasked).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Priority</head><p>: This is the priority of the task assigned by the user (a low value indicates a high priority). Tasks with low priority, such as scanning the area for moving objects, become default tasks that are performed whenever a camera would otherwise be idle. These default tasks are preempted whenever a higher priority task, such as tracking a particular object, are scheduled.</p><p>A matrix of cost values is created where each value represents the cost of sensor performing task . The cost function is where the s are tuning constants. If is zero, it indicates that the task cannot be performed by the sensor. Otherwise, a small nonzero value indicates a desirable tasking. The assignment of tasks to sensors at each time step is scheduled as follows.</p><p>• If a user has taken direct control of a sensor, remove it from consideration. • For each task, select the sensor that has minimum nonzero cost to perform it and remove that sensor from the list of untasked sensors. • After all tasks are assigned, any sensors left untasked are assigned to perform their minimum cost task. These sensors thus provide redundancy in carrying out that task. This arbitration scheme automatically allocates sensors such that: 1) high priority tasks are performed at the expense of less important ones; 2) no sensors are ever idle; and 3) sensors with better viewpoints of a particular area are favored over those farther away. Note that arbitration does not simply choose the closest sensor-a more distant sensor can be selected if the closest sensor is occluded or busy with a higher priority task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multisensor Cooperative Control</head><p>If a sensor is actively tracking an object that is moving out of its field of view and into the field of view of another sensor, the cost associated with having the first sensor track it will increase, while the cost associated with the second sensor will decrease, until the point where the second sensor will automatically be tasked to take over the surveillance. Thus, cost-based sensor arbitration allows the OCU to automatically coordinate multiple sensors to seamlessly track moving objects over an extended area. Just before the hand-off between sensors occurs, the second sensor is commanded by the OCU to point in the right direction at the right zoom factor. The OCU then issues a command, passing the estimated object image location to the SPU along with a target description that uses the same view-independent classification and color features used for data fusion matching. The SPU compares all moving objects in its field of view with the same matching function used in data fusion comparison (Section III-C)  A second form of sensor cooperation is sensor slaving. This is not a side effect of cost-based tasking, as hand-off is, but is instead a special, user-selectable task programmed into the system. A camera slaving system has one master camera and at least one slave camera. The master camera is set to have a wide-angle view of the scene so that it can track all objects in a wide area without moving. The object trajectories generated by the master camera are sent to the OCU, where they are converted into 3-D trajectories. After estimating the 3-D location of an object from the first camera's viewpoint, the OCU transforms the location into a pan-tilt command to control the slave camera. The slave camera, which is highly zoomed in, can then follow the trajectory to generate close-up imagery of the object. If more than one object is detected by the master camera at one time, the OCU will "multitask" the slave camera to cycle through the objects, visiting each one in turn for one second to generate an updated close-up view. Fig. <ref type="figure" target="#fig_12">10</ref> shows an example of camera slaving. A person has been detected and is being tracked in the wide-angle view shown in the left image. A second narrow field of view camera is continually tasked by the OCU to move slightly ahead of the person's estimated 3-D trajectory to generate a close-up view, as shown in the right image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DYNAMIC SCENE VISUALIZATION</head><p>Keeping track of people, vehicles, and their interactions over a large area is a difficult job for a human observer. It certainly cannot be done effectively by looking at a wall of video screens each showing a disparate sensor view. Our approach is to provide an interactive, graphical visualization by automatically placing dynamic agents representing people and vehicles into a synthetic view of the environment. This graphical approach to operator display has the benefit that visualization of an object is no longer tied to the original resolution and viewpoint of the video sensor, since a synthetic replay of the dynamic events can be constructed from any perspective. Particularly striking is the amount of data compression that can be achieved by transmitting only symbolic object information instead of raw video data. Currently, we can process NTSC color imagery with a frame size of 320 240 pixels at ten frames per second on a Pentium II computer, so that data is streaming into the system at a rate of roughly 2.3 Mb/s per sensor. After automated video processing, detected object hypotheses contain information about object type, location, and velocity, as well as measurement statistics such as a time stamp and a description of the sensor (current pan, tilt, and zoom, for example). Each object data packet takes up roughly 50 bytes. Therefore, with our current communication protocol, a sensor tracking three objects for 1 s at ten frames per second ends up transmitting 1500 bytes back to the OCU, well over a thousandfold reduction in data bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Map-Based Operator GUI</head><p>The human operator interacts with the VSAM system through a single, map-based graphical user interface (GUI). The GUI is dominated by a scalable and scrollable georeferenced map of the site, overlaid with all current object locations, camera locations, and camera fields of view (see Fig. <ref type="figure" target="#fig_13">11</ref>). Each of these graphical entities can be selected using the workstation's mouse. To the right of the map is a status pane, showing how many sensors are active, how many objects are currently being tracked, and other vital statistics of the system. The lower right pane is a low-bandwidth, compressed video stream from one of the cameras, which can be selected by the user. The video from that sensor is compressed by transmitting a spatially subsampled version of the adaptive background model used for motion detection (Section II-A), updated once every few seconds, overlaid with spatially subsampled pixels from within the bounding box of detected moving objects, updated at 10 Hz. These moving foreground objects overlaid on the background can also be selected using the mouse. The lower left pane of the GUI is the sensor-suite tasking interface, through which the operator can task individual sensor units, as well as the entire testbed, to perform surveillance operations such as generating a quick summary of all object activities in the area or creating a region of interest event trigger (Section IV-A). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Immersive 3-D Visualization</head><p>Ultimately, we believe that the key to comprehending large-scale, multiagent events is a full, 3-D immersive visualization that allows the human operator to move at will through the environment to view dynamic events unfolding in real-time from any viewpoint. This goal guided our selection of the Compact Terrain Database (CTDB) format for representing our 3-D model of the surveillance site. The CTDB format was originally designed to represent large expanses of terrain for U.S. Department of Defense distributed interactive simulation (wargaming) exercises <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>. In addition to terrain elevation, the CTDB format also represents relevant cartographic features on top of the terrain skin, including buildings, roads, bodies of water, and tree canopies. An important benefit to using CTDB as a site model representation is that it allows us to easily interface with third-party cartographic modeling and visualization tools developed to provide synthetic training environments <ref type="bibr" target="#b29">[30]</ref>. The Modular Semi-Automated Forces (ModSAF) program provides a 2-D graphical interface similar to our VSAM GUI, with the ability to insert computer-generated human and vehicle avatars that provide simulated opponents for training <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>. The ModStealth program generates an immersive, realistic 3-D visualization of texture-mapped scene geometry and computer generated avatars <ref type="bibr" target="#b28">[29]</ref>.</p><p>We have built an interface to the ModSAF and ModStealth programs from the VSAM testbed system. At the OCU, 3-D object hypotheses are repackaged into a data packet format specifying the type of avatar wanted (various human and ve-hicle types are available) and the current 3-D location and orientation of the avatar within the CTDB site model. These data packets are then broadcast (multicast) on the VSAM network, where any running ModSAF or ModStealth visualization clients pick them up and display them within their respective 2-D or 3-D synthetic environments. Fig. <ref type="figure" target="#fig_14">12(a</ref>) and (b) shows an example of three people being tracked by the VSAM system and a screen dump of their avatars represented within a 2-D ModSAF display. Fig. <ref type="figure" target="#fig_14">12(c)</ref> and<ref type="figure">(d)</ref> shows a person being tracked and the corresponding avatar being viewed within a 3-D ModStealth viewer. Fig. <ref type="figure" target="#fig_15">13</ref> shows an example of multiple objects detected automatically and inserted as avatars within a 3-D ModStealth view of the CMU campus. These experiments demonstrate that it is possible to automatically detect, track, and classify multiple people and vehicles using an automated surveillance system and then insert them as avatars in a synthetic environment for real-time visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Preliminary Work: Web-Page Event Reporting</head><p>In addition to on-line interactive use by an operator, another useful mode of operation for an automated surveillance system is unsupervised data logging, followed by off-line operator review days or months after collection of the data. We have begun to develop a prototype web-based data logging system to explore this application. For each object detected and tracked by the system, all symbolic data (e.g., object classification; color information; time-stamped trajectory data) is  saved, along with a cropped image of the object. All observations can then be explored by web browsing via CGI through an HTTP server <ref type="bibr" target="#b85">[86]</ref>, so that a human reviewer can access the data from anywhere. Fig. <ref type="figure" target="#fig_16">14</ref> user selects an object, the system automatically brings up a page showing other objects of the same class having similar color features. In this way, it might be possible for a user to detect the same vehicle or person being observed at different places and times around the surveillance site.</p><p>Unfortunately, in a high-traffic area, data on dozens of people and vehicles can be collected in just a few minutes of observation. Browsing the huge volume of raw surveillance data collected over even a single day is unmanageable and some type of higher-level semantic organization of the data is desired. To this end, we have begun developing an event detection program that scans the log files for common "events" that can be given a semantic label. There has been much work on parsing sequences of low-level surveillance observations, particularly time-stamped trajectories, into events that signify object interactions <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b86">[87]</ref>- <ref type="bibr" target="#b89">[90]</ref>. Our prototype event detector is based on hidden Markov models (HMMs) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b86">[87]</ref>, trained to recognize simple object interactions. Briefly, output from the low-level detection and tracking surveillance algorithms is quantized into the following discrete set of attributes and values for each motion blob:</p><p>1) object class: Human, Vehicle, HumanGroup; 2) object action: Appearing, Moving, Stopped, Disappearing; 3) interaction: Near, MovingAwayFrom, MovingToward, NoInteraction. The activities that can be labeled are: a) human entered a vehicle; b) human exited a vehicle; c) human entered a building; d) human exited a building; e) a vehicle parked; and f) human rendezvous. To train the activity classifier, conditional and joint probabilities of attributes and actions are obtained by generating many synthetic activity occurrences in simulation. This simulation-based training approach is motivated by <ref type="bibr" target="#b9">[10]</ref>. Fig. <ref type="figure" target="#fig_16">14</ref>(b) shows a preliminary example of a generated activity report. The activity report shows labeled events such as a "car parked," or "human entered a building," sorted by time. If a user wants more detail, a hypertext link brings up a page showing a cropped image of the object, along with its class and color information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY</head><p>The paper has presented an overview of video understanding algorithms developed at CMU to perform cooperative, multisensor surveillance. A network of smart sensors are deployed that are independently capable of performing real-time, autonomous object detection, tracking, classification and gait analysis. Results from these single-sensor technologies are combined into a coherent overview of the dynamic scene by multisensor fusion algorithms running on a central operator control station. The key to data integration from these multiple, widely spaced sensors is computation of object location with respect to a 3-D site model, followed by object hypothesis comparison and matching using a set of viewpoint-independent descriptors.</p><p>A single user tasks the system through an intuitive graphical user interface. The system automatically allocates sensors to perform these tasks using an arbitration function that determines the cost of assigning each sensor to each task. The system performs a greedy optimization over this cost table to maximize overall system performance. Through this cost-based scheduling approach, multiple sensors are automatically tasked to cooperatively track objects over long distances and through occlusion.</p><p>Visualizing the relative locations of people and vehicles over a large area is a difficult task. We provide the user with 2-D and 3-D synthetic views of the environment, within which detected people and vehicles are displayed as dynamic agents. This approach has the benefit that visualization of scene events is no longer tied to the original resolution and viewpoint of a single video sensor and the operator can therefore infer proper spatial relationships between sets of objects and between objects and scene features such as roads and buildings, leading to a better understanding of the evolving scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Logical layout of tasks in a multisensor surveillance system.</figDesc><graphic coords="2,128.04,25.92,318.00,176.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Placement of cameras in the current VSAM testbed system. (b) Schematic overview of the testbed system architecture. (c) Three sensors used for cooperative surveillance. (d) Central operator control station for integrating information from all sensors.</figDesc><graphic coords="3,92.28,394.36,426.00,160.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (Continued.) (e) Operator console where a user interacts with the system through a graphical user interface. (f) Screen dump of a map-based visualization tool that displays sensor resources and all detected objects and events.</figDesc><graphic coords="4,74.22,25.90,426.00,170.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Example analysis of intensity changes over time at a single pixel as a car enters the scene and stops, a second car enters and stops in front of the first, a person gets out and walks to the first car, the person returns to the second car, the second car drives away, and finally the first car drives away. Each of these steps is visible in the pixel's intensity profile. (b) Detection results for one timestep during the described events. The algorithm has correctly detected and represented that there are three overlapping objects, namely, the first stopped car, the second stopped car, and the person walking in front of them.</figDesc><graphic coords="6,172.56,383.98,229.00,123.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample object trajectories and class labels.</figDesc><graphic coords="7,139.44,25.93,331.00,220.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(A)]. Fig. 6(B) shows how two properties extracted from the skeleton provide cues to the person's gait. Assume the uppermost skeleton segment represents the torso and measure the angle between this segment and vertical. Assume the lower left segment represents one of legs and measure angle between this segment and vertical. Fig. 6(C) shows two star skeleton motion sequences for a walking and running human and plots the values and over time. Examining the average values of shows that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sample results for LDA classification of object type and color.</figDesc><graphic coords="8,74.04,198.88,426.00,160.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (A) The star skeleton is created by "unwrapping" the silhouette boundary as a distance function from the centroid, and extracting extremal points. (B) Determination of posture features from the skeleton: is the angle the left cyclic point (leg) makes with the vertical, and is the angle the torso makes with the vertical.</figDesc><graphic coords="9,217.02,338.88,175.68,128.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (Continued.) (C) Skeleton motion sequences. The periodic motion of provides cues to the person's gait, as does the mean value of .</figDesc><graphic coords="10,129.72,25.92,314.40,429.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Estimating object geolocations by intersecting backprojected viewing rays with a terrain model. (b) A Bresenham-like traversal algorithm determines which DEM cell contains the first intersection of a viewing ray and the terrain.</figDesc><graphic coords="11,202.26,303.52,206.00,224.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) Ground truth trajectory determined by a theodolite, overlaid with geolocations estimated by the system while automatically tracking the same object from one camera. (b) Geolocation error boxes computed by the system. (c) Plotted covariances of the horizontal displacement errors between estimate geolocations and ground truth locations for corresponding time stamps. All scales are in meters.</figDesc><graphic coords="12,334.20,365.40,168.48,132.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Cooperative, multisensor tracking (see text for description).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Example of camera slaving. (a) Wide-angle view in which a person is detected. (b) A better view from a second camera, which has been tasked to intercept the person's estimated 3-D path.</figDesc><graphic coords="16,90.72,25.90,393.00,144.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Map-based graphical user interface, through which a human operator tasks the VSAM sensor suite and visualizes objects tracked by the system.</figDesc><graphic coords="17,105.06,25.91,400.00,297.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Sample synthetic environment visualizations of VSAM detection and tracking data. (a) Automated tracking of three soldiers. (b) ModSAF 2-D orthographic map display of estimated geolocations. (c) Tracking of a soldier walking out of town. (d) Immersive, texture-mapped 3-D visualization of the same event, seen from a user-specified viewpoint.</figDesc><graphic coords="18,86.82,25.91,401.00,311.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Real-time, 3-D ModStealth visualization of objects detected and classified by the VSAM testbed system.</figDesc><graphic coords="18,87.12,401.08,400.00,238.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. (a) Object report web page. (b) Activity (event) report web page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,107.34,25.88,396.00,135.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,107.04,181.11,396.00,137.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,107.04,337.59,396.00,137.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,107.04,494.06,396.00,139.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="19,77.88,25.91,454.00,240.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="19,77.22,300.35,456.00,182.29" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PROCEEDINGS OF THE IEEE, VOL. 89, NO. 10, OCTOBER 2001</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT The authors would like to thank the CMU VSAM team members: D. Duggins, Y. Tsin, R. Patil, D. Tolliver, O. Hasegawa, N. Enomoto, Y.-T. Do, and A. Lee, for their tireless efforts and good humor. They also thank the members of the Sarnoff Corporation: P. Burt, L. Wixson, J. Eledath, and D. Mishra, who developed and demonstrated automated real-time airborne surveillance techniques within the scope of this effort.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Defense Advanced Research Projects Agency (DARPA) Image Understanding under Contract DAAB07-97-C-J031 and by the Office of Naval Research (ONR) under Grant N00014-99-1-0646.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robert T. Collins received the Ph.D. degree in computer science from the University of Massachusetts (UMass), Amherst, in 1993 for work on scene reconstruction using stochastic projective geometry.</p><p>He is a Member of the Research Faculty at the Robotics Institute, Carnegie Mellon University (CMU), Pittsburgh, PA. From 1992 to 1996, he was technical director of the DARPA RADIUS project at UMass, culminating in the ASCENDER system for populating 3-D site models from multiple, oblique aerial views. From 1996 to 1999, he was technical co-director of the DARPA Video Surveillance and Monitoring (VSAM) project at CMU. This project developed real-time, automated video understanding algorithms that guide a network of active video sensors to monitor the activities of people and vehicles in a complex scene. He has published for over a decade on topics in video surveillance, 3-D site modeling, multi-image stereo, projective geometry, and knowledge-based scene understanding.</p><p>Alan J. Lipton received the Ph.D. degree in electrical and computer systems engineering from Monash University, Melbourne, Australia, in 1996. For his thesis, he studied the problem of mobile robot navigation by natural landmark recognition using on-board vision sensing.</p><p>He is currently with Diamondback Vision, Inc., Reston, VA. From 1997 to 2000, he was on the faculty at the Robotics Institute of Carnegie Mellon University, Pittsburgh, PA, where he was technical co-director of the DARPA Video Surveillance and Monitoring (VSAM) effort. Under this program, he performed research in the areas of real-time object detection, tracking, and recognition from video.</p><p>Hironobu Fujiyoshi received the Ph.D. degree in electrical engineering from Chubu University, Japan, in 1997. For his thesis, he developed a fingerprint verification method using spectrum analysis, which has been incorporated into a manufactured device sold by a Japanese security company.</p><p>He is a Member of Faculty at the Department of Computer Science, Chubu University. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A real-time computer vision system for measuring traffic parameters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 Conf. Computer Vision and Pattern Recognition</title>
		<meeting>1997 Conf. Computer Vision and Pattern Recognition<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="495" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrated person tracking using stereo, color and pattern detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual surveillance for moving vehicles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Worrall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time system for video surveillance of unattended outdoor environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">697</biblScope>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental recognition of traffic situations from video image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time surveillance of people and their activities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="809" to="830" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A real-time system for monitoring of cyclists and pedestrians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Silven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd IEEE Int. Workshop Visual Surveillance</title>
		<meeting>2nd IEEE Int. Workshop Visual Surveillance<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object identification: A Bayesian analysis with application to traffic surveillance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="77" to="93" />
			<date type="published" when="1998-08">Aug. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-based object tracking in monocular image sequences of road traffic scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="257" to="281" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Bayesian computer vision system for modeling human interactions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="831" to="843" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiagent visual surveillance of dynamic scenes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="529" to="532" />
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D trajectory recovery for tracking multiple objects and trajectory-guided recognition of actions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="117" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Section I, video surveillance and monitoring</title>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bogaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chleq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cornez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teschioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<title level="m">The passwords project,&quot; in Int. Conf. Image Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video sequence interpretation for visual surveillance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Workshop on Visual Surveillance</title>
		<meeting>3rd IEEE Int. Workshop on Visual Surveillance<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Views project and wide-area surveillance</title>
		<idno>PM-02- ECCV92-01</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>VIEWS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual surveillance in a dynamic and uncertain world</title>
		<author>
			<persName><forename type="first">H</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="431" to="459" />
			<date type="published" when="1995-10">Oct. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cooperative distributed vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Image Understanding Workshop</title>
		<meeting>DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="365" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CDVWS</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st/2nd/3rd Int. Workshop Cooperative Distributed Vision</title>
		<meeting>1st/2nd/3rd Int. Workshop Cooperative Distributed Vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 1998/1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VS&apos;2000</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Workshop Visual Surveillance</title>
		<meeting>3rd IEEE Int. Workshop Visual Surveillance<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VS&apos;98</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st IEEE Int. Workshop Visual Surveillance</title>
		<meeting>1st IEEE Int. Workshop Visual Surveillance<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VS&apos;99</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd IEEE Int. Workshop Visual Surveillance</title>
		<meeting>2nd IEEE Int. Workshop Visual Surveillance<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Workshop Performance Evaluation of Tracking and Surveillance</title>
	</analytic>
	<monogr>
		<title level="m">Proc. First IEEE Int</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</editor>
		<meeting>First IEEE Int<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
	<note>PETS&apos;2000</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Introduction to the special section on video surveillance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="745" to="746" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction -surveillance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="173" to="173" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frame-rate omnidirectional surveillance and tracking of camouflaged and occluded targets</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Micheals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd IEEE Int. Workshop Visual Surveillance</title>
		<meeting>2nd IEEE Int. Workshop Visual Surveillance<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust video motion detection and event recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Flinchbaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 DARPA Image Understanding Workshop</title>
		<meeting>1997 DARPA Image Understanding Workshop<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
			<biblScope unit="page" from="51" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image alignment for precise camera fixation and aim</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wixson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mandelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="594" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The OpenScene ModStealth</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cauble</surname></persName>
		</author>
		<idno>97S-SIW-008</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Simulation Interoperability Workshop</title>
		<meeting>Simulation Interoperability Workshop</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling and distributed simulation techniques for synthetic training environments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Cavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Overstreet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Maly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling and Simulation of Advanced Computer Systems: Applications and Systems</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Bagchi</surname></persName>
		</editor>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Gordon &amp; Breach</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="237" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using a DEM to determine geospatial object trajectories</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 DARPA Image Understanding Workshop</title>
		<meeting>1998 DARPA Image Understanding Workshop<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Calibration of an outdoor active camera system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="528" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Influence of an explicitly modeled 3D scene on the tracking of partially occluded vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kollnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="206" to="225" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wallflower: Principles and practice of background maintenance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision</title>
		<meeting>Int. Conf. Computer Vision<address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Change detection and tracking using pyramid transformation techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Der Wal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Intelligent Robots and Computer Vision</title>
		<meeting>SPIE Intelligent Robots and Computer Vision</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">579</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image difference threshold strategies and shadow detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
		<meeting>British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pfinder: Real-time tracking of the human body</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="780" to="785" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motion-based recognition: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cedras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Motion of disturbances: Detection and tracking of multi-body nonrigid motion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Applicat</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="122" to="137" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust model-based motion tracking through the integration of search and estimation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="113" to="122" />
			<date type="published" when="1992-08">Aug. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking persons in monocular image sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="174" to="192" />
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">C-conditional density propagation for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1998-08">Aug. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient region tracking with parametric models of geometry and illumination</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1025" to="1039" />
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic feature point extraction and tracking in image sequences for arbitrary camera motion</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="31" to="76" />
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Tracking and Data Association</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fortmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Academic</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review of statistical data association techniques for motion correspondence</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="53" to="66" />
			<date type="published" when="1993-02">Feb. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The active recovery of 3D motion trajectories and their use in prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="219" to="234" />
			<date type="published" when="1997-03">Mar. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<title level="m">Digital Picture Processing</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Local application of optic flow to analyze rigid versus nonrigid motion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop Frame-Rate Vision</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09">Sept. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Virtual postman-an illustrative example of virtual video</title>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Automat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adaptive target recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Applicat</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="289" to="299" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic target scoring system using machine vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Mobasseri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vis. Applicat</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Moving target detection and classification from real-time video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 Workshop Applications of Computer Vision</title>
		<meeting>1998 Workshop Applications of Computer Vision<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Classifier and shift-invariant automatic target recognition neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Casasent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Neiberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="1117" to="1129" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic target recognition for naval traffic control using neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pasquariello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Satalino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laforgia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Spilotros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="67" to="73" />
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tolliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
		<idno>CMU-RI-TR-00-12</idno>
	</analytic>
	<monogr>
		<title level="m">A system for video surveillance and monitoring: VSAM final report</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Generating spatiotemporal models from examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="525" to="532" />
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning and recognizing human dynamics in video sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 Conf. Computer Vision and Pattern Recognition</title>
		<meeting>1997 Conf. Computer Vision and Pattern Recognition<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="568" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust real-time periodic motion detection, analysis and applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="781" to="796" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Tracking human body motion based on a stick figure model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Model-based vision: A program to see a walking person</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Real-time tracking of moving persons by exploring spatio-temporal image slices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ricquebourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="797" to="808" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Toward model-based recognition of human movements in image sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vis., Graphics Image Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cardboard people: A parameterized model of articulated image motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Face and Gesture Analysis</title>
		<meeting>Int. Conf. Face and Gesture Analysis<address><addrLine>Killington, VT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Human motion analysis: A review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="428" to="440" />
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: A survey</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="82" to="98" />
			<date type="published" when="1999-01">Jan. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Real-time human motion analysis by image skeletonization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 Workshop Applications of Computer Vision</title>
		<meeting>1998 Workshop Applications of Computer Vision<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Linear Algebra, Geodesy and GPS</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Wellesley-Cambridge</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Advances in cooperative multi-sensor video surveillance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wixson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 DARPA Image Understanding Workshop</title>
		<meeting>1998 DARPA Image Understanding Workshop<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">American Society of Photogrammetry</title>
		<author>
			<persName><forename type="first">Manual</forename><surname>Of</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Photogrammetry</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>Falls Church</pubPlace>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Monitoring activities from multiple video streams: Establishing a common coordinate frame</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="758" to="767" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Recognizing objects on the ground-plane</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="164" to="172" />
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">An algorithm for computer control of a digital plotter</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Bresenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Syst. J</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="30" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Special issue on sensor data fusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="161" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Special issue on data fusion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="3" to="5" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multi-camera color tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd IEEE Int. Workshop Visual Surveillance</title>
		<meeting>2nd IEEE Int. Workshop Visual Surveillance<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Segmenting images using normalized color</title>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="1992-01">Jan. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mathematical tools for representing uncertainty in perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ramparany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Spatial Reasoning and Multi-Sensor Fusion Workshop</title>
		<meeting>Spatial Reasoning and Multi-Sensor Fusion Workshop<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Distributed interactive simulation in the evolution of DoD warfare modeling and simulation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1995-08">Aug. 1995</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1138" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Recent developments in ModSAF terrain representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Conf. Computer Generated Forces and Behavioral Representation</title>
		<meeting>5th Conf. Computer Generated Forces and Behavioral Representation<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
			<biblScope unit="page" from="375" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">ModSAF development status</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Courtemanche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ceranowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Conf. Computer Generated Forces and Behavioral Representation</title>
		<meeting>5th Conf. Computer Generated Forces and Behavioral Representation<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Distributed Interactive Simulation Systems for Simulation and Training in the Aerospace Environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Petty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-04">Apr. 1995</date>
			<biblScope unit="page" from="251" to="280" />
		</imprint>
	</monogr>
	<note>Computer generated forces in distributed interactive simulation</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">CGI Programming in C and Perl</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boutell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Discovery and segmentation of activities in video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kettnaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="844" to="851" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Conceptual descriptions from monitoring and watching image sequences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="105" to="135" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Recognition of visual activities and interactions by stochastic parsing</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="852" to="872" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multiobject behavior recognition by event driven selective attention method</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="873" to="867" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
