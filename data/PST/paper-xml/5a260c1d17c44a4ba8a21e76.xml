<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Fusion with Global and Local Features for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Xu</surname></persName>
							<email>chengxushu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering and Science</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering and Science</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongtian</forename><surname>Liu</surname></persName>
							<email>ztliu@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering and Science</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Fusion with Global and Local Features for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/978-3-319-70087-8_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text classification</term>
					<term>Semantic feature</term>
					<term>Global average pooling</term>
					<term>Global feature</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is a crucial task in natural language processing. Due to the characteristics of text structure, achieving the best result remains an ongoing challenge. In this paper, we propose an ensemble model which outperforms the state-of-the-art. We first utilize rule-based n-gram approach to extend corpus. Then two different features, global dependencies of word and local semantic feature, are extracted by gated recurrent unit and global average pooling model respectively. In order to take advantage of the complementarity of the global and local features, a decision-level fusion is applied to fuse those different kinds of features. We evaluate the quality of our model on various public datasets, including sentiment analysis, ontology classification and text categorization. Experimental results show that our model can effectively learn representations for language modeling, and achieves the best accuracy of text categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification, a crucial task in natural language processing, has attracted extensive research attention in recent years. Due to the characteristics of text structure, how to extract text features more effectively and optimize the algorithm for higher accuracy are the main challenges <ref type="bibr" target="#b0">[1]</ref>. Some conventional machine learning models are simple but have yield strong baselines. For example, Pang et al. <ref type="bibr" target="#b1">[2]</ref> proposed a SVM categorization model based on n-gram approach and achieved good performance. Wang et al. <ref type="bibr" target="#b2">[3]</ref> developed the NBSVM model which combines SVM with Naive Bayes features to improve the accuracy of text classification. However, the conventional models usually capture count-based features which are not sufficient to represent the text information.</p><p>Compared to the conventional approaches, neural network has gained significant popularity since it can extract deep level semantic features <ref type="bibr" target="#b3">[4]</ref>. Both long short-term memory (LSTM) <ref type="bibr" target="#b4">[5]</ref> and gated recurrent unit (GRU) <ref type="bibr" target="#b5">[6]</ref> can capture the long-term dependencies of text sequences, thus they can deal with the information that depends on time. Convolutional neural networks (CNN) <ref type="bibr" target="#b6">[7]</ref> can extract local features accurately and efficiently. Unfortunately, either the global dependency features or local semantic features alone are inadequate to represent the text comprehensively. Some ensemble approaches <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> fused different text features and achieved promising results, but they are still limited because of the overlap between different features which leads to low text understanding.</p><p>In this paper, we propose a novel learnt representations model for language modeling, that utilizes different approaches to extract global and local features with low overlap from multiple modalities. The global feature with long term dependencies of words is extracted by the gated recurrent unit, while the local feature with short term semantic within a sliding context is extracted by global average pooling <ref type="bibr" target="#b10">[11]</ref>. Then two kinds of complementary features are fused to get a more comprehensive understanding of the language modeling. We analyze the classification accuracy before and after feature fusion through experiments, and also compare our model with the existing methods, the result demonstrates that our model outperforms the state-of-the-art approach.</p><p>The rest of the paper is organized as follows. Section 2 introduces our model architecture in detail. Section 3 presents the experiment results on public datasets, and the conclusion is drawn in Sect. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ensemble Model</head><p>In this section, we will introduce the detailed architecture of our model. As shown in Fig. <ref type="figure">1</ref> from bottom to up, first, new corpuses are generated by rule-based n-gram approach, which we will discuss in Sect. 2.1. Then, global average pooling is applied to extract the local semantic feature in Sect. 2.2, and GRU for global dependency feature extraction in Sect. 2.3. Section 2.4 presents the implementation of decision-level fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rule-Based Corpus Expansion</head><p>The first step of our model is to generate new corpuses from the original text by n-gram. The approach can capture both the word frequency and hidden semantic information, thus it is advantageous to model text sequence. However, the basic n-gram approach faces the problem of data explosion because of the numerous combinations of words. To deal with this problem, in our model, we adapt the basic n-gram by setting up rules which can guarantee a linear increase of the words.</p><p>Assuming it follows the Markov assumption <ref type="bibr" target="#b8">[9]</ref> that the selection probability of any word only depends on the previous N-1 words, which can be considered as the history of the word. It can effectively reduce the computational complexity based on the Markov assumption.</p><p>Upon the basic n-gram approach, we set up rules for new corpus generation, that is the order of combining adjacent words within a sliding context should be either the same as the original word order or reversed. As shown in Fig. <ref type="figure">2</ref>, only if the window size equals one, we combine the adjacent words in both forward and backward directions, otherwise, we conduct forward combination. The new corpuses are stored and named separately for ease of next operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X1 X2 X3 X4</head><p>Xk-1 Xk Comparing to the basic n-gram approach through which the total number of words generated will grow exponentially with the increase of window length N, our rule-based n-gram approach only increases linearly, thus can avoid data explosion. Moreover, the new corpuses still maintain significant word order information. We calculate the frequency of each word in each corpus and remove low-frequency words that are uncommon or redundant to increase the efficiency of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Local Semantic Feature Extraction</head><p>After processing the original text, global average pooling is adopted to extract local semantic feature from the new corpuses. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, it directly calculates the average value of the word vectors on each dimension <ref type="bibr" target="#b10">[11]</ref>. Here the variable a t,n denotes the word vector of t-th word and X ′ denotes result matrix with size T × N, where T is the total number of words and N is the dimension of the word vector. The calculation of X ′ can be represented as the following formulas.</p><formula xml:id="formula_0">X ′ = ( x ′ 1 , x ′ 2 , x ′ 3 , ⋯ , x ′ n−1 , x ′ n ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">x ′ n = 1 T × ∑ T t=1 a t,n<label>(2)</label></formula><p>Global average pooling takes the average value of the low-dimensional word vector as the feature of the text. The strategy behind is that the low-dimensional space, selected as mapping of the word vectors, can be considered as a feature space. By taking the average value, the text information in this feature space can be well represented. The feature space information is utilized to improve the robustness of the classifier by this strategy. It can effectively extract useful and rich local features although it is simple <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Global Dependency Feature Extraction</head><p>Another kind of feature to be extracted in our model is the global dependency feature, and we adopt GRU instead of conventional neural network to extract it. In feedforward neural network model, the nodes of each layer are independent when processing the samples, so the changes of sequence information cannot be modeled. GRU is a novel recurrent neural network which can be used in time series analysis and avoid the problem of gradient vanishing <ref type="bibr" target="#b11">[12]</ref>.</p><p>In our model, GRU extract long-term dependency features from the forward 1-gramcorpus and backward 1-gram-corpus-reverse corpus. In order to make the input text the same length, we truncate the text that exceeds specific length. In the reversed corpus, the lost words are at the forefront of the original corpus. Thus the forward and the backward corpus can form complementary information. The experiment result proves this trick can improve the categorization accuracy by about 0.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Decision Level Fusion Approach</head><p>Our model utilizes decision-level fusion to concatenate the complementary information of different kinds of features <ref type="bibr" target="#b0">[1]</ref>. The decision vector [x 1 , ⋯ , x k−1 , x k ] represents the corpus from GRU, and the decision vector</p><formula xml:id="formula_3">[ x ′ 1 , ⋯ , x ′ n−1 , x ′ n ]</formula><p>is obtained from global average pooling, where K and N are output dimensions. These decision vectors are normalized to [−1, 1] and then concatenated as the following formula.</p><formula xml:id="formula_4">Con {[ x 1 , ⋯ , x k−1 , x k ] , [ x ′ 1 , ⋯ , x ′ n−1 , x ′ n ]} = {[ x 1 , ⋯ x k , x ′ k+1 , ⋯ , x ′ k+n−1 , x ′ k+n ]}<label>(3)</label></formula><p>The new decision vector size is T × (K + N), where T is the total number of samples and the vector dimension is equal to the sum of the dimensions of different decision vectors.</p><formula xml:id="formula_5">x l j = 𝜎 ( ∑ k+n w l j(k+n) x l−1 k+n + b l j )<label>(4)</label></formula><p>The above formula calculates the new hidden layer, where w l j(k+n) is connection weights vector of the (k + n)-th neuron in (l − 1)-th layer and the j-th neuron in l-th layer, and b l j is j-th neurons bias in l-th layer. Our model uses cross entropy as the loss function to minimize the categorization error <ref type="bibr" target="#b3">[4]</ref>. The loss function is as follows.</p><formula xml:id="formula_6">L ( {x, y} M C ) = argmin ( ∑ M m=1 ∑ C c=1 y (m) c log ( f ( x (m) c )) )<label>(5)</label></formula><p>Where M is the total number of text, C is the total number of categories, and y (m) c is one-hot encoding indicating whether the m-th sample belongs to the c-th category, and y (m)  c represents the predictive probability of the categorization model for the M-th sample belonging to the C-th category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we show and analysis the performance of our model for various datasets including ontology classification, sentiment analysis, and text categorization. Table <ref type="table" target="#tab_0">1</ref> is a summary. The IMDB<ref type="foot" target="#foot_2">1</ref>  <ref type="bibr" target="#b12">[13]</ref> data set consists of numerous film movie reviews. It is commonly used for emotional categorization. The ELEC<ref type="foot" target="#foot_3">2</ref>  <ref type="bibr" target="#b13">[14]</ref> data set is part of Amazon's electronic product review data. Similar to the IMDB data, it only has two categories with the same number of documents. The 20Newsgroup<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b14">[15]</ref> data set is a standard database for machine learning evaluation, we chose the version which including 18,828 documents. The AG<ref type="foot" target="#foot_5">4</ref>  <ref type="bibr" target="#b15">[16]</ref> dataset is a collection of more than 1 million news articles, and we choose the 4 largest classes from this corpus. The Yelp<ref type="foot" target="#foot_6">5</ref>  <ref type="bibr" target="#b15">[16]</ref> reviews dataset is obtained from the Yelp Dataset Challenge in 2015, and include two classification task, the predicting full number of starts or polarity label the user has given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effectiveness of Rule-Based N-gram Approach</head><p>In Sect. 2.1, we utilize the rule-based n-gram approach to extend original corpus. To prove its effectiveness, Fig. <ref type="figure" target="#fig_2">4</ref>. shows how the average length of new corpus and the length of dictionary change with the sliding window size on 20Newsgroup, under rulebased n-gram and basic n-gram approach respectively.</p><p>With the basic n-gram approach, the size of new corpus increase rapidly when the sliding window increases. It is because the adjacent words in the text seldom co-occur with each other. Hence, a large number of new sequences that never appeared are generated. The new corpus generated by rule-based n-gram is greatly different from the basic n-gram approach. Both the average length of new corpuses and length of dictionary increase slowly. When the sliding window size equals to 3, the average length of new corpus generated by basic n-gram approach is 6 times more than rule-based one, and 24 times when the window size equals to 4. The results clearly show that the rule-based model can effectively constrain the number of word combinations and avoid the data explosion problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Between Different Implementations of the Model</head><p>Here we vary the sliding window size and feature selection approach to get different implementations of our model, as shown in Table <ref type="table" target="#tab_1">2</ref>. Noted that the word vector input to GRU is initialized with the Glove 6 <ref type="bibr" target="#b16">[17]</ref> word vector matrix, others are all initialized with uniform distribution. We can see clearly that, for the implementations of n-gram + Global average pooling, the categorization error reduces when the sliding window length increases. For example, when the window size equals to one, the model can only extract feature from single word itself. But when the window length increases to two or three, the model can extract semantic relation between two words or three words. Therefore, the semantic information obtained is more complete and the error rate can reduce correspondingly. The last two lines compare the implementations of GRU approach. The new corpus with both forward and backward combination of words performs better than that with one forward combination. The accuracy is improved by about 0.61%. 6 https://nlp.stanford.edu/projects/glove/. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Confusion Matrix of Different Implementations</head><p>In order to measure the contribution of each operation to the ensemble classifier, we randomly choose 8 categories and 7,682 documents from 20Newsgroup dataset, where 60% for training and 40% for testing. We set the maximum length of window sliding to three. The best accuracy of GRU and global average pooling is 91.21% and 90.96% respectively, and the accuracy of the ensemble model achieves 92.15%.</p><p>From the confusion matrices in Fig. <ref type="figure" target="#fig_3">5</ref>, we can conclude that different feature selection approaches have different attention. GRU focuses on extracting global dependencies feature, and the categorization accuracy is 94.1% in categories comp.graphics. Global average pooling focuses on extracting local semantic feature <ref type="bibr" target="#b3">[4]</ref> and the accuracy is 88.3% in the same categories. The large gap shows that the global feature is better than local feature in representing accurate information for these categories. But for categories sci.crypt and sci.electronics, the global average pooling performs better than GRU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal Fusion with Global and Local Features</head><p>The confusion matrix of the ensemble model is showed in Fig. <ref type="figure" target="#fig_4">6</ref>. The average categorization accuracy of the ensemble model is higher than that of GRU alone and global average pooling alone as well. For each category, the ensemble model can perform better than either model alone. This proves that the global dependency feature and local semantic feature can be fused effectively, and the fusion of different features provides a more precise and completed information of the text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with State-of-the-Art Models</head><p>At last, we compare our ensemble model with state-of-the-art models. Since the NBSVM <ref type="bibr" target="#b2">[3]</ref> model is only suitable for binary categorization, we do not use it on multi-classification tasks.</p><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, our model can achieve the best performance on various datasets. The error rate is lower than the best result ever, even decreasing by more than 2.78% on 20Newsgroup task. The oh-CNN and oh-2LSTMp model stay the second best, for they can utilize the sliding window to reserve and extract word order feature. The result proves that our ensemble model cans effective learnt representation language modeling by fuse the different but complementary global and local feature.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Conclusion</head><p>In this paper, we propose a novel ensemble model which achieves astonishing performance on various datasets. We first utilize rule-based n-gram approach to expand the original text to generate new corpus with richer information. These low-overlap local semantic feature and global dependence feature are complement with each other and can represent the language modeling more precisely and comprehensively. Beyond that, a simple algorithm is used to fuse these different but complementary features. Later, we use our model for various public tasks, proving it can effectively extract features of the text and improve the accuracy of text categorization. How good the learnt representations are for language modeling is a crucial question <ref type="bibr" target="#b15">[16]</ref>. In the future, we intend to apply these semi-supervised learning and transfer learning to our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 1 .</head><label>21</label><figDesc>Fig. 2. Composition of adjacent words within different sliding context through the rule-based ngram approach. The names of new corpuses are shown in the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Global average pooling approach.</figDesc><graphic url="image-1.png" coords="4,150.24,247.76,160.15,150.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The change curve of the average length of new corpuses and length of dictionary length with the sliding window length</figDesc><graphic url="image-2.png" coords="7,42.48,58.08,340.32,130.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The classification confusion matrices of GRU and global average pooling approach.</figDesc><graphic url="image-3.png" coords="8,139.92,234.12,173.88,312.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The classification confusion matrix of ensemble model.</figDesc><graphic url="image-4.png" coords="9,122.88,142.08,179.52,150.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of various datasets</figDesc><table><row><cell cols="4">Dataset Classes Train samples Test samples</cell></row><row><cell>IMDB</cell><cell>2</cell><cell>25000</cell><cell>25000</cell></row><row><cell>20NG</cell><cell>20</cell><cell>11300</cell><cell>7528</cell></row><row><cell>ELEC</cell><cell>2</cell><cell>25000</cell><cell>25000</cell></row><row><cell>AG</cell><cell>4</cell><cell>120000</cell><cell>7600</cell></row><row><cell>Yelp P</cell><cell>2</cell><cell>560000</cell><cell>38000</cell></row><row><cell>Yelp F</cell><cell>5</cell><cell>650000</cell><cell>50000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental error rates of different implementations of our model on IMDB task.</figDesc><table><row><cell>Implementations</cell><cell>Error rates</cell></row><row><cell>1-gram + Global average pooling</cell><cell>10.99%</cell></row><row><cell>1-gram + 2-gram + Global average pooling</cell><cell>8.93%</cell></row><row><cell>1-gram + 2-gram + 3-gram + Global average pooling</cell><cell>8.68%</cell></row><row><cell>1-gram + GRU</cell><cell>9.21%</cell></row><row><cell>1-gram + 1-gram-reverse + GRU</cell><cell>8.60%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The categorization error by different models</figDesc><table><row><cell>Model</cell><cell>IMDB</cell><cell>ELEC</cell><cell>20NG</cell><cell>AG</cell><cell>Yelp P</cell><cell>Yelp F</cell></row><row><cell>SVM bow</cell><cell>11.36</cell><cell>11.76</cell><cell>17.47</cell><cell>11.19</cell><cell>7.76</cell><cell>42.01</cell></row><row><cell>SVM 1-3grams [2]</cell><cell>9.42</cell><cell>8.71</cell><cell>15.85</cell><cell>7.96</cell><cell>4.36</cell><cell>43.74</cell></row><row><cell>NBSVM-uni [3]</cell><cell>11.74</cell><cell>11.53</cell><cell>-</cell><cell>-</cell><cell>7.82</cell><cell>-</cell></row><row><cell>NBSVM-bi [3]</cell><cell>8 . 7 8</cell><cell>8 . 3 5</cell><cell>-</cell><cell>-</cell><cell>4 . 8 9</cell><cell>-</cell></row><row><cell>oh-2LSTMp [18]</cell><cell>8 . 1 4</cell><cell>7.33</cell><cell>13.32</cell><cell>6.92</cell><cell>4.73</cell><cell>35.13</cell></row><row><cell>oh-CNN [18]</cell><cell>8.04</cell><cell>7.48</cell><cell>13.55</cell><cell>7.22</cell><cell>5.02</cell><cell>35.36</cell></row><row><cell>FastText-bi [4]</cell><cell>9.04</cell><cell>8.79</cell><cell>16.45</cell><cell>7.50</cell><cell>4.32</cell><cell>36.14</cell></row><row><cell>Our</cell><cell>6.97</cell><cell>6.55</cell><cell>10.54</cell><cell>6.08</cell><cell>3.42</cell><cell>34.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Multimodal Fusion with Global and Local Features</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="128" xml:id="foot_1">C. Xu et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2">http://www.imdb.com/interfaces.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">http://ai.stanford.edu/~amaas/data/sentiment/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4">http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5">http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6">https://www.yelp.com/dataset_challenge.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by the Special Funds of the National Natural Science Foundation of China (Grant No. 51227803).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining multimodal features with hierarchical classifier fusion for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
				<meeting>the 16th International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="481" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thumbs up? Sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-INNS-ENNS International Joint Conference on Neural Networks</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Analysis and optimization of fasttext linear text classifier</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zolotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05531</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5335</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alleviating overfitting for polysemous words for word representation estimation using lexicons</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2164" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An auto-encoder for learning conversation representation using LSTM</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-26532-2_34</idno>
	</analytic>
	<monogr>
		<title level="m">Multimodal Fusion with Global and Local Features</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9489</biblScope>
			<biblScope unit="page" from="310" to="317" />
		</imprint>
	</monogr>
	<note>ICONIP 2015</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Conference on Recommender Systems</title>
				<meeting>the 7th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Newsweeder: learning to filter netnews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning</title>
				<meeting>the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="919" to="927" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
