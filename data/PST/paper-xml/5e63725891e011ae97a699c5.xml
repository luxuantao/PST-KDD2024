<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TASK2VEC: Task Embedding for Meta-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-10">10 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alessandro</forename><forename type="middle">Achille</forename><surname>Ucla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Lam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soattos@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
							<email>peronapp@amazon.com</email>
						</author>
						<title level="a" type="main">TASK2VEC: Task Embedding for Meta-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-10">10 Feb 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1902.03545v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a "probe network" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar). We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of Deep Learning hinges in part on the fact that models learned for one task can be used on other related tasks. Yet, no general framework exists to describe and learn relations between tasks. We introduce the TASK2VEC embedding, a technique to represent tasks as elements of a vector space based on the Fisher Information Matrix. The norm of the embedding correlates with the complexity of the task, while the distance between embeddings captures semantic similarities between tasks (Fig. <ref type="figure">1</ref>). When other natural distances are available, such as the taxonomical distance in biological classification, we find that the embedding distance correlates positively with it (Fig. <ref type="figure" target="#fig_1">2</ref>). Moreover, we introduce an asymmetric distance on tasks which correlates with the transferability between tasks.</p><p>Computation of the embedding leverages a duality between network parameters (weights) and outputs (activations) in a deep neural network (DNN): Just as the activations of a DNN trained on a complex visual recognition task are a rich representation of the input images, we show that the gradients of the weights relative to a task-specific loss are a rich representation of the task itself. Specifically, given a task defined by a dataset D = {(x i , y i )} N i=1 of labeled samples, we feed the data through a pre-trained reference convolutional neural network which we call "probe network", and compute the diagonal Fisher Information Matrix (FIM) of the network filter parameters to capture the structure of the task (Sect. 2). Since the architecture and weights of the probe network are fixed, the FIM provides a fixed-dimensional representation of the task. We show this embedding encodes the "difficulty" of the task, characteristics of the input domain, and which features of the probe network are useful to solve it (Sect. 2.1).</p><p>Our task embedding can be used to reason about the space of tasks and solve meta-tasks. As a motivating example, we study the problem of selecting the best pre-trained feature extractor to solve a new task. This can be particularly valuable when there is insufficient data to train or finetune a generic model, and transfer of knowledge is essential. TASK2VEC depends solely on the task, and ignores interactions with the model which may however play an important role. To address this, we learn a joint task and model embedding, called MODEL2VEC, in such a way that models whose embeddings are close to a task exhibit good perfmormance on the task. We use this to select an expert from a given collection, improving performance relative to</p><formula xml:id="formula_0">Task Embeddings Domain Embeddings Actinopterygii (n) Amphibia (n) Arachnida (n) Aves (n) Fungi (n) Insecta (n) Mammalia (n) Mollusca (n) Plantae (n) Protozoa (n) Reptilia (n) Category (m) Color (m) Gender (m) Material (m)</formula><p>Neckline (m) Pants (m) Pattern (m) Shoes (m) Figure <ref type="figure">1</ref>: Task embedding across a large library of tasks (best seen magnified). (Left) T-SNE visualization of the embedding of tasks extracted from the iNaturalist, CUB-200, iMaterialist datasets. Colors indicate ground-truth grouping of tasks based on taxonomic or semantic types. Notice that the bird classification tasks extracted from CUB-200 embed near the bird classification task from iNaturalist, even though the original datasets are different. iMaterialist is well separated from iNaturalist, as it entails very different tasks (clothing attributes). Notice that some tasks of similar type (such as color attributes) cluster together but attributes of different task types may also mix when the underlying visual semantics are correlated. For example, the tasks of jeans (clothing type), denim (material) and ripped (style) recognition are close in the task embedding. (Right) T-SNE visualization of the domain embeddings (using mean feature activations) for the same tasks. Domain embedding can distinguish iNaturalist tasks from iMaterialist tasks due to differences in the two problem domains. However, the fashion attribute tasks on iMaterialist all share the same domain and only differ in their labels. In this case, the domain embeddings collapse to a region without recovering any sensible structure. fine-tuning a generic model trained on ImageNet and obtaining close to ground-truth optimal selection. We discuss our contribution in relation to prior literature in Sect. 6, after presenting our empirical results in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Embeddings via Fisher Information</head><p>Given an observed input x (e.g., an image) and an hidden task variable y (e.g., a label), a deep network is a family of functions p w (y|x) parametrized by weights w, trained to approximate the posterior p(y|x) by minimizing the (possibly regularized) cross entropy loss H pw, p(y|x) = E x,y? p[-log p w (y|x)], where p is the empirical distribution defined by the training set D = {(x i , y i )} N i=1 . It is useful, especially in transfer learning, to think of the network as composed of two parts: a feature extractor which computes some representation z = ? w (x) of the input data, and a "head," or classifier, which encodes the distribution p(y|z) given the representation z.</p><p>Not all network weights are equally useful in predicting the task variable: the importance, or "informative content," of a weight for the task can be quantified by considering a perturbation w = w + ?w of the weights, and measuring the average Kullbach-Leibler (KL) divergence between the original output distribution p w (y|x) and the perturbed one p w (y|x). To second-order approximation, this is</p><formula xml:id="formula_1">E x? p KL(p w (y|x) p w (y|x)) = ?w ? F ?w + o(?w 2 ),</formula><p>where F is the Fisher information matrix (FIM):</p><formula xml:id="formula_2">F = E x,y? p(x)pw(y|x) ? w log p w (y|x)? w log p w (y|x) T .</formula><p>that is, the expected covariance of the scores (gradients of the log-likelihood) with respect to the model parameters.</p><p>The FIM is a Riemannian metric on the space of probability distributions <ref type="bibr" target="#b6">[7]</ref>, and provides a measure of the information a particular parameter (weight or feature) contains about the joint distribution p w (x, y) = p(x)p w (y|x): If the classification performance for a given task does not depend strongly a parameter, the corresponding entry in the FIM will be small. The FIM is also related to the (Kolmogorov) complexity of a task, a property that can be used to define a computable metric of the learning distance between tasks <ref type="bibr" target="#b2">[3]</ref>. Finally, the FIM can be interpreted as an easy-tocompute positive semidefinite upper-bound to the Hessian of the cross-entropy loss, and coincides with it at local minima <ref type="bibr" target="#b23">[24]</ref>. In particular, "flat minima" correspond to weights that have, on average, low (Fisher) information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">TASK2VEC embedding using a probe network</head><p>While the network activations capture the information in the input image which are needed to infer the image label, the FIM indicates the set of feature maps which are more informative for solving the current task. Following this intuition, we use the FIM to represent the task itself. However, the FIMs computed on different networks are not directly comparable. To address this, we use single "probe" network pre-trained on ImageNet as a feature extractor and re-train only the classifier layer on any given task, which usually can be done efficiently. After training is complete, we compute the FIM for the feature extractor parameters.</p><p>Since the full FIM is unmanageably large for rich probe networks based on CNNs, we make two additional approximations. First, we only consider the diagonal entries, which implicitly assumes that correlations between different filters in the probe network are not important. Second, since the weights in each filter are usually not independent, we average the Fisher Information for all weights in the same filter. The resulting representation thus has fixed size, equal to the number of filters in the probe network. We call this embedding method TASK2VEC.</p><p>Robust Fisher computation Since the FIM is a local quantity, it is affected by the local geometry of the training loss landscape, which is highly irregular in many deep network architectures <ref type="bibr" target="#b20">[21]</ref>, and may be too noisy when trained with few samples. To avoid this problem, instead of a direct computation, we use a more robust estimator that leverages connections to variational inference. Assume we perturb the weights ? of the network with Gaussian noise N (0, ?) with precision matrix ?, and we want to find the optimal ? which yields a good expected error, while remaining close to an isotropic prior N ( ?, ? 2 I). That is, we want to find ? that minimizes:</p><formula xml:id="formula_3">L( ?; ?) = E w?N ( ?,?) [H pw, pp(y|x)] + ? KL(N (0, ?) N (0, ? 2 I)),</formula><p>where H is the cross-entropy loss and ? controls the weight of the prior. Notice that for ? = 1 this reduces to the Evidence Lower-Bound (ELBO) commonly used in variational inference. Approximating to the second order, the optimal value of ? satisfies (see Supplementary Material):</p><formula xml:id="formula_4">? 2N ? = F + ?? 2 2N I.</formula><p>Therefore, ? 2N ? ? F +o(1) can be considered as an estimator of the FIM F , biased towards the prior ? 2 I in the lowdata regime instead of being degenerate. In case the task is trivial (the loss is constant or there are too few samples) the embedding will coincide with the prior ? 2 I, which we will refer to as the trivial embedding. This estimator has the advantage of being easy to compute by directly minimizing the loss L( ?; ?) through Stochastic Gradient Variational Bayes <ref type="bibr" target="#b17">[18]</ref>, while being less sensitive to irregularities of the loss landscape than direct computation, since the value of the loss depends on the cross-entropy in a neighborhood of ? of size ? -1 . As in the standard Fisher computation, we estimate one parameter per filter, rather than per weight, which in practice means that we constrain ? ii = ? jj whenever w i and w j belongs to the same filter. In this case, optimization of L( ?; ?) can be done efficiently using the local reparametrization trick of <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Properties of the TASK2VEC embedding</head><p>The task embedding we just defined has a number of useful properties. For illustrative purposes, consider a twolayer sigmoidal network for which an analytic expression can be derived (see Supplementary Materials). The FIM of the feature extractor parameters can be written using the Kronecker product as</p><formula xml:id="formula_5">F = E x,y? p(x)pw(y|x) [(y -p) 2 ? S ? xx T ]</formula><p>where p = p w (y = 1|x) and the matrix S = ww T zz T (1 -z)(1 -z) T is an element-wise product of classifier weights w and first layer feature activations z. It is informative to compare this expression to an embedding based only on the dataset domain statistics, such as the (non-centered) covariance C 0 = E xx T of the input data or the covariance C 1 = E zz T of the feature activations. One could take such statistics as a representative domain embedding since they only depend on the marginal distribution p(x) in contrast to the FIM task embedding, which depends on the joint distribution p(x, y). These simple expressions highlight some important (and more general) properties of the Fisher embedding we now describe.</p><p>Invariance to the label space: The task embedding does not directly depend on the task labels, but only on the predicted distribution p w (y|x) of the trained model. Information about the ground-truth labels y is encoded in the weights w which are a sufficient statistic of the task <ref type="bibr" target="#b4">[5]</ref>. In particular, the task embedding is invariant to permutations of the labels y, and has fixed dimension (number of filters of the feature extractor) regardless of the output space (e.g., k-way classification with varying k).</p><p>Encoding task difficulty: As we can see from the expressions above, if the fit model is very confident in its predictions, E[(y -p) 2 ] goes to zero. Hence, the norm of the task embedding F scales with the difficulty of the task for a given feature extractor ?. Figure <ref type="figure" target="#fig_1">2</ref> (Right) shows that even for more complex models trained on real data, the FIM norm correlates with test performance.</p><p>Encoding task domain: Data points x that are classified with high confidence, i.e., p is close to 0 or 1, will have a lower contribution to the task embedding than points  For tasks extracted from iNaturalist and CUB, we compare the cosine distance between tasks to their taxonomical distance. As the size of the task embedding neighborhood increases (measured by number of tasks in the neighborhood), we plot the average taxonomical distance of tasks from the neighborhood center. While the task distance does not perfectly match the taxonomical distance (whose curve is shown in orange), it shows a good correlation. Difference are both due to the fact that taxonomically close species may need very different features to be classified, creating a mismatch between the two notions of distance, and because for some tasks in iNaturalist too few samples are provided to compute a good embedding. (Right) Correlation between L 1 norm of the task embedding (distance from origin) and test error obtained on the task. near the decision boundary since p(1 -p) is maximized at p = 1/2. Compare this to the covariance matrix of the data, C 0 , to which all data points contribute equally. Instead, in TASK2VEC information on the domain is based on data near the decision boundary (task-weighted domain embedding).</p><p>Encoding useful features for the task: The FIM depends on the curvature of the loss function with the diagonal entries capturing the sensitivity of the loss to model parameters. Specifically, in the two-layer model one can see that, if a given feature is uncorrelated with y, the corresponding blocks of F are zero. In contrast, a domain embedding based on feature activations of the probe network (e.g., C 1 ) only reflects which features vary over the dataset without indication of whether they are relevant to the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Similarity Measures on the Space of Tasks</head><p>What metric should be used on the space of tasks? This depends critically on the meta-task we are considering. As a motivation, we concentrate on the meta-task of selecting the pre-trained feature extractor from a set in order to obtain the best performance on a new training task. There are several natural metrics that may be considered for this meta-task. In this work, we mainly consider:</p><p>Taxonomic distance For some tasks, there is a natural notion of semantic similarity, for instance defined by sets of categories organized in a taxonomic hierarchy where each task is classification inside a subtree of the hierarchy (e.g., we may say that classifying breeds of dogs is closer to clas-sification of cats than it is to classification of species of plants). In this setting, we can define</p><formula xml:id="formula_6">D tax (t a , t b ) = min i?Sa,j?S b d(i, j),</formula><p>where S a , S b are the sets of categories in task t a , t b and d(i, j) is an ultrametric or graph distance in the taxonomy tree. Notice that this is a proper distance, and in particular it is symmetric.</p><p>Transfer distance. We define the transfer (or fine-tuning) gain from a task t a to a task t b (which we improperly call distance, but is not necessarily symmetric or positive) as the difference in expected performance between a model trained for task t b from a fixed initialization (random or pretrained), and the performance of a model fine-tuned for task t b starting from a solution of task t a :</p><formula xml:id="formula_7">D ft (t a ? t b ) = E[ a?b ] -E[ b ] E[ b ]</formula><p>,</p><p>where the expectations are taken over all trainings with the selected architecture, training procedure and network initialization, b is the final test error obtained by training on task b from the chosen initialization, and a?b is the error obtained instead when starting from a solution to task a and then fine-tuning (with the selected procedure) on task t b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Symmetric and asymmetric TASK2VEC metrics</head><p>By construction, the Fisher embedding on which TASK2VEC is based captures fundamental information about the structure of the task. We may therefore expect that the distance between two embeddings correlate positively with natural metrics on the space of tasks. However, there are two problems in using the Euclidean distance between embeddings: the parameters of the network have different scales, and the norm of the embedding is affected by complexity of the task and the number of samples used to compute the embedding. Symmetric TASK2VEC distance To make the distance computation robust, we propose to use the cosine distance between normalized embeddings:</p><formula xml:id="formula_8">d sym (F a , F b ) = d cos F a F a + F b , F b F a + F b ,</formula><p>where d cos is the cosine distance, F a and F b are the two task embeddings (i.e., the diagonal of the Fisher Information computed on the same probe network), and the division is element-wise. This is a symmetric distance which we expect to capture semantic similarity between two tasks. For example, we show in Fig. <ref type="figure" target="#fig_1">2</ref> that it correlates well with the taxonomical distance between species on iNaturalist. On the other hand, precisely for this reason, this distance is ill-suited for tasks such as model selection, where the (intrinsically asymmetric) transfer distance is more relevant.</p><p>Asymmetric TASK2VEC distance In a first approximation, that does not consider either the model or the training procedure used, positive transfer between two tasks depends both on the similarity between two tasks and on the complexity of the first. Indeed, pre-training on a general but complex task such as ImageNet often yields a better result than fine-tuning from a close dataset of comparable complexity. In our case, complexity can be measured as the distance from the trivial embedding. This suggests the following asymmetric score, again improperly called a "distance" despite being asymmetric and possibly negative:</p><formula xml:id="formula_9">d asym (t a ? t b ) = d sym (t a , t b ) -?d sym (t a , t 0 ),</formula><p>where t 0 is the trivial embedding, and ? is an hyperparameter. This has the effect of bring more complex models closer. The hyper-parameter ? can be selected based on the meta-task. In our experiments, we found that the best value of ? (? = 0.15 when using a ResNet-34 pretrained on ImageNet as the probe network) is robust to the choice of meta-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MODEL2VEC: task/model co-embedding</head><p>By construction, the TASK2VEC distance ignores details of the model and only relies on the task. If we know what task a model was trained on, we can represent the model by the embedding of that task. However, in general we may not have such information (e.g., black-box models or handconstructed feature extractors). We may also have multiple models trained on the same task with different performance characteristics. To model the joint interaction between task and model (i.e., architecture and training algorithm), we aim to learn a joint embedding of the two.</p><p>We consider for concreteness the problem of learning a joint embedding for model selection. In order to embed models in the task space so that those near a task are likely to perform well on that task, we formulate the following meta-learning problem: Given k models, their MODEL2VEC embedding are the vectors m i = F i + b i , where F i is the task embedding of the task used to train model m i (if available, else we set it to zero), and b i is a learned "model bias" that perturbs the task embedding to account for particularities of the model. We learn b i by optimizing a k-way cross entropy loss to predict the best model given the task distance (see Supplementary Material):</p><formula xml:id="formula_10">L = E[-log p(m | d asym (t, m 0 ), . . . , d asym (t, m k ))].</formula><p>After training, given a novel query task t, we can then predict the best model for it as the arg max i d asym (t, m i ), that is, the model m i embedded closest to the query task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We test TASK2VEC on a large collection of tasks and models, related to different degrees. Our experiments aim to test both qualitative properties of the embedding and its performance on meta-learning tasks. We use an off-the-shelf ResNet-34 pretrained on ImageNet as our probe network, which we found to give the best overall performance (see Sect. 5.2). The collection of tasks is generated starting from the following four main datasets. iNaturalist <ref type="bibr" target="#b35">[36]</ref>: Each task extracted corresponds to species classification in a given taxonomical order. For instance, the "Rodentia task" is to classify species of rodents. Notice that each task is defined on a separate subset of the images in the original dataset; that is, the domains of the tasks are disjoint. : We use the same procedure as iNaturalist to create tasks. In this case, all tasks are classifications inside orders of birds (the aves taxonomical class), and have generally much less training samples than corresponding tasks in iNaturalist. iMaterialist <ref type="bibr" target="#b0">[1]</ref> and DeepFashion <ref type="bibr" target="#b22">[23]</ref>: Each image in both datasets is associated with several binary attributes (e.g., style attributes) and categorical attributes (e.g., color, type of dress, material). We binarize the categorical attributes, and consider each attribute as a separate task. Notice that, in this case, all tasks share the same domain and are naturally correlated.</p><p>In total, our collection of tasks has 1460 tasks (207 iNaturalist, <ref type="bibr">25 CUB,</ref><ref type="bibr">228</ref> iMaterialist, 1000 DeepFashion). While a few tasks have many training examples (e.g., hundred thousands), most have just hundreds or thousands of samples. This simulates the heavy-tail distribution of data in real-world applications.  TASK2VEC mostly recover the optimal, or close to optimal, feature extractor to use without having to perform an expensive brute-force search over all possibilities. Columns are ordered by norm of the task embedding: Notice tasks with lower embedding norm have lower error and more "complex" task (task with higher embedding norm) tend to benefit more from a specialized expert.</p><p>Together with the collection of tasks, we collect several "expert" feature extractors. These are ResNet-34 models pre-trained on ImageNet and then fine-tuned on a specific task or collection of related tasks (see Supplementary Materials for details). We also consider a "generic"expert pretrained on ImageNet without any finetuning. Finally, for each combination of expert feature extractor and task, we trained a linear classifier on top of the expert in order to solve the selected task using the expert.</p><p>In total, we trained 4,100 classifiers, 156 feature extractors and 1,460 embeddings. The total effort to generate the final results was about 1,300 GPU hours.</p><p>Meta-tasks. In Sect. 5.2, for a given task we aim to predict, using TASK2VEC , which expert feature extractor will yield the best classification performance. In particular, we formulate two model selection meta-tasks: iNat + CUB and Mixed. The first consists of 50 tasks and experts from iNaturalist and CUB, and aims to test fine-grained expert selection in a restricted domain. The second contains a mix of 26 curated experts and 50 random tasks extracted from all datasets, and aims to test model selection between different domains and tasks (see Supplementary Material for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Task Embedding Results</head><p>Task Embedding qualitatively reflects taxonomic distance for iNaturalist For tasks extracted from the iNaturalist dataset (classification of species), the taxonomical distance between orders provides a natural metric of the semantic similarity between tasks. In Figure <ref type="figure" target="#fig_1">2</ref> we compare the symmetric TASK2VEC distance with the taxonomical distance, showing strong agreement.</p><p>Task embedding for iMaterialist In Fig. <ref type="figure">1</ref> we show a t-SNE visualization of the embedding for iMaterialist and iNaturalist tasks. Task embedding yields interpretable results: Tasks that are correlated in the dataset, such as binary classes corresponding to the same categorical attribute, may end up far away from each other and close to other tasks that are semantically more similar (e.g., the jeans category task is close to the ripped attribute and the denim material). This is reflected in the mixture of colors of semantically related nearby tasks, showing non-trivial grouping.</p><p>We also compare the TASK2VEC embedding with a domain embedding baseline, which only exploits the input distribution p(x) rather than the task distribution p(x, y). While some tasks are highly correlated with their domain (e.g., tasks from iNaturalist), other tasks differ only on the labels (e.g., all the attribute tasks of iMaterialist, which share the same clothes domain). Accordingly, the domain Table <ref type="table">1</ref>: Choice of probe network. Mean relative error increase over the ground-truth optimum on the iNat+CUB meta-task for different choices of the probe-network. We also report the performance on the top 10 tasks with more samples to show how data size affect different architectures.</p><p>embedding recovers similar clusters on iNaturalist. However, on iMaterialst domain embedding collapses all tasks to a single uninformative cluster (not a single point due to slight noise in embedding computation).</p><p>Task Embedding encodes task difficulty The scatterplot in Fig. <ref type="figure">3</ref> compares the norm of embedding vectors vs. performance of the best expert (or task specific model for cases where we have the diagonal computed). As shown analytically for the two-layers model, the norm of the task embedding correlates with the complexity of the task also on real tasks and architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model Selection</head><p>Given a task, our aim is to select an expert feature extractor that maximizes the classification performance on that task. We propose two strategies: (1) embed the task and select the feature extractor trained on the most similar task, and (2) jointly embed the models and tasks, and select a model using the learned metric (see Section 4). Notice that (1) does not use knowledge of the model performance on various tasks, which makes it more widely applicable but requires we know what task a model was trained for and may ignore the fact that models trained on slightly different tasks may still provide an overall better feature extractor (for example by over-fitting less to the task they were trained on).</p><p>In Table <ref type="table" target="#tab_2">2</ref> we compare the overall results of the various proposed metrics on the model selection meta-tasks. On both the iNat+CUB and Mixed meta-tasks, the Asymmetric TASK2VEC model selection is close to the ground-truth optimal, and significantly improves over both chance, and over using an generic ImageNet expert. Notice that our method has O(1) complexity, while searching over a collection of N experts is O(N ).</p><p>Error distribution In Fig. <ref type="figure">3</ref> we show in detail the error distribution of the experts on multiple tasks. It is interesting to notice that the classification error obtained using most experts clusters around some mean value, and little improvement is observed over using a generic expert. On the other hand, a few optimal experts can obtain a largely better performance on the task than a generic expert. This confirms the importance of having access to a large collection of experts when solving a new task, especially if few training data are available. But this collection can only be efficiently exploited if an algorithm is given to efficiently find one of the few experts for the task, which we propose.</p><p>Dependence on task dataset size Finding experts is especially important when the task we are interested in has relatively few samples. In Fig. <ref type="figure">4</ref> we show how the performance of TASK2VEC varies on a model selection task as the number of samples varies. At all sample sizes TASK2VEC is close to the optimum, and improves over selecting a generic expert (ImageNet), both when fine-tuning and when training only a classifier. We observe that the best choice of experts is not affected by the dataset size, and that even with few examples TASK2VEC is able to find the optimal experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of probe network</head><p>In Table <ref type="table">1</ref> we show that DenseNet <ref type="bibr" target="#b14">[15]</ref> and ResNet architectures <ref type="bibr" target="#b10">[11]</ref> perform significantly better when used as probe networks to compute the TASK2VEC embedding than a VGG <ref type="bibr" target="#b31">[32]</ref> architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Task and Domain embedding. Tasks distinguished by their domain can be understood simply in terms of image statistics. Due to the bias of different datasets, sometimes a benchmark task may be identified just by looking at a few images <ref type="bibr" target="#b33">[34]</ref> Average optimal error obtained on two meta-learning tasks by exhaustive search over the best expert, and relative error increase when using cheaper model selection methods. Always picking a fixed good general model (e.g., a model pretrained on ImageNet) performs better than picking an expert at random (chance). However, picking an expert using the Asymmetric TASK2VEC distance can achieve an overall better performance than using a general model. Notice also the improvement over the Symmetric version, especially on iNat + CUB, where experts trained on very similar tasks may be too simple to yield good transfer, and should be avoided.</p><p>statistics are useful (analogous to our choice of probe network) has also been considered, for example <ref type="bibr" target="#b8">[9]</ref> train an autoencoder that learns to extract fixed dimensional summary statistics that can reproduce many different datasets accurately. However, for general vision tasks which apply to all natural images, the domain is the same across tasks. Taskonomy <ref type="bibr" target="#b38">[39]</ref> explores the structure of the space of tasks, focusing on the question of effective knowledge transfer in a curated collection of 26 visual tasks, ranging from classification to 3D reconstruction, defined on a common domain. They compute pairwise transfer distances between pairs of tasks and use the results to compute a directed hierarchy. Introducing novel tasks requires computing the pairwise distance with tasks in the library. In contrast, we focus on a larger library of 1,460 fine-grained classification tasks both on same and different domains, and show that it is possible to represent tasks in a topological space with a constant-time embedding. The large task collection and cheap embedding costs allow us to tackle new meta-learning problems.</p><p>Fisher kernels Our work takes inspiration from Jaakkola and Hausler <ref type="bibr" target="#b15">[16]</ref>. They propose the "Fisher Kernel", which uses the gradients of a generative model score function as a representation of similarity between data items K(x (1) , x (2) ) = ? ? log P (x (1) |?) T F -1 ? ? log P (x (2) |?).</p><p>Here P (x|?) is a parameterized generative model and F is the Fisher information matrix. This provides a way to utilize generative models in the context of discriminative learning. Variants of the Fisher kernel have found wide use as a representation of images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, and other structured data such as protein molecules <ref type="bibr" target="#b16">[17]</ref> and text <ref type="bibr" target="#b29">[30]</ref>. Since the generative model can be learned on unlabelled data, several works have investigated the use of Fisher kernel for unsupervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr" target="#b34">[35]</ref> learns a metric on the Fisher kernel representation similar to our metric learning approach. Our approach differs in that we use the FIM as a representation of a whole dataset (task) rather than using model gradients as representations of individual data items.</p><p>Fisher Information for CNNs Our approach to task embedding makes use of the Fisher Information matrix of a neural network as a characterization of the task. Use of Fisher information for neural networks was popularized by Amari <ref type="bibr" target="#b5">[6]</ref> who advocated optimization using natural gradient descent which leverages the fact that the FIM is an appropriate parameterization-independent metric on statistical models. Recent work has focused on approximates of FIM appropriate in this setting (see e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>). FIM has also been proposed for various regularization schemes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>, analyze learning dynamics of deep networks <ref type="bibr" target="#b3">[4]</ref>, and to overcome catastrophic forgetting <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-learning and Model Selection</head><p>The general problem of meta-learning has a long history with much recent work dedicated to problems such as neural architecture search and hyper-parameter estimation. Closely related to our problem is work on selecting from a library of classifiers to solve a new task <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b19">20]</ref>. Unlike our approach, these usually address the question via land-marking or active testing, in which a few different models are evaluated and performance of the remainder estimated by extension. This can be viewed as a problem of completing a matrix defined by performance of each model on each task.</p><p>A similar approach has been taken in computer vision for selecting a detector for a new category out of a large library of detectors <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>TASK2VEC is an efficient way to represent a task, or the corresponding dataset, as a fixed dimensional vector. It has several appealing properties, in particular its norm correlates with the test error obtained on the task, and the cosine distance between embeddings correlates with natural distances between tasks, when available, such as the taxonomic distance for species classification, and the fine-tuning distance for transfer learning. Having a representation of tasks paves the way for a wide variety of meta-learning tasks. In this work, we focused on selection of an expert feature extractor in order to solve a new task, especially when little training data is present, and showed that using TASK2VEC to select an expert from a collection can sensibly improve test performance while adding only a small overhead to the training process.</p><p>Meta-learning on the space of tasks is an important step toward general artificial intelligence. In this work, we introduce a way of dealing with thousands of tasks, enough to enable reconstruct a topology on the task space, and to test meta-learning solutions. The current experiments highlight the usefulness of our methods. Even so, our collection does not capture the full complexity and variety of tasks that one may encounter in real-world situations. Future work should further test effectiveness, robustness, and limitations of the embedding on larger and more diverse collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analytic FIM for two-layer model</head><p>Assume we have data points (x i , y i ), i = 1 . . . n and y i ? {0, 1}. Assume that a fixed feature extractor applied to data points x yields features z = ?(x) ? R d and a linear model with parameters w is trained to model the conditional distribution p i = P (y i = 1|x i ) = ? w T ?(x i ) , where ? is the sigmoid function. The gradient of the cross-entropy loss with respect to the linear model parameters is:</p><formula xml:id="formula_11">? ?w = 1 N i (y i -p i )?(x i ),</formula><p>and the empirical estimate of the Fisher information matrix is:</p><formula xml:id="formula_12">F = E ? ?w ? ?w T = E y?pw(y|x) 1 N i ?(x i )(y i -p i ) 2 ?(x i ) T = 1 n i ?(x i )(1 -p i )p i ?(x i ) T</formula><p>In general, we are also interested in the Fisher information of the parameters of the feature extractor ?(x) since this is independent of the specifics of the output space y (e.g., for k-way classification). Consider a 2-layer network where the feature extractor uses a sigmoid non-linearity:</p><formula xml:id="formula_13">p = ?(w T z) z k = ?(U T k x)</formula><p>and the matrix U specifies the feature extractor parameters and w are parameters of the task-specific classifier. Taking the gradient w.r.t. parameters we have:</p><formula xml:id="formula_14">? ?w j = (y -p)z j ? ?U kj = (y -p)w k z k (1 -z k )x j</formula><p>The Fisher Information Matrix (FIM) consists of blocks:</p><formula xml:id="formula_15">? ?w i ? ?w j T = (y -p) 2 z i z j ? ?U ki ? ?w j T = (y -p) 2 z j z k (1 -z k )x i ? ?U li ? ?U kj T = (y -p) 2 w k z k (1 -z k )w l z l (1 -z l )x i x j</formula><p>We focus on the FIM of the probe network parameters which is independent of the dimensionality of the output layer and write it in matrix form as:</p><formula xml:id="formula_16">? ?U l ? ?U k T = (y -p) 2 (1 -z k )z k (1 -z l )z l w k w l xx T</formula><p>Note that each block {l, k} consists of the same matrix (y -p) 2 ? xx T multiplied by a scalar S kl given as:</p><formula xml:id="formula_17">S kl = (1 -z k )z k (1 -z l )z l w k w l</formula><p>We can thus write the whole FIM as the expectation of a Kronecker product:</p><formula xml:id="formula_18">F = E[(y -p) 2 ? S ? xx T ]</formula><p>where the matrix S can be written as where we take expectation over y w.r.t. the predictive distribution y ? p w (y|x).</p><formula xml:id="formula_19">S = ww T zz T (1 -z)(1 -z) T</formula><p>Example toy task embedding As noted in the main text, the FIM depends on the domain embedding, the particular task and its complexity. We illustrate these properties of the task embedding using an "toy" task space illustrated in Figure <ref type="figure">5</ref>. We generate 64 binary classification tasks by clustering a uniform grid of points in the XY plane into k ? <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> clusters using k-means and assigning a half of them to one category. We consider two different feature extractors, which play the role of "probe network". One is a collection of polynomial functions of degree d = 3, the second is 10 random linear features of the form max(0, ax + by + c) where a and b are sampled uniformly between [-1/2, 1/2] and c between [-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robust Fisher Computation</head><p>Consider again the loss function (parametrized with the covariance matrix ? instead of the precision matrix ? for convenience of notation):</p><formula xml:id="formula_20">L( ?; ?) = E w?N ( ?,?) [H pw, p(y|x)] + ? KL(N ( ?, ?) N (0, ? 2 I)).</formula><p>We will make use of the fact that the Fisher Information matrix is a positive semidefinite approximation of the Hessian H of the cross-entropy loss, and coincide with it in local minima <ref type="bibr" target="#b23">[24]</ref>. Expanding to the second order around ?, we have:</p><formula xml:id="formula_21">L( ?; ?) =E w?N ( ?,?) [H p ? , p(y|x) + ? w H p ? , p(y|x)(w -?) + 1 2 (w -?) T H(w -?)] + ? KL(N ( ?, ?) N (0, ? 2 I)) =H p ? , p(y|x) + 1 2 tr(?H) + ? KL(N ( ?, ?) N (0, ? 2 I)) =H p ? , p(y|x) + 1 2 tr(?H) + ? 2 [ ?2 ? 2 + 1 ? 2 tr? + k log ? 2 -log(|?|) -k]</formula><p>where in the last line used the known expression for the KL divergence of two Gaussian. Taking the derivative with respect to ? and setting it to zero, we obtain that the expression loss is minimized when ? -1 = 2 ? H + ? 2? 2 I , or, rewritten in term of the precision matrices, when</p><formula xml:id="formula_22">? = 2 ? H + ?? 2 2 I ,</formula><p>where we have introduced the precision matrices ? = ? -1 and ? 2 I = 1/? 2 I. We can then obtain an estimate of the Hessian H of the cross-entropy loss at the point ?, and hence of the FIM, by minimizing the loss L( ?, ?) with respect to ?. This is a more robust approximation than the standard definition, as it depends on the loss in a whole neighborhood of ? of size ? ?, rather than from the derivatives of the loss at a point. To further make the estimation more robust, and to reduce the number of parameters, we constrain ? to be diagonal, and constrain weights w ij belonging to the same filter to have the same precision ? ij . Optimization of this loss can be performed easily using Stochastic Gradient Variational Bayes, and in particular using the local reparametrization trick of <ref type="bibr" target="#b17">[18]</ref>.</p><p>The prior precision ? 2 should be picked according to the scale of the weights of each layer. In practice, since the weights of each layer have a different scale, we found it useful to select a different ? 2 for each layer, and train it together with ?,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of the experiments C.1. Training of experts and classifiers</head><p>Given a task, we train an expert on it by fine-tuning an off-the-shelf ResNet-34 pretrained on ImageNet 1 . Fine-tuning is performed by first fixing the weights of the network and retraining from scratch only the final classifier for 10 epochs using Adam, and then fine-tuning all the network together with SGD for 60 epochs with weight decay 5e-4, starting from learning rate 0.001 and decreasing it by a factor 0.1 at epochs 40.</p><p>Given an expert, we train a classifier on top of it by replacing the final classification layer and training it with Adam for 16 epochs. We use weight decay 5e-4 and learning rate 1e-4.</p><p>The tasks we train on generally have different number of samples and unbalanced classes. To limit the impact of this imbalance on the training procedure, regardless of the total size of the dataset, in each epoch we always sample 10,000 images with replacement, uniformly between classes. In this way, all epochs have the same length and see approximately the same number of examples for each class. We use this balanced sampling in all experiments, unless noted otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Computation of the TASK2VEC embedding</head><p>As the described in the main text, the TASK2VEC embedding is obtained by choosing a probe network, retraining the final classifier on the given task, and then computing the Fisher Information Matrix for the weights of the probe network.</p><p>Unless specified otherwise, we use an off-the-shelf ResNet-34 pretrained on ImageNet as the probe network. The Fisher Information Matrix is computed in a robust way minimizing the loss function L( ?; ?) with respect to the precision matrix ?, as described before. To make computation of the embedding faster, instead of waiting for the convergence of the classifier, we train the final classifier for 2 epochs using Adam and then we continue to train it jointly with the precision matrix ? using the loss L( ?; ?). We constrain ? to be positive by parametrizing it as ? = exp(L), for some unconstrained variable L. While for the classifier we use a low learning rate (1e-4), we found it useful to use an higher learning rate (1e-2) to train L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Training the MODEL2VECembedding</head><p>As described in the main text, in the MODEL2VECembedding we aim to learn a vector representation m j = F j + b j of the j-th model in the collection, which represents both the task the model was trained on (through the TASK2VEC embedding F j ), and the particularities of the model (through the learned parameter b j ).</p><p>We learn b j by minimizing a k-way classification loss which, given a task t, aims to select the model that performs best on the task among a collection of k models. Multiple models may perform similarly and close to optimal: to preserve this information, instead of using a one-hot encoding for the best model, we train using soft-labels obtained as follows:</p><formula xml:id="formula_23">p(y i ) = Softmax -? error i -mean(error i ) std(error i ) ,</formula><p>where error i,j is the ground-truth test error obtained by training a classifier for task i on top of the j-th model. Notice that for ? 1, the soft-label y j i reduces to the one-hot encoding of the index of the best performing model. However, for lower ?'s, the vector y i contains richer information about the relative performance of the models.</p><p>1 https://pytorch.org/docs/stable/torchvision/models.html</p><p>We obtain our prediction in a similar way: Let d i,j = d asym (t i , m j ), then we set our model prediction to be</p><formula xml:id="formula_24">p(y|d i,0 , . . . , d i,k ) = Softmax(-? d i ),</formula><p>where the scalar ? &gt; 0 is a learned parameter. Finally, we learn both the m j 's and ? using a cross-entropy loss:</p><formula xml:id="formula_25">L = 1 N N i=0 E yi? p[p(y i |d i,0 , . . . , d i,k )],</formula><p>which is minimized precisely when p(y|d i,0 , . . . , d i,k ) = p(y i ).</p><p>In our experiments we set ? = 20, and minimize the loss using Adam with learning rate 0.05, weight decay 0.0005, and early stopping after 81 epochs, and report the leave-one-out error (that is, for each task we train using the ground truth of all other tasks and test on that task alone, and report the average of the test errors obtained in this way).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets, tasks and meta-tasks</head><p>Our two model selection meta-tasks, iNat+CUB and Mixed, are curated as follows. For iNat+CUB, we generated 50 tasks and (the same) experts from iNaturalist and CUB. The 50 tasks consist of 25 iNaturalist tasks and 25 CUB tasks to provide a balanced mix from two datasets of the same domain. We generated the 25 iNaturalist tasks by grouping species into orders and then choosing the top 25 orders with the most samples. The number of samples for tasks shows the heavy-tail distribution typical of real data, with the top task having 64,100 samples (the Passeriformes order classification task), while most tasks have around 6, 000 samples.</p><p>The 25 CUB tasks were similarly generated with 10 order tasks but additionally has 15 Passeriformes family tasks: After grouping CUB into orders, we determined 11 usable order tasks (the only unusable order task, Gaviiformes, has only one species so it makes no sense to train on it). However, one of the orders-Passeriformes-dominated all other orders with 134 species when compared to 3-24 species of other orders. Therefore, we decided to further subdivide the Passeriformes order task into family tasks (i.e., grouping species into families) to provide a more balanced partition. This resulted in 15 usable family tasks (i.e., has more than one species) out of 22 family tasks. Unlike iNaturalist, tasks from CUB have only a few hundreds of samples and hence benefit more from carefully selecting an expert.</p><p>In the iNAT+CUB meta-task the classification tasks are the same tasks used to train the experts. To avoid trivial solutions (always selecting the expert trained on the task we are trying to solve) we test in a leave-one-out fashion: given a classficication task, we aim to select the best expert that was not trained on the same data.</p><p>For the Mixed meta-task, we chose 40 random tasks and 25 curated experts from all datasets. The 25 experts were generated from iNaturalist, iMaterialist and DeepFashion (CUB, having fewer samples than iNaturalist, is more appropriate as tasks). For iNaturalist, we trained 15 experts: 8 order tasks and 7 class tasks (species ordered by class), both with number of samples greater than 10,000. For DeepFashion, we trained 3 category experts (upper-body, lower-body, full-body). For iMaterialist, we trained 2 category experts (pants, shoes) and 5 multi-label experts by grouping attributes (color, gender, neckline, sleeve, style). For the purposes of clustering attributes into larger groups for training experts (and color coding the dots in Figure <ref type="figure">1</ref>), we obtained a de-anonymized list of the iMaterialist Fashion attribute names from the FGCV contest organizers.</p><p>The 40 random tasks were generated as follows. In order to balance tasks among all datasets, we selected 5 CUB, 15 iNaturalist, 15 iMaterialist and 5 DeepFashion tasks. Within those datasets, we randomly pick tasks with a sufficient number of validation samples and maximum variety. For the iNaturalist tasks, we group the order tasks into class tasks, filter out the number of validation samples less than 100 and randomly pick order tasks within each class. For the iMaterialist tasks, we similarly group the tasks (e.g. category, style, pattern), filter out tasks with less than 1,000 validation samples and randomly pick tasks within each group. For CUB, we randomly select 2 order tasks and 3 Passeriformes family tasks, and for DeepFashion, we randomly select the tasks uniformly. All this ensures that we have a balanced variety of tasks.</p><p>For the data efficiency experiment, we trained on a subset of the tasks and experts in the Mixed meta-task: We picked the Accipitriformes, Asparagales, Upper-body, Short Sleeves for the tasks, and the Color, Lepidoptera, Upper-body, Passeriformes, Asterales for the experts. Tasks where selected among those that have more than 30,000 training samples in order to represent all datasets. The experts were also selected to be representative of all datasets, and contain both strong and very weak experts (such as the Color expert).      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Error matrices</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distance between species classification tasks. (Left) Task similarity matrix ordered by hierarchical clustering. Note that the dendrogram produced by the task similarity matches the taxonomic clusters (indicated by color bar). (Center)For tasks extracted from iNaturalist and CUB, we compare the cosine distance between tasks to their taxonomical distance. As the size of the task embedding neighborhood increases (measured by number of tasks in the neighborhood), we plot the average taxonomical distance of tasks from the neighborhood center. While the task distance does not perfectly match the taxonomical distance (whose curve is shown in orange), it shows a good correlation. Difference are both due to the fact that taxonomically close species may need very different features to be classified, creating a mismatch between the two notions of distance, and because for some tasks in iNaturalist too few samples are provided to compute a good embedding. (Right) Correlation between L 1 norm of the task embedding (distance from origin) and test error obtained on the task.</figDesc><graphic url="image-1.png" coords="4,68.08,27.10,198.00,196.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[</head><label></label><figDesc>C U B ] B o m b y c il li d a e [ C U B ] T h r a u p id a e [ C U B ] L a n ii d a e [ C U B ] P a s s e r id a e [ C U B ] M im id a e [ C U B ] A n s e r if o r m e s [ C U B ] F r in g il li d a e [ C U B ] C a r d in a li d a e [ C U B ] C a p r im u lg if o r m e s [ C U B ] P r o c e ll a r ii f o r m e s [ C U B ] A p o d if o r m e s [ C U B ] H ir u n d in id a e [ C U B ] C u c u li f o r m e s [ C U B ] C o r a c ii f o r m e s [ C U B ] P o d ic ip e d if o r m e s [ C U B ] P e le c a n if o r m e s [ C U B ] P ic if o r m e s [ C U B ] C o r v id a e [ C U B ] Ic t e r id a e [ C U B ] T r o g lo d y t id a e [ C U B ] T y r a n n id a e [ C U B ] V ir e o n id a e [ C U B ] C h a r a d r ii f o r m e s [ C U B ] P a r u li d a e [ C U B ] E m b e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :( 1 -</head><label>51</label><figDesc>Figure 5: Task embeddings computed for a probe network consisting of (a) 10 random linear + ReLU features and (b)degree three polynomial features projected to 2D using t-SNE. The tasks are random binary partitions of the unit square visualized in each icon (three tasks are visualized on the left) and cannot be distinguished based purely on the input domain without considering target labels. Note that qualitatively similar tasks group together, with more complex tasks (requiring complicated decision boundaries) separated from simpler tasks.</figDesc><graphic url="image-2.png" coords="12,126.56,66.76,171.61,167.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[</head><label></label><figDesc>C U B ] P ro c e ll a ri if o rm e s (A v e s ) [C U B ] C u c u li fo rm e s (A v e s ) [C U B ] C h a ra d ri if o rm e s (A v e s ) [C U B ] C a p ri m u lg if o rm e s (A v e s ) [C U B ] P e le c a n if o rm e s (A v e s ) [C U B ] P ic if o rm e s (A v e s ) [C U B ] A n s e ri fo rm e s (A v e s ) [C U B ] P o d ic ip e d if o rm e s (A v e s ) [C U B ] A p o d if o rm e s (A v e s ) [C U B ] C o ra c ii fo rm e s (A v e s ) [C U B ] Ic te ri d a e (A v e s ) [C U B ] C a rd in a li d a e (A v e s ) [C U B ] M im id a e (A v e s ) [C U B ] P a ru li d a e (A v e s ) [C U B ] E m b e ri z id a e (A v e s ) [C U B ] C o rv id a e (A v e s ) [C U B ] F ri n g il li d a e (A v e s ) [C U B ] T y ra n n id a e (A v e s ) [C U B ] L a n ii d a e (A v e s ) [C U B ] P a s s e ri d a e (A v e s ) [C U B ] H ir u n d in id a e (A v e s ) [C U B ] T h ra u p id a e (A v e s ) [C U B ] V ir e o n id a e (A v e s ) [C U B ] B o m b y c il li d a e (A v e s ) [C U B ] T ro g lo d y ti d a e (A v e s ) [i N a t] P a s s e ri fo rm e s (A v e s ) [i N a t] L e p id o p te ra (I n s e c ta ) [i N a t] S q u a m a ta (R e p ti li a ) [i N a t] O d o n a ta (I n s e c ta ) [i N a t] C h a ra d ri if o rm e s (A v e s ) [i N a t] A s te ra le s (M a g n o li o p s id a ) [i N a t] P e le c a n if o rm e s (A v e s ) [i N a t] A n s e ri fo rm e s (A v e s ) [i N a t] L a m ia le s (M a g n o li o p s id a ) [i N a t] A n u ra (A m p h ib ia ) [i N a t] A c c ip it ri fo rm e s (A v e s ) [i N a t] C a ry o p h y ll a le s (M a g n o li o p s id a ) [i N a t] C o le o p te ra (I n s e c ta ) [i N a t] A s p a ra g a le s (L il io p s id a ) [i N a t] R o d e n ti a (M a m m a li a ) [i N a t] C a rn iv o ra (M a m m a li a ) [i N a t] P ic if o rm e s (A v e s ) [i N a t] F a b a le s (M a g n o li o p s id a ) [i N a t] R o s a le s (M a g n o li o p s id a ) [i N a t] C o lu m b if o rm e s (A v e s ) [i N a t] P e rc if o rm e s (A c ti n o p te ry g ii ) [i N a t] S a p in d a le s (M a g n o li o p s id a ) [i N a t] R a n u n c u la le s (M a g n o li o p s id a ) [i N a t] G e n ti a n a le s (M a g n o li o p s id a ) [i N a t] E ri c a le s (M a g n o li o p s id a )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>U p p e</head><label></label><figDesc>r-b o d y cl o th e s L o w e r-b o d y cl o th e s F u ll -b o d y cl o th e s C o lo r G e n d e r N e ck li n e S le e v e S ty le P a n ts S h o e s A v e s M a g n o li o p si d a In se ct a R e p ti li a M a m m a li a L il io p si d a A m p h ib ia P a ss e ri fo rm e s L e p id o p te ra S q u a m a ta O d o n a ta C h a ra d ri if o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Meta-tasks ground-truth error matrices. (Best viewed magnified). (Left) Error matrix for the CUB+iNat meta-task. The numbers in each cell is the test error obtained by training a classifier on a given combination of task (rows) and expert (columns). The background color represent the Asymmetric TASK2VEC distance between the target task and the task used to train the expert. Numbers in red indicate the selection made by the model selection algorithm based on the Asymmetric TASK2VEC embedding. The (out-of-diagonal) optimal expert (when different from the one selected by our algorithm), is highlighted in blue. (Right) Same as before, but for the Mixed meta-task.</figDesc><graphic url="image-6.png" coords="15,105.98,103.80,237.22,237.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Performance of model selection on a subset of 4 tasks as a function of the number of samples available to train relative to optimal model selection (dashed orange). Training a classifier on the feature extractor selected by TASK2VEC (solid red) is always better than using a generic ImageNet feature extractor (dashed red). The same holds when allowed to fine-tune the feature extractor (blue curves). Also notice that in the low-data regime fine-tuning the ImageNet feature extractor is more expensive and has a worse performance than accurately selecting a good fixed feature extractor.</figDesc><table><row><cell>(lower is better) Error relative to brute force</cell><cell>-10% 0% 10%</cell><cell>10 2</cell><cell cols="2">10 3 Number of samples 10 4 Brute force fixed ImageNet fixed Task2Vec fixed ImageNet finetune Task2Vec finetune</cell></row><row><cell cols="5">Figure 4: TASK2VEC improves results at different</cell></row><row><cell cols="5">dataset sizes and training conditions: Probe network Top-10</cell><cell>All</cell></row><row><cell></cell><cell cols="3">Chance</cell><cell>+13.95% +59.52%</cell></row><row><cell></cell><cell cols="3">VGG-13</cell><cell>+4.82%</cell><cell>+38.03%</cell></row><row><cell></cell><cell cols="3">DenseNet-121</cell><cell>+0.30%</cell><cell>+10.63%</cell></row><row><cell></cell><cell cols="3">ResNet-13</cell><cell>+0.00%</cell><cell>+9.97%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>. The question of determining what summary Model selection performance of different metrics.</figDesc><table><row><cell>Meta-task</cell><cell>Optimal</cell><cell>Chance</cell><cell cols="4">ImageNet TASK2VEC Asymmetric TASK2VEC MODEL2VEC</cell></row><row><cell>iNat + CUB</cell><cell>31.24</cell><cell>+59.52%</cell><cell>+30.18%</cell><cell>+42.54%</cell><cell>+9.97%</cell><cell>+6.81%</cell></row><row><cell>Mixed</cell><cell>22.90</cell><cell cols="2">+112.49% +75.73%</cell><cell>+40.30%</cell><cell>+29.23%</cell><cell>+27.81%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>15 18 18 20 16 17 19 16 22 17 22 17 18 23 24 19 19 16 22 17 17 23 23 24 20 17 20 15 18 14 30 18 15 25 26 24 26 18 19 20 19 25 18 26 19 27 24 20 22 24 12 15 17 18 18 23 17 20 18 22 23 18 14 18 18 21 20 20 20 16 22 23 22 18 19 12 16 20 19 19 32 14 19 25 17 20 29 21 27 19 17 16 26 29 19 20 31 27 25 25 45 44 41 45 43 45 45 46 46 46 47 49 46 47 47 45 46 47 48 47 44 47 48 44 48 36 42 45 45 32 51 41 43 47 48 46 50 42 47 48 45 44 50 48 43 44 49 48 50 48 21 24 29 16 27 21 16 23 27 23 21 25 25 27 20 27 23 23 25 27 24 20 29 21 17 27 24 16 19 23 33 27 20 23 24 23 23 27 25 28 23 17 29 35 35 21 29 27 31 31 18 17 22 15 17 19 19 20 20 19 17 19 17 19 20 15 18 16 21 18 17 20 23 17 21 20 19 17 20 19 24 17 18 20 24 22 25 20 23 17 19 20 19 20 18 16 25 25 23 24 13 11 15 13 10 8 11 12 14 14 13 13 14 11 15 12 16 17 13 15 16 14 15 16 16 6 9 14 11 10 13 9 12 14 14 16 14 10 12 13 12 3 15 15 15 10 16 16 15 13 16 14 16 13 12 15 12 12 12 11 14 16 17 12 13 13 15 15 13 17 14 15 15 14 14 12 11 12 12 12 20 10 8 17 14 14 12 11 14 14 10 12 14 14 15 11 17 16 15 17 20 17 27 16 21 21 13 16 19 16 17 24 24 20 20 18 22 20 22 17 17 18 20 17 22 17 16 24 21 13 26 17 15 18 25 20 17 15 18 17 22 17 19 25 22 18 23 20 17 17 22 27 24 22 24 27 25 21 23 27 23 29 27 25 24 22 27 27 26 27 22 27 30 25 22 18 19 24 21 22 26 24 19 27 26 24 27 31 28 22 21 22 32 22 25 24 27 26 28 28 19 21 21 19 19 20 18 20 21 19 21 23 22 26 21 22 22 30 17 20 21 19 26 22 25 17 16 20 20 15 30 18 16 29 25 19 20 19 24 21 18 21 31 25 21 24 30 24 29 21 28 31 32 27 32 31 30 31 34 30 25 30 29 28 32 32 29 29 30 28 31 34 28 30 28 20 25 30 29 26 35 26 27 35 35 28 34 29 34 33 30 28 32 36 29 28 37 36 36 36 48 45 52 44 47 46 45 44 49 48 44 45 50 50 46 48 43 48 43 45 41 35 45 43 46 48 46 48 47 47 51 48 41 46 49 48 46 48 49 48 29 50 50 48 46 64 68 70 67 63 64 67 64 64 66 70 65 63 67 67 69 67 68 67 66 69 64 65 65 63 64 61 61 65 65 64 67 70 59 63 67 60 62 64 65 67 64 56 61 70 57 53 60 61 62 50 48 50 50 49 48 53 51 49 48 50 47 46 52 53 45 51 51 50 49 52 49 50 49 50 51 48 49 51 52 35 48 50 33 50 54 38 49 37 50 49 49 39 37 54 47 42 35 39 39 57 54 52 55 51 54 56 55 53 54 54 53 54 55 54 52 50 53 53 53 52 53 55 54 55 53 51 51 50 57 41 54 57 42 51 55 44 48 45 57 56 51 46 45 54 48 47 45 40 43 49 46 52 47 48 48 47 49 47 48 46 44 47 49 48 48 51 49 49 47 50 47 49 52 48 44 45 49 45 51 38 48 47 37 46 51 38 45 35 47 50 49 40 40 50 45 40 39 41 35</figDesc><table><row><cell>12 13 10 14 8 11 12 14 12 9 14 7 13 14 16 13 9 14 10 12 15 16 13 12 13 6 10 16 14 9 18 9 13 18 13 10 19 11 13 13 14 12 16 18 11 12 15 18 20 19 7 14 6 8 9 10 8 14 12 12 13 10 11 8 11 9 14 13 11 12 11 12 15 10 11 5 5 8 10 10 17 9 4 16 10 9 14 11 14 8 11 10 10 14 5 8 10 15 13 15 35 37 41 36 36 36 38 42 36 40 37 37 36 23 36 36 35 39 40 40 40 38 34 38 35 14 26 36 33 33 42 33 34 42 37 40 41 29 39 36 37 30 42 42 34 33 43 45 43 41 36 42 43 38 38 42 38 43 40 41 41 44 40 39 29 41 42 43 41 38 42 43 42 42 39 18 31 40 38 32 53 39 40 50 43 42 49 34 45 43 41 36 47 50 35 40 50 50 51 49 29 30 32 29 29 29 29 29 33 27 30 30 32 30 33 25 32 30 30 28 27 30 31 29 30 23 26 29 32 27 35 30 32 33 30 32 35 27 33 27 25 31 31 33 28 34 35 35 34 35 7 5 11 7 12 6 9 9 7 9 7 8 8 4 8 11 6 8 10 7 8 7 8 10 6 4 7 7 6 6 8 6 7 13 7 9 7 7 11 7 8 7 9 13 7 6 13 10 9 11 42 43 42 41 38 40 40 41 43 43 38 42 41 42 40 41 43 36 46 44 40 42 40 42 42 24 39 40 41 38 46 40 37 47 40 41 49 37 40 43 42 39 45 44 37 41 46 50 46 46 35 35 32 28 27 38 40 42 35 33 33 33 45 22 30 32 35 33 22 33 35 35 30 28 42 25 27 37 23 28 32 20 17 42 33 30 37 27 35 37 28 27 35 37 30 33 37 35 37 32 38 27 32 38 40 35 33 32 35 37 32 42 30 35 33 35 32 30 28 32 33 27 37 37 27 18 35 22 30 33 38 35 25 38 30 32 35 28 35 32 32 28 33 35 25 35 35 43 37 42 27 23 29 20 21 21 19 24 27 23 25 27 24 22 24 21 25 20 24 23 23 27 23 28 22 12 19 26 29 17 32 18 18 30 25 24 28 20 29 25 24 23 28 26 23 27 34 26 28 26 10 13 7 7 13 8 13 13 13 10 5 7 10 10 5 8 8 12 12 8 13 8 15 13 3 3 5 7 5 8 13 8 10 15 7 10 8 7 8 5 7 8 12 13 7 12 10 18 12 13 32 33 39 35 36 31 34 38 33 35 33 35 34 27 34 29 35 33 35 34 36 33 32 41 30 15 24 32 32 27 41 27 33 45 37 35 44 34 43 29 33 28 44 42 31 37 48 46 42 40 10 10 15 7 10 8 13 20 10 10 10 12 13 10 13 15 10 7 8 12 10 8 12 13 8 7 15 12 12 18 17 12 13 15 18 10 18 13 13 12 7 10 17 12 13 12 17 8 15 13 32 32 33 27 31 32 30 36 31 32 30 31 31 31 30 33 26 31 35 30 29 34 30 30 22 18 24 29 33 29 35 30 31 32 28 29 33 32 35 30 30 31 40 35 34 30 37 38 32 34 78 81 81 78 78 81 79 82 80 81 79 81 80 80 81 80 78 81 82 78 79 81 81 81 80 56 73 79 78 76 86 75 77 84 82 78 85 77 82 79 79 77 85 85 75 78 86 85 87 84 61 60 62 58 60 59 61 63 60 60 60 61 60 62 60 60 60 63 62 60 60 62 62 60 60 54 31 54 56 60 62 59 60 60 55 60 60 54 62 60 59 60 62 62 60 57 61 62 63 61 73 74 73 70 72 73 73 75 74 72 75 74 70 75 74 72 73 76 74 72 74 72 75 72 73 68 68 61 71 71 75 71 74 74 73 73 73 71 74 71 72 73 77 76 71 70 76 77 77 76 71 73 73 69 70 70 73 72 70 70 72 74 71 71 73 71 71 74 72 72 71 73 71 71 72 65 59 67 45 69 72 68 71 69 70 71 70 64 69 71 70 68 67 72 71 69 74 72 73 71 62 60 61 59 65 62 63 65 64 62 65 64 59 63 65 64 60 66 63 60 61 62 65 64 65 54 59 59 62 48 70 57 57 65 66 60 66 60 69 62 63 60 68 70 61 64 69 69 69 65 69 68 69 69 69 69 70 70 69 67 68 67 66 69 68 67 68 69 68 70 69 69 69 69 69 66 64 66 66 67 49 69 69 54 69 69 57 65 59 69 68 69 58 60 68 66 63 58 58 59 58 61 55 49 54 56 54 57 58 59 62 55 58 64 60 59 57 64 57 54 57 57 63 53 59 48 50 54 60 53 67 46 51 59 56 51 62 57 64 51 47 56 64 64 54 59 67 67 67 61 60 60 63 61 61 60 60 63 64 60 66 65 60 62 63 59 61 66 60 62 58 66 67 59 62 57 59 66 62 56 69 59 57 61 67 61 66 63 65 64 62 62 64 65 63 65 66 65 68 64 62 58 61 60 59 58 61 63 57 59 58 59 58 60 61 56 60 60 60 61 61 59 61 61 63 55 57 57 56 57 43 59 61 38 57 60 47 54 45 60 58 59 46 45 61 58 52 49 47 48 69 69 73 65 71 72 72 71 69 72 71 73 70 73 71 69 71 71 73 69 70 75 70 74 66 68 60 64 70 70 75 68 71 71 58 70 74 64 71 72 73 70 70 74 70 69 76 75 71 71 69 69 69 68 73 72 68 75 74 76 70 73 68 74 69 70 70 77 73 69 71 76 71 72 71 62 67 68 68 68 77 69 68 76 66 66 74 66 71 72 68 66 74 76 71 72 71 77 73 72 60 57 59 56 58 55 60 58 55 58 55 54 57 57 59 53 58 56 57 57 56 55 57 55 54 55 53 53 56 57 44 56 57 45 54 57 42 53 48 53 56 57 47 50 56 54 50 46 48 46 45 42 44 41 43 42 42 46 43 43 43 43 42 40 45 42 44 45 45 40 43 43 41 40 43 34 29 40 37 41 49 41 43 44 39 44 44 29 43 42 42 40 42 46 43 38 52 48 45 46 51 50 53 48 50 51 51 54 50 50 52 48 52 52 55 50 52 54 52 51 53 45 51 50 52 44 47 49 51 51 38 48 50 38 49 53 40 48 34 51 49 52 41 42 52 48 44 44 42 44 56 50 54 51 48 49 48 58 49 49 53 51 48 50 52 46 52 50 47 50 50 48 48 46 46 43 51 44 50 53 56 52 56 58 51 50 58 51 54 44 50 52 54 54 50 48 59 61 59 59 52 47 58 48 47 54 49 56 49 51 53 51 50 55 54 52 50 49 52 47 52 49 53 49 50 52 49 49 53 54 58 53 54 58 54 53 58 53 56 54 46 52 54 59 49 49 57 57 57 54 59 62 60 52 60 56 57 52 59 62 53 63 61 64 62 63 58 63 61 60 61 63 66 59 63 41 50 60 51 52 69 55 54 72 61 59 66 50 60 53 56 48 63 67 58 60 70 66 68 69 61 60 59 57 61 64 65 61 60 59 58 58 59 61 59 59 60 63 61 61 60 58 63 58 60 56 54 60 58 59 49 58 61 46 58 60 49 57 48 59 60 60 43 49 62 57 50 52 50 50 65 65 65 66 64 63 71 70 64 63 63 64 65 68 64 62 62 66 65 66 68 63 69 67 66 59 61 62 67 68 51 62 66 54 62 63 57 61 62 64 62 66 59 49 64 63 56 57 58 58 53 50 55 51 50 58 55 56 59 59 57 57 50 62 59 50 55 58 55 52 57 59 59 60 57 41 50 51 59 50 67 55 51 62 58 50 61 50 56 59 58 59 67 61 50 64 64 63 59 65 45 46 48 45 44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>40 38 40 92 45 44 47 43 39 41 16 39 26 35 28 38 33 14 25 36 33 32 44 33 33 39 44 42 92 43 43 48 46 42 47 21 48 30 39 36 45 38 20 32 40 39 30 50 38 35 34 43 44 80 41 40 44 42 42 46 26 44 38 40 39 40 43 25 37 39 41 37 44 34 39 44 47 49 89 46 45 48 46 43 47 35 46 40 42 45 42 48 37 41 42 45 32 51 40 38 69 70 68 93 69 70 74 77 69 76 59 69 67 68 65 72 69 67 64 71 72 67 75 68 70 58 61 57 95 63 64 71 62 58 60 56 36 49 54 56 44 53 56 51 54 54 60 43 58 58 43 45 45 95 49 51 58 52 45 47 33 39 22 37 38 42 39 33 29 39 38 42 46 43 39 52 56 54 86 54 54 61 56 54 52 41 49 46 47 49 54 50 45 52 45 54 52 51 52 49 12 22 16 69 20 24 24 20 17 16 14 22 19 14 20 18 25 20 14 18 20 20 26 19 14 51 55 53 94 57 52 59 62 60 55 46 51 44 54 48 53 53 50 47 54 50 55 57 56 54 63 66 63 96 66 64 71 71 63 63 37 63 52 52 54 64 62 35 47 58 56 60 71 58 57 61 65 60 95 65 62 73 64 63 62 56 37 50 59 57 45 57 55 56 60 60 56 47 59 61 66 66 69 95 69 64 71 70 66 68 59 46 61 66 63 56 63 61 62 66 63 65 53 62 61 50 59 61 90 51 48 50 57 50 60 37 55 48 56 51 63 52 40 57 54 57 50 62 55 50 48 49 52 97 50 47 57 56 44 50 38 43 35 42 45 42 45 39 36 45 44 47 47 47 47 60 62 61 94 67 62 68 67 61 65 52 61 43 53 53 60 57 53 49 55 56 59 64 57 54 64 65 66 89 62 59 68 65 63 63 54 61 57 59 58 62 56 54 56 56 58 58 65 61 54 44 47 52 95 53 53 60 53 51 49 39 45 27 39 44 43 43 39 30 40 43 44 50 44 45 62 72 63 88 63 70 67 73 65 62 42 63 62 55 60 50 65 48 48 57 53 58 65 53 50 54 52 56 89 60 60 67 60 54 56 52 60 52 45 52 54 42 51 50 49 59 50 55 55 52 12 10 13 30 6 9 9 10 10 11 14 11 16 14 14 14 18 13 15 10 11 8 10 11 14 9 8 10 21 9 7 9 7 7 6 5 6 5 9 5 8 5 5 6 5 6 4 6 7 6 25 33 22 19 41 21 40 17 31 39 40 21 42 47 22 32 32 27 44 65 37 41 32 38 32 33 22 17 66 24 40 26 27 26 32 30 28 28 28 20 34 29 27 26 35 24 35 30 35 34 9 9 12 28 9 10 8 7 7 8 10 12 9 12 9 10 10 11 5 8 12 6 8 11 17 17 26 13 30 18 27 17 17 16 14 18 19 22 23 18 17 20 24 20 17 21 17 24 20 25 26 26 14 20 20 14 21 22 19 27 20 19 20 26 14 14 22 27 26 26 21 16 22 23 23</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.kaggle.com/c/imaterialist-challenge-fashion-2018" />
		<title level="m">FGVC5 workshop, CVPR 2018</title>
		<imprint/>
	</monogr>
	<note>iMaterialist Challenge (Fashion</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speeding up algorithm selection using average ranking and active testing by introducing runtime</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Abdulrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="108" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The dynamic distance between learning tasks: From Kolmogorov complexity to transfer learning via quantum physics and the information bottleneck of the weights of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mbeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>ArXiv: 1810.02440</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NIPS Workshop on Integration of Deep Learning Theories</title>
		<meeting>of the NIPS Workshop on Integration of Deep Learning Theories</meeting>
		<imprint>
			<date type="published" when="2002">October 2018. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Critical learning periods in deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rovere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>ArXiv:1711.08856</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intl. Conf. on Learning Representations (ICLR)</title>
		<meeting>of the Intl. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>ArXiv 1706.01350</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Methods of information geometry, volume 191 of translations of mathematical monographs</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nagaoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
		<title level="m">Towards a neural statistician</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On natural learning and pruning in multilayered perceptrons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="881" to="901" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining generative models and fisher kernels for object recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using the fisher kernel method to detect remote protein homologies</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diekhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">201611835. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selecting classification algorithms with active testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on machine learning and data mining in pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="117" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09913</idno>
		<title level="m">Visualizing the loss landscape of neural nets</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stokes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01530</idno>
		<title level="m">Fisher-rao metric, geometry, and complexity of neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">New perspectives on the natural gradient method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<idno>CoRR, abs/1412.1193</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model recommendation for action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Matikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2256" to="2263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><surname>Fisher Gan</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2513" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">String kernels, fisher kernels and finite state automata</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinokourov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning with labeled and unlabeled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<idno>EPFL-REPORT-161327</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Institute for Adaptive and Neural Computation, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recommending learning algorithms and their associated hyperparameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud-Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.1890</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discriminative fisher kernels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Model recommendation: Generating object detectors from few samples</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1619" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting failures of vision systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3566" to="3573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cub] Procellariiformes</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Aves) [CUB] Cuculiformes (Aves</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Aves) [CUB] Caprimulgiformes (Aves) [CUB] Pelecaniformes (Aves) [CUB] Piciformes (Aves) [CUB] Anseriformes (Aves) [CUB] Podicipediformes (Aves) [CUB] Apodiformes (Aves)</title>
		<author>
			<persName><surname>Charadriiformes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Aves) [CUB] Icteridae (Aves) [CUB] Cardinalidae (Aves) [CUB] Mimidae (Aves) [CUB] Parulidae (Aves) [CUB] Emberizidae (Aves) [CUB] Corvidae (Aves) [CUB] Fringillidae (Aves) [CUB] Tyrannidae (Aves) [CUB] Laniidae (Aves) [CUB] Passeridae (Aves) [CUB] Hirundinidae</title>
		<author>
			<persName><surname>Coraciiformes</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Aves) [CUB] Thraupidae (Aves) [CUB] Vireonidae (Aves</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bombycillidae</surname></persName>
		</author>
		<imprint>
			<publisher>Aves</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Troglodytidae</surname></persName>
		</author>
		<imprint>
			<publisher>Aves</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Passeriformes</surname></persName>
		</author>
		<imprint>
			<publisher>Aves</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Insecta) [iNat] Squamata (Reptilia) [iNat] Odonata (Insecta)</title>
		<author>
			<persName><surname>Lepidoptera</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Charadriiformes</surname></persName>
		</author>
		<imprint>
			<publisher>Aves</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Asterales</surname></persName>
		</author>
		<author>
			<persName><surname>Magnoliopsida</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>iNat] Pelecaniformes (Aves) [iNat] Anseriformes (Aves</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Lamiales (Magnoliopsida) [iNat] Anura (Amphibia)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Aves) [iNat] Caryophyllales (Magnoliopsida) [iNat] Coleoptera (Insecta)</title>
		<author>
			<persName><surname>Accipitriformes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Liliopsida) [iNat] Rodentia (Mammalia) [iNat] Carnivora (Mammalia)</title>
		<author>
			<persName><surname>Asparagales</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>iNat] Piciformes (Aves</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Fabales</surname></persName>
		</author>
		<author>
			<persName><surname>Magnoliopsida</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>iNat] Rosales (Magnoliopsida) [iNat] Columbiformes (Aves</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Perciformes</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Actinopterygii) [iNat] Sapindales (Magnoliopsida) [iNat] Ranunculales (Magnoliopsida) [iNat] Gentianales (Magnoliopsida) [iNat] Ericales (Magnoliopsida</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
