<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experiences with an Interactive Museum Tour-Guide Robot</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1999-06-20">20 June 1999</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Armin</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department and Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Hähnel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gerhard</forename><surname>Lakemeyer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department V</orgName>
								<orgName type="institution">Technological University of Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Schulz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><surname>Steiner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department and Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department III</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Experiences with an Interactive Museum Tour-Guide Robot</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="1999-06-20">20 June 1999</date>
						</imprint>
					</monogr>
					<idno type="MD5">493AB3D7B22AE5CA34718ECB8FE46891</idno>
					<note type="submission">submitted to Elsevier Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mobile robotics</term>
					<term>probabilistic reasoning</term>
					<term>localization</term>
					<term>mapping</term>
					<term>planning</term>
					<term>collision avoidance</term>
					<term>logic</term>
					<term>human robot interaction</term>
					<term>machine learning</term>
					<term>entertainment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the software architecture of an autonomous, interactive tour-guide robot. It presents a modular and distributed software architecture, which integrates localization, mapping, collision avoidance, planning, and various modules concerned with user interaction and Web-based telepresence. At its heart, the software approach relies on probabilistic computation, on-line learning, and any-time algorithms. It enables robots to operate safely, reliably, and at high speeds in highly dynamic environments, and does not require any modifications of the environment to aid the robot's operation. Special emphasis is placed on the design of interactive capabilities that appeal to people's intuition. The interface provides new means for human-robot interaction with crowds of people in public places, and it also provides people all around the world with the ability to establish a "virtual telepresence" using the Web. To illustrate our approach, results are reported obtained in mid-1997, when our robot "RHINO" was deployed for a period of six days in a densely populated museum. The empirical results demonstrate reliable operation in public environments. The robot successfully raised the museum's attendance by more than 50%. In addition, thousands of people all over the world controlled the robot through the Web. We conjecture that these innovations transcend to a much larger range of application domains for service robots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ever since the Czech novelist Karel Čapek invented the term "robot" <ref type="bibr" target="#b153">[154]</ref>-which was later popularized by Isaak Asimov <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>-the dream of building autonomous robots-willing, intelligent and human-like machines that make life pleasant by doing the type work we don't like to do-has been an active dream in people's minds. With universal personal robots still far beyond reach, we are currently witnessing a rapid revolution in robots that directly interact with people and affect their lives (see, e.g., <ref type="bibr" target="#b127">[128,</ref><ref type="bibr" target="#b154">155]</ref>). This paper describes one such robot, which is really just a step in this direction. Presented here is the software architecture of an interactive robot named RHINO, which has been built to assist and entertain people in public places, such as museums. RHINO is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Its primary task is to give interactive tours through an exhibition, providing multi-modal explanations to the various exhibits along the way (verbal, graphical, sound). In May 1997, RHINO was deployed in the "Deutsches Museum Bonn" (see Figure <ref type="figure">2</ref>). During a six-day installation period the robot gave tours to more than 2,000 visitors. Through an interactive Web-Interface, people from all over the world could watch the robot's operation and even control its operation-and more than 2,000 did.</p><p>On the software side, on which this article focuses, RHINO employs some of the most recent developments in the field of artificial intelligence (AI) and robotics. At its core, RHINO relies upon data-driven probabilistic representation and reasoning to cope with the uncertainties that necessarily arise in complex and highly dynamic environments. RHINO can also learn models (maps) of its environment and change its plans on-the-fly. It is equipped with an easy-to-understand multi-modal user interface, and it can react to the presence of people in various ways.</p><p>The necessity to employ state-of-the-art AI technology arose from the complexity of the task domain. The majority of RHINO's users were complete novices in robotics; yet, since the typical tour lasted for less than ten minutes, appealing to visitors' intuition was essential for the success of the concept. RHINO's environment, the museum, was densely populated. Most of the time, RHINO was "lucky" in that it lead the way when giving a tour with people following. At times, however, we counted more than a hundred people that surrounded the robot from all sides, making it difficult for the robot to reach the exhibits as planned while not losing track of its orientation. The museum itself, its geometry and its exhibits, posed further challenges on the software. While there were several narrow passages in the environment in which accurate motion control was essential, most of the museum consisted of wide open spaces that, to a large extent, lacked the necessary structure for the robot to orient itself. One of the key constraints was the necessity to avoid collisions with obstacles at all costs, humans and exhibits alike. Many of the obstacles, however, were literally "invisible," i.e., they could physically not be detected with the robot's sensors. The inability to sense certain obstacles was not necessarily due to the lack of an appropriate sensor suite-in fact, RHINO used four different sensor systems, ranging from laser range finders, sonar, and active infrared sensors to touch-sensitive panels-rather, it was the nature of the obstacles. For example, many exhibits were protected by glass cases, whose very purpose implied that they were not detectable by light-based sensors such as cameras, laser, or infrared, and whose smoothness made it impossible to detect them reliably even with sonar. Other exhibits were placed on solid metal plates, many of which were below the range of our lowest sensors.</p><p>Not all objects in the museum were static. In particular, the museum provided stools for the visitors to rest, and people tended to leave them behind at random places, usually close to exhibits. The problem of safe navigation was made more difficult by the speed requirements in the museum. To be interesting to the people, the robot had to move at walking speed whenever possible. At speeds of up to 80 cm/sec, the inertia of the robot is significant; turning or stopping on the spot is impossible. Lastly, some of the users were not at all cooperative, imposing further difficulties for the software design. Often museum visitors tried to "challenge" the robot. For example, by permanently blocking its way, they sometimes sought to make the robot leave the designated exhibition area towards other parts of the museum, where several unmapped and undetectable hazards existed (including a staircase).</p><p>We quickly learned that one cannot necessarily expect humans to be cooperative, so the safety of the system may not depend on specific behavioral requirements on the side of the users. On the other hand, people are thrilled if robots interact with them-just like they are if people interact with them. Thus, a primary component of a successful tour-guide is the ability to notice the presence of people, and to interact with them in a meaningful, appealing way. In fact, when we interviewed museum visitors, most of them assigned more weight to the robot's interactive capabilities than to its ability to navigate.</p><p>These challenges mandated the development of a collection of new, innovative software solutions. Our past work (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b148">149]</ref>) focused primarily on office navigationwhich involved moving through static corridors and into offices with well-detectable obstacles and cooperative people-where many of the difficulties simply did not exist. Many of the assumptions underlying this work do not apply in populated public places, such as a museum. As a result, we had to develop several new techniques and, more importantly, changed our view on several well-established methodologies in the field.</p><p>For example, most successful mobile robot architectures employ sensor-based, reactive methods for real-time collision avoidance <ref type="bibr" target="#b84">[85]</ref>. The typical paradigm is to consider a short time window of past sensor readings when setting the speed and final motion direction of the robot. Of course, the purely sensor-based approach is inappropriate if obstacles cannot be sensed. RHINO employs a hybrid approach, which incorporates a map of the environment in addition to sensor readings. The map contains "invisible" obstacles and hazards such as staircases. For the map to be useful, however, the robot has to know where it is with high accuracy. RHINO employs an efficient, fine-grain variant of Markov localization, capable of tracking the robot's position with high accuracy. Localization proved more difficult in the museum than in our office environment. Markov localization relies on the assumption that the robot's location is the only state in the world. However, large numbers of people that followed the robot closely, thereby blocking most of its sensors constantly violate this assumption. As demonstrated here and elsewhere <ref type="bibr" target="#b52">[53]</ref>, the basic Markov localization algorithm would have failed under such conditions. RHINO employs an extended localization method that filters out corrupted sensor readings. This approach, described in detail below, considers only sensor data that reduce the uncertainty of the robot during localization. As a result, the robot pays only attention to sensor data that confirms its current belief while ignoring all other data. While this approach critically departs from the basic Markov approach, it makes localization succeed in this highly non-Markovian environment.</p><p>Most navigation approaches either rely on a fixed, pre-given map of the environment, or learn such a map by themselves. Neither of these two options was appropriate-a fixed map would have made it impossible to navigate successfully when obstacles blocked the robot's path persistently (such as the stools). A pure learning approach, which was used successfully in a prototype tour-guide robot that was previously installed in our university building <ref type="bibr" target="#b148">[149]</ref>, was inappropriate since the robot was unable to map those invisible obstacles. RHINO's software integrates both types maps, using a hand-crafted CAD map as a starting point and a map learned on-the-fly from sensor data. To cope with changing maps, RHINO uses an efficient motion planner that can quickly react to changes in the map.</p><p>Finally, a key ingredient of any robot that interacts with people in public places is its interactive component. To interact with people, a method for finding people is called for. Unfortunately, sensor differencing (e.g., image differencing) is inapplicable since it typically assumes that the people move and the robot doesn't-in our case, the robot was almost always in motion, while people often didn't move. Thus, RHINO finds people by comparing the map with its range measurements, using the inverse of the filter described above. The robot then invokes a series of means that inform people of the robot's intentions and goals. RHINO also possesses two user interfaces, one for visitors and one for Web-users, which are designed to be simple enough for people to operate even without prior exposure to robotics.</p><p>This article provides an overview of the major components of RHINO's software Fig. <ref type="figure">3</ref>: Major components of the RHINO system and major flow of information.</p><p>architecture. As this description of the museum suggests, operating a robot in public environments as complex (and dangerous) as it poses research challenges that go beyond many of those found in most office environments. To cope with them, this paper describes a collection of algorithms which provide the robot with some unique features, such as its ability navigate smoothly and safely at high speed, to determine its location in an unmodified environment and populated, the ability to quickly find detours if paths are blocked, and the ability to engage and interact with people. We believe that these characteristics are prototypical for a large variety of application domains for mobile robots, and conjecture that virtually all of the technology described in this paper can be applied to a much larger variety of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architectural Overview</head><p>The overall software architecture consists of 20 modules (processes) listed in Table 1, which are executed in parallel on 3 on-board PCs and 2 off-board SUN workstations, connected via Ethernet. The software modules communicate using TCX <ref type="bibr" target="#b45">[46]</ref>, a decentralized communication protocol for point-to-point socket communication. Figure <ref type="figure">3</ref> shows the overall software architecture along with the major software modules and the flow of information between them. Similar to other robot control architectures <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b137">138]</ref>, the RHINO system is also organized in a hierarchical manner, with the device drivers at the lowest level and the user interfaces at the highest. The hierarchy, however, is not strict in the sense that modules would pass information only within the same or across adjacent layers. In RHINO's software,   Complete listing of all software modules. Modules labeled "(*)" must be and those labeled "(**)" should be run on board the robot. The exact on-board configuration during the exhibition varied; however, a dual Pentium computer is sufficient to execute all major components on-board.</p><p>modules often communicate across multiple layer boundaries.</p><p>Among the various principles that can be found in RHINO's software system, the following three are the most important ones:</p><p>(1) Probabilistic representations, reasoning, and learning. Robot perception is inaccurate and incomplete. Therefore, robots are inherently unable to determine the state of the world. Probabilistic data structures lend themselves nicely to the inherent uncertainty inside a robot. Instead of extracting just a single interpretation from sensor data, as is traditionally done in the field of robotics, probabilistic methods extract multiple interpretations (often all possible ones), weighted by a numeric plausibility factor that is expressed as a conditional probability. By considering multiple hypotheses, the robot can deal in a mathematically elegant way with ambiguities and uncertainty. In our experience, robots that use probabilistic representations recover easier from false beliefs and therefore exhibit more robust behavior. In addition, probability theory provides nice and elegant ways to integrate evidence from multiple sources over time, and to make optimal decisions under uncertainty. Recently, probabilistic methods have been employed in a variety of successful mobile robots <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b138">139]</ref>, for reasons similar to the ones given here. (2) Resource flexibility. Most of RHINO's software can adapt to the available computational resources. For example, modules that consume substantial processing time, such as the motion planner or the localization module, can produce results regardless of the time available for computation. The more processing cycles are available, however, the better or more accurate the result. In RHINO's software, resource flexibility is achieved by two mechanisms: selective data processing and any-time algorithms <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b161">162]</ref>. Some modules consider only a subset of the available data, such as the localization routine.</p><p>Other modules, such as the motion planning module, can quickly draft initial solutions which are then refined incrementally, so that an answer is available when needed. (3) Distributed, asynchronous processing with decentralized decision making. RHINO's software does not possess a centralized clock or a centralized communication module. Synchronization of different modules is strictly decentral. Time-critical software (e.g., all device drivers), and software that is important for the safety of the robot (e.g., collision avoidance), are run on the robot's on-board computers. Higher-level software, such as the task control module, is run on the stationary computers. This software organization has been found to yield robust behavior even in the presence of unreliable communication links (specifically the radio link which connected the on-board and off-board computers) and various other events that can temporarily delay the message flow or reduce the available computational time. The modular, decentralized software organization eases the task of software configuration. Each module adds a certain competence, but not all modules are required to run the robot. The idea of decentralized, distributed decision making has been at the core of research on behavior-based robotics over the last decade <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b120">121]</ref>, but here modules are typically much lower in complexity (e.g., simple finite state machines).</p><p>The remainder of this paper will describe those software modules that were most essential to RHINO's success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">State Estimation</head><p>To find its way safely through a populated environment with invisible obstacles, RHINO employs several methods to estimate its current state. State comprises the robot's position and the position of people and obstacles. This section describes RHINO's approach to localization and mapping, both of which use probabilistic estimators for interpreting and integrating sensor evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Localization</head><p>At the core of RHINO's navigation routines is a module that continuously estimates the robot's position in x-yspace, where x and y are the coordinates of the robot in a 2D Cartesian coordinate system and is its orientation. RHINO employs a variant of Markov localization, which is a probabilistic method for robot localization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b138">139,</ref><ref type="bibr" target="#b144">145]</ref>. Its input is a stream of sensor readings from the robot's proximity sensors, interleaved with a sequence of action commands. Throughout this paper, this sequence will be denoted d = fo (1) ; o (2) : : : ; o (T) g</p><p>where each o (t) with t 2 f1; : : : ; T g is either a sensor reading or an action command. The localization module computes, incrementally and in real-time, a probability distribution P ( (t) ) that expresses the robot's belief to be at location (t) at time t where each (t) is a location in the three-dimensional x-yspace.</p><p>The robot's belief at time t is described by the conditional probability P ( (t) ) = P ( j o (1) ; o (2) : : :</p><formula xml:id="formula_1">; o (t) )<label>(2)</label></formula><p>To compute this probability, three aspects have to be discussed: (1) initialization, (2) sensing, and (3) motion. The latter two, sensing and motion, have opposite effects on the robot's belief. While sensor readings convey information about the robot's position, thereby often decreasing the entropy of P ( (t) ), actions generally cause a loss of information due to the inaccurate nature of robot motion, and increase the entropy of P ( (t ). The entropy of P ( (t) ) will be discussed further below in Section 3.1.6. Fig. <ref type="figure">4</ref>: The conditional probability P (o j dist( )) obtained from 11,000,000 laser measurements (left) and its approximation using a mixture of a Gaussian, a uniform and a Dirac density (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Initialization</head><p>Initially, at time t = 0, P ( (0) ) reflects the robot's initial state of knowledge in the absence of any data d. If the robot's initial position is 0 and if it knows exactly where it is, P ( (0) ) is initialized with a Dirac distribution</p><formula xml:id="formula_2">P ( (0) ) = 8 &gt; &lt; &gt; : 1; if = 0 0; if 6 = 0 (3)</formula><p>If the robot does not know where it is, P ( ) is initialized with a uniform distribution. Of particular interest in this paper is the latter case, since the robot was often placed somewhere in the museum without initial knowledge of its position. Thus, the robot had to localize itself under global uncertainty, a problem also known as global localization or the kidnapped robot problem <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Robot Perception</head><p>Suppose at time t, the robot receives a sensor measurement o (t) . In RHINO's localization module, o (t) is either a laser scan or a sonar scan. This measurement is used to update the internal belief as to where the robot is, according to the following rule: P ( (t) j o (1) ; : : : ; o (t) ) = P (o (t) j (t) ; o (1) ; : : : ; o (t?1) ) P ( (t) j o (1) ; : : : ; o (t?1) ) = P (o (t) j (t) ) P ( (t) j o (1) ; : : : ; o (t?1) ) Here is the Bayes normalizer that ensures that the probabilities on the left-hand side of (4) sum up to 1, and P ( (t) j o (1) ; : : : ; o (t?1) ) is the robot's belief just before sensing o (t) . The first step of the derivation of (4) follows from Bayes rule. The second step rests on the following conditional independence assumption, also called Markov assumption:</p><p>P (o (t) j (t) ; o (1) ; : : : ; o (t?1) ) = P (o (t) j (t) )</p><p>This conditional independence assumption states that if the robot's location at time t is known, knowledge of past sensor readings o (1) ; : : : ; o (t?1) do not convey any information relevant to the prediction of o (t) . In other words, the Markov property assumes there is no state in the environment other than the robot's own location.</p><p>In most realistic environments such as the museum, this assumption is violated; for example, people often block the robot's sensors for multiple time steps, which makes sensor readings conditionally dependent even if the exact robot location is known. Section 3.1.6 will explicitly address this problem. For now, the Markov assumption will be adopted, as it is mathematically convenient and as it justifies a simple, incremental update rule.</p><p>The update equation (4) relies on the probability P (o (t) j (t) ) of observing o (t)   at location (t) , which henceforth is called the perceptual model. The perceptual model does not depend on t; thus, for the reader's convenience we will omit the superscript (t) and write P (o j ) instead.</p><p>RHINO uses its proximity sensors (sonars, lasers) for localization. Its perceptual model is obtained using a generic noise model of the robot's sensors along with a map of the environment. More specifically, P (o j ) is computed in two steps: </p><p>Here the function dist: ?! &lt; computes the expected measurement that a noisefree sensor would obtain in a stationary environment. The value of dist( ) is computed by ray tracing in a map of the robot's environment. The remaining probability, P (o j dist( )), models the noise in perception. It is learned from data. The left diagram in Figure <ref type="figure">4</ref> shows the empirical distribution of P (o j dist( )) obtained from 11 10 6 measurements; here "expected distance" refers to dist( ), "measured distance" refers to o, and the vertical axis plots the probability P (o j dist( )). In RHINO's software, P (o j dist( )) is approximated by a mixture of a Gaussian, a geometric, and a Dirac distribution, as shown in the right diagram in Figure <ref type="figure">4</ref>. The coefficients of these distribution are learned from data, using the maximum likelihood estimator <ref type="bibr" target="#b7">[8]</ref>.</p><p>Figure <ref type="figure" target="#fig_2">5</ref> illustrates the perceptual model in practice. An example laser range scan is shown in Figure <ref type="figure" target="#fig_2">5a</ref>. Figure <ref type="figure" target="#fig_2">5b</ref> shows, for each position , the likelihood P (o j ) of this specific range scan in a pre-supplied map (projected into 2D). As is easy to be seen, P (o j ) is high in the main corridor, whereas it is low in the rooms.</p><p>In our implementation, the parameters of the perceptual model are obtained through maximum likelihood estimation from data, i.e., pairs of measurements o and "true" distances dist( ). Since such data is difficult to obtain-it requires knowledge of the exact robot location, a bootstrapping algorithm was used to automatically derive position data. More specifically, our approach relies with position labels derived using an approximate perpetual model, and used these approximate positions to optimize the model parameters. Once the model parameters have been fit, new position labels are computed using the improved perceptual model. This approach is iterated in an EM-like fashion <ref type="bibr" target="#b39">[40]</ref>, leading to increasingly better data fits. Notice that this approach bears close similarity to a rich body on research on learning from labeled and unlabeled data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b131">132]</ref>, and is commonly used in other data-intense fields such as speech recognition <ref type="bibr" target="#b155">[156]</ref>. As a result, our approach can effortlessly use millions of data items gathered during everyday operation, for building a highly accurate perceptual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Robot Motion</head><p>Motion changes the location of the robot. If o (t) is a motion command, the robot's belief changes according to the following rule:</p><p>P ( (t+1) j o (1) ; : : : ; o (t) ) = Z P ( (t+1) j (t) ; o (1) ; : : : ; o (t) ) P ( (t) j o (1) ; : : : ; o (t) ) d (t)   = Z P ( (t+1) j (t) ; o (t) ) P ( (t) j o (1) ; : : : ; o (t?1) ) d (t)</p><p>This update rule is incremental, just like the perceptual update rule (4). The first step in its derivation is obtained using the Theorem of total probability, and the second step is based on a similar Markov assumption as the one above:</p><p>P ( (t+1) j (t) ; o (t) ) = P ( (t+1) j (t) ; o (1) ; : : : ; o (t) )</p><p>In fact, both Markov assumptions described in this section are consequences of a single one, which states that the location of the robot is the only state in the environment.</p><p>Equation ( <ref type="formula" target="#formula_6">7</ref>) relies on P ( (t+1) j (t) ; o (t) ), which is a probabilistic kinematic model of robot motion. Since the motion model does not depend on t, we will henceforth denote it by P ( j 0 ; o). In our implementation, P ( j 0 ; o) is realized using a mixture of two independent, zero-centered distributions, which model rotational and translational error, respectively <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b149">150]</ref>. The width of these distributions are proportional to the length of the motion command. Figure <ref type="figure" target="#fig_3">6</ref> illustrates RHINO's motion model for two example motion commands. Shown there are "banana-shaped" distributions P ( j 0 ; o), which result if the robot starts at 0 and executes the motion commands specified in the figure caption. Both distributions are of course three-dimensional (in x-y--space); Figure <ref type="figure" target="#fig_3">6</ref> shows their 2D projections into x-yspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Grid-based Markov Localization</head><p>The generic, incremental Markov localization algorithm is depicted in Table <ref type="table">2</ref>. Here the time index is omitted, to emphasize the incremental nature of the algorithm. In experimental tests this method has been demonstrated to localize the robot reliably in static environments even if it does not have any prior knowledge about the robot's position <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Recently, different variants of Markov localization have been developed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b138">139]</ref>. These methods can be roughly distinguished by the nature of the state space representation. Virtually all published implementations of Markov localization, with</p><p>(1) Initialization: P ( ) ? Bel pri ( (0) )</p><p>(2) For each observation o do:</p><formula xml:id="formula_8">P ( ) ? P (o j ) P ( )<label>(9)</label></formula><p>P ( ) ? P ( ) Z P ( 0 ) d 0 ?1</p><formula xml:id="formula_9">(normalization)<label>(10)</label></formula><p>(3) For each action command o do:</p><formula xml:id="formula_10">P ( ) ? Z P ( j 0 ; o) P ( 0 ) d 0<label>(11)</label></formula><p>Table <ref type="table">2</ref> Markov localization-the basic algorithm.</p><p>the more recent exception of <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">48]</ref>, are based on coarse-grained representations of space, often with a spatial resolution of less than one meter and an angular resolution of 90 degrees. For example, in <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b138">139]</ref> Markov localization is used for landmark-based corridor navigation and the state space is organized according to the topological structure of the environment. Unfortunately, coarse-grained, topological representations are insufficient for navigating in the close vicinity of invisible (but known) obstacles, such as the glass cases described above. Thus, RHINO's localization algorithm differs from previous approaches in that it employs a finegrained, grid-based decomposition of the state space <ref type="bibr" target="#b18">[19]</ref>. In all our experiments reported here, the spatial resolution was 15cm and the angular distribution was 2 .</p><p>The advantage of this approach is that it provides a high accuracy with respect to the position and orientation of the robot. Its disadvantage, however, is the huge state space which has to be maintained and updated. With such a high resolution, the number of discrete entities is huge, and the basic algorithm cannot be run fast enough on our current low-cost computer hardware for the algorithm to be of practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Selective Computation</head><p>To cope with the large numbers of grid cells, RHINO updates them selectively.</p><p>The legitimacy of selectively updating P ( )-instead of updating all values at all times-is based on the observation that most of the time, the vast majority of grid cells have probability vanishingly close to zero and, thus, can safely be ignored. This is because in most situations, the robot knows its location with high certainty, and only a small number of grid cells close to the true location have probabilities that differ significantly from zero.</p><p>In RHINO's localization algorithm, grid cells whose probability are smaller than a robot Fig. <ref type="figure">7</ref>: Global localization in the Deutsches Museum Bonn. The left image shows the belief state after incorporating one laser scan. After incorporating a second scan, the robot uniquely determined its position (right).</p><p>threshold are not updated. Instead, they are represented by a single value, which uniformly represents the probability of all non-updated grid cells <ref type="bibr" target="#b17">[18]</ref>. In the museum exhibit, the threshold was set to 0.1% of the a priori position probability. This led to an average savings of two orders of magnitude while not reducing the accuracy of the localization algorithm in any noticeable way.</p><p>Figure <ref type="figure">7</ref> shows a typical example of global localization in the Deutsches Museum Bonn. RHINO is started with a uniform distribution over its belief state. The probability distribution after integrating the first sensor scan is shown on the left side of Figure <ref type="figure">7</ref>. Thus, after incorporating a single sensor scan, the probability mass is readily centered on a much smaller number of grid cells. After incorporating a few more sensor scans, the robot knows its position with high certainty. In the museum exhibit, the localization algorithm was run on a single-processor SUN 170Mhz Ul-traSparc station, equipped with 256MB RAM. The time required to process a sensor scan varied, depending on the uncertainty in the robot's position. Initially, when the robot was maximally uncertain about its position and therefore had to update every single value in P ( ), processing a sensor scan required approximately 20 seconds.</p><p>After the initial localization, the robot's uncertainty was consistently low, which reduced the computational complexity tremendously. The average processing time for processing a sensor scan was approximately 0.5 sec. Since our sensors (sonar and laser) generate approximately 8 scans per second, not every sensor reading was considered in localization. In addition, only a subset of the 360 range readings generated with the laser range finder were considered, since these readings are highly redundant. The practical success of the localization algorithm, however, demonstrates that sufficiently much sensor data was incorporated while the robot was moving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Entropy Gain Filters: Beyond The Markov Assumption</head><p>Unfortunately, the basic Markov localization approach is bound to fail in densely populated environments. Markov localization approaches, by definition, assume that the environment is static-a direct consequence of the underlying Markov assumption. The presence of people violates the Markov assumption by introducing additional state.</p><p>In the museum, people often followed the robot closely for extended durations of time. In such situations, the Markov assumption can be fatal. For example, when multiple visitors collaboratively blocked the robot's path, the sensor readings often suggested the presence of a wall in front of the robot. For such readings o, P (o j )</p><p>is maximal for locations next to walls. Since the Markov localization algorithm incorporates P (o j ) in a multiplicative way every time a sensor reading is taken, multiple iterations of this algorithm will ultimately make the robot believe that it is next to a wall. This property is a direct consequence of the conditional independence assumption (Markov assumption) that was used in the derivation of the Markov localization algorithm, At first glance, one might attempt to remedy the problem by introducing additional state features in the Markov approach. Instead of estimating the location of the robot as the only state, one could extend Markov localization to simultaneously estimate the locations of the people. With such an enriched state representation, the Markov assumption would be justified and the approach would therefore be applicable. Unfortunately, such an extension is computationally expensive, since the computational and memory complexity increases exponentially in the number of state variables. In addition, such an approach requires probabilistic models of the behavior of the various non-stationary obstacles, such as humans, which may be difficult to obtain.</p><p>In our approach, we pursued a different line of thought: filtering. The idea is to sort sensor readings into two buckets, one that corresponds to known obstacles such as walls, and one that corresponds to dynamic obstacles such as humans. Only the former readings are incorporated into the position estimation, whereas the latter ones are simply discarded. The filtering approach does not explicitly estimate the full state of the environment; rather, it reduces the damaging effect that arises from state other than the robot's location.</p><p>The specific filter used in our implementation is called entropy gain filter <ref type="bibr" target="#b52">[53]</ref> and works as follows. The entropy H(P ) of a distribution P is defined by <ref type="bibr" target="#b22">[23]</ref> H(P ) = ? Z P ( ) log P ( ) d :</p><formula xml:id="formula_11">(12)</formula><p>Entropy is a measure of uncertainty: The larger the entropy, the higher the robot's uncertainty as to where it is. Entropy gain measures the relative change of entropy upon incorporating a sensor reading into P . More specifically, let o denote a sensor scan, and let o i denote an individual component of the scan (i.e., a single range measurement). The entropy gain of a probability distribution P with respect to a sensor measurement o i is defined as: H(P j o i ) := H(P ( (t) j o (t) i )) ? H(P ( (t?1) ))</p><p>Entropy gain measures the change of certainty. A positive entropy gain indicates that after incorporating o i , the robot is less certain about its position. A negative entropy gain indicates an increase in certainty upon incorporating o i .</p><p>RHINO's entropy gain filter filters out sensor measurements that, if used, would decrease the robot's certainty. This is achieved by considering only those o i for which H(P j o i ) 0. The entropy gain filter makes robot perception highly selective, in that only sensor readings are considered that confirm the robot's current belief. The resulting approach does not comply with the original Markov assumption. successfully filtered out. We did observe, however, that the robot occasionally filtered out measurements that stemmed from stationary obstacles that were part of the map.</p><p>The entropy gain filter proved to be highly effective in identifying non-static obstacles and in filtering sensor readings accordingly. Throughout the complete deployment period, the robot incorporated sufficiently many sensor readings that it never lost track of its position. Using the data gathered in the museum, we evaluated the accuracy of our localization algorithm systematically using 118 reference positions, whose coordinates were determined manually <ref type="bibr" target="#b52">[53]</ref>. One of the data sets, shown in Figure <ref type="figure">9</ref>, contains data collected during 4.8 hours of robot operation in peak traffic, in which the robot traversed 1,540 meters. In this data set, more than 50% of all sensor readings were corrupted by people for extended periods of time.</p><p>The average localization error was found to be smaller than 10cm <ref type="bibr" target="#b52">[53]</ref>. In only one case did we observe some noticeable error. Here the robot's internal belief deviated approximately 30cm from the real position. As a result, the robot touched a large, invisible metal plate in the center of the museum. The localization error was preceded by a failure of the robot's sonar sensors for an unknown duration of time.</p><p>Unfortunately, the basic entropy gain filter also has a disadvantage. When applied as described above, it impairs the robot's ability to recover from large errors in its localization. This is because if the robot's position estimate is wrong, the entropy gain filter might filter out those sensor readings that convey information about its correct position, making a recovery impossible. To solve this problem we always incorporated a randomly chosen set of readings. A successor of the entropy gain filter, which is better suited to proximity sensors and outperforms the entropy gain filter, is described in <ref type="bibr" target="#b52">[53]</ref>. As discussed in more depth there, Markov localization combined with the entropy gain filter was able to accurately estimate the position of the robot throughout the entire deployment period, and the entropy filter played a crucial role in its success. Additional comparative experimental results, using the data obtained in the museum, can be found in <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">Finding People</head><p>As an aside, it is interesting to notice that the entropy gain filter fulfills a secondary purpose in RHINO's software. Sensor measurements o i with H(P j o i ) &gt;</p><p>(with &gt; 0) indicate the presence of an unexpected obstacle, such as people and other objects not contained in the map. Thus, the inverse of the entropy gain filter is a filter that can detect people. This filter differs from many other approaches in the literature on people detection <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b75">76]</ref> in that it can find people who do not move, and it can do this even while the robot itself is in motion. As will be described in more detail below, this filter, in combination with a criterion that measures the robot's progress towards its goal, was used to activate the robot's horn. As a result, the robot blew its horn whenever humans blocked its path; an effect, that most visitors found highly entertaining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mapping</head><p>The problem of mapping is the problem of estimating the occupancy of all hx; yi locations in the environment <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b146">147]</ref> from sensor data. Mapping is essential if the environment changes over time, specifically if entire passages can be blocked.</p><p>In the museum, stools or people often blocked certain regions or passages for extended durations of time. RHINO's ability to acquire maps on-the-fly enabled it to dynamically plan detours, which prevented it from getting stuck in many cases.</p><p>The statistical estimation involved in building occupancy maps from sensor data is similar to the probabilistic estimation of the robot's location. Let c xy denote a random variable with events in f0; 1g that corresponds to the occupancy of a location hx; yi (in world coordinates). Here 1 stands for occupied, and 0 stands for free. The problem of mapping is to estimate P (fc xy g j o (1) ; : : : ; o (t) )</p><p>where the set of all occupancy values fc xy g denotes the map. Since the set of variables to be estimated-the map-is usually extremely high-dimensional (many of our maps contain 10 6 or more grid cells), it is common practice to treat the occupancy estimation problem independently for each location hx; yi <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b146">147]</ref>. This ! ?1</p><p>Table <ref type="table">3</ref> Mapping-the basic algorithm.</p><p>effectively transforms the high-dimensional occupancy estimation problem into a collection of one-dimensional estimation problems f P (c xy j o (1) ; : : : ; o (t) ) g <ref type="bibr" target="#b14">(15)</ref> which can be tackled efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Temporal Integration of Evidence</head><p>The temporal integration of sensor evidence is analogous to Markov localization. Just like Markov localization, our mapping approach relies on the following Markov assumption P (o (t) j c xy ; (t) ; dno (t) ) = P (o (t) j (t) ; c xy ) <ref type="bibr" target="#b15">(16)</ref> which renders sensor data conditionally independent given the true occupancy of the grid cell hx; yi. Here o (t) stands for the sensor reading taken at time t. To separate the problem of mapping from the localization problem, it is assumed that the robot's location (t) is known 1 ; henceforth, the estimation of (t) will be omitted in the mathematical derivation. In our implementation, the maximum likelihood estimation</p><formula xml:id="formula_15">^ (t) = argmax (t) P ( (t) )<label>(17)</label></formula><p>is used as the robot's location.</p><p>Armed with all necessary assumptions, we are now ready to derive an efficient algorithm for statistical occupancy estimation from data. The probability that a</p><p>1 See <ref type="bibr" target="#b149">[150]</ref> for an approach to concurrent localization and mapping that relaxes these assumption and estimates both the robot's location and the location of the obstacles using a single, mathematical approach.</p><p>location hx; yi is occupied given the data is given by P (c xy j o (1) ; : : : ; o (t) ) = P (o (t) j c xy ; o (1) ; : : : ; o (t?1) ) P (c xy j o (1) ; : : : ; o (t?1) ) P (o (t) j o (1) ; : : : ; o (t?1) ) = P (o (t) j c xy ) P (c xy j o (1) ; : : : ; o (t?1) ) P (o (t) j o (1) ; : : : ; o (t?1) ) = P (c xy j o (t) ) P (o (t) ) P (c xy j o (1) ; : : : ; o (t?1) ) P (c xy ) P (o (t) j o (1) ; : : : ; o (t?1) )</p><p>This transformation is obtained via Bayes rule, followed by applying the Markov assumption and a second application of Bayes rule.</p><p>The binary nature of occupancy-a location hx; yi is either occupied or not-can be exploited to derive a more compact update equation than in the real-valued case of position estimation <ref type="bibr" target="#b105">[106]</ref>. In analogy to <ref type="bibr" target="#b18">(19)</ref>, the probability that a location hx; yi is free (unoccupied) is given by P (:c xy j o (1) ; : : : ; o (t) ) = P (:c xy j o (t) ) P (o (t) ) P (:c xy j o (1) ; : : : ; o (t?1) ) P (:c xy ) P (o (t) j o (1) ; : : : ; o (t?1) )</p><p>To see, just replace every occurrence of c xy by :c xy in Equation <ref type="bibr" target="#b18">(19)</ref>.</p><p>Dividing <ref type="bibr" target="#b18">(19)</ref> by ( <ref type="formula" target="#formula_17">20</ref>) yields the following expression, which is often referred to as the odds of hx; yi being occupied <ref type="bibr" target="#b115">[116]</ref>:</p><p>P (c xy j o (1) ; : : : ; o (t) ) P (:c xy j o (1) ; : : : ; o (t) ) = P (c xy j o (t) ) P (:c xy ) P (c xy j o (1) ; : : : ; o (t?1) ) P (:c xy j o (t) ) P (c xy ) P (:c xy j o (1) ; : : : ; o (t?1) ) (21)   it follows that the desired probability is given by P (c xy j o (1) ; : : : ; o (t) )</p><p>= 1 ? 1 + P (c xy j o (t) ) 1 ? P (c xy j o (t) ) 1 ? P (c xy ) P (c xy ) P (c xy j o (1) ; : : : ; o (t?1) ) 1 ? P (c xy j o (1) ; : : : ; o (t?1) ) ! ?1</p><p>Here P (c xy ) represents the prior distribution of c xy (in the absence of data), which in out implementation is set to 0.5 and can therefore be ignored.</p><p>As is easily seen, the latter estimation equation can be computed incrementally, leading to the mapping algorithm shown in Table <ref type="table">3</ref>. The probability P (c xy j o) is called the inverse sensor model (or sensor interpretation), whose description is subject to the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Neural Network Sensor Interpretation</head><p>In RHINO's mapping approach, P (c xy j o) maps a sensor reading to a conditional probability that location hx; yi is occupied (under knowledge of the actual position ). In traditional approaches to mobile robot mapping, P (c xy j o) is usually crafted by hand, based on knowledge of the characteristics of the respective sensor. In our approach, which is described in more detail in <ref type="bibr" target="#b146">[147]</ref>, an artificial neural network is trained with Back-Propagation <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b122">123]</ref> to approximate P (c xy j o) from data. This interpretation network, which is shown in Figure <ref type="figure" target="#fig_0">10a</ref>, accepts as input an encoding of a specific hx; yi-location, encoded in polar coordinates, relative to the robot's local coordinate system. Part of the input are also the four sensor readings that are geometrically closest to hx; yi. The output of the network, after training, is an approximation of P (c xy j o). Training data for learning P (c xy j o) is obtained by placing the robot at random places in a known environment, and recording its sensor readings. For each hx; yi within the robot's perceptual range (which in our implementation is between 3 and 5 meters), a training pattern is generated, whose label reflects whether or not the hx; yi is occupied. After appropriate training <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b146">147]</ref>, which in RHINO's case is carried out using a simulator <ref type="bibr" target="#b146">[147]</ref>, the output of the network can be interpreted as the desired conditional probability P (c xy j o). Figure <ref type="figure" target="#fig_0">10b</ref> shows examples of sonar sensor readings and the corresponding probabilities generated by the trained neural network.</p><p>In conjunction with any of the approaches presented in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b142">143,</ref><ref type="bibr" target="#b146">147,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b151">152]</ref>, the mapping algorithm is powerful enough to generate consistent maps from scratch. Two example maps are shown in Figure <ref type="figure" target="#fig_8">11</ref>. Both maps were constructed in less than one hour. The scarceness of the map shown in Figure <ref type="figure" target="#fig_8">11a</ref>. however, illustrates the large number of undetectable obstacles (cf. the hand-crafted map shown in Figure <ref type="figure" target="#fig_4">18</ref>). Because of this, we chose to provide RHINO with a hand-crafted CAD map instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Integration of Multiple Maps</head><p>RHINO possesses two major proximity sensor systems, a ring of 24 ultrasonic transducers (sonars) and a pair of laser range finders. Both sensor systems cover a full 360 degree range. Since the perceptual characteristics of both systems are quite different, and since they are mounted at different heights, separate maps are built for each sensor.</p><p>From those, and from the hand-supplied CAD map, a single map is compiled using the conservative rule </p><p>where the superscript "int" marks the integrated map and the various superscripts on the right hand-side correspond to the respective maps. The integrated map is used for navigation. The reader may notice that the integration rule ( <ref type="formula" target="#formula_14">18</ref>) is inapplicable if different sensors detect different obstacles, which is the case for the specific sensor systems considered in this article.</p><p>Figure <ref type="figure" target="#fig_0">12</ref> shows an example of the various maps and their integration. Other examples of integrated maps are shown in Figure <ref type="figure" target="#fig_10">13</ref>. These examples were recorded during peak traffic hours. In both cases, a massive congestion made it impossible to make progress along the original path. The robot's ability to modify its map and hence its paths on-the-fly was absolutely essential for the high reliability with which the robot reached its destinations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Planning and Execution</head><p>RHINO motion control is implemented hierarchically, using three different modules for generating control. These are, in increasing levels of abstraction:</p><p>(1) Collision avoidance. This module directly sets the velocity and the motion direction of the robot so as to move in the direction of a target location while avoiding collisions with obstacles. It is the only module that considers the dynamics of the robot. (2) Motion planner. The motion planner consults the map to find shortest paths to an exhibit. The path is communicated to the collision avoidance module for execution. Since maps are updated continuously, the motion planner continuously revises its plans. (3) Task control module. The task control module coordinates the various robot activities related to motion and interaction. For example, it determines the sequence at which exhibits are visited during a tour, and it also determines the sequence of steps involved in the dialogue with the user.</p><p>The hierarchical organization is fairly classical <ref type="bibr" target="#b84">[85]</ref>. Each module has its own way to monitor the execution and reacts accordingly. In the museum, the robot was always in motion-unless, of course, it intentionally stopped to explain an exhibit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Collision Avoidance</head><p>The task of the collision avoidance module is to determine the actual motion direction and velocity of the robot so as to operate the robot safely while maximizing its progress towards its goal location. The majority of literature on mobile robot collision avoidance suffers from two limitations, both of which are critical in environments like the museum. (1) Inability to handle invisible obstacles. Virtually all existing methods for collision avoidance are purely sensor-based, i.e., they only consult the robot's sensors to determine collision-free motion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b136">137]</ref>. If all obstacles can be sensed, such strategies suffice. However, since some of the obstacles in the museum were invisible, a purely sensor-based approach would have been likely to fail. (2) Inability to consider dynamics. With few exceptions <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b136">137]</ref>, existing approaches model only the kinematics of the robot and ignore dynamic effects such as inertia. At lower speeds (such as 20 cm/sec), the dynamics of mobile robots can safely be ignored. At higher speeds (such as 80 cm/sec), however, the robot's inertia can prohibit certain maneuvers, such as sudden stops or sharp turns. Since one of the requirements in the museum was to operate at walking speed, it was essential that the robot's dynamics were taken into account.</p><p>RHINO's collision avoidance module, which is called DWA (short for: modelbased dynamic window algorithm), specifically addresses these limitations <ref type="bibr" target="#b51">[52]</ref>. DWA consults a hand-supplied map of the environment to avoid collisions with obstacles that cannot be sensed. The map is also used to bound the area in which the robot operates. To ensure safe motion at high speed, constraints imposed by the robot's dynamics are explicitly considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The Dynamic Window Algorithm</head><p>The key idea of DWA is to choose control directly in the velocity space of the robot, that is the translational and rotational velocity. As shown in <ref type="bibr" target="#b49">[50]</ref>, robots with fixed velocity always travel on a circular trajectory whose diameter is determined by the ratio of translational and rotational velocity. Motor current (torque) change the velocity of the robot and, as a consequence, its motion direction. The problem   of collision avoidance is, thus, the problem of selecting appropriate velocities for translation and rotation.</p><p>In regular time intervals (every .25 seconds), DWA chooses velocities so as to best obey various hard and soft constraints (see also <ref type="bibr" target="#b121">[122]</ref>):</p><p>(1) Hard constraints are vital for a robot's safety and are imposed by torque limits. DWA considers two types of hard constraints: torque constraints and safety constraints. Torque constraints rule out velocities that physically cannot be attained (e.g., a fast moving robot cannot take a 90 degree turn). Safety constraints rule out velocity settings that would inevitably lead to a collision with an obstacle. Notice that hard constraints do not specify preferences among the different control options; neither do they take into account the robot's goal. (2) Soft constraints express preferences for both the motion direction and the velocity of the robot. DWA measures the progress towards the goal by trading off three different soft constraints, which measure (1) translational velocity, (2) heading towards the target position, and (3) forward clearance. If combined in the right ratio <ref type="bibr" target="#b49">[50]</ref>, these criteria lead to goal-directed behavior while graciously avoiding collisions.</p><p>Consider the situation depicted in Figure <ref type="figure" target="#fig_11">14</ref>, in which the robot is nearly in straight motion at a translational speed of about 40cm/sec (the location of obstacles as perceived by the robot are shown in Figure <ref type="figure" target="#fig_12">15</ref>). Figure <ref type="figure" target="#fig_0">17</ref> depicts the whole velocity space, in which each axis corresponds to a velocity (translational and rotational). The robot's current velocity is in the center of the small rectangular box in the diagram, called the dynamic window. This window includes all velocities that can be attained in the next 0.25 seconds under the robot's torque limits. Nearby obstacles carve out regions in the diagram (shown there in white), as those velocities Fig. <ref type="figure" target="#fig_0">17</ref>: Each control is a combination of translational (y-axis) and rotational (xaxis) velocity. The darker a control, the higher its value, Also shown here is the dynamic window of velocities that can actually be attained. The cross marks the control chosen by DWA.</p><p>would inevitably lead to a collision. The remaining velocities are then evaluated according to a superposition of the three soft constraints listed above, which favors velocity vectors with high translational velocity and for which the robot's heading direction points towards the goal. The overall evaluation of each velocity pair is represented by its grey level, where darker values correspond to velocity pairs with higher value. The cross marks DWA's final selection, which makes the robot follow the (circular) trajectory shown in Figure <ref type="figure" target="#fig_13">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Integrating Sensor Readings and Maps</head><p>DWA integrates "real" proximity measurements, obtained from the robot's various sensors (tactile, infrared, sonar, laser), with "virtual" proximity measurements, generated using a map of the environment. Figure <ref type="figure" target="#fig_4">18</ref> shows the map that was used in the museum for this purpose. This map marks as dark grey all regions that contain obstacles that cannot be sensed. This map was also used to limit the robot's operational range. By adding appropriate virtual obstacles (shown in light grey) it can be ensured that the robot does not accidentally leave the area where it is supposed to operate.</p><p>Just like the real sensors (tactile, infrared, sonar, laser), the virtual sensors in DWA are assumed to be mounted on a circular array around the robot. The generation of virtual measurements is not straightforward, as the robot never knows exactly where it is; instead, it is given the belief P ( ) that assigns conditional probabilities </p><p>to generate virtual proximity sensor measurements. However, such an approach would be brittle, since it ignores the robot's uncertainty. DWA uses a more robust rule which takes uncertainty into account, by generating virtual measurements so that with high likelihood (e.g., 99%), virtual measurements underestimate the actual distance to the nearest object. To explain how this is done, let us first consider situations in which the robot position is known. Recall that dist( ) denotes the distance an ideal (noise-free) sensor would measure if the robot's position was , and let X denote a random variable that models the measurement of this ideal sensor. Obviously, the probability P (X = o j ) is given by a Dirac distribution:</p><formula xml:id="formula_21">P (X = o j ) = 8 &gt; &lt; &gt; : 1; if o = dist( ) 0; if o 6 = dist( )<label>(25)</label></formula><p>In our case, the robot only has a probabilistic belief P ( ) as to where it might be.</p><p>Under this belief, the probability that the sensor returns a value o is given by</p><formula xml:id="formula_22">P (X = o) = Z P (X = o j ) P ( ) d :<label>(26)</label></formula><p>The probability that the sensor measures a value larger than o is given by</p><formula xml:id="formula_23">P (X &gt; o) = Z P (X &gt; o j ) P ( ) d = Z Z o 0 &gt;o P (X = o 0 j ) P ( ) do 0 d = Z o 0 &gt;o P (X = o 0 ) do 0<label>(27)</label></formula><p>DWA generates virtual measurements using a conservative rule: The measurement of a virtual sensor is the largest distance that, with 99% probability, underestimates the true distance. o = supfo : P (X &gt; o) :99g <ref type="bibr" target="#b27">(28)</ref> Let us illustrate this approach using two examples. Figure <ref type="figure">7</ref> show two situations, one in which the robot is uncertain about its position, and one in which it is fairly certain. Both situations induce different densities P (X = o), which are shown in Figure <ref type="figure" target="#fig_0">19</ref>. The solid curve depicts P (X = o) in the uncertain case, whereas the dashed curve illustrates P (X = o) when the robot is certain. As is easy to be seen, P (X = o) is fairly unspecific in the uncertain case, whereas it is narrow in the certain case. The vertical lines (solid and dashed) indicate the virtual reading that DWA generates in either situation. With 99% probability, the real distance is larger than the distance suggested by the virtual reading. This conservative rule ensures that the robot does not collide with any of the invisible obstacles, unless it assigns less than 1% probability to its actual position.</p><p>Both virtual and real measurements form the basis for determining the robot's motion direction and velocity. Figure <ref type="figure" target="#fig_13">16</ref> shows the integrated sensor information (real and virtual). Figure <ref type="figure" target="#fig_13">16</ref> also shows the trajectory chosen by DWA, which safely avoids collision with the center obstacle. This figure demonstrates that a purely sensor-based approach is inappropriate.</p><p>The collision avoidance module proved to be highly effective in the museum. Because of the unified approach to setting speed and motion direction, the approach often maintained walking speed even in cluttered environments. The robot reacted quickly when people blocked its way, which prevented visitors from perceiving the robot as a potential threat. We never observed that parents kept their childrenmany of whom were much shorter than the robot-from approaching the robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Motion Planning</head><p>The collision avoidance module only considers local constraints. As any local motion planning method, cannot cope with U-shaped obstacles and similar configurations that require unbounded look-ahead. RHINO's motion planning module takes a more global view. Its task is to determine globally shortest paths to arbitrary target points. Paths generated by the motion planner are then communicated to the Fig. <ref type="figure">20:</ref> The motion planner uses dynamic programming to compute the shortest path to the nearest goal(s) for every location in the unoccupied space, as indicated by the gray shading. Once the distance has been computed, paths are generated by hill-climbing in distance space. An additional post-processing step increases the side-clearance to obstacles. collision avoidance routine for execution.</p><p>The idea for path planning is to let the robot always move on a minimum-cost path to the next exhibit. The cost for traversing a grid cell hx; yi is proportional to its occupancy value P (c int xy ) (cf. Equation ( <ref type="formula" target="#formula_19">23</ref>)). The minimum-cost path is computed using a modified version of value iteration, a popular dynamic programming algorithm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b69">70]</ref>:</p><p>(1) Initialization. The grid cell that contains the target location is initialized with 0, all others with 1:</p><p>V x;y ? Value iteration updates the value of all grid cells by the value of their best neighbors, plus the costs of moving to this neighbor (just like A* <ref type="bibr" target="#b109">[110]</ref>). Cost is here equivalent to the probability P (c x;y ) that a grid cell hx; yi is occupied.</p><p>The update rule is iterated. When the update converges, each value V x;y measures the cumulative cost for moving to the nearest goal. However, control can be generated at any time, long before value iteration converges.</p><p>(3) Determine motion direction. To determine where to move, the robot generates a minimum-cost path to the goal. This is done by steepest descent in V , starting at the actual robot position. The steepest descent path is then postprocessed to maintain a minimum clearance to the walls. Determining the motion direction is done in regular time intervals and is fully interleaved with updating V .</p><p>Figure <ref type="figure">20</ref> shows V after convergence for a typical situation in the museum, using the map shown in Figure <ref type="figure" target="#fig_4">18</ref>. The grey-level indicates the cumulative costs V</p><p>for moving towards the goal point. Notice that every local minimum in the value function corresponds to a goal, if multiple goals exist. Thus, for every point hx; yi, steepest descent in V leads to the nearest goal point.</p><p>Unfortunately, plain value iteration is too inefficient to allow the robot to navigate while simultaneously learning maps. Strictly speaking, the basic value iteration algorithm can only be applied if the cost function does not increase (which frequently happens when the map is modified). This is because when the cost function increases, previously adjusted values V might become too small. While value iteration quickly decreases values that are too large, increasing too small a value can be arbitrarily slow <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b146">147]</ref>. Consequently, the basic value iteration algorithm requires that the value function be initialized completely (Step 1) whenever the map-and thus the cost function-is updated. This is very inefficient, since the map is updated almost constantly.</p><p>To avoid complete re-initializations, and to further increase the efficiency of the approach, the basic paradigm was extended in the following way:</p><p>4. Selective reset phase. Every time the map is updated, values V x;y that are too small are identified and reset. This is achieved by the following loop, which is iterated:</p><p>For all non-goal hx; yi do: V x;y ? 1 if V x;y &lt; min x=?1;0;1 y=?1;0;1 fV x+ x;y+ y + P (c x+ x;y+ y )g</p><p>Notice that the remaining V x;y -values are not affected. Resetting the value table bears close resemblance to value iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Bounding box.</head><p>To focus value iteration, a rectangular bounding box x min ; x max ] y min ; y max ] is maintained that contains all grid cells in which V x;y may change. This box is easily determined in the value iteration update. As a result, value iteration focuses on a small fraction of the grid only, hence converges much faster. Notice that the bounding box bears similarity to prioritized sweeping <ref type="bibr" target="#b104">[105]</ref>.</p><p>Value iteration is a very general procedure, which has several properties that make it attractive for real-time mobile robot navigation:</p><p>Any-time algorithm. Value iteration can be understood as an any-time planner <ref type="bibr" target="#b37">[38]</ref>, since it allows the generation of a robot action at (almost) any time, long before value iteration has converged. It allows the robot to move in realtime, even though some of its motion commands might be sub-optimal. Full exception handling. Value iteration pre-plans for arbitrary robot locations. This is because V is computed for every location in the map, not just the current location of the robot. Consequently, the robot can quickly react if it finds itself in an unexpected location, and generate appropriate motion directions without any additional computational effort. This is particularly important in our approach, since the collision avoidance module adjusts the motion direction commanded by the planner based on sensor readings and dynamic constraints.</p><p>In the museum, the motion planner was fast enough for real-time operation. In grid maps of size 30 by 30 meter and with a spatial resolution of 15cm, optimized value iteration, done from scratch, requires approximately one to five seconds on a SUN Sparc station. In cases where the selective reset step does not reset large fractions of the map (which is the common situation), value iteration converges in much less than a second. Figure <ref type="figure" target="#fig_10">13</ref> shows situations in which a passage is temporarily blocked, along with partially executed plans generated by the motion planner. Such situations occurred frequently in the museum, and without the ability to dynamically change the map and generate new plans, the robot would have frequently been stuck for extended durations of time (e.g., waiting for a stool to be moved away).</p><p>The motion planner, together with the collision avoidance and the various state estimation modules described above, provided the robot with the ability to safely move from one exhibit to another, while adjusting the velocity to the circumstances, circumventing obstacles when possible, but choosing completely different trajectories when passages were blocked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">High-Level Task Control</head><p>RHINO also employs a high-level planner, which is responsible for the composition of entire tours.</p><p>The task control module coordinates the various robot activities related to motion and user interaction at the highest level. It transforms abstract, user-level commands (such as: "give tour number three") into a sequence of appropriate actions, where actions either correspond to motion commands (e.g., "move to exhibit number five") or control the robot's user interface (e.g., "display image four" and "play pre-recorded message number seventeen"). The task control module also monitors the execution and modifies task-level plans if necessary.</p><p>In the museum exhibition, the primary role of the task control module was to deter- mine the order at which exhibits were visited, and to control the user interaction to ensure the robot functioned in accordance to the user's demands. When tours were given to real visitors, the job of the task control module was to monitor and control the dialogue with the visitor, and to monitor plan execution. Internet users were able to compose tours by selecting individual tour items. Since multiple Internet users often sent commands at the same time, there was a combinatorial problem of sequencing exhibits appropriately. RHINO's task control monitor is an augmented version of GOLOG, which has been described in depth elsewhere <ref type="bibr" target="#b94">[95]</ref>. GOLOG was chosen for two primary reasons: First, its seamless integration of programming and planning makes it a promising framework to specify high-level control. However, GOLOG lacks several features necessary for robot control in unpredictable and dynamic environments. The second reason for choosing GOLOG, thus, underlines a general interest to better understand how to integrate physical robot control with high level planning and decision tools, which have been subject of intense AI research ever since its interception.</p><p>GOLOG is a first-order logical language that represents knowledge in the situation action calculus <ref type="bibr" target="#b95">[96]</ref>. It uses a built-in theorem prover to generate plans and to verify their correctness <ref type="bibr" target="#b102">[103]</ref>. Programs (and plans) in GOLOG are sequences of elemental actions expressed in a logical language using if-then-else rules and recursive procedures. GOLOG also requires the programmer to provide an abstract model of the robot's environment (a domain theory), describing the effects of its actions. The key benefit of GOLOG is that it facilitates designing high-level controllers by seamlessly integrating programming and problem solving <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>. Table <ref type="table" target="#tab_6">4</ref> depicts an example GOLOG program for scheduling requests by Internet users. It basically specifies that the robot shall serve all pending requests by moving to the corresponding position and explaining it, and return to its homing position thereafter.</p><p>Unfortunately, GOLOG, in its current form, suffers from several limitations such as the lack of sensing, interaction, and execution monitoring. 2 Also, there is a mismatch between the level of abstraction of GOLOG actions and those the robot is able to perform, thus making it difficult to directly control the low-level software from GOLOG:</p><p>Sensing and Interaction: GOLOG is unable to accept and react to exogenous events. It cannot handle plans conditioned on events not known in the beginning of program execution. In the museum, the robot's actions are, of course, conditioned on user input and various other circumstances, and the ability to react to exogenous events is essential. Execution monitoring: By default, GOLOG assumes that actions always succeed if their preconditions are met. It does not monitor the execution of its actions.</p><p>In practice, however, actions can fail, and it is important that the robot reacts adequately. For example, the action wait for user request() often does not result in a user response, and timeout mechanisms have to be employed to avoid getting stuck indefinitely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Level of Abstraction:</head><p>The primitive actions provided by RHINO's low-level software components are too fine-grained to be used directly in GOLOG. For example, the action goto(exhibit) involves a collection of low-level control directives, such as setting track-points for the cameras, setting target locations for the motion planner, turning the robot towards the exhibit after arrival, etc. While in principle, GOLOG can cope with any granularity, dealing with low-level action sequencing at the most abstract level would make GOLOG programs cumbersome and difficult to understand.</p><p>These difficulties are overcome by an intermediate software layer, called GOLEX <ref type="bibr" target="#b62">[63]</ref>. GOLEX is a runtime and execution monitoring system for GOLOG, which extends GOLOG in three aspects:</p><p>GOLEX integrates sensing and user interaction capabilities into GOLOG. It enables the programmer to formulate conditional programs. Action sequences can be conditioned upon exogenous events (such as the arrival of a tour item request) and timer events. If necessary, GOLEX can activate GOLOG's planner to generate new plans in reaction to unexpected events. GOLEX permanently monitors the execution of the various concurrent actions of the underlying robot control system. If it detects a failure, it chooses appropriate actions for recovery and updates the internal state accordingly, so that GOLOG can resume its operation. GOLEX decomposes the primitive actions specified in GOLOG into a macrolike sequence of appropriate directives for the robot control system, thereby bridging the gap between GOLOG's abstract task-level programs, and the remaining robot control software. Table <ref type="table">5</ref> Implementation of primitive actions in GOLEX.</p><p>Table <ref type="table">5</ref> shows an excerpt of RHINO's GOLEX program. This program segment implements, in a Prolog-like notation, the re-scheduling of new tour items by calling GOLOG to compute a new plan, and the various commands involved when moving from one item to another. This program illustrates how high-level actions, such as drive(location), are decomposed into sequences of lower level actions. In particular, the drive(location) action involves setting the appropriate tracking point for the cameras, blowing its horn, playing music thereafter, initiating the motion by informing the motion planner, turning the robot towards the exhibit upon arrival, and continuing with the next action in the schedule. The program segment also illustrates how GOLEX can react to exogenous events (in this case: requests for tour items) and change the execution accordingly.</p><p>As an example, consider a situation where RHINO is waiting in its parking position and receives a tour request for the exhibits 1 and 12. GOLOG then generates the plan</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>do(drive(p), do(explain(e12), do(drive(e12), do(explain(e1), do(drive(e1), s0))))).</head><p>which is graphically illustrated by Figure <ref type="figure" target="#fig_18">22</ref>. Now suppose RHINO receives a new request for exhibit 5 while it explains exhibit 1. In this case GOLEX uses the predicate updateSchedule to initiate replanning using GOLOG's planner. Since exhibit 5 is closer than exhibit 12, the plan is revised to do(drive(p), do(explain(e12), do(drive(e12), do(explain(e5), do(drive(e5), . . . ))))).</p><p>The resulting plan for the tour-guide is illustrated in Figure <ref type="figure" target="#fig_18">22</ref>. By providing these methods for executing primitive actions, reacting to user requests, and monitoring  the progress of the robot, GOLEX provides the necessary "glue" between GOLOG and the rest of the robot control software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Human-Robot Interaction</head><p>An important aspect of the tour-guide robot is its interactive component. User interfaces are of great importance for robots that are to interact with "normal" people.</p><p>In settings such as the museum, where people typically do not spend extensive amounts of time with the robot, two criteria are most important: ease of use, and interestingness. The user interfaces must be intuitive, so that untrained and nontechnical users can operate the system without instruction. Interestingness is an important factor in capturing people's attention.</p><p>RHINO possesses two user interfaces, one on-board interface to interact with people directly, and one on the Web. The on-board interface is a mixed-media interface that integrates graphics, sound, and motion. The Web-based interface uses graphics and text. The interactive component was critical for RHINO's user acceptance in the museum. Visitors of the museum paid considerably little attention to the fact that the robot navigated safely from exhibit to exhibit. Instead, many seemed to be most intrigued when the robot interacted with them in some noticeable way. Some of RHINO's most enthusiastic users were less than six years old; others were over 80. The vast majority of users had not been exposed to robots prior to visiting the museum. Since the majority of visitors stayed less than 15 minutes, it was critical that RHINO's interface was easy-to-use and robust. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mixed-Media User Interface</head><p>RHINO's on-board control interface integrates graphics, motion, and spoken, prerecorded language and sound.</p><p>(1) Initially, visitors can select a tour or, alternatively, listen to a brief, pre-recorded explanation of the system (the "help text"). They indicate their choice by pressing one out of four colored buttons, shown in Figure <ref type="figure" target="#fig_19">23</ref>. (2) When RHINO moves towards an exhibit, it displays an announcement on its screen. It also uses its camera-head to indicate the direction of its destination, by continually pointing the camera towards the next exhibit. While in motion, the robot plays music in order to entertain the people. (3) At each exhibit, the robot first plays a brief pre-recorded verbal explanation.</p><p>Users are then given the choice to listen to more text or to move on to the next exhibit. Users are informed about their choice through the graphical display, and they can indicate their selection by pressing one of two lit buttons next to the screen. If no user presses a button within five seconds, the robot defaults to the fast version of the tour where a minimum of verbal explanations are provided. (4) When a tour is finished, the robot returns to a pre-defined starting position in the entrance area where it waits for new visitors.</p><p>Figure <ref type="figure" target="#fig_19">23</ref> illustrates the interaction between visitors in the museum and the robot. The left images shows an example screen of the graphical display. All text was in German. The four colored buttons adjacent to the screen were the sole input interface. Unfortunately, we were unable to systematically evaluate the various components of the user interface (see <ref type="bibr" target="#b128">[129]</ref> for a detailed comparison based on a later tour-guide robot). All evidence is therefore anecdotal. We observed that people consistently grasped the main aspects of the interface within seconds, and without the need for further explanation. They were able to select a tour without being given any instructions. Often, they did not realize immediately that they were given the choice to obtain more detailed explanations, once a tour was started. After visiting the third exhibit or so, users were usually aware of this option and made use of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Web Interface</head><p>RHINO's Web interface consists of a collection of Web pages 3 , which serves four main purposes.</p><p>(1) Monitoring.</p><p>(2) Control.</p><p>(3) Providing background information.</p><p>(4) Providing a forum for discussion.</p><p>The interface enabled remote users to establish a "virtual tele-presence" in the museum, by controlling the robot and to observing it, along with the museum and the people therein.</p><p>Three of the most frequently visited pages of the Web interface are shown in Fig- <ref type="figure">3</ref> See http://www.cs.uni-bonn.de/˜rhino/tourguide/ ure 24. The page on the left enables users to observe the robot's operation on-line.</p><p>In regular intervals, users receive live camera images. The left image includes camera images obtained with one of RHINO's cameras (left) and taken by a fixed, wall-mounted camera (right). The center of this page shows a map of the robot's environment from a bird's eye perspective, along with the actual location of the robot and the exhibits. The bottom portion of this page has two functions. When the robot is moving, it indicates which exhibit RHINO is moving towards. Once an exhibit is reached, information is provided about this specific exhibit, including hyper-links to more detailed background information. All information on this page is updated synchronously in approximately five second intervals, or when the robot reaches or leaves an exhibit. Each user's Web browser automatically reloads this page in periodic time intervals. The update rate can be specified by the user in accordance to the speed of his communication link (the default is 15 seconds).</p><p>To provide more frequent updates of the robot's state we additionally provided a Java page illustrating the current position of the robot and explaining the robot's current action (see middle image of Figure <ref type="figure" target="#fig_20">24</ref>). This Java applet directly connects to a dedicated server and updates the position of the robot every 0.3 seconds, thereby providing smooth animations of the robot's motion in the map. At the bottom of the map this applet also scrolls text explaining the current robot mode (e.g., "approaching exhibit 5").</p><p>The middle image of Figure <ref type="figure" target="#fig_20">24</ref> serves as the remote control interface of the robot.</p><p>To send the robot to an exhibit, users can click on exhibits directly in the map or, alternatively, highlight one or more exhibits listed on the left side of this Web page.</p><p>For four of the exhibits (9-12) the users could additionally specify the heading from which they wanted to see the images. Up to 12 different viewpoints were admissible for these exhibits. A particular viewpoint could be chosen by clicking on the appropriate region closely around the exhibit. When hitting the "select" button, the selected exhibits are queued. Users can also identify themselves by typing their name (or acronyms) into the name box at the top of this page. This name is then displayed in the queue. The task planner schedules exhibits so as to minimize the travel distance. Requests of multiple users for the same exhibits are accommodated in parallel, so that the exhibit is visited only once. Thus, the length of the queue is limited by the number of different exhibits (13 exhibits plus 36 view points in the museum). At times, there were over 100 pending requests of different individual users. Particularly popular was the tour item "the crew," which was only available to Web users. Upon sending the robot to the crew, the robot positioned itself so that it could see the off-board computers and, at rare occasions, some of us.</p><p>When the robot was controlled through the Internet, we quickly learned that the robot was too fast to convince some Web-users that there was actually a physical machine involved. This was specifically the case during a so-called "Internet night," a scheduled event that took place outside the opening hours of the museum.</p><p>Because the museum was empty, most of the time the robot traveled close to its Fig. <ref type="figure" target="#fig_2">25</ref>: Typical situation in which visitors try to challenge the robot, here by intentionally blocking its path.</p><p>maximum speed of 80cm/sec. With an update rate of 15 seconds per update, Web users saw mostly images of exhibits (where the robot waited for 30 seconds), but they rarely saw the robot traveling from one exhibit to another. This problem was remedied by lowering the maximum speed of the robot to 30 cm/sec. Now Web users saw images recorded along the way, raising their confidence that there might actually be a real machine (as if we couldn't have pre-recorded those images as well).</p><p>One of the major drawback of the interface was the fact that the control and monitor interface was implemented on two separate pages. This forced Web users to open multiple windows when operating the machine. In a later implementation <ref type="bibr" target="#b129">[130,</ref><ref type="bibr" target="#b147">148]</ref>, the problem was remedied by placing everything necessary for the control of the robot on a single page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reaction to People</head><p>RHINO's ability to react directly to people was, for most visitors, the most fascinating aspects of the entire system. If people stepped in the robot's way, RHINO reacted in multiple ways, each of which was characterized by a different time constant:</p><p>(1) The first behavior of the robot is initiated by its collision avoidance, slowing the robot down so as to avoid a collision. If the obstacle is not contained in the map (and thus most likely a person), the robot blows its horn. Such obstacles are detected using the inverse entropy gain filter, as described in Section 3.1.7. (2) The next visible behavior is an attempt to find a local detour around the person.</p><p>This behavior is also driven by the collision avoidance method. At the same time, the map is modified. (3) If the blockage persists and no local detour exists, the modification of the map leads, after a few seconds, to a global detour. The robot then turns away and During all this, the robot uses its cameras to indicate the desired direction of travel.</p><p>In the museum, people were all but cooperative. Many shared an ambition to "break the system." Attempts to do so included intentionally blocking the robot's way for extensive periods of time, by trying to "push" it outside its operational boundary (e.g., close to the hazardous staircase), or by lining up in a way that looked like a wall to the robot, in order to confuse its sense of location. Typical examples of such a situations are show in Figure <ref type="figure" target="#fig_2">25</ref>. Luckily none of these attempts succeeded. We attribute this robustness to the extensive use of methods that are non-reactive, in the sense that they did not base their decisions on the most sensor readings only, as advocated elsewhere <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. RHINO's ability to react to people proved to be one of the most entertaining aspects, which contributed enormously to its popularity. Many visitors were amazed by the fact that the robot acknowledged their presence by blowing its horn, and repeatedly stepped in its way to get the acoustic "reward." The ability of RHINO to decelerate in the presence of people and to "ask" for clearance proved to be one of the most entertaining aspects of the entire systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Statistics</head><p>The results of the six day deployment are summarized in Table 6 <ref type="bibr" target="#b16">[17]</ref>. RHINO operated for approximately 47 hours without any significant down-time (i.e., more than one hour). Over this period of time, the robot traveled approximately 18.6km. More than 2,000 real visitors and over 2,000 "virtual" Web-based visitors were guided by RHINO. We counted over 200,000 accesses to RHINO's Web site. The robot served a total of 2,400 tour requests by real and virtual visitors of the museum.</p><p>Only six requests were not fulfilled, all but one of them due to scheduled battery changes at the time of the request. Thus, RHINO's overall success-rate was 99:75%.</p><p>Whenever possible, RHINO chose its maximum speed (80 cm/sec when guiding real people, between 30 and 50 cm/sec when controlled through the Web). The discrepancy between the top and the average speed (36.6cm/sec) was due to the fact that in the presence of obstacles, the collision avoidance module was forced to slow the robot down.</p><p>To the best of our knowledge, during its 47 hours of operation RHINO suffered a total of six collisions with obstacles, all of which occurred at low speed and did not cause any damage. Only one of these collisions was caused by a software failure. Here the localization module failed to compute the robot's position with the necessary accuracy. All other collisions were results of various hardware failures (which were usually caused by neglect on our side to exchange the batteries in time) and by omissions in the manually generated map (which were fixed after the problem was observed).</p><p>Overall, RHINO was received with enthusiasm in all age groups. We estimate that more than 90% of the museum's visitors followed the robot for at least a fraction of a tour. Kids often followed the robot for more than an hour. According to the director of the museum, RHINO raised the overall number of visitors by at least 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Localization</head><p>Mobile robot localization has frequently been recognized as a key problem in robotics with significant practical importance. Cox <ref type="bibr" target="#b32">[33]</ref> noted that "Using sensory information to locate the robot in its environment is the most fundamental problem to providing a mobile robot with autonomous capabilities." A recent book by Borenstein, Everett, and Feng <ref type="bibr" target="#b10">[11]</ref> provides an excellent overview of the stateof-the-art in localization. Localization plays a key role in various successful mobile robot architectures <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b137">138,</ref><ref type="bibr" target="#b157">158]</ref> and various chapters in <ref type="bibr" target="#b84">[85]</ref>. While some localization approaches, such as those described in <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b138">139]</ref> localize the robot relative to some landmarks in a topological map, RHINO's approach localizes the robot in a metric space, just like those methods proposed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b149">150]</ref>.</p><p>The vast majority of approaches is incapable of localizing a robot globally; instead, they are designed to track the robot's position by compensating small odometric errors. Thus, they differ from the approach described here in that they require knowl -edge of the robot's initial position; and they are not able to recover from global localizing failures. Probably the most popular method for tracking a robot's position is Kalman filtering <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b101">102]</ref>, which represent uncertainty by single-modal distributions <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b124">125,</ref><ref type="bibr" target="#b139">140]</ref>. While these approaches are computationally extremely fast, they are unable to localize robots under global uncertainty-a problem which Engelson called the "kidnapped robot problem" <ref type="bibr" target="#b43">[44]</ref>. Recently, several researchers proposed Markov localization, which enables robots to localize themselves under global uncertainty <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b138">139]</ref>. Global approaches have two important advantages over local ones: First, the initial location of the robot does not have to be specified, and second, they provide an additional level of robustness, due to their ability to recover from localization failures. Unfortunately, none of these methods were appropriate for the museum. This is because previous Markov localization methods relied on extremely coarse-grained, topological representations of the environment, making it impossible to navigate close to "invisible" obstacles. By using a fine-grained, metric representation of space, our approach can localize a robot with much higher accuracy, and it can also deal with a wider variety of environments, including those that do not possess obvious geometric features such as corridors, intersections and doors.</p><p>In addition, the vast majority of approaches differ from the method described here in that they can only cope with static environments, that is, environments that do not possess measurable state other than the robot's location. These approaches are typically brittle in dynamic environments. The approach described in <ref type="bibr" target="#b79">[80]</ref> uses cameras pointed towards the ceiling and thus cannot perceive most of the changes that occur in typical office environments. Unfortunately, such an approach is only applicable if the ceiling contains enough structure for accurate position estimation. RHINO's approach, by filtering out sensor data, has been demonstrated to function even in highly dynamic environments. The results obtained in the museum illustrate that it is capable of reliably localizing a robot even if more than 50% of all sensor readings are corrupted by people (see also <ref type="bibr" target="#b52">[53]</ref>).</p><p>Finally, most existing approaches are restricted in the type features that they consider. Many approaches reviewed in a recent survey book on this topic <ref type="bibr" target="#b10">[11]</ref> are limited in that they require modifications of the environment. Some require artificial landmarks such as bar-code reflectors <ref type="bibr" target="#b44">[45]</ref>, reflecting tape, ultrasonic beacons, or visual patterns that are easy to recognize, such as black rectangles with white dots <ref type="bibr" target="#b9">[10]</ref>. Of course, modifying the environment is not an option in many application domains. Some of the more advanced approaches use more natural landmarks that do not require modifications of the environment. For example, the approaches of Kortenkamp and Weymouth <ref type="bibr" target="#b87">[88]</ref> and Matarić <ref type="bibr" target="#b100">[101]</ref> use gateways, doors, walls, and other vertical objects to determine the robot's position. The Helpmate robot uses ceiling lights to position itself <ref type="bibr" target="#b79">[80]</ref>. Dark/bright regions and vertical edges are used in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b158">159]</ref>, and hallways, openings and doors are used by the approach described in <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b134">135,</ref><ref type="bibr" target="#b137">138]</ref>. Others have proposed methods for learning what feature to extract, through a training phase in which the robot is told its location <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b145">146]</ref>. These are just a few representative examples of many different features used for localization. RHINO's approach differs from all these approaches in that it does not extract predefined features from the sensor values. Instead, it directly processes raw sensor data. Such an approach has two key advantages: First, it is more universally applicable since fewer assumptions are made on the nature of the environment; and second, it can utilize all sensor information, typically yielding more accurate results. Other approaches that process raw sensor data can be found in <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b97">98]</ref>; however, these approaches do not address the global localization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Mapping</head><p>RHINO's mapping approach is a variant of the well-known occupancy grid method. Occupancy grids have originally been proposed by Elfes and Moravec <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b105">106]</ref> and since been adopted in numerous robotic systems (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b160">161]</ref>). To date, occupancy grids have become the most successful metric approach to mobile robot map acquisition. Our approach differs from previous ones in that neural networks are used to learn the mapping from sensors to occupancy values; as a result, sensor readings are interpreted in the context of their neighbors, which increases the accuracy of the resulting maps <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b146">147]</ref>.</p><p>Occupancy grids, however, are not the only approach to mobile robot mapping. Chatila and Laumond <ref type="bibr" target="#b25">[26]</ref> proposed to represent objects by polyhedra in a global coordinate frame. Cox <ref type="bibr" target="#b33">[34]</ref> proposed to construct probabilistic trees to represent different, alternative models of the environment. In his work, Kalman filters and Bayesian methods are used for handling uncertainty. Lu, Milios and Gutmann <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b98">99]</ref> presented an approach that memorizes raw proximity sensor data in a metric coordinate system, using an alignment procedure that extracts lines from laser range finder data. Jeeves <ref type="bibr" target="#b143">[144]</ref>, an award-winning robot at the 1996 AAAI mobile robot competition <ref type="bibr" target="#b86">[87]</ref>, constructs geometric maps incrementally by concatenating wall segments detected in temporal sequences of sonar measurements. Jeeves's design was strongly inspired by the work presented here; its inability to handle dynamic environments and its strong commitment to parallel/orthogonal walls make its software approach more brittle than the approach described here.</p><p>A statistical approach, which addresses both mobile robot localization and metric mapping, has been proposed in <ref type="bibr" target="#b149">[150]</ref><ref type="bibr" target="#b150">[151]</ref><ref type="bibr" target="#b151">[152]</ref>. This approach uses efficient statistical estimators to interleave localization and mapping. It is specifically tailored towards building maps of large, indoor environments, where the natural drift of the robot makes it difficult to maintain an accurate sense of a robot's position. In the current application, this is not an issue, as an initial map of the environment is readily available.</p><p>All approaches discussed thus far fall into the metric paradigm <ref type="bibr" target="#b146">[147]</ref>. There exists a second, major paradigm to mapping, called topological methods. This family of algorithms represents environments by graphs, where nodes correspond to places and arcs correspond to actions for moving from one place to another. Often, topological graphs are enriched by local metric information to facilitate the navigation from one place to another. Among the earliest successful work in this field is an approach by Kuipers and Byun <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b89">90]</ref>. In their approach, topological places are defined as points that maximize the number of equidistant obstacles (a similar idea can be found in Choset's work, who refers to such points as "meetpoints" <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>). Topological places are connected by arcs, which contain metric information for locally moving from one place to another. The approach disambiguates different places by local sensor information (taken at a single node or, if necessary, at a small number of neighboring nodes). In systematic simulations, this approach has been found to reliably learn large maps of indoor environments, even if sensor data is noisy. However, in these experiments the robot was equipped with a compass, which simplifies the localization problem significantly.</p><p>A different approach to learning topological maps was proposed by Matarić <ref type="bibr" target="#b100">[101]</ref>. Her algorithm acquires topological maps of the environment in which nodes correspond to pre-defined landmarks such as straight wall segments. Neighboring topological entities are connected by links. The topological representation is enriched by distance information to help keeping track of the location of the robot. The approach was evaluated on a physical robot and was found to be robust in practice. Its inability to maintain an exact position estimate imposes intrinsic scaling limitations. Moreover, since the recognition of landmarks in this approach involves robot motion, the approach might have severe difficulties in recognizing previously visited locations when approaching them from different directions (e.g., T-junctions).</p><p>Shatkay and Kaelbling proposed a method that learns topological map from landmark observations <ref type="bibr" target="#b133">[134,</ref><ref type="bibr" target="#b134">135,</ref><ref type="bibr" target="#b132">133]</ref>. Their work extends work by Koenig and Simmons <ref type="bibr" target="#b81">[82]</ref>, who investigated the problem of learning topological maps if a topological sketch of the environment is readily available. In Shatkay and Kaelbling's work, no such assumption is made. Their approach considers local topological information along with landmark label information (which is assumed to be observable), to disambiguate different locations. A key feature of their approach is the use of a recursive estimation routine (the Baum-Welsh algorithm <ref type="bibr" target="#b117">[118]</ref>) that enables the topological mapper to refine position estimates backwards in time. As a result, their approach has built fairly large topological maps. Their work is similar to the metric approach in <ref type="bibr" target="#b149">[150,</ref><ref type="bibr" target="#b151">152]</ref> in that it also employs the Baum-Welsh algorithm to interleave mapping and localization. Other topological approaches can be found in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b162">163]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Collision Avoidance</head><p>In the field of collision avoidance for mobile robots, potential field methods <ref type="bibr" target="#b78">[79]</ref> are highly popular. They determine the steering direction of the robot by (hypothetically) assuming that obstacles assert negative forces on the robot and that the target location asserts a positive force. By restricting the field of view to the close vicinity of the robot, these methods are computationally highly efficient. While the physical analogy of considering the robot as a free-flying object is attractive, Borenstein and Koren <ref type="bibr" target="#b83">[84]</ref> identified that in practice, such methods often fail to find trajectories between narrowly spaced obstacles; they also can produce oscillatory behavior in narrowly confined corridors. An extended version of the potential field approach is introduced in <ref type="bibr" target="#b77">[78]</ref>. By modifying the potential function the motion of the robot becomes more efficient and different behaviors such as wall following and tracking can be achieved.</p><p>The virtual force field histogram algorithm <ref type="bibr" target="#b11">[12]</ref>, which closely resembles potential field methods, uses occupancy grids to represent information about obstacles close to the robot. This grid is generated and updated continuously using ultrasonic proximity sensors. Borenstein later extended this approach to the vector field histogram <ref type="bibr" target="#b12">[13]</ref>. In this algorithm. occupancy information is transformed into a histogram description of the free space around the robot, which is used to compute the motion direction and velocity for the robot.</p><p>All of the approaches discussed above generate motion commands for mobile robots under the assumption that infinite forces can be asserted on the robot-a common assumption in the field of collision avoidance for indoor robots, as most robots are operated at low speed where inertial effect can be neglected. However, to operate robots safely at higher speeds (such as walking speed), it is necessary to take the robot dynamics into account. Both the dynamic window approach <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref> and the "curvature velocity method" <ref type="bibr" target="#b136">[137]</ref>, which despite their similarity were developed independently, are designed to deal with the dynamics of mobile robots.</p><p>To deal with obstacles that cannot be detected by the robot's sensors it is necessary to integrate model-based information into reactive collision avoidance. Little attention has been payed to this problem in the literature. In <ref type="bibr" target="#b125">[126]</ref>, an approach to motion planning is proposed which in principle could solve this problem. The emphasis of their work lies in the combination of global path planning and local collision avoidance. Here, motion commands are generated based on a global model of the environment, which is updated based on sensory input. They propose a method which efficiently extracts a path to a goal point based on such a map. Unfortunately, the authors do not discuss the problem of robot dynamics and uncertain position estimation. Furthermore it is not clear how the static model of the environment is combined with the fast-changing information obtained on-the-fly. RHINO's current method, called DWA <ref type="bibr" target="#b52">[53]</ref>, is specifically designed for environments where not all obstacles can be perceived, guaranteeing safe navigation with high probability even if the robot is not certain as to where it is. To the best of our knowledge, this feature is unique in the literature on mobile robot collision avoidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Motion Planning</head><p>Robot motion planning has been subject to intense research, as documented by a large body of literature on this topic (see e.g., <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b130">131]</ref>). The majority of work addresses more complicated problems than the one addressed in this article, such as motion planning in higher-dimensional and continuous space. Motion planning for circular mobile robots is often performed in 2D, ignoring costs of rotation and the dynamics of the robot. Such a methodology yields only sub-optimal results, but greatly reduces the complexity of motion planning, which is known to be exponential in the number of degrees of freedom <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b118">119]</ref>.</p><p>A popular algorithm for robot motion planning is A* <ref type="bibr" target="#b109">[110]</ref>, which bears close resemblance to the value iteration approach proposed here (see <ref type="bibr" target="#b80">[81]</ref>). Both approaches can be used to find shortest paths to multiple goals, and they also generate values for arbitrary locations, facilitating rapid replanning in the face of unexpected events. The issue of efficient re-planning has also been addressed in <ref type="bibr" target="#b142">[143]</ref> and by Stentz <ref type="bibr" target="#b140">[141]</ref>. Stenz's D* (dynamic A*) planning algorithm is designed to quickly repair motion plans if the map is updated. RHINO's motion planner does the same; given sufficient computation time, it is guaranteed to generate optimal plans with respect to the underlying representation.</p><p>In the AI community, conditional planning has also been a subject of intense research. A recent textbook <ref type="bibr" target="#b123">[124]</ref> provides a collection of references to the most important work in this field. Almost all of the work surveyed here is focused on domains where the model is specified by a collection of logical axioms, often expressed as STRIPS-like operators <ref type="bibr" target="#b109">[110]</ref>. This work usually does not exploit the highly geometric nature of robot motion planning, and is therefore not directly applicable. State-based representations, such as the one used here, are commonly used in the literature on partially observable Markov decision processes (POMDPs) <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b96">97]</ref>. POMDPs are the mathematically the most appropriate framework to formulate the motion planning problem, since they can encompass uncertainty both in the map and the robot's location. Unfortunately, the complexity of the domain does not permit efficient planning even if today's best POMDP algorithms are employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Task Planning and Control</head><p>Recent research has produced a variety of frameworks for task-level control (see e.g., <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b135">136]</ref> and various chapters in <ref type="bibr" target="#b84">[85]</ref>). A recent AAAI symposium on this topic illustrates the interest in this area <ref type="bibr" target="#b4">[5]</ref>. To our knowledge, GOLOG is unique in its seamless integration of a programming language and a powerful theorem prover for planning. Other special-purpose programming languages for mobile robots, such as COLBERT <ref type="bibr" target="#b82">[83]</ref>, do not feature built-in problem solvers. COLBERT also stays below the level of abstraction of GOLOG in that it is designed to deal with raw sensor input and motor commands. However, since the expressive power of the situation calculus exceeds that of STRIPS-like planners (which rely on an implicit close-world assumption), GOLOG is less efficient than most existing planning algorithms (see <ref type="bibr" target="#b123">[124]</ref>).</p><p>Historically, Nilsson's SHAKEY robot was the first, successful demonstration of the synthesis of symbolic, AI-type problem solving and low-level control. While SHAKEY was a milestone in the history of mobile robotics, it suffered from a lack of robustness on the low-level side, making it too brittle to operate in domains as dynamic as the one considered in this paper. More recently, Haigh integrated PRODIGY <ref type="bibr" target="#b21">[22]</ref> with a suite of low-level software developed for CMU's XAVIER project <ref type="bibr" target="#b137">[138]</ref>. Her system, called ROUGE <ref type="bibr" target="#b63">[64]</ref>, uses PRODIGY to generate costoptimal motion plans for XAVIER, a robot navigating in the corridors of an office building. This approach differs from ours in that it does not offer a programming option on the task control level, i.e., all plans are generated by the planner, not by the programmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Human Robot Interfaces</head><p>Most existing robots do not possess user-friendly interfaces. This is because mobile robotics research tends to focus on issues related to navigation, not on human robot interaction. Nevertheless, the need for more effective human robot interfaces has clearly been recognized. For example, in his M.Sc. thesis, Torrance developed a natural language interface for teaching mobile robots names of places in an indoor environment <ref type="bibr" target="#b152">[153]</ref>. Due to the lack of a speech recognition system, his interface still required the user to operate a keyboard; however, the natural language component made instructing the robot significantly easier. More recently, Asoh and colleagues <ref type="bibr" target="#b3">[4]</ref> developed an interface that integrates a speech recognition system into a phrase-based natural language interface. They successfully instructed their "office-conversant" robot to navigate to office doors and other significant places in their environment, using verbal commands. Other researchers have proposed vision-based interfaces that allow people to instruct mobile robots via arm gestures. For example, Kortenkamp and colleagues <ref type="bibr" target="#b85">[86]</ref> recently developed a gesture-based interface, which is capable of recognizing arm poses such as pointing towards a location on the ground. In a similar effort, Kahn and colleagues <ref type="bibr" target="#b75">[76]</ref> developed a gesture-based interface which has been demonstrated to reliably recognize static arm poses (pose gestures) such as pointing. This interface was successfully integrated into Firby's reactive plan-execution system RAP <ref type="bibr" target="#b46">[47]</ref>, where it enabled people to instruct a robot to pick up free-standing objects. A recent paper by Waldherr and colleagues <ref type="bibr" target="#b156">[157]</ref> extends these approaches to dynamic gestures, i.e., gestures which are defined through motion, not just poses. Motion gestures, which are commonly used for communication among people, provide additional freedom in the design of gestures. In addition, they reduce the chances of accidentally classifying arm poses as gestures that were not intended as such.</p><p>Unfortunately, while these interfaces are important steps in the right direction, they are not quite suited for application domains as museums, where people typically interact with a robot for an extremely short duration. Most visitors spent less than 15 minutes following the robot through the museum, and even RHINO's most enthusiastic supporters stayed rarely for more than two hours. Under such circumstances, it is important that the robot appeals to the "intuitions" of its users. It is generally undesirable that users have to learn how to interact with a robot, even if the nature of the interface (language, gestures) facilitates this process. We believe that we currently lack a convincing methodology for "intuitive" human robot interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Integrations and Applications</head><p>Various researchers have devised integrated systems similar to the one described here <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b127">128]</ref>. A good survey of fielded systems is provided in a recent book by Schraft and Schmierer <ref type="bibr" target="#b127">[128]</ref>. For example, Siemens Corp. (Germany) has recently developed a mobile cleaning robot, which it successfully deployed in a supermarket <ref type="bibr" target="#b42">[43]</ref>. Unfortunately, much of their technology is proprietary. The robot differs from RHINO in that does not interact with people; it also moves at much lower speed. We suspect that the basic software architecture is similar to the one used by RHINO. A second, prominent example of a successfully deployed robot is Helpmate Inc.'s Helpmate robot <ref type="bibr" target="#b79">[80]</ref>. This robot has been deployed at dozens of hospitals worldwide, where it carries food and other items through hospital corridors. The Helpmate does not interact with people either, but it features an easy-to-use interface which enables nurses to tell the robot where to go.</p><p>To the best of our knowledge, the concept of a tour-guide robot was first proposed by Horswill <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>, who built a robot that guided visitors through the AI Lab at MIT. This robot, designed to demonstrate Horswill's fast vision routines, did not require modifications of the environment, just like RHINO (apart from colored tape at the boundary of the robot's operational range). However, it lacked the ability to modify its map; neither could it avoid collisions with invisible obstacles or recover from global localization failures. RHINO's approach relies to a much larger degree on internal state (location, map), which accounts for its enhanced level of robustness and capability required in environments such as the one considered here.</p><p>Motivated by RHINO, Nourbaksh and colleagues recently installed a similar tourguide robot in the Carnegie Museum of Natural History in Pittsburgh, PA <ref type="bibr" target="#b111">[112]</ref>. This robot, called CHIPS, has been operated successfully over a period of several months. CHIPS differs from RHINO in several aspect. To aid the navigation, CHIPS's environment is marked by bright orange-green markers, whose location is known. The robot does not possess a planner; instead, it closely follows a pre-given path. When the robot's sensors are blocked, it typically waits for the blockage to disappear. CHIPS uses high-end multi-media technology to provide detailed explanations of exhibits, lasting in the order of 3 minutes per exhibits. RHINO, in comparison, provides much shorter explanations. Finally, CHIPS lacks a Web interface.</p><p>Similarly motivated by RHINO's success, we recently installed a more advanced tour-guide (called MINERVA) in the Smithsonian's National Museum of American History in Washington, DC <ref type="bibr" target="#b147">[148]</ref>. This robot was similar to RHINO, in that it shared many of the basic navigation modules. However, it features an additional module for learning ceiling maps and using those for localization, and it also possessed a drastically revised user interface, aimed at providing the robot with a "lifelike" character. Further details can be found in <ref type="bibr" target="#b147">[148]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8">Robots on the Web</head><p>In recent years, several research teams have connected robots to the Web, enabling people all over the world to command robots remotely. One of the most prominent examples is CMU's XAVIER robot <ref type="bibr" target="#b137">[138]</ref>, which predates the work reported here. XAVIER is equipped with an interface similar to the one described here. Webusers can schedule requests for moving to pre-specified locations where the robot tells a user-selected "joke," and they can watch camera images recorded in regular time intervals. RHINO's interface offers a choice between JPEG images and Java applets; the latter option enables Web users to run a robot simulator on their own machine, in which the robot's location is animated continuously. Users command both robots at an abstract level, and the low-level navigation software ensures the robot's safety.</p><p>Others have designed interfaces for the control of robot arms. The MERCURY system <ref type="bibr" target="#b55">[56]</ref>, which started its operation in 1994, was one of the first tele-operated manipulators on the Web. It enabled Web users to excavate artifacts buried in a sand-filled terrarium. This system required users to assume exclusive control over the manipulator. The TELE-GARDEN, successor of MERCURY, enabled people to plant flowers <ref type="bibr" target="#b56">[57]</ref>. Just like RHINO, this system was able to coordinate requests by multiple users. The "Mechanical Gaze Project" <ref type="bibr" target="#b114">[115]</ref> enables visitors of a museum to examine exhibits from various viewpoints, and with different distances. Other Web-connected robots include a tele-operated "Eyebot", a robot that carries a camera whose pictures are posted on the Web (http://www.dma.nl/eyebot), and "KhepOnTheWeb", a table-top robot that Web users can manually move through a maze (http://KhepOnTheWeb.epfl.ch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary</head><p>This article described the software architecture of a fully autonomous mobile robot designed to interact with people. The robot has been proven to function reliably in unmodified and densely populated environments, and it has equally been proven to interact successfully with people. To reach this level of reliability, our approach extends a collection of well-known algorithms for mobile robot state estimation, control, and user interaction, and integrates them into a single system. While most of the work reported here is well-grounded in previously approaches to mobile robotics, some critical innovations were necessary to provide the robot with the necessary level of robustness. RHINO's localization method is based on Markov localization, a popular probabilistic localization method. Our approach extends the Markov localization paradigm by a method for filtering out noisy sensor data, which makes it extremely robust in highly dynamic environments. It also estimates the location of a robot at much higher resolution; a necessary requirement for navigating close to obstacles that cannot be sensed. The robot inherits its mapping algorithm from the decade-old literature on occupancy grid maps. It employs artificial neural networks for sensor interpretation, which allow it to interpret sensor readings in the context of its neighbors. As argued elsewhere <ref type="bibr" target="#b146">[147]</ref>, such an approach improves the robustness of the mapping algorithm to noise (such as spectral reflection). RHINO also integrates maps from different sensor modalities.</p><p>RHINO employs a hybrid collision avoidance method ( DWA), specifically designed to operate in environments where not all obstacles can be sensed reliably. In addition to the sensor readings, this approach assembles "virtual" sensor readings using a map of the environment. A key feature is its ability to generate safe motion even if the robot does not quite know where it is. This contrasts with previous methods, which are typically purely sensor-based and would thus fail in an environment as the museum. RHINO's motion planner is a version of dynamic programming, which, by spreading activation through free-space, effectively pre-plans for all possible exceptions. To accommodate the continuous change of the map, the path planner contains a method for identifying where and when replanning becomes necessary. Task-level control is performed by GOLOG, an integrated pro-gramming language and theorem prover embedded in the situation action calculus. While this approach provides a powerful framework for designing high-level controllers, it alone is inappropriate to deal with various contingencies arising in robot control. We therefore designed GOLEX, an interface which provides the glue between GOLOG and the rest of the software. GOLEX provides macros, conditional plans and an execution monitor.</p><p>RHINO exhibits a degree of interactiveness typically not found on other mobile robots. It reacts to the presence of people in various ways-an aspect that we found to be essential to spark people's enthusiasm. Both its command interface and its interface to the Web was specifically designed to appeal to novices. All these components are integrated through a modular software architecture, designed to accommodate the various bottlenecks in distributed computer networks. All computation is carried out completely asynchronously, and the different modules communicate by sending messages to each other. Computationally-intense modules were capable of adapting their requirements to the available resources, which is critical in a communication network that was often subject to lengthy communication delays.</p><p>During its six-day installation period, the robot performed reliably and excited the visitors. The robot guided thousands of users to exhibits with almost perfect reliability, traversing more than 18.6km at an average speed of 35 cm/sec. We did not modify the museum in any way that would facilitate the robot's operation; in fact, RHINO's software successfully coped with various "invisible" obstacles and the large number of people. We believe that RHINO's reliability and effectiveness in complex environments is unprecedented. The robot also raised the museum's attendance by more than 50%, suggesting that robotic applications in the field of entertainment and education might be commercially viable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion</head><p>One of the key limitations of the current approach is its reliance on an accurate map of the environment. In the Deutsches Museum Bonn, it took us about a week to manually acquire the map, partially because of the non-orthogonal arrangement of exhibits and walls. To remedy this problem, we recently devised a family of more powerful mapping techniques <ref type="bibr" target="#b149">[150,</ref><ref type="bibr" target="#b151">152]</ref>. These techniques make it possible to acquire maps of large-scale, cyclic environments from raw sensor data, collected while joy-sticking the robot through its environments (see also <ref type="bibr">Section 7.2,</ref><ref type="bibr">Figure 11,</ref><ref type="bibr" target="#b147">and [148]</ref>). Augmenting such maps with "invisible" obstacles is straightforward, as the robot's localization methods can be used to accurately determine its position. These new techniques overcome an important limitation of the approach described here, effectively reducing the installation time for a robot by more than an order of magnitude.</p><p>Another limitation arises from the fact that the pure entropy gain filter, used to filter out all corrupted sensor readings, impairs the robot's ability to recover from gl obal localization failure. As noted above, once the robot has lost track of its position entirely the entropy gain filter may filter out all authentic sensor readings, making it impossible for the robot to re-localize itself. Recently, we proposed novelty filters <ref type="bibr" target="#b52">[53]</ref>, an extension of entropy gain filters that takes into account the nature of the surprise in a sensor reading. In a systematic study using the data collected in the museum we found that novelty filters do not suffer this limitation, while still retaining the full advantage of the entropy gain filter. We envision that this new filter will lead to more robust behavior in highly dynamic environments.</p><p>Finally, the interactive component of the robot can certainly be considered a first step only. As reported above, user interaction was essential for the overall validity of the concept. RHINO's interaction is quite limited, when compared to ongoing research on human computer interaction and believable characters. While this paper was under review, we developed a second generation tour-guide robot, called MIN-ERVA <ref type="bibr" target="#b147">[148]</ref>, which exhibited a much richer repertoire of interactive means. MIN-ERVA was equipped with a face, used a finite state machine to emulate "moods," and employed reinforcement learning <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b141">142]</ref> to learn how to best attract people. In addition, MINERVA possessed a more effective Web interface, which gave users the freedom to send the robot to arbitrary points in the environment. Results of this more advanced interface are described in <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b129">130]</ref>. Nevertheless, we believe that the issue of people interaction with service robots in public places remains an open and promising research issue-which we have barely touched.</p><p>RHINO is just one out of a series of recent successful mobile robots <ref type="bibr" target="#b127">[128]</ref>. Recent research in the field of mobile robotics has led to significant progress along various dimensions. Applications such as robots that guide blind or mentally handicapped people, robots that clean large office buildings and department stores, robots that assist people in recreational activities, etc., are clearly in reach, and for many of those target domains prototypes are readily available <ref type="bibr" target="#b127">[128]</ref>. This recent, ongoing revolution has been triggered by advances along various dimensions. Robotic hardware has steadily become cheaper and more reliable. Robotic software has matured, reaching critical levels of reliability, robustness, and flexibility.</p><p>We believe that the museum tour-guide is a prototypical example of a new generation of mobile service robots. Many of the challenges in the museum tour-guide domain apply to a wide variety of mobile robot applications: the necessity to navigate through highly dynamic and unstructured environments; the necessity to interact with people; and the necessity to operate in environments that cannot be modified. It is quite likely that robots similar to the one described here will soon be deployed in shopping malls, amusement centers, technology fairs, etc., where they will be receptionists, information kiosks, waiters, guides, but most of all: magnets that attract people. Similar robots may soon perform janitorial services, operate at sites too dangerous for humans, or assist people in various aspects of their lives.</p><p>Through developing RHINO's software and watching it operate in the Deutsches Museum, we learned more than we can possibly describe in a single article. Among the most important lessons is the recognition that mobile robot navigation has progressed to a level at which robots can now navigate reliably even in densely populated spaces. In most aspects, such robots can now be installed in new sites within days, without having to modify the environment (see also <ref type="bibr" target="#b149">[150]</ref>). We also learned that human robot interfaces are key prerequisites if robots are to become part of people's everyday lives. We found that adaptive mechanisms are essential for operating robots in highly dynamic and unpredictable environments, as without the capability to revise its maps and its plans, the robot would often have been stuck. Finally, we learned that entertainment is a highly promising application domain for mobile robotics. In most envisioned service robot applications, robots have to compete with human labor, whereas in the entertainment sector, robots may generate revenue by simply exploiting the fact that they differ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The robot and its sensors. Fig. 2: RHINO, pleasing the crowd.</figDesc><graphic coords="3,287.52,80.22,183.41,141.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Probabilistic model of perception: (a) Laser range scan, projected into a previously acquired map. (b) The probability P (o j ), evaluated for all positions and projected into the map (shown in grey). The darker a position, the larger P (o j ).</figDesc><graphic coords="10,238.80,88.06,247.96,125.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Probabilistic model of robot motion: Accumulated uncertainty after moving (a) 40 meter, (b) 80 meter.</figDesc><graphic coords="11,170.44,73.93,87.07,87.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: Sensor measurements (black) selected by the entropy filter from a typical scan (left image) and endpoints of selected scans on a longer trajectory (right image)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 Fig. 9 :</head><label>89</label><figDesc>Figure8shows a prototypical situation which illustrates the entropy gain filter. Shown there are examples where RHINO has been projected into the map at its most likely position. The lines indicate the current proximity measurements, some of which correspond to static obstacles that are part of the map, whereas others are caused by humans (max-range measurements are not shown). The different shading of the measurements demonstrates the result of the entropy gain filter. The black values reduce entropy, whereas the gray values would increase the robot's entropy and are therefore filtered out. Here all measurements of humans are successfully filtered out. These examples are prototypical. In the museum exhibit, we never observed that a reading caused by a dynamic obstacle (such as a human) was not</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 ) 5 ( 2 )</head><label>152</label><figDesc>Initialization: P (c xy ) ? 0:For each observation o do: P (c xy ) ? 1 ? 1 + P (c xy j o) 1 ? P (c xy j o) P (c xy ) 1 ? P (c xy )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 10. (a) An artificial neural network maps sensor measurements to probabilities of occupancy. (b) An example sonar scan, along with the local map generated by the neural network. The darker a region, the more likely it is to be occupied.</figDesc><graphic coords="21,133.08,76.74,128.25,201.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Maps learned from scratch: (a) the Deutsches Museum Bonn, and (b) the Dinosaur Hall of Pittsburgh's Carnegie Museum of Natural History, built in preparation of the installation of a similar tour-guide robot. Both maps were acquired in less than an hour.</figDesc><graphic coords="22,283.44,69.25,203.20,113.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 12. Integrating multiple maps: (a) CAD map, (b) laser map, (c) sonar map, and (d) the integrated map. The scarceness of the sensor-based maps, when compared to the CAD map, indicates how few of the obstacles are actually detectable.</figDesc><graphic coords="23,297.84,95.02,81.58,70.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Two integrated maps, acquired in situations where a massive congestion of the museum forced the robot to take a detour.</figDesc><graphic coords="24,113.52,80.59,158.73,147.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Scan of the laser sensors, missing much of the large center obstacle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Obstacle line field, purely sensor-based. Without map information, the robot would collide with an "invisible" obstacle.</figDesc><graphic coords="25,238.93,81.09,97.48,96.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Obstacle line field enriched using virtual sensors. Here the collision is avoided. The circular trajectory visualizes the control, as chosen by DWA for the next 0.25 seconds.</figDesc><graphic coords="25,369.49,80.66,97.30,96.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 :Fig. 19 :</head><label>1819</label><figDesc>Fig.18: This map depicts, in dark grey, obstacles which could be detected, and in light grey, the boundary of the robot's operational area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>exec( [], Done, Schedule ). exec([explain(Exhibit)|ToDo], Done, Schedule ) :explain( Exhibit ), updateSchedule( [explain(Exhibit)|Done], Schedule, NewToDo, NewSchedule ), exec( NewToDo, [explain(Exhibit)|Done], NewSchedule ). exec([drive(L)|ToDo], Done, Schedule ) :position(L, (X, Y)), panTiltSetTrackPoint((X, Y)), soundPlay(horn), soundPlay(jamesBondTheme) robotDrivePath([(X, Y)]), robotTurnToPoint((X, Y)), exec(ToDo, [drive(L)|Done], Schedule).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>11 Fig. 21 :</head><label>1121</label><figDesc>Fig. 21: Schedule of tour items for the exhibits 1 and 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 22 :</head><label>22</label><figDesc>Fig.22: Re-scheduled plan after receiving a request for a exhibit 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 23 :</head><label>23</label><figDesc>Fig. 23: On-board interface of the tour-guide robot. The users were required to press the buttons right from the display to choose between different options (left image). During the tour the robot used its camera to point to the exhibits. It showed text and graphics on its display and played pre-recorded sounds from CD to explain the exhibits (right image).</figDesc><graphic coords="36,121.75,81.41,87.71,123.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 24 :</head><label>24</label><figDesc>Fig. 24: Popular pages of the Web interface. The left image shows a page with online images from the museum, including background information about the exhibits and further hyper links. The middle image contains a screen dump of a page for smooth animations of the robot's trajectory and actions based on a Java applet. The right image shows the control page, where Web-users can specify where they want the robot to go.</figDesc><graphic coords="37,92.90,74.30,145.89,130.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Fraction of the GOLOG program for the Internet tour-guide robot.</figDesc><table><row><cell>proc internet-tourguide while (9 exhibit) request(exhibit) ^next(exhibit) do</cell></row><row><cell>( exhibit).goto(exhibit) ; explain(exhibit)</cell></row><row><cell>endWhile</cell></row><row><cell>goto(homing-position)</cell></row><row><cell>endProc</cell></row><row><cell>proc goto(loc) if robotLocation(robotloc) ^robotloc 6 = loc then drive(loc) endIf</cell></row><row><cell>endProc</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Summary of the robot's six-day deployment period.chooses the second best global path.</figDesc><table><row><cell>hours of operation</cell><cell>47</cell></row><row><cell>number of visitors</cell><cell>&gt;2,000</cell></row><row><cell>number of Web visitors</cell><cell>2,060</cell></row><row><cell>total distance</cell><cell>18.6 km</cell></row><row><cell>maximum speed</cell><cell>&gt;80 cm/sec</cell></row><row><cell cols="2">average speed during motion 36.6 cm/sec</cell></row><row><cell>number of collisions</cell><cell>6</cell></row><row><cell>exhibits explained</cell><cell>2,400</cell></row><row><cell>success rate</cell><cell>99.75%</cell></row><row><cell>increased attendance</cell><cell>&gt;50%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In recent work, which was not available when RHINO's software was developed, exten- sions of GOLOG were proposed which address these shortcomings<ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b90">91]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors thank Peter Frieß and his staff from the Deutsches Museum Bonn for their enthusiastic support of this project. This research is sponsored in part by DARPA via AFMSC (contract number F04701-97-C-0022), TACOM (contract number DAAE07-98-C-L032), and Rome Labs (contract number F30602-98-2-0137), and also by the EC (contract number ERBFMRX-CT96-0049) under the TMR programme. The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of DARPA, AFMSC, TACOM, Rome Labs, the United States Government, or the EC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Behavior-Based Robotics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Asimov</surname></persName>
		</author>
		<author>
			<persName><surname>Runaround</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1942">1942</date>
			<pubPlace>Faucett Crest, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Asimov</surname></persName>
		</author>
		<author>
			<persName><surname>Robot</surname></persName>
		</author>
		<author>
			<persName><surname>Doubleday</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Socially embedded learning of office-conversant robot jijo-2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayamizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Motomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-97</title>
		<meeting>IJCAI-97</meeting>
		<imprint>
			<publisher>IJCAI, Inc</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lakemayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<title level="m">AAAI Fall Symposium, Workshop on Cognitive Robotics</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998-10">October 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<title level="m">Dynamic Programming</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mobile robot localization using landmarks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gurvits</surname></persName>
		</author>
		<idno>SCR-94-TR-474</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1993-12">December 1993</date>
			<publisher>Siemens Corporate Research</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>will also appear in the</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory (COLT)</title>
		<meeting>the Conference on Learning Theory (COLT)<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Nursing Robot System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-06">June 1987</date>
			<pubPlace>Technion, Haifa, Israel</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Navigating Mobile Robots: Systems and Techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<editor>A. K. Peters, Ltd.</editor>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Wellesley, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time obstacle avoidance for fast mobile robots in cluttered environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation</title>
		<meeting>of the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="572" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The vector field histogram -fast obstacle avoidance for mobile robots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="1991-06">June 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A robust layered control system for a mobile robot</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
		<idno>AI memo 864</idno>
		<imprint>
			<date type="published" when="1985">1985</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intelligence without reason</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-91</title>
		<meeting>IJCAI-91</meeting>
		<imprint>
			<publisher>IJCAI, Inc</publisher>
			<date type="published" when="1991-07">July 1991</date>
			<biblScope unit="page" from="569" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The mobile robot Rhino</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The interactive museum tour-guide robot</title>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hähnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lakemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fifteenth National Conference on Artificial Intelligence</title>
		<meeting>the AAAI Fifteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrating global position estimation and position tracking for mobile robots: The dynamic Markov localization approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;98)</title>
		<meeting>of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the absolute position of a mobile robot using position probability grids</title>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence<address><addrLine>Menlo Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI, AAAI Press/MIT Press</publisher>
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active mobile robot localization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Complexity of Robot Motion Planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Prodigy: An integrated architecture for planning and learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
		<editor>K. Van Lehn</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Architectures for Intelligence. Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Statistical Inference</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Wadsworth &amp; Brooks, Pacific Grove, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the exponential value of labeled samples</title>
		<author>
			<persName><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="111" />
			<date type="published" when="1995-01">January 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The relative value of and unlabeled samples in pattern recognition with an unknown mixing parameter</title>
		<author>
			<persName><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2101" to="2117" />
			<date type="published" when="1996-11">November 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Position referencing and consistent world modeling for mobile robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Laumond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1985 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 1985 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sensor Based Motion Planning: The Hierarchical Generalized Voronoi Graph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sensor Based Planning for a Planar Rod Robot</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Konuksven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/SICE/RSJ Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems</title>
		<meeting>IEEE/SICE/RSJ Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sensor Based Planning: A Control Law for Generating the Generalized Voronoi Graph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Konuksven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to Proc. IEEE Int. Advanced Robotics</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototypes, location, and associative networks (plan): Towards a unified theory of cognitive mapping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Landmark learning in bees</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Cartwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Physiology</title>
		<imprint>
			<date type="published" when="1985-01">January 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Minimalist Mobile Robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Connell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Blanche-an experiment in guidance and navigation of an autonomous robot vehicle</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling a dynamic environment using a Bayesian multiple hypothesis approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="311" to="344" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Wilfong</surname></persName>
		</author>
		<title level="m">Autonomous Robot Vehicles</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An incremental interpreter for high-level programs with sensing</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 1998 Fall Symposium on Cognitive Robotics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Execution monitoring of highlevel robot programs</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soutchanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Knowledge Representation and Reasoning: Proceedings of the Sixth International Conference (KR&apos;98)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of time-dependent planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Seventh National Conference on Artificial Intelligence AAAI-92</title>
		<meeting>eeding of Seventh National Conference on Artificial Intelligence AAAI-92<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI, AAAI Press/The MIT Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monte Carlo localization for mobile robots</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sonar-based real-world mapping and navigation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elfes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="1987-06">June 1987</date>
		</imprint>
	</monogr>
	<note>RA</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Occupancy Grids: A Probabilistic Framework for Robot Perception and Navigation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elfes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Field test of a navigation system: Autonomous cleaning in supermarkets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lawitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1998 IEEE International Conference on Robotics &amp; Automation</title>
		<meeting>of the 1998 IEEE International Conference on Robotics &amp; Automation</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>ICRA 98</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Passive Map Learning and Visual Place Recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Engelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-world issues in warehouse navigation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Gage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gilbreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Smurlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference on Mobile Robots IX</title>
		<meeting>the SPIE Conference on Mobile Robots IX<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
			<biblScope unit="volume">2352</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">TCX. An interprocess communication system for building robotic architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fedor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">15213. December 1993</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Programmer&apos;s guide to version 10</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An architecture for active vision and action</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Firby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Prokopowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-95</title>
		<meeting>IJCAI-95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monte Carlo localization: Efficient position estimation for mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Controlling synchro-drive robots with the dynamic window approach to collision avoidance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;96)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;96)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The dynamic window approach to collision avoidance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Active Markov localization for mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="195" to="207" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A hybrid collision avoidance method for mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Position estimation for mobile robots in dynamic environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fifteenth National Conference on Artificial Intelligence</title>
		<meeting>the AAAI Fifteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Navigation system based on ceiling landmark recognition for autonomous mobile robot</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Industrial Electronics Control and Instrumentation (IECON&apos;93)</title>
		<meeting>Int&apos;l Conference on Industrial Electronics Control and Instrumentation (IECON&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Esl: A language for supporting robust plan execution in embedded autonomous agents</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working notes of the AAAI Fall Symposium on Plan Execution</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Desktop tele-operation via the world wide web</title>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mascha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rothenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiegley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The telegarden</title>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santarromana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bekey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiegley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGGRAPH</title>
		<meeting>of ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to select useful landmarks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isukapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 AAAI Conference</title>
		<meeting>1994 AAAI Conference<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press / The MIT Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1251" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Vergleich von Algorithmen zur Selbstlokalisierung eines mobilen Roboters. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Gutmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Ulm, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Ulm</orgName>
		</respStmt>
	</monogr>
	<note>in German</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Navigation mobiler Roboter mit Laserscans</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonome Mobile Systeme</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>In German</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Amos: Comparison of scan matching approaches for self-localization in indoor environments</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schlegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Euromicro Workshop on Advanced Mobile Robots</title>
		<meeting>the 1st Euromicro Workshop on Advanced Mobile Robots</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Many robots make short work</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Julia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">GOLEX: Bridging the gap between logic (GOLOG) and a real robot</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hähnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lakemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22st German Conference on Artificial Intelligence (KI 98)</title>
		<meeting>the 22st German Conference on Artificial Intelligence (KI 98)<address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">High-level planning and low-level execution: Towards a complete robotic agent</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Haigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First International Conference on Autonomous Agents</title>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lewis</forename><surname>Johnson</surname></persName>
		</editor>
		<meeting>First International Conference on Autonomous Agents<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Introduction to the theory of neural computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Addison-Wesley Pub. Co</publisher>
			<pubPlace>Redwood City, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Landmark-based autonomous navigation in sewerage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hertzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kirchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Euromicro Workshop on Advanced Mobile Robots</title>
		<meeting>of the First Euromicro Workshop on Advanced Mobile Robots</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Environment perception with a laser radar in a fast moving robot</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Knieriemen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Robot Control</title>
		<meeting>Symposium on Robot Control<address><addrLine>Karlsruhe, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-10">October 1988</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="68" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Polly: A vision-based artificial agent</title>
		<author>
			<persName><forename type="first">I</forename><surname>Horswill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93)</title>
		<meeting>the Eleventh National Conference on Artificial Intelligence (AAAI-93)</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Specialization of perceptual processes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Horswill</surname></persName>
		</author>
		<idno>AI TR-1511</idno>
		<imprint>
			<date type="published" when="1994-09">September 1994</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT, AI Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Markov Processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>MIT Press and Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A Bayesian approach to real-time obstacle avoidance for a mobile robot</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous Robots</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="69" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Using stereo vision to pursue moving agents with a mobile robot</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Reinforcement learning algorithm for partially observable decision problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kurien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Gesture recognition using the perseus architecture</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Prokopowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Firby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. ASME, Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">An extended potential field approach for mobile robot sensor-based motions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Intelligent Autonomous Systems (IAS&apos;4)</title>
		<meeting>International Conference on Intelligent Autonomous Systems (IAS&apos;4)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Real-time obstacle avoidance for robot manipulator and mobile robots</title>
		<author>
			<persName><forename type="first">O</forename><surname>Khatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Helpmate autonomous mobile robot navigation system</title>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Conference on Mobile Robots</title>
		<meeting>the SPIE Conference on Mobile Robots<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-11">November 1990</date>
			<biblScope unit="volume">2352</biblScope>
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">The complexity of real-time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<idno>CMU-CS-92-145</idno>
		<imprint>
			<date type="published" when="1992-04">April 1992</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Passive distance learning for robot navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Saitta</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Colbert: A language for reactive control in saphira</title>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KI-97: Advances in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="31" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Potential field methods and their inherent limitations for mobile robot navigation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation</title>
		<meeting>IEEE Int. Conf. Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1991-04">April 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">AI-based Mobile Robots: Case studies of successful robot systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Bonasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Recognizing and interpreting gestures on a mobile robot</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-96</title>
		<meeting>AAAI-96</meeting>
		<imprint>
			<publisher>AAAI Press/The MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="915" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The 1996 AAAI mobile robot competition and exhibition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hinkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Topological mapping for mobile robots using a combination of sonar and vision sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weymouth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth National Conference on Artificial Intelligence</title>
		<meeting>the Twelfth National Conference on Artificial Intelligence<address><addrLine>Menlo Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI, AAAI Press/MIT Press</publisher>
			<date type="published" when="1994-07">July 1994</date>
			<biblScope unit="page" from="979" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A robust qualitative method for spatial learning in unknown environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Eighth National Conference on Artificial Intelligence AAAI-88</title>
		<meeting>eeding of Eighth National Conference on Artificial Intelligence AAAI-88<address><addrLine>Menlo Park, Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press / The MIT Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">On sensing and off-line interpreting in GOLOG</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lakemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 1998 Fall Symposium on Cognitive Robotics</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Robot Motion Planning</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Latombe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Directed Sonar Sensing for Mobile Robot Navigation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Dynamic map building for an autonomous mobile robot</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="89" to="96" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">GOLOG: A logic programming language for dynamic domains</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lespérance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scherl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="84" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
		<title level="m">State constraints Journal of Logic and Computation, special issue on actions and processes</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="665" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning policies for partially observable environments: Scaling up</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Prieditis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</editor>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Globally consistent range scan alignment for environment mapping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="333" to="349" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Robot pose estimation in unknown environments by matching 2d range scans</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent and Robotic Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<title level="m">Bayesian Methods for Adaptive Models</title>
		<meeting><address><addrLine>Pasadena, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A distributed model for mobile robot environment-learning and navigation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Matarić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="1990-01">January 1990</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
	<note>also available as MIT AI Lab Tech Report AITR-1228</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">The Kalman filter: An introduction to concepts</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous Robot Vehicles</title>
		<imprint>
			<publisher>Springer verlag</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Situations, actions and causal laws</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Information Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1968">1968</date>
			<biblScope unit="page" from="410" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less time</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Moravec</surname></persName>
		</author>
		<title level="m">Sensor fusion in certainty grids for mobile robots. AI Magazine</title>
		<meeting><address><addrLine>Summer</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Location recognition in a mobile robot using self-organizing feature maps</title>
		<author>
			<persName><forename type="first">U</forename><surname>Nehmzow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smithers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Autonomous Mobile Robots</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Schmidt</surname></persName>
		</editor>
		<imprint>
			<publisher>springer Verlag</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Dynamics parametrically controlled by image correlations organize robot navigation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schöner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="293" to="307" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning to classify text from labeled and unlabeled documents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Principles of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Springer Publisher</publisher>
			<pubPlace>Berlin, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">DERVISH an office-navigating robot</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="1995">1995</date>
			<pubPlace>Summer</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">The failures of a self-reliant tour robot with no planner</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/illah/SAGE/index.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Using unsupervised learning to assist supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Australian Joint Conference on AI</title>
		<meeting>the Eighth Australian Joint Conference on AI</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A mobile robot that learns its place</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="683" to="699" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Delivering real reality to the world wide web via telerobotics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Paulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems: networks of plausible inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Surmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huser</surname></persName>
		</author>
		<title level="m">Moria: Fuzzy Logik gesteuertes, autonomes Fahrzeug. internal report</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>In German</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page">8825949</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Complexity of the mover&apos;s problem and generalizations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Reif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 20th IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="421" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Concurrent localisation and map building for mobile robots using ultrasonic sensors</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Rencken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-07">July 1993</date>
			<biblScope unit="page" from="2129" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">DAMN: A Distributed Architecture for Mobile Navigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenblatt</surname></persName>
		</author>
		<idno>CMU-RI-TR-97-01</idno>
		<imprint>
			<date type="published" when="1997-01">January 1997</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The distributed architecture for mobile navigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental and Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="339" to="360" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">I + II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Modern Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A comparison of position estimation techniques using occupancy grids</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 1994 IEEE International Conference on Robotics and Automation<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
			<biblScope unit="page" from="1628" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Mobile robot navigation in a dynamic world using an unsteady diffusion equation strategy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azarm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Sensorinterpretation und Kartenerstellung für mobile Roboter</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Schneider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-12">December 1994</date>
			<biblScope unit="volume">53117</biblScope>
			<pubPlace>Bonn</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science III, University of Bonn</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
	<note>In German</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Schraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schmierer</surname></persName>
		</author>
		<author>
			<persName><surname>Serviceroboter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer verlag</publisher>
		</imprint>
	</monogr>
	<note>In German</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Spontaneous short-term interaction with mobile robots in public places</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Web interfaces for mobile robots in public places</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Magazine on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Planning, Geometry and Complexity of Robot Motion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Ablex Publishing Corporation</publisher>
			<pubPlace>Norwood, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahshahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="1994-09">Sept 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Learning Models for Robot Navigation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shatkay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Learning topological maps with weak local odometric information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shatkay</surname></persName>
		</author>
		<author>
			<persName><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-97</title>
		<meeting>IJCAI-97</meeting>
		<imprint>
			<publisher>IJCAI, Inc</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Learning hidden Markov models with geometric information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shatkay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<idno>CS-97-04</idno>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Concurrent planning and execution for autonomous robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="50" />
			<date type="published" when="1992-02">February 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">The curvature-velocity method for local obstacle avoidance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation</title>
		<meeting>of the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A layered architecture for office delivery robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Haigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Autonomous Agents</title>
		<meeting>the First International Conference on Autonomous Agents<address><addrLine>Marina del Rey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-02">February 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Probabilistic robot navigation in partially observable environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-95</title>
		<meeting>IJCAI-95<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IJCAI, Inc</publisher>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="1080" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Estimating uncertain spatial relationships in robotics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Self</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheeseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomous Robot Vehnicles</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Wilfong</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="167" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">The focussed D* algorithm for real-time replanning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-95</title>
		<meeting>IJCAI-95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Exploration and model building in mobile robot domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Ruspini</surname></persName>
		</editor>
		<meeting>the IEEE International Conference on Neural Networks<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Neural Network Council</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">To know or not to know: On the utility of models in mobile robotics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="54" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Bayesian landmark learning for mobile robot localization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Finding landmarks for mobile robot navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Learning metric-topological maps for indoor mobile robot navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="71" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">MINERVA: A second generation mobile tour-guide robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hähnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Map learning and high-speed navigation in RHINO</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bücken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fröhlinghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI-based Mobile Robots: Case Studies of Successful Robot Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Kortenkamp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Bonasso</surname></persName>
		</editor>
		<editor>
			<persName><surname>Murphy</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">A probabilistic approach to concurrent mapping and localization for mobile robots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="253" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Probabilistic mapping of an environment by a mobile robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Integrating topological and metric maps for mobile robot navigation: A statistical approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fifteenth National Conference on Artificial Intelligence</title>
		<meeting>the AAAI Fifteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Natural communication with robots</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Torrance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-01">January 1994</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT Department of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Rossum&apos;s Universal Robots)</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R U R</forename><surname>Čapek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
	<note>out of print</note>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Randow</surname></persName>
		</author>
		<title level="m">Roboter: Unsere Nächsten Verwandten. Rohwolt Verlag</title>
		<meeting><address><addrLine>Reinbek, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m">Readings in Speech Recognition</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-F</forename><surname>Lee</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Template-based recognition of pose and motion gestures on a mobile robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Waldherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Margaritis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fifteenth National Conference on Artificial Intelligence</title>
		<meeting>the AAAI Fifteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="977" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Keeping track of position and orientation of moving indoor systems by correlation of range-finder scans</title>
		<author>
			<persName><forename type="first">G</forename><surname>Weiß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Puttkamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent Robots and Systems</title>
		<meeting>the International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="595" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Position refinement for a navigating robot using motion information based on honey bee strategies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wolfart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Robotic Systems (SIR 95)</title>
		<meeting>the International Symposium on Robotic Systems (SIR 95)<address><addrLine>Pisa, Italy; U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh, Department of Artificial Intelligence</orgName>
		</respStmt>
	</monogr>
	<note>also appeared as DAI Research Paper No 751</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Place recognition in dynamic environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<ptr target="http://www.aic.nrl.navy.mil/yamauchi/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Robotic Systems</title>
		<imprint/>
	</monogr>
	<note>Special Issue on Mobile Robots, (to appear</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Magellan: An integrated adaptive architecture for mobile robots</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Adams</surname></persName>
		</author>
		<idno>98-2</idno>
	</analytic>
	<monogr>
		<title level="m">Institute for the Study of Learning and Expertise (ISLE)</title>
		<meeting><address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Approximate reasoning using anytime algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<editor>S. Natarajan</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
	<note>Imprecise and Approximate Computation</note>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Robust world-modeling and navigation in a real world</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Zimmer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
