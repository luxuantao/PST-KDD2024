<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenchao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D61F43999C831A810771B7C87235471B</idno>
					<idno type="DOI">10.1109/LGRS.2018.2813094</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bounding box</term>
					<term>convolutional neural network</term>
					<term>optical remote-sensing image</term>
					<term>orientation angle information</term>
					<term>ship detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ship detection is a challenging problem in complex optical remote-sensing images. In this letter, an effective ship detection framework in remote-sensing images based on the convolutional neural network is proposed. The framework is designed to predict bounding box of ship with orientation angle information. Note that the angle information which is added to bounding box regression makes bounding box accurately fit into the ship region. In order to make the model adaptable to the detection of multiscale ship targets, especially small-sized ships, we design the network with feature maps from the layers of different depths. The whole detection pipeline is a single network and achieves real-time detection for a 704×704 image with the use of Titan X GPU acceleration. Through experiments, we validate the effectiveness, robustness, and accuracy of the proposed ship detection framework in complex remote-sensing scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S HIP detection in remote-sensing images is important in both civilian and military applications, such as traffic dynamic monitoring, fishery management, security threats uncovering, illegal activities uncovering. However, optical remote-sensing images suffer from the complex scene condition and the large data quantity <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, which makes ship detection a challenging task. Furthermore, the appearance and orientation of the ships are diverse in remote-sensing images, which further increase the complexity of ship detection.</p><p>Researchers have made a lot of effort in this field. Zhu et al. <ref type="bibr" target="#b3">[4]</ref> proposed a hierarchical complete and operational ship detection model based on a sequential false alarm elimination approach by using shape and texture features. In addition, Liu et al. <ref type="bibr" target="#b1">[2]</ref> presented a ship detection method using shape and context information. Li et al. <ref type="bibr" target="#b4">[5]</ref> went further and proposed a method for ship detection via ship head classification and body boundary determination. By using the human-designed features (i.e., shape, texture, gray intensity, bag of words, and so on) and various classification methods, most of the ship detection models can get satisfying results in  good-quality images. However, such kind of methods cannot fill the gap well when the ship objects are not presented in ideal quality and can hardly overcome the disturbance of complex background, such as harbor and mist.</p><p>As known, convolutional neural network-based models perform well and do not need human-designed features, which make these models widely used for object detection in remotesensing images. Common convolutional neural network-based models include pixelwise labeling methods and bounding box labeling methods. Lin et al. <ref type="bibr" target="#b5">[6]</ref> designed a ship detection framework based on a pixelwise labeling method and achieved a robust performance. In addition, Lin et al. <ref type="bibr" target="#b6">[7]</ref> proposed a novel fully convolutional network to accomplish ship detection. However, the pixelwise labeling methods generally suffer from the problem of class imbalance when used for ship detection. This problem causes high false-positive rate and high false-alarm rate, especially for small object detection. In addition, the output of the pixelwise labeling method is always blurred near the edge of the object, which may lead to highposition error risk. The bounding box labeling method <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> breaks the above limits-it has an advantage in terms of object detection performance and positioning accuracy. However, as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, in the case that the ships dock side by side, the traditional bounding box strategy using scale and aspect ratio parameters will cause a larger intersection over union (IoU). These bounding boxes with large IoU may be suppressed in nonmaximum suppression (NMS) step, and hence, being missed. In contrast, as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, rotated bounding boxes with arbitrary rotation angles make a small IoU, therefore more suitable than the traditional ones for ship detection. For this purpose, most studies have suggested to rotate the image <ref type="bibr" target="#b11">[12]</ref> or to generate the rotated region proposal <ref type="bibr" target="#b12">[13]</ref>, but there still remains room for improvement in this field. In this letter, we propose an arbitrary-oriented ship detection framework that is designed to directly predict the rotated bounding box by a convolutional neural network so as to robustly and effectively detect ships in optical remotesensing images. The main contributions of this letter mainly lie in the following two aspects.</p><p>1) We present an arbitrary-oriented ship detection framework and introduce the loss function for the training of the ship detection framework. 2) We enhance the framework performance for the ship detection especially small ones via fine-grained features and also describe the training tricks for the proposed framework. In summary, the proposed framework depicts a concise procedure to achieve high ship detection performance with a small number of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED ARCHITECTURE</head><p>In this letter, the network is designed to predict the rotated bounding box of arbitrary-oriented ship. First, we elaborate the construction of the rotation-based network architecture. The details of each phase will be described in Sections II-B and II-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>The proposed network architecture is inspired by the YOLO <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> network which models detection as a regression problem. The YOLO can run in real time and, therefore, is suitable to be applied in ship detection for large-scale remote-sensing images.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the proposed network employs the Darknet-19-based YOLOv2 architecture as a fundamental network for ship detection. Same as YOLOv2 network'C the proposed network divides the input image into grid cells, which enables the network to realize end-to-end training, real-time performance, and meanwhile high average precision (AP).</p><p>For small object detection, the network can benefit from fine-grained features. Considering the ships are generally small, thin, and long, we modify the network through integrating feature maps from different depth layers-we add reorganization and route layers to the YOLOv2 network, which together bring features from the earlier 22 × 22 feature map, the earlier 44 × 44 feature map, and the earlier 88 × 88 feature map. Fig. <ref type="figure" target="#fig_2">3</ref> briefly demonstrates the implementation detail.</p><p>We set the size of the integrated feature maps as 44 × 44. Two convolutional layers are followed by the integrated feature map. Convolutional kernel sizes of these two layers are 3 × 3 and 1 × 1, respectively. Between these two convolution layers are batch normalization layer and rectified linear unit layer sequentially. The final output works as a tensor to predict the parameters and confidence score c of each rotated bounding box, which will be discussed in Sections II-B and II-C.</p><p>For each grid cell, the network predicts B-rotated bounding boxes. And for each bounding parameter, confidence score of 1 is predicted. Hence, the size of final prediction tensor is 44 × 44 × (B × (5 + 1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Definition of Rotated Bounding Box</head><p>The rotated bounding box is defined by the parameters including coordinate, width, height, and orientation. The coordinate (x, y) represents geometric center of the bounding box. Note that the coordinate (x, y) is parameterized as an offset of a particular grid cell location and it is normalized between 0 and 1. The orientation parameter θ controls the orientation of a bounding box. It is defined as the angle between the x-axis positive direction and the bounding box side whose slope is less than or equal to 0. The orientation parameter θ ranges from 0 to π/2. We normalize the orientation parameter θ by π/2 so that it falls between 0 and 1. Note that we use a logistic activation to constrain the network's predictions of coordinate and orientation parameters to fall in the range 0-1. The bounding box width w indicates the length of the bounding box side whose slope is less than or equal to 0. The bounding box height h indicates the length of the bounding box side whose slope is larger than 0. Note that in the final layer, we use an exponential function to obtain the network's prediction of the bounding box width and height.</p><p>The confidence score of each rotated bounding box indicates the probability that the predicted bounding box is a ship. We use a logistic activation to constrain the network's predictions of confidence score to fall in the range of 0-1.</p><p>The bounding box that is near the image border may exceed the image border when rotating. Note that we simply reserve such bounding boxes for network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Skew IoU Computation</head><p>Although the orientation of the rotated bounding box is arbitrary, any overlap area between two bounding boxes is always a polygon. So, the skew IoU can be calculated with the method of triangulation <ref type="bibr" target="#b14">[15]</ref>, the geometric principle is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>First, the linear functions of each bounding box sides can be obtained by the bounding box parameters.</p><p>Next, we can get all the intersection points of the linear functions by the following:</p><formula xml:id="formula_0">(x i j , y i j ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ null if((a i b j -a j b i ) == or i == j) b i c j -b j c i a i b j -a j b i , a j c i -a i c j a i b j -a j b i if((a i b j -a j b i )! = 0 and i ! = j).</formula><p>(1) Then, the intersection points which belong to the intersection range are added to the point set of overlap area.</p><p>After that, the points in the point set are sorted in anticlockwise order.</p><p>Finally, the overlap area can be computed by triangulation. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, we can obtain the overlap area S o and the union area S u as follows:</p><p>S o = S P 0 P 1 P 2 + S P 0 P 2 P 3 + S P 0 P 3 P 4 + S P 0 P 4 P 5 (2)</p><formula xml:id="formula_1">S u = (w 1 • h 1 ) + (w 0 • h 0 ) -S o .</formula><p>(</p><formula xml:id="formula_2">)<label>3</label></formula><p>The skew IoU can be obtained by the following:</p><formula xml:id="formula_3">IoU = S o /S u . (<label>4</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning of Rotated Bounding Box</head><p>During training, the loss function is a crucial ingredient in learning of the rotated bounding box for ship detection. The overall objective loss function is a weighted sum of the bounding box parameters loss and the confidence loss. Unlike YOLOv2, we add the orientation loss to the loss function.</p><p>During training, we optimize the following multipart loss function:</p><p>where 1 obj i j denotes that the j th bounding box in cell i predicts ship, 1 noobj i j denotes predicting no ship, λ coord , λ obj , and λ noobj are the weight parameters of each part, S denotes the grid cell number, and B denotes the rotated bounding number of each grid cell. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>This section reports several experiments conducted with our proposed ship detection framework. Using relatively few training examples, we manage to demonstrate the high adaptability and high location accuracy of our framework in complex scenes. The proposed network is implemented using an opensource neural network framework DARKNET which is written in C and CUDA. The proposed network is run on a GTX Titan X GPU with 12 GB of memory donated by the NVIDIA Corporation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set Description</head><p>A data set containing a total of 3417 images obtained from Google Earth and GaoFen-2 satellite is used in this letter. This data set contains over 18 000 ships with various types and sizes. Among these, 3417 images are obtained from Google Earth, each with a resolution between 1 and 0.3 m. The other 855 images are panchromatic ones obtained from GaoFen-2 satellite, each with a resolution of 1 m. The dimension of all images is 704 pixels × 704 pixels. Our data set is randomly separated into three parts: 1886 images are used to train the network, 452 images are used for validation, and the remaining images are used for testing. The ground truth is labeled manually by an open annotation tool LabelMe. Moreover, each patch of training set is rotated around its center point in 20°i ntervals, which makes the data set augmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training the Network</head><p>However, the remote-sensing images in training set are still insufficient to gain a promising network for ship detection. To remedy this, fine-tuning is applied to the proposed network training. The benefit of fine-tuning is that it is not necessary to feed the model with abundant image samples to find a local optimum point. So, we initialize the proposed network with parameters which have been pretrained using the visual object classes 2007 + 2012 detection data set.</p><p>1) Evaluation Criterion: Average skew IoU with ground truth and recall (R) rate are used as a criterion to quantify the goodness of fit of the network. The recall is defined as follows:</p><formula xml:id="formula_5">R = TP TP + FN = N i=1 TP i N i=1 (TP i + FN i ) (6)</formula><p>where TP stands for the number of correctly identified ships, FP stands for the number of falsely identified ships, FN stands for the number of missed ships, and N indicates the image number of validation set. If the skew IoU between ground truth and predicted bounding box is larger than 0.5, the predicted bounding box is considered as a true positive. 2) Learning Rate: Throughout the training, we use a batch size of 48, a momentum of 0.9, and a decay of 0.0005. And we set the learning rate to 0.001 and slowly decrease it to 0.00001, except for the first five epochs, in which we use a learning rate value of 0.0001 in case that a value greater than 0.0001 may lead to model instability. 3) Weight Parameter: In remote-sensing images, ship objects are generally sparse. Thus, many grid cells do not contain any ship. This leads to the problem of imbalance between positive and negative grid cells.</p><p>In addition, a large number of grid cells are misclassified as negative during the initial training period. All these problems can push the "confidence" scores of all cells toward zero, thus overpowering the gradient of those cells that do contain objects. This can lead to the model instability, causing training to diverge early. To solve this problem, we adjust the weight parameters of loss function. Our loss function weight parameters schedule is as follows: We set λ coord to constant 1. The value of λ noobj and λ obj are empirically set to 1 and 5, respectively. 4) Bounding Box Prior: Better prior information can make it easier for the network to learn to predict good detections. The prior information of width and height of ship is obtained by the k-means clustering on the training set. We can get no prior information of the parameter θ , because the orientation of the ship is arbitrary. The parameter θ can only be predicted by the network without the prior information. Here, experiments were designed to analyze the effects of the number of rotated bounding box on the detection result. The value of B was set to 1, 3, 5, 7, and 9. From Fig. <ref type="figure" target="#fig_3">4</ref>, we can find that the larger B is directly proportional to the higher recall rate and faster convergence rate. From Table <ref type="table" target="#tab_1">I</ref>, we can also find that the larger the value of B is, the lower the computational efficiency is. Comprehensively, taking into account the performance and the computational efficiency, we set B to 5 in this letter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation</head><p>We design an experiment to test proposed model on the test data set. In the experiment, threshold for NMS is set to 0.2, because the overlap area of the rotated bounding boxes is small even though the ships are located very close together.</p><p>In the test data set, ships with various scale may appear in both harbor and sea area, some of which are connected with the harbor, some are not, some are moored side by side, and some are moored alone. In addition, the ship detection in some of the images is under complex background disturbance. Fig. <ref type="figure" target="#fig_5">5</ref> shows the typical ship detection result in above cases. This visual evaluation result demonstrates the robustness of the proposed method.</p><p>We use the precision-recall curve to quantitatively evaluate our ship detection framework. Fig. <ref type="figure" target="#fig_5">5</ref>(e) shows the precisionrecall curve for the proposed ship detection network evaluated on the test data set. The recall rate is calculated by <ref type="bibr" target="#b6">(7)</ref>. The precision (P) is calculated by the following:</p><formula xml:id="formula_6">P = TP TP + FP = N i=1 TP i N i=1 (TP i + FP i )<label>(7)</label></formula><p>where N indicates the image number of test set. In terms of efficiency, the ship detection framework runs 0.85 s per 704 × 704 image with the use of Titan X GPU acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison and Discussion</head><p>To demonstrate the effect of fine features on ship detection, we present an experiment based on original Darknet-19 network which predicts detections on a 22 × 22 feature map. Figs. <ref type="figure" target="#fig_5">5</ref> and<ref type="figure" target="#fig_6">6</ref> show the experimental results. The comparison of experimental results indicates that concatenating fine feature can effectively improve the performance of our model in ship detection, especially for small ship detection. As our data set contains a lot of small ships, the performance differences are obvious to note.</p><p>We regard the state-of-the-art method [rotated region-based CNN (RR-CNN)] [14] as a baseline. The performance comparison metric is given in terms of the average precision. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>  in dealing with small ship. To sum up, our method outperforms the baseline in that its procedure is more concise and is robust in complex remote-sensing scenes.</p><p>IV. CONCLUSION In this letter, we have proposed an arbitrary-oriented ship detection framework remote-sensing images. We defined a rotated bounding box for the arbitrary-oriented ship. The rotated bounding box is directly predicted by the framework. In addition, the framework is designed to access the finegrained features to improve the performance of small ship object detection. In training stage, the framework gained high performance by fine-tuning with a small number of training data. The experiments confirmed that the proposed framework is robust to complex remote-sensing scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the bounding box strategy. (a) Strategy without rotation. (b) Strategy with rotation.</figDesc><graphic coords="1,443.51,173.45,94.58,82.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Rotated bounding box-based ship detection pipeline.</figDesc><graphic coords="2,153.83,121.49,116.42,62.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Implementation of integrating feature maps from different depth layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Intersection area computation of two bounding boxes in arbitrary orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 (</head><label>4</label><figDesc>b) shows the changes of skew IoU and recall rate R in each epoch evaluated on the validation data set. In epoch 92, prediction has both high IoU with the ground truth and high recall rate value. Thus, the result from epoch 92 is adopted as the final model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a)-(d) Performance on the validation set. (e) Recall-precision curve on the test data.</figDesc><graphic coords="4,436.91,230.21,125.66,84.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Ship detection results in typical scenes. (a)-(c), (e)-(g), and (j)-(l) Google Earth images. (d), (e), (h), (i), (m), and (n) GaoFen-2 satellite images. (a)-(e) Result of RR-CNN. (e)-(i) Result on 22 × 22 feature map. (j)-(n) Result on 44 × 44 feature map.</figDesc><graphic coords="5,457.91,276.41,99.86,99.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received October 3, 2017; revised January 5, 2018 and February 6, 2018; accepted March 4, 2018. This work was supported in part by the Chang Jiang Scholars Program under Grant T2012122 and in part by the Hundred Leading Talent Project of Beijing Science and Technology under Grant Z141101001514005. (Corresponding author: He Chen.) W. Liu and H. Chen are with the Beijing Key Laboratory of Embedded Realtime Information Processing Technology, Beijing Institute of Technology, Beijing 100081, China (e-mail: chenhe@bit.edu.cn).</figDesc><table /><note><p>L. Ma is with the School of Information Engineering, Zhengzhou University, Zhengzhou 450001, China. Color versions of one or more of the figures in this letter are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/LGRS.2018.2813094</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I TIME</head><label>I</label><figDesc>CONSUMPTION OF DIFFERENT VALUES OF B (IN SECONDS)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, RR-CNN adopts epoch 63 as final model and gives 78.09% AP. The proposed method adopts epoch 92 as final model and can achieve 79.79% AP. The visual comparison shows that the proposed method has advantages</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on object detection in optical remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="11" to="28" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new method on inshore ship detection in highresolution satellite images using shape and context information</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="617" to="621" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compressed-domain ship detection on spaceborne optical image using deep neural network and extreme learning machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1174" to="1185" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel hierarchical method of ship detection from spaceborne optical image based on shape and texture features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3446" to="3456" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel inshore ship detection via ship head classification and body boundary determination</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1920" to="1924" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional network with task partitioning for inshore ship detection in optical remote sensing images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1665" to="1669" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maritime semantic labeling of optical remote sensing images with multi-scale fully convolutional network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">480</biblScope>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ship detection in spaceborne optical image with SVD networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5832" to="5845" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.08242" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SSD: Single shot MultiBox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.02325" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthetic aperture radar ship detection using Haar-like features</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Schwegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kleynhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="158" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1074" to="1078" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rotated region based CNN for ship detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two algorithms for constructing a Delaunay triangulation</title>
		<author>
			<persName><forename type="first">D.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Schachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="242" />
			<date type="published" when="1980-02">Feb. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
