<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tom-vs-Pete Classifiers and Identity-Preserving Alignment for Face Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Berg</surname></persName>
							<email>tberg@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
							<email>belhumeur@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tom-vs-Pete Classifiers and Identity-Preserving Alignment for Face Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FE3AB1D29313C16257D89835FCDE8F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method of face verification that takes advantage of a reference set of faces, disjoint by identity from the test faces, labeled with identity and face part locations. The reference set is used in two ways. First, we use it to perform an "identity-preserving" alignment, warping the faces in a way that reduces differences due to pose and expression but preserves differences that indicate identity. Second, using the aligned faces, we learn a large set of identity classifiers, each trained on images of just two people. We call these "Tom-vs-Pete" classifiers to stress their binary nature. We assemble a collection of these classifiers able to discriminate among a wide variety of subjects and use their outputs as features in a same-or-different classifier on face pairs. We evaluate our method on the Labeled Faces in the Wild benchmark, achieving an accuracy of 93.10%, significantly improving on the published state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In face verification, we are given two face images and must determine whether they are the same person or different people. The images may vary in pose, expression, lighting, occlusions, image quality, etc. The difficulty lies in teasing out features in the image that indicate identity and ignoring features that vary with differences in environmental conditions.</p><p>One might argue it is easy to find features that correspond with identity. For example, to distinguish the faces of actors Lucille Ball and Orlando Bloom from each other, we can consider hair color. "Red hair" is a simple feature that consistently indicates Lucille Ball. To distinguish images of Stephen Fry and Brad Pitt, the best feature might be "crooked nose." With a sufficiently large and diverse set of these features, we should have a discriminating feature for almost any pair of subjects. Kumar et al. <ref type="bibr" target="#b17">[18]</ref> explored this approach, calling these features "describable visual attributes," implementing them as classifiers, and using them for verification. A limitation of the approach is that the set of reliable features can only be as big as the relevant vocabulary one can come up with and get training data labelers to consistently label.</p><p>In this paper, we automatically find features that can distinguish between two people, without requiring the features to be describable in words and without requiring workers to label images with the feature. A simple way to find such a feature is to train a linear classifier to distinguish between two people. If the training data includes many images of each person under varied conditions, the projection found by the classifier will be insensitive A reference set of images labeled with parts and identities is used to train a parts detector and a large number of "Tom-vs-Pete" classifiers. Then given a pair of test image, parts are detected and the detected parts are used to perform an "identity-preserving" alignment. The Tom-vs-Pete classifiers are run on the aligned images, with the results passed to a same-or-different classifier to produce a decision.</p><p>to the conditions and consistently correspond to identity. We call classifiers trained in this way "Tom-vs-Pete" classifiers to emphasize that each is trained on just two individuals. We will show that they can be applied to any individual and used for face verification.</p><p>To demonstrate the Tom-vs-Pete classifiers, we collect a "reference set" of face images, labeled by identity and with many images of each subject. We build a library of Tom-vs-Pete classifiers by considering all possible pairs of subjects in the reference set. We then assemble a subset of these classifiers such that, for any pair of subjects, it is highly likely that we have at least a few classifiers able to distinguish them from each other. When presented with a pair of faces (of subjects not in the reference set) for verification, we apply these classifiers to each face and use the classifier outputs as features for a second-stage classifier that makes the "same-or-different" verification decision. Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of the method.</p><p>To allow us to build a large and diverse collection of Tom-vs-Pete classifiers, and to make it more likely that each classifier will generalize beyond the two subjects it is trained on, each classifier looks at just a small portion of the face. These small regions must correspond to each other across images and identities for the classifiers to be effective, so alignment of the faces becomes particularly important. With this in mind, we adopt an alignment procedure, based on the detection of a set of face parts, that enforces a fairly strict correspondence across images. Our alignment procedure also includes a novel use of the reference dataset to distinguish geometry differences due to pose and expression, which should be normalized out by the alignment, from those that pertain to identity (such as thicker lips or a wider nose) and should be preserved. We call this an "identity-preserving alignment."</p><p>We evaluate our method on the Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b15">[16]</ref>, a face verification benchmark using uncontrolled images collected from Yahoo News. We achieve an accuracy of 93.10%, reducing the error rate of the previous state of the art by 26.86%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The field of face recognition and verification has too long a history to survey here, so we focus on previous approaches that are similar to ours. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our method is essentially a hierarchical classifier, where the outputs of the Tom-vs-Pete face classifiers are combined to form features for input to a second stage pair classifier. Wolf et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> also use a two-level classifier, for each test pair training a small number of "one-shot" and "twoshot" classifiers using one or both test images as positive samples and an additional fixed set of reference images as negative samples. Yin et al. <ref type="bibr" target="#b33">[34]</ref> take a similar approach but augment the positive training set with a set of images of subjects similar to the test image, chosen from a reference set independent from the test set. In both cases specialized classifiers must be trained for each test image. In contrast, we train a single set of classifiers as an offline preprocessing step and have no need to train during testing. Kumar et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> also adopt this two-level classifier structure, using a set of attribute (gender, race, hair color, etc.) classifiers in the first stage. This requires a great deal of manual effort in choosing and labeling attributes, while our method automatically generates a very large number of classifiers from data labeled only with identity. The "simile classifiers" method from the same work uses a set of one-vs-all identity classifiers trained on an independent reference dataset for the first stage. This is most similar to our work, but the one-vs-one nature of the Tom-vs-Pete classifiers allow us to produce a much larger set of classifiers, quadratic instead of linear in the number of subjects. The simplicity of the onevs-one concept to be learned also allows us to use fast linear SVMs, while the attribute and simile classifiers use an RBF kernel.</p><p>Other well-performing verification methods that do not follow this precise two-level structure still follow the pattern of first learning how to extract features, then learning the same-vs-different classifier. For example, Pinto and Cox <ref type="bibr" target="#b24">[25]</ref> use a validation set of face pairs to experiment with a large number of features and choose those most effective for verification, then feed these to an SVM for verification of the test data. Nguyen and Bai <ref type="bibr" target="#b22">[23]</ref> split their training data and use one part to learn metrics in the image feature space and the other to learn a verification classifier whose input is the learned distances between the image pairs.</p><p>Our Tom-vs-Pete classifiers themselves are based on features extracted at detected parts of the face such as the corners of the mouth and tip of the nose. Similar parts-based features are demonstrated for face recognition by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34]</ref>. Generation of the Tom-vs-Pete classifiers can also be viewed as a feature selection process where we seek features suitable for face verification. There is an extremely large body of work on feature selection from the machine learning community (for surveys, see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>), but we do not adopt a formal feature selection methodology here.</p><p>It is well known that alignment is critical for good performance in face recognition with uncontrolled images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. One method often applied is Huang et al.'s <ref type="bibr" target="#b14">[15]</ref> "funneling," which extends the congealing method of Learned-Miller <ref type="bibr" target="#b19">[20]</ref> to handle noisy, realworld images. These methods find transformations that minimize differences in images that are initially only roughly aligned. Funneled images provided by the LFW maintainers have been widely used <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>, but the Tom-vs-Pete classifiers require a correspondence more precise than funneling provides. Another common technique is to apply a transformation to the images based on the locations of detected parts such as the corners of the eyes and mouth. This is the approach of Kumar et al. <ref type="bibr" target="#b17">[18]</ref> and Wolf et al. <ref type="bibr" target="#b32">[33]</ref>, who detect a small number of parts with a commercial system and then apply an affine transformation to bring the parts close to fixed locations. The aligned LFW images from <ref type="bibr" target="#b32">[33]</ref> have been released as the LFW-a dataset, which has subsequently been used by many authors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>, but we desire an alignment that gives tighter correspondence between the faces.</p><p>Alignments based on active appearance models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> and 3D models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> are appealing, but have not yet been demonstrated on images captured in the wild and displaying simultaneous variation in pose, lighting, expression, occlusion, and image quality. AAM-based methods perform alignment by creating a triangulation on a set of detected parts and performing a piecewise affine transformation that takes the triangles to their positions in the desired pose. We follow this warping procedure, but detect the parts in a different way, and introduce an adjustment to the part locations to avoid losing identity information in the transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reference Dataset</head><p>The identity-preserving alignment and the Tom-vs-Pete classifiers both rely on a dataset of reference face images labeled with identities and face part locations. We describe this dataset here for clarity of explanation in the following sections.</p><p>The dataset consists of 20,639 images of 120 people. So that we can train on our dataset and evaluate our methods on LFW, none of the people in our dataset are in LFW. The images were collected by searching for the names of public figures on web sites such as Flickr and Google Images. We filtered the resulting images by running a commercial face detector <ref type="bibr" target="#b23">[24]</ref> to discard images without faces and using Amazon Mechanical Turk to discard images that were not of the target person, following the procedure outlined in <ref type="bibr" target="#b17">[18]</ref>. In addition we removed the majority of "near-duplicate" images -images derived from the same original, but with different crops, compression, or other processing -following the method of <ref type="bibr" target="#b26">[27]</ref>, which is based on a simple image similarity measure. Images for 60 of the 120 people are from the "development" part of PubFig <ref type="bibr" target="#b17">[18]</ref> dataset (which was collected as described above), while the remainder are new.</p><p>For all 20,639 images, we have obtained the human-labeled locations of 95 face parts, again using Mechanical Turk. Each point was marked by five labelers, with the mean of the three-label subset having the smallest variance taken as the final location. We divide the parts into two categories: a set of 55 "inner" parts that occur at edges and corners of relatively well-defined points on the face, such as the corners of the eyes and the tip of the nose, and a set of 40 "outer" parts that show the overall shape of the face but are less well-defined and so harder to precisely localize. The part locations are shown on a sample image in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Identity-preserving Alignment</head><p>We have constructed our alignment procedure with three criteria in mind. First, although our classification problem concerns pairs of faces, the Tom-vs-Pete classifiers used in the first stage operate on single faces. To accommodate this, all images must be aligned to a standard pose and expression. A "pairwise" alignment in which the images in each pair are brought into correspondence only with each other, which can produce less distortion than a single "all images" alignment, is not sufficient. We design our alignment to bring all images to a frontal pose with neutral expression.</p><p>Second, for the Tom-vs-Pete classifiers to be effective, the regions on which they are trained must have very good correspondence. This is because each classifier uses only a small part of the face, so the regions will have little or no overlap if the correspondence is not good, and because the linear nature of the classifiers makes it difficult to learn the more complex concepts that would be required to deal with poor alignment. Global similarity or affine transformations, for example, are not ideal because they can bring only two or three points into perfect alignment, respectively.</p><p>Third, we must be careful not to over-align. The perfect alignment procedure for face recognition removes differences due to pose and expression but not those due to identity.</p><p>Our alignment should turn faces to a frontal pose and close open mouths, but it should not warp a prominent jaw to a receding chin.</p><p>The alignment procedure we have designed to satisfy these criteria requires a set of part locations on each face. We use the ninety-five parts defined in the reference dataset. To find them automatically in a test image, we first use the detector of <ref type="bibr" target="#b3">[4]</ref>, which combines the results of an independent detector for each part with global models of the parts' relative positions, to detect the fifty-five inner parts. Then we find the image in the reference dataset whose inner parts, under similarity transformation, are closest in an L 2 sense to the detected inner parts, and "inherit" the outer part positions from that image. The parts detector is trained on a subset of the images in the reference set.</p><p>Each part also has a canonical location, where it occurs in an average, frontal face with neutral expression. To align the image, we adopt the piecewise affine warp often used with parts detected using active appearance models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. We take a Delaunay triangulation of the canonical part positions and the corresponding triangulation on the part positions in the image, then map each triangle in the image to the corresponding canonical triangle by the affine transformation determined by the three vertices. The three correspondences at the vertices of each triangle produce a unique, exact solution for the affine transformation, so all the parts are mapped perfectly to their canonical locations. Provided we have a sufficiently dense set of parts, this ensures the tight correspondence we require.</p><p>This system of alignment produces very tight correspondences and effectively compensates for pose and expression. However the warping is so strict, moving the ninety-five parts to exactly the same locations in every image, that features indicating identity are lost; the third criterion for our alignment is not satisfied. This can be seen in Figure <ref type="figure" target="#fig_2">3</ref> (c), whose images are somewhat anonymized compared with (a), (b), and (d). To understand why this happens, note that since there are parts at both sides of the base of the nose, aligned images of all subjects will have noses of the same width. To avoid this over-alignment, we will perform the alignment based not on the part locations in the image itself, but on "generic" parts -where the parts would be for an average person with the pose and expression in the image. For a wide-nosed person, these points will be not on the edge of the nose but slightly inside, and the above-average width of the nose will be preserved by the piecewise affine warp.</p><p>To find the generic parts, we modify the procedure for locating the parts in a test image as follows. We run the detector of <ref type="bibr" target="#b3">[4]</ref> to get the fifty-five inner part locations as before. Then we find the image with the most similar configuration of parts for each of the 120 subjects in the reference dataset. We include the additional forty outer parts of these images to get a full set of ninety-five parts for each of 120 reference faces. These represent the part locations of 120 different individuals with nearly the same pose and expression as the test image. We take the mean of the 120 sets of part locations to get the generic part locations for the test image. We use these generic part locations in place of the originally detected locations to produce an identity-preserving aligned image with a piecewise affine warp as described above.</p><p>For large yaw angles, we cannot produce a warp to frontal that looks good on the side of the face originally turned away from the camera. To reduce the difficulty this presents to the classifiers, we make a very simple guess at the yaw direction of the face (we use the detected parts to find the shorter eyebrow and assume the subject is facing that direction), then reflect the image if necessary so that all faces are facing the left side of the image. In this way, our classifiers can learn learn to assign more importance to the reliable, right side of the image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tom-vs-Pete Classifiers and Verification</head><p>Each Tom-vs-Pete classifier is a binary classifier trained using a low-level feature on images of two people from the reference dataset. If there are N subjects in the reference set and k low-level features, we can train k • N 2 Tom-vs-Pete classifiers. In our experiments, each low-level feature is a concatenation of SIFT descriptors <ref type="bibr" target="#b20">[21]</ref> extracted at several points and scales in one region of the face. By limiting each classifier to a small region of the face, we hope to learn a concept that will generalize to individuals other than the two people used for training. The regions, shown in Figure <ref type="figure" target="#fig_3">4</ref>, cover the distinctive features inside the face, such as the nose and eyes, as well as the boundary of the face. The classifiers are linear support vector machines trained using the LIBLINEAR package <ref type="bibr" target="#b8">[9]</ref>.</p><p>For each face in a verification pair, we evaluate a set of Tom-vs-Pete classifiers and construct a vector of the signed distances from the decision boundaries. This vector serves as a descriptor of the face. Following the example of <ref type="bibr" target="#b17">[18]</ref>, we then concatenate the absolute difference and element-wise product of the the two face descriptors and pass the result to an RBF SVM to make the same-or-different decision.</p><p>We use 5,000 Tom-vs-Pete classifiers to build the face descriptors. Experiments suggest that additional classifiers beyond this number are of little benefit. With 120 subjects in the reference dataset and 11 low-level features, we can train tens of thousands of classifiers, so we have to choose a subset. There are many reasonable ways to do so, but we design our procedure motivated by the desire for a subset of classifiers that (a) can handle a wide variety of subject pairs and (b) complement each other. To achieve the first, we will choose evenly from classifiers that excel at each reference subject pair. To achieve the second, we will use Adaboost <ref type="bibr" target="#b9">[10]</ref>. We begin by constructing a ranked list of classifiers for each subject pair (S i , S j ), as follows:</p><p>1. Let H i j be the set {h 1 , ..., h n } of Tom-vs-Pete classifiers that are not trained on S i or S j . 2. Consider each h k in H i j as an S i -vs-S j classifier. Do this by fixing the subject with the greater median output of h k as the positive class, then finding the threshold that produces equal false positive and false negative rates. 3. Treating H i j as a set of weak S i -vs-S j classifiers, run the Adaboost algorithm. This  <ref type="figure">5</ref>: (a) A comparison with the best published results on the LFW image-restricted benchmark, including the Associate-predict method <ref type="bibr" target="#b33">[34]</ref>, Brain-inspired features <ref type="bibr" target="#b24">[25]</ref>, and Cosine Similarity Metric Learning (CSML) <ref type="bibr" target="#b22">[23]</ref>, (b) The log scale highlights the performance of our method at the low-false-positive rates desired by many security applications. assigns a weight to each h k . 4. Sort H i j by descending Adaboost weights to get a list of classifiers L i j . An initial subsequence of L i j will be a set of classifiers, not trained on S i or S j , that combine effectively to distinguish S i from S j .</p><p>We construct an overall ordered list of classifiers, L, by taking the first classifier in each of the L i j , then the second in each, then the third, etc. Within each group we randomly order the classifiers, and each classifier is included in L only the first time it occurs. To choose a subset of classifiers of any size, we take an initial subsequence of L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We evaluate our system on Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b15">[16]</ref>, a face verification benchmark using images collected from Yahoo News. The LFW benchmark consists of 6,000 pairs of faces, half of them "same" pairs and half "different," divided into ten folds for crossvalidation. In our method the parts detection, alignment, and Tom-vs-Pete classifiers are based on the reference dataset, so the LFW training folds are used only to train the final same-vs-different classifier. Note that none of the subjects in our reference set appear in LFW, so neither the Tom-vs-Pete classifiers nor the parts detector have seen these individuals in training. We follow the "image-restricted" protocol, in which the training face pairs are marked only with a same or different label and not with the identities of each face (which would allow the generation of additional training pairs).</p><p>We obtain a mean accuracy of 93.10% ± 1.35%. LFW is widely reported on, with accuracies of twenty-five published methods listed on the maintainers' web site <ref type="bibr" target="#b29">[30]</ref> at time of writing. Figure <ref type="figure">5</ref> compares our performance with the top three previously published results. We achieve a 26.86% reduction in the error rate of the previous best results reported by Yin et al. <ref type="bibr" target="#b33">[34]</ref>. Figure <ref type="figure">5</ref> (b) demonstrates our performance at the low false positive rates required by many security applications. At 10 -3 false positive rate we achieve a true positive rate of 55.07%, where the previous best is 40.33% <ref type="bibr" target="#b33">[34]</ref>. Kumar et al. <ref type="bibr" target="#b17">[18]</ref> have made the output of their attribute classifiers on the LFW images available on the LFW web site <ref type="bibr" target="#b29">[30]</ref>. These classifiers are similar to our Tom-vs-Pete classifiers, but are trained on images hand labeled with attributes such as gender and age. Appending the attributes classifier outputs to our vector of Tom-vs-Pete outputs boosts our accuracy to 93.30% ± 1.28%.</p><p>Our method is efficient. Training and selection of the Tom-vs-Pete classifiers is done offline. The eleven low-level features are constructed from SIFT descriptors at a total of just 34 points on the face, at one to three scales each. These features are shared by all of the Tom-vs-Pete classifiers. Evaluation of each Tom-vs-Pete classifier requires evaluation of a single dot product. Finally, the RBF SVM verification classifier must be evaluated on a single feature vector.</p><p>To demonstrate the relative importance of each part of our system, we run the benchmark with several stripped-down variants of the algorithm:</p><p>• Random projection: Replace each Tom-vs-Pete classifier with a random projection of the low-level feature it uses. Shown in Figure <ref type="figure">6</ref>  Figure <ref type="figure">6</ref> includes ROC curves from these experiments and from the full system, showing that each part of the method contributes to the high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a method of face verification that takes advantage of a reference set of labeled images in two novel ways. First, our identity-preserving alignment warps face images in a way that corrects for pose and expression but preserves geometry distinctive to the subject, based on face parts in similar configurations in the reference set. Second, the verification decision itself is based on the output from our fast, linear Tom-vs-Pete classifiers, which are trained to distinguish anyone from anyone by considering every possible pair of subjects in the reference set. The method is efficient and accurate. We evaluate on the standard Labeled Faces in the Wild image-restricted benchmark, on which it achieves state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An overview of the verification system. A reference set of images labeled with parts and identities is used to train a parts detector and a large number of "Tom-vs-Pete" classifiers. Then given a pair of test image, parts are detected and the detected parts are used to perform an "identity-preserving" alignment. The Tom-vs-Pete classifiers are run on the aligned images, with the results passed to a same-or-different classifier to produce a decision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Labeled face parts. (a) There are fifty-five "inner" points at well-defined landmarks and (b) forty "outer" points that are less well-defined but give the general shape of the face. (c) The triangulation of the parts used to perform a piecewise affine warp. (Additional fixed points are placed well outside the face to ensure a rectangular warped image.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Warping images to frontal. (a) Original images. (b) Aligning by an affine transformation based on the locations of the eyes, tip of the nose, and corners of the mouth does not achieve tight correspondence between the images. (c) Warping to put all 95 parts at their canonical positions gives tight correspondence, but de-identifies the face by altering its shape. (d) Warping based on genericized part locations gives tight correspondence without obscuring identity. In all methods, we ensure that the side of the face presented to the camera is on the right side of the image by performing a left-right reflection when necessary. This restricts the worst distortions to the left side of the image (shown with a gray wash here), which the classifiers can learn to weight less important than the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The top left image is produced by the alignment procedure. Each of the remaining images shows the region from which one low-level feature is extracted. SIFT descriptors are extracted from each square and concatenated. Concentric squares indicate SIFT descriptors at the same point but different scales.</figDesc><graphic coords="7,26.13,29.58,364.72,142.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>warp with... Tom-vs-Pete classifiers (93.10%) Random projection (88.05%) Low-level features only (85.33%) -Pete classifiers using... Identity-preserving warp (93.10%) Non-generic warp (91.20%) Affine alignment (90.47%) (a) (b) Figure 6: LFW benchmark results. (a) The contribution of the Tom-vs-Pete classifiers, compared with random projection or direct use of the low-level features. (b) The contribution of the alignment method, compared with a piecewise affine warp using non-generic part locations or a global affine transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a). • Low-level features only: Discarding the Tom-vs-Pete classifiers, concatenate the lowlevel features to produce a descriptor of each face, and use the absolute difference of these descriptors as the feature vector for the same-or-different classifier. Shown in Figure 6 (a). • Non-generic warp: Train and use Tom-vs-Pete classifiers, but use the detected part locations directly in a piecewise affine warp, rather than the genericized locations that produce the identity-preserving warp. Shown in Figure 6 (b). • Affine alignment: Train and use Tom-vs-Pete classifiers, but align all images with global affine transformations based on the detected locations of the eyes and mouth instead of our identity-preserving warp. Shown in Figure 6 (b).</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by NSF award 1116631 and ONR award N00014-08-1-0638.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A face recognition system based on automatically determined facial fiducial points</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose normalization via learned 2d warping for fully automatic face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully automatic pose-invariant face recognition via 3d pose normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View-based active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the IEEE Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpreting face images using active appearnce models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the IEEE Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hello! My name is... Buffy&quot; -automatic naming of characters in TV video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A generative shape regularization model for robust face alignment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large scale strongly supervised ensemble metric learning, with applications to face verification and retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>NEC</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LFW Results Using a Combined Nowak Plus MERL Recognizer</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Faces in Real Life Images workshop at ECCV</title>
		<meeting>the Faces in Real Life Images workshop at ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Describable visual attributes for face verification and image search</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data driven image models through continuous joint alignment</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m">Computational Methods of Feature Selection</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>Chapman and Hall / CRC</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cosine similarity metric learning for face verification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OKAO vision</title>
		<author>
			<persName><surname>Omron</surname></persName>
		</author>
		<ptr target="http://www.omron.com/r_d/coretech/vision/okao.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond Simple Features: A Large-Scale Feature Search Approach to Unconstrained Face Recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the IEEE Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How far can you get with a modern face recognition test set using only simple features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scaling-up biologically-inspired computer vision: A case-study on facebook</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Biologically Consistent Vision at the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Workshop on Biologically Consistent Vision at the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face verification using the lark representation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple one-shots for utilizing class label information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="http://vis-www.cs.umass.edu/lfw/" />
		<title level="m">LFW web site</title>
		<imprint/>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving face recognition by online image alignment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Descriptor based methods in the wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Faces in Real Life Images workshop at ECCV</title>
		<meeting>the Faces in Real Life Images workshop at ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Similarity scores based on background samples</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An associate-predict model for face recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distance metric learning with eigenvalue optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
