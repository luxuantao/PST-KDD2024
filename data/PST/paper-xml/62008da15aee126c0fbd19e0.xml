<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-Coupled Oscillator Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-04">4 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">Konstantin</forename><surname>Rusch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Seminar for Applied Mathematics (SAM)</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<addrLine>D-MATH</addrLine>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Twitter Inc</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Twitter Inc</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Seminar for Applied Mathematics (SAM)</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<addrLine>D-MATH</addrLine>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">ETH AI Center</orgName>
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Twitter Inc</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-Coupled Oscillator Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-04">4 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.02296v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Graph-Coupled Oscillator Networks (GraphCON), a novel framework for deep learning on graphs. It is based on discretizations of a second-order system of ordinary differential equations (ODEs), which model a network of nonlinear forced and damped oscillators, coupled via the adjacency structure of the underlying graph. The flexibility of our framework permits any basic GNN layer (e.g. convolutional or attentional) as the coupling function, from which a multi-layer deep neural network is built up via the dynamics of the proposed ODEs. We relate the oversmoothing problem, commonly encountered in GNNs, to the stability of steady states of the underlying ODE and show that zero-Dirichlet energy steady states are not stable for our proposed ODEs. This demonstrates that the proposed framework mitigates the oversmoothing problem. Finally, we show that our approach offers competitive performance with respect to the state-of-the-art on a variety of graph-based learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b52">Sperduti (1994)</ref>; <ref type="bibr" target="#b25">Goller &amp; Kuchler (1996)</ref>; <ref type="bibr" target="#b53">Sperduti &amp; Starita (1997)</ref>; <ref type="bibr" target="#b22">Frasconi et al. (1998)</ref>; <ref type="bibr" target="#b26">Gori et al. (2005)</ref>; <ref type="bibr" target="#b47">Scarselli et al. (2008)</ref>; <ref type="bibr" target="#b6">Bruna et al. (2014)</ref>; <ref type="bibr" target="#b16">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b31">Kipf &amp; Welling (2017)</ref>; <ref type="bibr" target="#b36">Monti et al. (2017)</ref>; <ref type="bibr" target="#b24">Gilmer et al. (2017)</ref> are a widely-used class of models for learning on relations and interaction data. These models have recently been successfully applied in a variety of tasks such as computer vision and graphics <ref type="bibr" target="#b36">Monti et al. (2017)</ref>, recommender systems <ref type="bibr" target="#b63">Ying et al. (2018)</ref>, transportation <ref type="bibr" target="#b17">Derrow-Pinion et al. (2021)</ref>, computational chemistry <ref type="bibr" target="#b24">(Gilmer et al., 2017)</ref>, drug discovery <ref type="bibr">Gaudelet et al. (2021)</ref>, physics <ref type="bibr" target="#b50">(Shlomi et al., 2020)</ref>, and analysis of social networks (see <ref type="bibr" target="#b64">Zhou et al. (2019)</ref>; <ref type="bibr" target="#b5">Bronstein et al. (2021)</ref> for additional applications).</p><p>Several recent works proposed Graph ML models based on differential equations coming from physics <ref type="bibr" target="#b1">Avelar et al. (2019)</ref>; <ref type="bibr" target="#b43">Poli et al. (2019b)</ref>; <ref type="bibr" target="#b65">Zhuang et al. (2020)</ref>; <ref type="bibr" target="#b60">Xhonneux et al. (2020b)</ref>, including diffusion <ref type="bibr" target="#b9">Chamberlain et al. (2021b)</ref> and wave <ref type="bibr" target="#b19">Eliasof et al. (2021)</ref> equations and geometric equations such as Beltrami <ref type="bibr" target="#b8">Chamberlain et al. (2021a)</ref> and Ricci <ref type="bibr" target="#b55">Topping et al. (2021)</ref> flows. Such approaches allow not only to recover popular GNN models as discretization schemes for the underling differential equations, but also, in some cases, can address problems encountered in traditional GNNs such as oversmoothing <ref type="bibr" target="#b38">Nt &amp; Maehara (2019)</ref>; <ref type="bibr" target="#b39">Oono &amp; Suzuki (2020)</ref> and bottlenecks <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref>.</p><p>In this paper, we propose a novel physically-inspired approach to learning on graphs. Our framework, termed GraphCON (Graph-Coupled Oscillator Network) builds upon suitable time-discretizations of a specific class of ordinary differential equations (ODEs) that model the dyanmics of a network of non-linear forced and damped oscillators, which are coupled via the adjacency structure of the underlying graph. Graph-coupled oscillators are often encountered in mechanical, electronic, and biological systems, and have been studied extensively Strogatz (2015), with a prominent example being functional circuits in the brain such as cortical columns <ref type="bibr" target="#b54">Stiefel &amp; Ermentrout (2016)</ref>. In these circuits, each neuron oscillates with periodic firing and spiking of the action potential. The network of neurons is coupled in the form of a graph, with neurons representing nodes and edges corresponding to synapses linking neurons.</p><p>Main Contributions. In the subsequent sections, we will demonstrate the following features of Graph-CON:</p><p>• GraphCON is flexible enough to accommodate any standard GNN layer (such as GAT or GCN) as its coupling function. As timesteps of our discretized ODE can be interpreted as layers of a deep neural network <ref type="bibr" target="#b12">Chen et al. (2018)</ref>; <ref type="bibr" target="#b27">Haber &amp; Ruthotto (2018)</ref>; <ref type="bibr" target="#b9">Chamberlain et al. (2021b)</ref>, one can view GraphCON as a wrapper around any underlying basic GNN layer allowing to build deep GNNs. Moreover, we will show that standard GNNs can be recovered as steady states of the underlying class of ODEs, whereas GraphCON utilizes their dynamic behavior to sample a richer set of states, which could lead to better expressive power.</p><p>• We mathematically formulate the frequently encountered oversmoothing problem for GNNs <ref type="bibr" target="#b38">Nt &amp; Maehara (2019)</ref>; <ref type="bibr" target="#b39">Oono &amp; Suzuki (2020)</ref> in terms of the stability of zero-Dirichlet energy steady states of the underlying equations. By a careful analysis of the dynamics of the proposed ODEs, we demonstrate that any zero-Dirichlet energy steady states are not (exponentially) stable. Consequently, we show that the oversmoothing problem for GraphCON is mitigated by construction.</p><p>• We provide an extensive empirical evaluation of GraphCON on a wide variety of graph learning tasks such as transductive and inductive node classification and graph regression and classification, demonstrating that GraphCON achieves competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GraphCON</head><p>Let G = (V, E ⊆ V) be an undirected graph with |V| = v nodes and |E| = e edges consisting of unordered pairs of nodes {i, j} and denoted i ∼ j. We will label nodes by the index i ∈ V = {1, 2, . . . , v}. For any i ∈ V, we denote its 1-neighborhood as N i = {j ∈ V : i ∼ j}. Furthermore, let X ∈ R v×m be given by X = {X i } for i ∈ V, denoting the m-dimensional feature vector at each node i.</p><p>Central to our framework is a graph dynamical system represented by the following nonlinear system of ODEs:</p><formula xml:id="formula_0">X = σ(F θ (X, t) + XW + b) − γX − αX .<label>(1)</label></formula><p>Here, X(t) denotes the time-dependent v × m-matrix of node features, σ is the activation function, F θ is a general learnable (possibly time-dependent) 1-neighborhood coupling function of the form</p><formula xml:id="formula_1">(F θ (X, t)) i = F θ (X i (t), X j (t), t) ∀i ∼ j,<label>(2)</label></formula><p>parametrized with a set of learnable parameters θ. W ∈ R m×m , b ∈ R m are learnable weights and biases that act as a residual connection. By introducing the auxiliary velocity variable Y(t) = X (t) ∈ R v×m , we can rewrite the second-order ODEs (1) as a first-order system:</p><formula xml:id="formula_2">Y = σ(F θ (X, t) + XW + b) − γX − αY, X = Y.</formula><p>(3)</p><p>The key idea of our framework is, given the input node features X(0) as an initial condition, to use the solution X(T ) at some time T as the output (more generally, one can also apply (linear) transformations (embeddings) to X(0) and X(T )). As will be shown in the following section, the space of solutions of our system is a rich class of functions that can solve many learning tasks on a graph.</p><p>The system (3) must be solved by an iterative numerical solver using a suitable time-discretization. It is highly desirable for a time-discretization to preserve the structure of the underlying ODEs (3) <ref type="bibr" target="#b28">Hairer et al. (1987)</ref>. In this paper, we use the following IMEX (implicit-explicit) time-stepping scheme, which extends the symplectic Euler method <ref type="bibr" target="#b28">Hairer et al. (1987)</ref> to systems with an additional damping term,</p><formula xml:id="formula_3">Y n = Y n−1 + ∆t[σ(F θ (X n−1 , t n−1 ) + X n−1 W + b) − γX n−1 − αY n−1 ], X n = X n−1 + ∆tY n ,<label>(4)</label></formula><p>for n = 1, . . . , N , where ∆t &gt; 0 is a fixed time-step and Y n , X n denote the hidden node features at time t n = n∆t. The iterative scheme (4) can be interpreted as an N -layer graph neural network (with potential additional linear input and readout layers, omitted here for simplicity), which we refer to as GraphCON (see section 3 for the motivation of this nomenclature). The coupling function F θ plays the role of a message passing mechanism <ref type="bibr" target="#b24">(Gilmer et al. (2017)</ref>, also referred to, in various contexts, as 'diffusion' or 'neighborhood aggregation') in traditional GNNs.</p><p>Choice of the coupling function F θ . Our framework allows for any learnable 1-neighborhood coupling to be used as F θ , including instances of message passing mechanisms commonly used in the Graph ML literature such as GraphSAGE <ref type="bibr" target="#b29">(Hamilton et al., 2017)</ref>, Graph Attention <ref type="bibr" target="#b57">Velickovic et al. (2018</ref><ref type="bibr">), Graph Convolution Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b31">Kipf &amp; Welling (2017)</ref>, SplineCNN <ref type="bibr" target="#b20">(Fey et al., 2018)</ref>, or MoNet <ref type="bibr" target="#b36">(Monti et al., 2017)</ref>). In this paper, we focus on two particularly popular choices: Attentional message passing of <ref type="bibr" target="#b57">Velickovic et al. (2018)</ref>:</p><formula xml:id="formula_4">F θ (X n , t n ) = A n (X n )X n W n ,</formula><p>with learnable weight matrices W n ∈ R m×m and attention matrices A n ∈ R n×n following the adjacency structure of the graph G, i.e., (</p><formula xml:id="formula_5">A n (X n )) ij = 0 if j / ∈ N i and (A n (X n )) ij = exp(LeakyReLU(a [W n X n i ||W n X n j ])) k∈Ni exp(LeakyReLU(a [W n X n i ||W n X n k ]))</formula><p>, otherwise (here X n i denotes the i-th row of X n and a ∈ R 2m ). We refer to (4) based on this attentional 1-neighborhood coupling as GraphCON-GAT. Graph convolution operator of <ref type="bibr" target="#b31">Kipf &amp; Welling (2017)</ref>:</p><formula xml:id="formula_6">F θ (X n , t n ) = D− 1 2 Â D− 1 2 X n W n ,<label>(5)</label></formula><p>with Â = A + I denoting the adjacency matrix of G with inserted self-loops, diagonal degree matrix D = diag( n l=1 Âkl ), and W n i ∈ R m×m being learnable weight matrices. We refer to (4) based on this convolutional 1-neighborhood coupling as GraphCON-GCN.</p><p>Steady States of GraphCON and relation to GNNs. It is straightforward to see that the steady states X * , Y * of the GraphCON dynamical system (4) with an autonomous coupling function F θ = F θ (X) (as in GraphCON-GAT or GraphCON-GCN) are given by Y * ≡ 0 and</p><formula xml:id="formula_7">X * = ∆t γ σ(F θ (X * )).<label>(6)</label></formula><p>Using a simple fixed point iteration to find the steady states (6) yields a multi-layer GNN of the form;</p><formula xml:id="formula_8">X n = ∆t γ σ(F θ (X n−1 )), for n = 1, 2, . . . , N.<label>(7)</label></formula><p>We observe that (up to a rescaling by the factor ∆t/γ) equation ( <ref type="formula" target="#formula_8">7</ref>) corresponds to the update formula for any standard N -layer message-passing <ref type="bibr">GNN Gilmer et al. (2017)</ref>, including such popular variants as <ref type="bibr">GAT Velickovic et al. (2018)</ref> or <ref type="bibr">GCN Kipf &amp; Welling (2017)</ref>. Thus, this interpretation of GraphCON (4) clearly brings out its relationship with standard GNNs. Unlike in standard multi-layer GNNs of the generic form (7) that can be thought of as steady states of the underlying ODEs (3), GraphCON evolves the underlying node features dynamically in time. Interpreting the multiple GNN layers as iterations at times t n = n∆t in (4), we observe that the node features in GraphCON follow the trajectories of the corresponding dynamical system and can explore a richer sampling of the underlying latent feature space, leading to possibly greater expressive power than standard GNNs (7), which might remain in the vicinity of steady states.</p><p>Moreover, this interpretation also reveals that, in principle, any GNN of the form (7) can be used within the GraphCON framework, offering a very flexible and broad class of architectures. Hence, one can think of GraphCON as an additional wrapper on top of any basic GNN layer allowing for a principled and stable design of deep multi-layered GNNs. In the following Section 3, we show that such an approach has several key advantages over standard GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Properties of GraphCON</head><p>To gain some insight into the functioning of GraphCON (4), we start by assuming no residual connections (i.e., W = 0, b = 0), setting the hyperparameter γ = 1 and assuming that the 1-neighborhood coupling F θ is given by either the GAT or GCN type coupling functions. In this case, the underlying ODEs (3) takes the following node-wise form,</p><formula xml:id="formula_9">X i = Y i , Y i = σ   j∈Ni A ij X j   − X i − αY i ,<label>(8)</label></formula><p>for all nodes i ∈ V, with A ij = A (X i (t), X j (t)) ∈ R stemming from the attention or convolution operators. Furthermore, the matrices are right stochastic i.e., the entries satisfy,</p><formula xml:id="formula_10">0 ≤ A ij ≤ 1, ∀j ∈ N i , ∀i ∈ V, j∈Ni A ij = 1, ∀i ∈ V. (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>Uncoupled case. The simplest case of (8), corresponds to setting σ ≡ 0 and α = 0. In this case, all nodes are uncoupled from each other and the solutions of the resulting ODEs are of the form,</p><formula xml:id="formula_12">X i (t) = X i (0) cos(t) + Y i (0) sin(t). (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Thus, the dynamics of the ODEs (3) in this special case correspond to a system of uncoupled oscillators, with each node oscillating at unit frequency.</p><p>Coupled linear case. Next, we introduce coupling between the nodes that are adjacent on the underlying graph G and assume identity activation function σ(x) = x. In this case, ( <ref type="formula" target="#formula_9">8</ref>) is a coupled linear system and an exact closed form solution, such as (10) may not be possible. However, we can describe the dynamics of (8) in the form of the following proposition (proved in SM C.1), Proposition 3.1. Let the node features X, Y evolve according to the ODEs (8) with activation function σ = id and time-independent matrix A (e.g. A ij = A(X i (0), X j (0)) using the initial features). Further assume that A is symmetric and α = 0. Then</p><formula xml:id="formula_14">i∈V Y i (t) 2 + i∈V j∈Ni A ij X i (t) − X j (t) 2 = i∈V Y i (0) 2 + i∈V j∈Ni A ij X i (0) − X j (0) 2 ,<label>(11)</label></formula><p>holds for all t &gt; 0.</p><p>Thus, in this case, we have shown that the dynamics of the underlying ODEs (8) preserves the energy,</p><formula xml:id="formula_15">E (t) := i∈V Y i (t) 2 + i∈V j∈Ni A ij X i (t) − X j (t) 2 , (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>and the trajectories of ( <ref type="formula" target="#formula_9">8</ref>) are constrained to lie on a manifold of the node feature space, defined by the level sets of the energy. In particular, energy ( <ref type="formula" target="#formula_15">12</ref>) is not produced or destroyed but simply redistributed among the nodes of the underlying graph G. Thus, the dynamics of (3) in this setting amounts to the motion of a linear system of coupled oscillators.</p><p>General nonlinear case. In the general case, we have (i) a nonlinear activation function σ; (ii) time-dependent non-linear coefficients A ij = A(X i (t), X j (t)); and (iii) possible unsymmetrical entries A ij = A ji . All these factors destroy the energy conservation property ( <ref type="formula" target="#formula_14">11</ref>) and can possibly lead to unbounded growth of the energy. Hence, we need to add some damping to the system. To this end, the damping term in ( <ref type="formula" target="#formula_9">8</ref>) is activated by setting α &gt; 0. Moreover, the residual terms corresponding to non-zero weights W and biases b in (3) can be interpreted as forcing or impulse terms. Similarly, γ = 1 corresponds to controlling frequencies of the nodes. Thus, the overall dynamics of the underlying ODEs (3) amounts to the motion of a nonlinear system of coupled, controlled, damped, and forced oscillators with the coupling structure being that of the underlying graph. This explains our choice of the name, Graph-Coupled Oscillatory Neural Network or 'GraphCON' for short.</p><p>We illustrate the dynamics of GraphCON in Fig. <ref type="figure" target="#fig_0">1</ref>, where the model is applied to the graph of a molecule from the ZINC database <ref type="bibr" target="#b30">Irwin et al. (2012)</ref>, with features X denoting the position of the nodes and they are propagated in time through the action of GraphCON (4). The oscillatory behavior of the node features, as well as their dependence on the adjacency structure of the underlying graph can be clearly observed in this figure.  <ref type="formula" target="#formula_3">4</ref>)) are represented by the 2-dimensional positions of the nodes, while the initial velocities (Y 0 in (4)) are set to the initial positions. The positions are propagated forward in time ('layers') using GraphCON-GCN with random weights. The molecular graph is plotted at initial time t = 0 as well as at t = 20.</p><p>Oversmoothing and GraphCON. One of the common plights of GNN models such as <ref type="bibr">GAT Velickovic et al. (2018)</ref>, <ref type="bibr">GCN Kipf &amp; Welling (2017)</ref> and their variants is oversmoothing Nt &amp; Maehara (2019); <ref type="bibr" target="#b39">Oono &amp; Suzuki (2020)</ref>, a phenomenon where all node features in a deep GNN converge to the same constant value as the number of hidden layers is increased. Consequently, one often must resort to shallow GNNs at the expense of expressive power <ref type="bibr" target="#b38">Nt &amp; Maehara (2019)</ref>; <ref type="bibr" target="#b39">Oono &amp; Suzuki (2020)</ref>. Many attempts have been made in recent years to mitigate the oversmoothing problem for GNNs, including regularization procedures such as DropEdge <ref type="bibr" target="#b44">Rong et al. (2020)</ref>, using intermediate representations <ref type="bibr" target="#b62">Xu et al. (2018b)</ref>, or adding residual connections <ref type="bibr" target="#b11">Chen et al. (2020)</ref>.</p><p>We will show that GraphCON allows to mitigate this problem by construction, and set off by formulating this problem in precise mathematical terms and to this end, we recall the Dirichlet energy, defined on the node features X of an undirected graph G as,</p><formula xml:id="formula_17">E(X) = 1 v i∈V j∈Ni X i − X j 2 . (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Next, we define oversmoothing as follows:</p><p>Definition 3.2. Let X n denote the hidden features of the nth layer of an N -layer GNN, with n = 0, . . . , N . We define oversmoothing as the exponential convergence to zero of the layer-wise Dirichlet energy as a function of n, i.e.,</p><formula xml:id="formula_19">E(X n ) ≤ C 1 e −C2n ,<label>(14)</label></formula><p>with some constants C 1 , C 2 &gt; 0.</p><p>In other words, oversmoothing happens when the graph gradients vanish quickly (see for instance the illustration in Fig. <ref type="figure">2</ref>) in the number of hidden layers of the GNN. As a result, the feature vectors across all nodes rapidly (exponentially) converge to the same constant value. This behavior is commonly observed in GNNs and is identified as one of the reasons for the difficulty in designing deep GNNs.</p><p>GraphCON behaves rather differently and allows to mitigate the oversmoothing problem in the sense of definition 3.2. To see this, we focus on the underlying ODEs (3). It is trivial to extend the definition of oversmoothing from the discrete case to the continuous one by requiring that oversmoothing happens for the ODEs (3) if the Dirichlet energy behaves as,</p><formula xml:id="formula_20">E(X(t)) ≤ C 1 e −C2t , ∀t &gt; 0, (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>for some C 1,2 &gt; 0.</p><p>We have the following simple proposition (proved in SM C.2) that characterizes the oversmoothing problem for the underlying ODEs in the standard terminology of dynamical systems <ref type="bibr" target="#b58">Wiggins (2003)</ref>, Proposition 3.3. The oversmoothing problem occurs for the ODEs (3) if and only if the hidden states (X * , Y * ) = (c, 0) are exponentially stable steady states (fixed points) of the ODE (3), for some c ∈ R m and 0 being the m-dimensional vector with zeroes for all its entries.</p><p>In other words, all the trajectories of the ODE (3), that start within the corresponding basin of attraction, have to converge exponentially fast in time (satisfy ( <ref type="formula" target="#formula_20">15</ref>)) to the corresponding steady state (c, 0) for the oversmoothing problem to occur for this system. Note that the basins of attraction will be different for different values of c.</p><p>Given this characterization, the key questions are a) whether (c, 0) are fixed points for the ODE (3), and b) whether these fixed points are exponentially stable. We answer these questions for the ODEs (8) in the following Proposition 3.4. Assume that the activation function σ in the ODEs (8) is ReLU. Then, for any c ∈ R m such that each entry of the vector c ≥ 0, for all 1 ≤ ≤ m, the hidden state (c, 0) is a steady state for the ODEs (8). However under the additional assumption of α ≥ 1 2 , this fixed point is not exponentially stable. The fact that (c, 0) is a steady state of (8), for any positive c is straightforward to see from the structure of (8) and the definition of the ReLU activation function. We can already observe from the energy identity (11) for the simplified symmetric linear system that the energy (12) for the small perturbations around the steady state (c, 0) is conserved in time. Hence, these small perturbations do not decay at all, let alone, exponentially fast in time. Thus, these steady states are not exponentially stable.</p><p>An extension of this analysis to the nonlinear time-dependent, possibly non-symmetric system (8) is more subtle and the proof relies on the identity (21) (expressed in Proposition C.1 in SM C.3) that describes how a suitably defined energy of the general system (8) evolves around small perturbations of the steady state (c, 0). A careful analysis of this identity reveals that these small perturbations can grow polynomially in time (at least for short time periods) and do not decay exponentially. Consequently, the fixed point (c, 0) is not stable. This shows that the oversmoothing problem, in the sense of definition 3.2, is mitigated for the ODEs (3) and structure preserving time-discretizations of it such as (4), from which, in simple words it follows that GraphCON mitigates oversmoothing by construction.</p><p>This analysis also illustrates the rich dynamics of (3) as we show that even if the trajectories reach a steady state of the form (c, 0), very small perturbations will grow and the trajectory will veer away from this steady state, possibly towards other constant steady states which are also not stable. Thus, the trajectories can sample large parts of the latent space, contributing to the expressive power of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Differential equations have historically played a role in designing and interpreting various algorithms in machine learning, including non-linear dimensionality reduction methods <ref type="bibr" target="#b3">Belkin &amp; Niyogi (2003)</ref>; <ref type="bibr" target="#b14">Coifman &amp; Lafon (2006)</ref>  Chamberlain et al. (2021b) used parabolic diffusion-type PDEs to design GNNs using graph gradient and divergence operators as the spatial differential operator, a transformer type-attention as a learnable diffusivity function ('1-neighborhood coupling' in our terminology), and a variety of time stepping schemes to discretize the temporal dimension in this framework. <ref type="bibr" target="#b8">Chamberlain et al. (2021a)</ref> applied a non-euclidean diffusion equation ('Beltrami flow') to a joint positional-feature space, yielding a scheme with adaptive spatial derivatives ('graph rewiring'), and <ref type="bibr" target="#b55">Topping et al. (2021)</ref> studied a discrete geometric PDE similar to Ricci flow to improve information propagation in GNNs. We can see the contrast between the diffusionbased methods of <ref type="bibr">Chamberlain et al. (2021b,a)</ref> and GraphCON in the simple case of identity activation σ(x) = x and no residual connection (W = 0 and b = 0). Then, under the further assumption that the second-order time derivative X is removed from (1) and α = γ = 1, we recover the graph diffusion-PDEs of <ref type="bibr" target="#b9">Chamberlain et al. (2021b)</ref>. Hence, the presence of the temporal second-order derivative distinguishes this approach from diffusion-based PDEs. <ref type="bibr" target="#b19">Eliasof et al. (2021)</ref> proposed a GNN framework arising from a mixture of parabolic (diffusion) and hyperbolic (wave) PDEs on graphs with convolutional coupling operators, which describe dissipative wave propagation. We point out that a particular instance of their model (damped wave equation, also called as the Telegrapher's equation) can be obtained as a special case of our model (1) with the identity activation function and no residual connection. This is not surprising as the zero grid-size limit of oscillators on a regular grid yields a wave equation. However, given that we use a nonlinear activation function and the specific placement of the activation layer in (3), a local PDE interpretation of the general form of our underlying ODEs (1) does not appear to be feasible.</p><p>Finally, the explicit use of networks of coupled, controlled oscillators to design machine learning models was proposed in context of recurrent neural networks (RNNs) by <ref type="bibr">Rusch &amp; Mishra (2021a,b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We present a detailed experimental evaluation of the proposed framework on a variety of graph learning tasks. We test two settings of GraphCON: GraphCON-GCN (using graph convolution as the 1-neighborhood coupling in (4)) and GraphCON-GAT (using the attentional coupling). Since in most experiments, these two configurations already outperform the state-of-the-art (SOTA), we only apply GraphCON with a more involved coupling functions if neither GraphCON-GCN nor GraphCON-GAT outperforms the current SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evolution of Dirichlet Energy.</head><p>We start by illustrating the dynamics of the Dirichlet energy (13) of GraphCON for an undirected graph representing a 2-dimensional 10 × 10 regular grid with 4-neighbor connectivity. The node features X are randomly sampled from U([0, 1]) and then propagated through 100-layer GNNs (with random weights): GAT, GCN, and their GraphCON-stacked versions (GraphCON-GAT and GraphCON-GCN) for two different values of the damping parameter α = 0, 0.5 in (4) and with fixed γ = 1. In Fig. <ref type="figure">2</ref>, we plot the (logarithm of) Dirichlet energy of each layer's output with respect to (logarithm) of the layer number. It can clearly be seen that GAT and GCN suffer from the oversmoothing problem as the Dirichlet energy converges exponentially fast to zero, indicating that the node features become constant, while GraphCON is devoid of this behavior. This holds true even for non-zero value of the damping parameter α, where the Dirichlet energy stabilizes after an initial decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transductive node classification</head><p>We evaluate GraphCON on both homophilic and heterophilic datasets, where high homophily implies that the features in a node are similar to those of its neighbors. The homophily level reported in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> is the measure proposed by <ref type="bibr" target="#b41">Pei et al. (2020)</ref>.  </p><formula xml:id="formula_22">−3 E(X n ) GAT GCN GraphCON-GAT (α = 0) GraphCON-GAT (α = 0.5) GraphCON-GCN (α = 0) GraphCON-GCN (α = 0.5)</formula><p>Figure <ref type="figure">2</ref>: Dirichlet energy E(X n ) of layer-wise node features X n propagated through a GAT and GCN as well as their GraphCON-stacked versions (GraphCon-GAT and GraphCON-GCN) for two different values of α = 0, 0.5 in (4) and fixed γ = 1.</p><p>Homophilic datasets. We consider three widely used node classification tasks, based on the citation networks Cora <ref type="bibr" target="#b35">(McCallum et al., 2000)</ref>, Citeseer <ref type="bibr" target="#b48">(Sen et al., 2008)</ref> and Pubmed <ref type="bibr" target="#b37">(Namata et al., 2012)</ref>. We follow the evaluation protocols and training, validation, and test splits of <ref type="bibr" target="#b49">Shchur et al. (2018)</ref>; <ref type="bibr" target="#b9">Chamberlain et al. (2021b)</ref>, using only on the largest connected component in each network.</p><p>Table <ref type="table" target="#tab_0">1</ref> compares GraphCON with standard GNN baselines: GCN <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b57">(Velickovic et al., 2018)</ref>, MoNet <ref type="bibr" target="#b36">(Monti et al., 2017)</ref>, GraphSAGE (GS) <ref type="bibr" target="#b29">(Hamilton et al., 2017)</ref>, CGNN <ref type="bibr" target="#b59">(Xhonneux et al., 2020a)</ref>, GDE <ref type="bibr" target="#b42">(Poli et al., 2019a)</ref>, and GRAND <ref type="bibr" target="#b9">Chamberlain et al. (2021b)</ref>. We observe that GraphCON-GCN and GraphCON-GAT outperform pure GCN and GAT consistently. We also provide results for GraphCON based on the propagation layer used in GRAND i.e., transformer <ref type="bibr" target="#b56">Vaswani et al. (2017)</ref> based graph attention, referred to as GraphCON-Tran, which also outperforms the basic underlying model. Overall, GraphCON models show the best performance on all these datasets. Heterophilic datasets. We also evaluate GraphCON on the heterophilic graphs; Cornell, Texas and Wisconsin from the WebKB dataset<ref type="foot" target="#foot_0">1</ref> . Here, the assumption on neighbor feature similarity does not hold. Many GNN models were shown to struggle in this settings as can be seen by the poor performance of baseline GCN and GAT in Table <ref type="table" target="#tab_1">2</ref>. On the other hand, we see from Table <ref type="table" target="#tab_1">2</ref> that not only do GraphCON-GCN and GraphCON-GAT dramatically outperform the underlying GCN and GAT models (e.g. for the most heterophilic Texas graph, GraphCON-GCN and GraphCON-GAT have mean accuracies of 85.4% and 82.2%, compared to accuracies of 55.1% and 52.2% for GCN and GAT), the GraphCON models also provide the best performance, outperforming recent baselines that are specifically designed for heterophilic graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Inductive node classification</head><p>In this experiment, we consider the Protein-Protein-Interaction (PPI) dataset of <ref type="bibr" target="#b66">Zitnik &amp; Leskovec (2017)</ref>, using the protocol of <ref type="bibr" target="#b29">Hamilton et al. (2017)</ref>. Table <ref type="table" target="#tab_2">3</ref> shows the test performance (micro-average F1) of GraphCON and several standard GNN baselines. We can see that GraphCON significantly improves the performance of the underling models (GAT from 97.4% to 99.4% and GCN from 98.5% to 99.6%, which is the top result on this benchmark).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Molecular graph property regression</head><p>We reproduce the benchmark proposed in <ref type="bibr" target="#b18">Dwivedi et al. (2020)</ref>, regressing the constrained solubulity of 12K molecular graphs from the ZINC dataset <ref type="bibr" target="#b30">(Irwin et al., 2012)</ref>. We follow verbatim the settings of <ref type="bibr" target="#b18">Dwivedi et al. (2020)</ref>; <ref type="bibr" target="#b2">Beani et al. (2021)</ref>: make no use of edge features and constrain the network sizes to ∼100K parameters. Table <ref type="table">4</ref> summarizes the performance of GraphCON and standard GNN baselines. Both GraphCON-GAT and GraphCON-GCN outperform GAT and GCN respectively, by a factor of 2. Moreover, the performance of GraphCON-GCN is on par with the recent state-of-the-art method DGN <ref type="bibr" target="#b2">(Beani et al., 2021)</ref> with significantly lower standard deviation. Given these results, it is instructive to ask why GraphCON models outperform their underlying base GNN models such as GCN. A part of the answer can be seen from SM Table <ref type="table">6</ref>, where the MAE for GCN and GraphCON-GCN for this task is shown for increasing number of layers. We observe from this table that while the MAE with GCN increases with the number of layers, the MAE for GraphCON-GCN decreases monotonically with increasing layers, allowing for the use of very deep GraphCON models with increased expressive power. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Micro-averaged F 1 VR-GCN <ref type="bibr" target="#b10">(Chen et al., 2017)</ref> 97.8 GraphSAGE <ref type="bibr" target="#b29">(Hamilton et al., 2017)</ref> 61.2 PDE-GCN <ref type="bibr" target="#b19">(Eliasof et al., 2021)</ref> 99.2 GCNII <ref type="bibr" target="#b11">(Chen et al., 2020)</ref> 99.5 Cluster-GCN <ref type="bibr" target="#b13">(Chiang et al., 2019)</ref> 99.4 GeniePath <ref type="bibr" target="#b34">Liu et al. (2019)</ref> 98.5 JKNet <ref type="bibr" target="#b62">(Xu et al., 2018b)</ref> 97.6</p><p>GAT <ref type="bibr" target="#b57">(Velickovic et al., 2018)</ref> 97.3 GraphCON-GAT 99.4</p><p>GCN <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> 98.5 GraphCON-GCN 99.6</p><p>Table <ref type="table">4</ref>: Test mean absolute error (MAE, averaged over 4 runs on different initializations) on ZINC (without edge features, small 12k version) restricted to small network sizes of ∼ 100k parameters.</p><p>Baseline results are taken from <ref type="bibr" target="#b2">Beani et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Test MAE</head><p>GIN <ref type="bibr" target="#b61">(Xu et al., 2018a)</ref> 0.41 ± 0.008 GatedGCN <ref type="bibr" target="#b4">(Bresson &amp; Laurent, 2017)</ref> 0.42 ± 0.006 GraphSAGE <ref type="bibr" target="#b29">Hamilton et al. (2017)</ref> 0.41 ± 0.005 MoNet <ref type="bibr" target="#b36">(Monti et al., 2017)</ref> 0.41 ± 0.007 PNA <ref type="bibr" target="#b15">(Corso et al., 2020)</ref> 0.32 ± 0.032 DGN <ref type="bibr" target="#b2">(Beani et al., 2021)</ref> 0.22 ± 0.010 GCN <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> 0.47 ± 0.002 GraphCON-GCN 0.22 ± 0.004</p><p>GAT <ref type="bibr" target="#b57">(Velickovic et al., 2018)</ref> 0.46 ± 0.002 GraphCON-GAT 0.23 ± 0.004</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MNIST Superpixel graph classification</head><p>This experiment, first suggested by <ref type="bibr" target="#b36">Monti et al. (2017)</ref>, is based on the MNIST dataset <ref type="bibr" target="#b32">(LeCun et al., 1998)</ref>, where the grey-scale images are transformed into irregular graphs, as follows: the vertices in the graphs represent superpixels (large blobs of similar color), while the edges represent their spatial adjacency. Each graph has a fixed number of 75 superpixels (vertices). We use the standard splitting of using 55K-5K-10K for training, validation, and testing.</p><p>Table <ref type="table" target="#tab_3">5</ref> shows that GraphCON-GCN dramatically improves the performance of a pure GCN (test accuracy of 88.89% vs 98.70%). We stress that both models share the parameters over all layers, i.e. GraphCON-GCN does not have more parameters despite being a deeper model. Thus, the better performance of GraphCON-GCN over GCN can be attributed to the use of more 'layers' (iterations) and not to a higher number of parameters (see SM Table <ref type="table">7</ref> for accuracy vs. number of layers for this testcase). Finally, Table <ref type="table" target="#tab_3">5</ref> also shows that GraphCON-GAT outperforms all other methods, including the recently proposed <ref type="bibr">PNCNN Finzi et al. (2021)</ref>, reaching a nearly-perfect test accuracy of 98.91%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test accuracy</p><p>ChebNet <ref type="bibr" target="#b16">(Defferrard et al., 2016)</ref> 75.62 MoNet <ref type="bibr" target="#b36">(Monti et al., 2017)</ref> 91.11 PNCNN <ref type="bibr" target="#b21">(Finzi et al., 2021)</ref> 98.76 GatedGCN <ref type="bibr" target="#b4">(Bresson &amp; Laurent, 2017)</ref> 97.95 SplineCNN <ref type="bibr" target="#b20">(Fey et al., 2018)</ref> 95.22</p><p>GCN <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> 88.89 GraphCON-GCN 98.68</p><p>GAT <ref type="bibr" target="#b57">(Velickovic et al., 2018)</ref> 96.19 GraphCON-GAT 98.91</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In conclusion, we proposed a novel framework for designing deep Graph Neural Networks called GraphCON, based on suitable time discretizations of ODEs (1) that model the dynamics of a network of forced and damped oscillators. The coupling between the nodes is conditioned on the structure of the underlying graph.</p><p>One can readily interpret GraphCON as a framework to propagate information through multiple layers of a deep GNN, where each hidden layer has the same structure as standard GNNs such as GAT, GCN etc. Unlike in canonical constructions of deep GNNs, which stack hidden layers in a straightforward iterative fashion (7), GraphCON stacks them in a more involved manner using the dynamics of the ODE (3). Hence, in principle, any GNN hidden layer can serve as the coupling function F θ in GraphCON (4), offering it as an attractive framework for constructing very deep GNNs.</p><p>The well-known oversmoothing problem for GNNs was described mathematically in terms of the stability of zero Dirichlet energy steady states of the underlying ODE (3). We showed that such zero Dirichlet energy steady states of (3), which lead to constant node features, are not (exponentially) stable. Even if a trajectory reaches a feature vector that is constant across all nodes, very small perturbations will nudge it away and the resulting node features will deviate from each other. Thus, by construction, we demonstrated that the oversmoothing problem, in the sense of definition 3.2, is mitigated for GraphCON.</p><p>Finally, we extensively test GraphCON on a variety of node-and graph-classification and regression tasks, including heterophilic datasets known to be challenging for standard GNN models. From these experiments, we observed that (i) GraphCON models significantly outperform the underlying base GNN such as GCN or GAT and (ii) GraphCON models are either on par with or outperform state-of-the-art models on these tasks. This shows that ours is a novel, flexible, easy to use framework for constructing deep GNNs with theoretical guarantees and solid empirical performance.</p><p>Limitations and Future work. The presented approach can be extended in various directions. First, the richer dynamics of the ODEs (3) underlying our model points to a potentially higher expressive power, as also corroborated by experimental results. A future detailed theoretical investigation of the expressivity of GraphCON will entail studying subtle questions of optimal control, in particular the controllability Sontag (1998); <ref type="bibr" target="#b33">Lions (1988)</ref> of the underlying ODEs (3), as one has to prove that the trainable parameters in GraphCON can be adjusted such that the resulting trajectories of (3) could be steered to reach any target state in a rich enough hypothesis class. Relating such results to traditional approaches classifying the expressive power of GNNs by analogy to the Weisfeiler-Lehman hierarchy could be another interesting direction.</p><p>Second, we focused primarily on the widely-used GNNs such as GAT and GCN to define the 1neighborhood coupling in GraphCON. Given the inherent flexibility of GraphCON, it would be interesting to deploy more sophisticated GNNs as the 1-neighborhood coupling to see if the expressivity of GraphCON is increased further for larger sized benchmarks, as it was in the case of transductive node classification with a Transformer-type attention.</p><p>Third, GraphCON can also serve as a foundation for designing other physics-based GNNs such as models that possess a Hamiltonian structure. Finally, high-order discretizations of the underlying ODEs (3), such as the structure preserving Stormer-Verlet algorithm and high-order Runge-Kutta methods, can be considered as variants of GraphCON. results in Table <ref type="table" target="#tab_4">8</ref>) and vary α in the range of α ∈ [0, 2]. The results are plotted in Fig. <ref type="figure" target="#fig_5">3</ref> and show that the accuracy is extremely robust to a very large parameter range in α. Only for large values α &gt; 1.6, we see that the accuracy deteriorates when the damping is too high.</p><p>Next for this model and task, we fix α = 1 (which provides the best performance as reported in Table <ref type="table" target="#tab_4">8</ref>) and vary γ ∈ [0, 2]. Again, for a large range of values corresponding to γ ∈ [0.2, 2], the accuracy is very robust. However, for very small values of γ, the accuracy falls significantly. This is to be expected as the model loses its interpretation as system of oscillators for γ ≈ 0.</p><p>Thus, these sensitivity results demonstrate that GraphCON performs very robustly with respect to variations of the parameters α, γ, within a reasonable range.   <ref type="table" target="#tab_4">8</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training details</head><p>All experiments were run on NVIDIA GeForce GTX 1080 Ti, RTX 2080 Ti as well as RTX 2080 Ti GPUs. The tuning of the hyperparameters was done using a standard random search algorithm. We fix the time-step ∆t in (4) to 1 in all experiments. The damping parameter α as well as the frequency control parameter γ are set to 1 for all Cora, Citeseer and Pubmed experiments, while we set them to 0 for all experiments based on the Texas, Cornell and Wisconsin network graphs. For all other experiments we include α and γ to the hyperparameter search-space. The tuned values can be found in Table <ref type="table" target="#tab_4">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Mathematical details for Section 3 of main text</head><p>In this section, we provide details for the mathematical results in section 3 of the main text. We start with, Proof. We multiply Y i to the second equation of ( <ref type="formula" target="#formula_9">8</ref>) and obtain,</p><formula xml:id="formula_23">Y i dY i dt = j∈Ni A ij Y i (X j − X i ) ,   as j∈Ni A ij = 1  </formula><p>Summing over i ∈ G and using the symmetry condition A ij = A ji in the above expression yields,</p><formula xml:id="formula_24">d dt i∈G Y i 2 2 = − i∈G j∈Ni A ij (Y j − Y i ) (X j − X i ) , = − i∈G j∈Ni A ij d(X j − X i ) dt (X j − X i ) ⇒ 1 2 d dt   i∈G Y i 2 2 + i∈G j∈Ni A ij X j − X i 2   = 0.</formula><p>Integrating the last line in the above expression over time [0, t] yields the desired identity (11)</p><p>C.2 Proof of Proposition 3.3</p><p>Proof. By the definition of the Dirichlet energy ( <ref type="formula" target="#formula_17">13</ref>), (15) implies that,</p><formula xml:id="formula_25">lim t→∞ X i (t) ≡ c, ∀i ∈ V,<label>(16)</label></formula><p>for some c ∈ R m . In other words, all the hidden node features converge to the same feature vector c as time increases. Moreover, by ( <ref type="formula" target="#formula_20">15</ref>), this convergence is exponentially fast. Plugging in ( <ref type="formula" target="#formula_25">16</ref>) in to the first equation of the ODE (3), we obtain that,</p><formula xml:id="formula_26">lim t→∞ Y i (t) ≡ 0, ∀i ∈ G,<label>(17)</label></formula><p>with 0 being the m vector with zeroes for all its entries. Thus, oversmoothing in the sense of definition 3.2, amounts to (c, 0) being an exponentially stable fixed point (steady state) for the dynamics of (8) On the other hand, if (c, 0) is an exponentially stable steady state of (8), then the trajectories converge to this state exponentially fast satisfying (15). Consequently, by the definition of the Dirichlet energy (13), we readily observe that the oversmoothing problem, in the sense of definition 3.2, occurs in this case.</p><p>where we have used the first equation of ( <ref type="formula">19</ref>) in the last line of ( <ref type="formula">22</ref>). Consequently, we have for all i ∈ V,</p><formula xml:id="formula_27">d dt Ŷi 2 2 + α Ŷi 2 + d dt j∈Ni Âij 2 Xj − Xi 2 2 = j∈Ni Âij Ŷi + Ŷj 2 Xj − Xi<label>(23)</label></formula><p>Summing ( <ref type="formula" target="#formula_27">23</ref>) over all nodes i ∈ V yields,</p><formula xml:id="formula_28">d dt i∈V Ŷi 2 2 + α i∈V Ŷi 2 + d dt i∈G j∈Ni Âij + Âji 2 Xj − Xi 2 2 = i∈V j∈Ni Âij − Âji 2 Ŷi + Ŷj Xj − Xi (24)</formula><p>Multiplying e 2αt to both sides of (24) and using the chain rule, we readily obtain, We readily obtain the desired identity ( <ref type="formula">21</ref>) from ( <ref type="formula">26</ref>).</p><formula xml:id="formula_29">d dt i∈V e 2αt   Ŷi 2 2 + j∈Ni Âij + Âji 2 Xj − Xi 2 2   = αe 2αt i∈V j∈Ni Âij + Âji 2 Xj − Xi 2 + e 2αt i∈V j∈Ni Âij − Âji 2 Ŷi + Ŷj Xj − Xi</formula><p>Next, we observe that the right-hand side of the nonlinear ODEs (8) is globally Lipschitz. Therefore, solutions exist for all time t &gt; 0, are unique and depend continuously on the data.</p><p>We assume that the initial perturbations around the steady state (c, 0) are small i.e., they satisfy Xi (0) − Xj (0) ≤ , ∀j ∈ N i , ∀i ∈ G, Ŷi (0) ≤ , ∀i ∈ G, for some 0 &lt; &lt;&lt; 1. Hence, there exists a small time τ &gt; 0 such that the time-evolution of these perturbations can be approximated to arbitrary accuracy by solutions of the linearized system (19).</p><p>Next, we see from the identity (21) that the evolution of the perturbations X, Ŷ from the fixed point (c, 0) for the linearized system (19) is balanced by three terms T 1,2,3 . The term T 1 is clearly a dissipative term and says that the initial perturbations are damped exponentially fast in time.</p><p>On the other hand, the term T 2 , which has a positive sign, is a production term and says that the initial perturbations will grow with time t. Given the continuous dependence of the dynamics evolved by the ODE (19), there exists a time, still called τ by choosing it even smaller than the τ encountered before, such that Xi (t) − Xj (t) ∼ O( ), ∀j ∈ N i , ∀i ∈ G, ∀t ∈ [0, τ ],</p><formula xml:id="formula_30">Ŷi (t) ∼ O( ), ∀i ∈ G, ∀t ∈ [0, τ ]. (<label>27</label></formula><formula xml:id="formula_31">)</formula><p>Plugging the above expression into the term T 2 in (21) and using the right-stochatisticity of the matrix Â, we obtain that, T 2 (t) ∼ O( 2 ) 1 − e −2αt , ∀t ≤ τ (28)</p><p>Thus, the leading term is T 2 grows algebraically with respect to the initial perturbations.</p><p>Next we turn our attention to the term T 3 in ( <ref type="formula">21</ref>). This term is proportional to the asymmetry in the graph-coupling matrix Â = A(c, c). If this matrix were symmetric, then T 3 vanishes. On the other hand, for many 1-neighborhood couplings considered in this article, the matrix Â is not symmetric. In fact, one can explicitly compute that for the GAT and Transformers attention and GCN-couplings, we have, Âij = 1 deg(i)</p><p>, ∀j ∈ N i , ∀i ∈ V.</p><p>Here, deg is refers to the degree of the node, with possibly inserted self-loops.</p><p>As the ordering of nodes of the graph G is arbitrary, we can order them in such a manner that Âij &gt; Âji . Even with this ordering, as long as the matrix Â is not symmetric, the term T 3 is of indefinite sign. If it is positive, then we have additional growth with respect to time in (21). On the other hand, if T 3 is negative, it will have a dissipative effect. The rate of this dissipation can be readily calculated for a short time t ≤ τ under the assumption (27) to be, Thus by combining ( <ref type="formula">28</ref>) with (30), we obtain,</p><formula xml:id="formula_33">T 2 + T 3 ∼ 1 − D − D 2αDD 1 − e −2αt O( 2 )<label>(32)</label></formula><p>In particular for α ≥ 1/2, we see from (32), that the overall balance (21) leads to an algebraic growth, rather than exponential decay, of the initial perturbations of the fixed point (c, 0). Thus, we have shown that this steady state is not exponentially stable and small perturbations will take the trajectories of the ODE (19) away from this fixed point, completing the proof of Proposition 3.4.</p><p>Remark C.2. We see from the above proof, the condition α ≥ 1 2 is only a sufficient condition for the proof of Proposition 3.4, we can readily replace it by, α ≥ D − D 2DD</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of GraphCON dynamics on a ZINC molecular graph. The initial positions of GraphCON (X 0 in (4)) are represented by the 2-dimensional positions of the nodes, while the initial velocities (Y 0 in (4)) are set to the initial positions. The positions are propagated forward in time ('layers') using GraphCON-GCN with random weights. The molecular graph is plotted at initial time t = 0 as well as at t = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and ranking<ref type="bibr" target="#b40">Page et al. (1999)</ref>;<ref type="bibr" target="#b7">Chakrabarti (2007)</ref> (all of which are related to closed-form solutions of diffusion PDEs). In the context of Deep Learning, differential equations have been used to derive various types of neural networks including Neural ODEs and their variants, that have been used to design and interpret residual Chen et al. (2018) and convolutional Haber &amp; Ruthotto (2018) neural networks. These approaches have recently gained traction in Graph ML, e.g. with ODE-based models for learning on graphs Avelar et al. (2019); Poli et al. (2019b); Zhuang et al. (2020); Xhonneux et al. (2020b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sensitivity (measured as test accuracy) plot for α and γ hyperparameters of GraphCON-GCN (with 32 layers) trained on MNIST superpixel 75 experiment. First, α = 1.0 is fixed and γ is varied in [0, 2]. Second, γ = 0.76 is fixed and α is varied in [0, 2]. The fixed α, γ are taken from the best performing GraphCON-GCN on the MNIST superpixel 75 task (Table8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Transductive node classification test accuracy (MAP in %) on homophilic datasets. Mean and standard deviation are obtained using 20 random initializations on 5 random splits each. The three best performing methods are highlighted in red (First), blue (Second), and violet (Third).</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Homophily level</cell><cell>0.81</cell><cell>0.74</cell><cell>0.80</cell></row><row><cell cols="4">GAT-ppr MoNet GraphSage-mean GraphSage-maxpool 76.6 ± 1.9 81.6 ± 0.3 81.3 ± 1.3 79.2 ± 7.7 CGNN 81.4 ± 1.6 GDE 78.7 ± 2.2 GCN 81.5 ± 1.3 GraphCON-GCN 81.9 ± 1.7 GAT 81.8 ± 1.3 GraphCON-GAT 83.2 ± 1.4 73.2 ± 1.8 79.5 ± 1.8 68.5 ± 0.2 76.7 ± 0.3 71.2 ± 2.0 78.6 ± 2.3 71.6 ± 1.9 77.4 ± 2.2 67.5 ± 2.3 76.1 ± 2.3 66.9 ± 1.8 66.6 ± 4.4 71.8 ± 1.1 73.9 ± 3.7 71.9 ± 1.9 77.8 ± 2.9 72.9 ± 2.1 78.8 ± 2.6 71.4 ± 1.9 78.7 ± 2.3 GRAND 83.6 ± 1.0 73.4 ± 0.5 78.8 ± 1.7 GraphCON-Tran 84.2 ± 1.3 74.2 ± 1.7 79.4 ± 1.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Transductive node classification test accuracy (MAP in %) on heterophilic datasets. All results represent the average performance of the respective model over 10 fixed train/val/test splits, which are taken from<ref type="bibr" target="#b41">Pei et al. (2020)</ref>.</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Cornell</cell></row><row><cell>Homophily level</cell><cell>0.11</cell><cell>0.21</cell><cell>0.30</cell></row><row><cell cols="4">GPRGNN H2GCN GCNII Geom-GCN PairNorm GraphSAGE MLP GAT GraphCON-GAT 82.2 ± 4.7 78.4 ± 4.4 84.9 ± 7.2 87.7 ± 5.0 82.7 ± 5.3 82.9 ± 4.2 80.3 ± 8.1 77.6 ± 3.8 80.4 ± 3.4 77.9 ± 3.8 66.8 ± 2.7 64.5 ± 3.7 60.5 ± 3.7 60.3 ± 4.3 48.4 ± 6.1 58.9 ± 3.2 82.4 ± 6.1 81.2 ± 5.6 76.0 ± 5.0 80.8 ± 4.8 85.3 ± 3.3 81.9 ± 6.4 52.2 ± 6.6 49.4 ± 4.1 61.9 ± 5.1 85.7 ± 3.6 83.2 ± 7.0 GCN 55.1 ± 5.2 51.8 ± 3.1 60.5 ± 5.3 GraphCON-GCN 85.4 ± 4.2 87.8 ± 3.3 84.3 ± 4.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test micro-averaged F 1 score on Protein-Protein Interactions (PPI) data set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy in % on MNIST Superpixel 75.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters α and γ of GraphCON (4) for each best performing GraphCON model (based on a validation set).</figDesc><table><row><cell>Model</cell><cell>Experiment</cell><cell>α</cell><cell>γ</cell></row><row><cell cols="2">GraphCON-GCN PPI GraphCON-GAT</cell><cell>0.242 0.785</cell><cell>1.0 1.0</cell></row><row><cell cols="2">GraphCON-GCN ZINC GraphCON-GAT</cell><cell cols="2">0.215 1.115 1.475 1.324</cell></row><row><cell cols="2">GraphCON-GCN MNIST (superpixel) GraphCON-GAT</cell><cell>1.0 0.76</cell><cell>0.76 0.105</cell></row><row><cell>C.1 Proof of Proposition 3.1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>The research of TKR and SM was performed under a project that has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant Agreement No. 770880). MB and JR are supported in part by ERC Grant No. 724228 (LEMAN).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for: Graph-Coupled Oscillator Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further experimental results</head><p>A.1 Performance of GraphCON with respect to number of layers As we have argued in the main text, GraphCON is designed to be a deep GNN architecture with many layers. Depth could enhance the expressive power of GraphCON and we investigate this issue in two of the datasets, presented in Section 5 of the main text. In both experiments, we will focus on the GraphCON-GCN model and compare and contrast its performance, with respect to increasing depth, with the baseline GCN model.</p><p>We start with the molecular graph property regression example for the ZINC dataset of <ref type="bibr" target="#b30">Irwin et al. (2012)</ref>. In Table <ref type="table">6</ref>, we present the mean absolute error (MAE) of the model on the test set with respect to increasing number of layers (up to 20 layers) of the respective GNNs. As observed from this table, the MAE with standard GCN increases with depth. On the other hand, the MAE with GraphCON decreases as more layers are added. Next, we consider the MNIST Superpixel graph classification task and present the test accuracy with increasing depth (number of layers) for both GCN and GraphCON-GCN. As in the previous example, we observe that increasing depth leads to worsening of the test accuracy for GCN. On the other hand, the test accuracy for GraphCON-GCN increases as more layers (up to 32 layers) are added to the model. Thus, both experiments demonstrate that GraphCON leverages more depth to improve performance. A.2 Sensitivity of performance of GraphCON to hyperparameters α and γ</p><p>We recall that GraphCON, (4) of the main text, has two additional hyperparameters, namely the damping parameter α ≥ 0 and the frequency control parameter γ &gt; 0. In Table <ref type="table">8</ref>, we present the values of α, γ that led to the best performance of the resulting GraphCON models. It is natural to ask how sensitive the performance of GraphCON is to the variation of these hyperparameters. To this end, we choose the MNIST Superpixel graph classification task and perform a sensitivity study of the GraphCON-GCN model with respect to these hyperparameters. First, we fix a value of γ = 0.76 (corresponding to the best</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Proposition 3.4</head><p>The main aim of the section is to show that steady states of ( <ref type="formula">8</ref>), of the form (c, 0) are not exponentially stable.</p><p>To this end, we fix c and start by considering small perturbations around the fixed point (c, 0). We define, Xi =</p><p>and evolve these perturbations by the linearized ODE,</p><p>As σ(x) = max(x, 0) and c ≥ 0, we have that σ (c) = ID and linearized system (19) reduces to,</p><p>We have the following proposition on the dynamics of linearized system (19) with respect to perturbations of the fixed point (c, 0), Proposition C.1. Perturbations X(t), Ŷ(t) of the fixed point (c, 0), which evolve according to (19) satisfy the following identity,  <ref type="bibr">(22)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discrete and continuous deep residual learning over graphs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H C</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Directional graph networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ICLR 2014</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic personalized pagerank in entity-relation graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Beltrami flow and neural diffusion on graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GRAND: graph neural diffusion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Gorinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
				<meeting>the 38th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1407" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10568</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion maps</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and computational harmonic analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Traffic Prediction with Graph Neural Networks in Google Maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eliasof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Treister</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Splinecnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic numeric convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bondesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Utilizing graph machine learning within drug discovery and development</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICNN</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<editor>IJCNN</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Solving ordinary differential equations I</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hairer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Norsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wanner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exact controllability, stabilization and perturbations for distributed systems</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><surname>Geniepath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434v4</idno>
		<title level="m">Revisiting graph neural networks: all we have is low pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07532</idno>
		<title level="m">Graph neural ordinary differential equations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A recurrent model for learning very long time dependencies</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><surname>Unicornn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Mathematical Control Theory</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Encoding labeled graphs by labeling RAAM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neurons as oscillators</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Stiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Ermentrout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strogatz, S. Nonlinear Dynamics and Chaos</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="2950" to="2960" />
			<date type="published" when="2015">2016. 2015</date>
		</imprint>
	</monogr>
	<note>Journal of Neurophysiology</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14522</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Introduction to nonlinear dynamical systems and chaos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiggins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Continuous graph neural networks</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Continuous graph neural networks</title>
		<author>
			<persName><forename type="first">L.-P</forename><forename type="middle">A C</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434v4</idno>
		<title level="m">Graph neural networks: a review of methods and applications</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Ordinary differential equations on graph networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
