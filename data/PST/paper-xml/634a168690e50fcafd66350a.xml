<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
							<email>chiyuan@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
							<email>bengio@googe.com</email>
						</author>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
							<email>brecht@berkeey.edu</email>
						</author>
						<author>
							<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<email>vinyas@googe.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep artificial neural networks often have far more trainable model parameters than the number of samples they are trained on. Nonetheless, some of these models exhibit remarkably small generalization error, i.e., difference between "training error" and "test error". At the same time, it is certainly easy to come up with natural model architectures that generalize poorly. What is it then that distinguishes neural networks that generalize well from those that don't? A satisfying answer to this question would not only help to make neural networks more interpretable, but it might also lead to more principled and reliable model architecture design.</p><p>To answer such a question, statistical learning theory has proposed a number of different complexity measures that are capable of controlling generalization error. These include VC dimension <ref type="bibr" target="#b29">(Vapnik, 1998)</ref>, <ref type="bibr">Rademacher complexity (Bartlett &amp; Mendelson, 2003)</ref>, and uniform stability <ref type="bibr" target="#b20">(Mukherjee et al., 2002;</ref><ref type="bibr" target="#b3">Bousquet &amp; Elisseeff, 2002;</ref><ref type="bibr" target="#b23">Poggio et al., 2004)</ref>. Moreover, when the number of parameters is large, theory suggests that some form of regularization is needed to ensure small generalization error. Regularization may also be implicit as is the case with early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">OUR CONTRIBUTIONS</head><p>In this work, we problematize the traditional view of generalization by showing that it is incapable of distinguishing between different neural networks that have radically different generalization performance.</p><p>Randomization tests. At the heart of our methodology is a variant of the well-known randomization test from non-parametric statistics <ref type="bibr" target="#b9">(Edgington &amp; Onghena, 2007)</ref>. In a first set of experiments, we train several standard architectures on a copy of the data where the true labels were replaced by random labels. Our central finding can be summarized as:</p><p>Deep neural networks easily fit random labels.</p><p>More precisely, when trained on a completely random labeling of the true data, neural networks achieve 0 training error. The test error, of course, is no better than random chance as there is no correlation between the training labels and the test labels. In other words, by randomizing labels alone we can force the generalization error of a model to jump up considerably without changing the model, its size, hyperparameters, or the optimizer. We establish this fact for several different standard architectures trained on the CIFAR10 and ImageNet classification benchmarks. While simple to state, this observation has profound implications from a statistical learning perspective:</p><p>1. The effective capacity of neural networks is sufficient for memorizing the entire data set.</p><p>2. Even optimization on random labels remains easy. In fact, training time increases only by a small constant factor compared with training on the true labels.</p><p>3. Randomizing labels is solely a data transformation, leaving all other properties of the learning problem unchanged.</p><p>Extending on this first set of experiments, we also replace the true images by completely random pixels (e.g., Gaussian noise) and observe that convolutional neural networks continue to fit the data with zero training error. This shows that despite their structure, convolutional neural nets can fit random noise. We furthermore vary the amount of randomization, interpolating smoothly between the case of no noise and complete noise. This leads to a range of intermediate learning problems where there remains some level of signal in the labels. We observe a steady deterioration of the generalization error as we increase the noise level. This shows that neural networks are able to capture the remaining signal in the data, while at the same time fit the noisy part using brute-force.</p><p>We discuss in further detail below how these observations rule out all of VC-dimension, Rademacher complexity, and uniform stability as possible explanations for the generalization performance of state-of-the-art neural networks.</p><p>The role of explicit regularization. If the model architecture itself isn't a sufficient regularizer, it remains to see how much explicit regularization helps. We show that explicit forms of regularization, such as weight decay, dropout, and data augmentation, do not adequately explain the generalization error of neural networks. Put differently:</p><p>Explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.</p><p>In contrast with classical convex empirical risk minimization, where explicit regularization is necessary to rule out trivial solutions, we found that regularization plays a rather different role in deep learning. It appears to be more of a tuning parameter that often helps improve the final test error of a model, but the absence of all regularization does not necessarily imply poor generalization error. As reported by <ref type="bibr" target="#b15">Krizhevsky et al. (2012)</ref>, 2 -regularization (weight decay) sometimes even helps optimization, illustrating its poorly understood nature in deep learning.</p><p>Finite sample expressivity. We complement our empirical observations with a theoretical construction showing that generically large neural networks can express any labeling of the training data. More formally, we exhibit a very simple two-layer ReLU network with p = 2n + d parameters that can express any labeling of any sample of size n in d dimensions. A previous construction due to <ref type="bibr" target="#b17">Livni et al. (2014)</ref> achieved a similar result with far more parameters, namely, O(dn). While our depth 2 network inevitably has large width, we can also come up with a depth k network in which each layer has only O(n/k) parameters.</p><p>While prior expressivity results focused on what functions neural nets can represent over the entire domain, we focus instead on the expressivity of neural nets with regards to a finite sample. In contrast to existing depth separations <ref type="bibr" target="#b8">(Delalleau &amp; Bengio, 2011;</ref><ref type="bibr" target="#b10">Eldan &amp; Shamir, 2016;</ref><ref type="bibr" target="#b28">Telgarsky, 2016;</ref><ref type="bibr" target="#b6">Cohen &amp; Shashua, 2016)</ref> in function space, our result shows that even depth-2 networks of linear size can already represent any labeling of the training data.</p><p>The role of implicit regularization. While explicit regularizers like dropout and weight-decay may not be essential for generalization, it is certainly the case that not all models that fit the training data well generalize well. Indeed, in neural networks, we almost always choose our model as the output of running stochastic gradient descent. Appealing to linear models, we analyze how SGD acts as an implicit regularizer. For linear models, SGD always converges to a solution with small norm. Hence, the algorithm itself is implicitly regularizing the solution. Indeed, we show on small data sets that even Gaussian kernel methods can generalize well with no regularization. Though this doesn't explain why certain architectures generalize better than other architectures, it does suggest that more investigation is needed to understand exactly what the properties are inherited by models that were trained using SGD.</p><p>1.2 RELATED WORK <ref type="bibr" target="#b11">Hardt et al. (2016)</ref> give an upper bound on the generalization error of a model trained with stochastic gradient descent in terms of the number of steps gradient descent took. Their analysis goes through the notion of uniform stability <ref type="bibr" target="#b3">(Bousquet &amp; Elisseeff, 2002)</ref>. As we point out in this work, uniform stability of a learning algorithm is independent of the labeling of the training data. Hence, the concept is not strong enough to distinguish between the models trained on the true labels (small generalization error) and models trained on random labels (high generalization error). This also highlights why the analysis of <ref type="bibr" target="#b11">Hardt et al. (2016)</ref> for non-convex optimization was rather pessimistic, allowing only a very few passes over the data. Our results show that even empirically training neural networks is not uniformly stable for many passes over the data. Consequently, a weaker stability notion is necessary to make further progress along this direction.</p><p>There has been much work on the representational power of neural networks, starting from universal approximation theorems for multi-layer perceptrons <ref type="bibr" target="#b7">(Cybenko, 1989;</ref><ref type="bibr" target="#b19">Mhaskar, 1993;</ref><ref type="bibr" target="#b8">Delalleau &amp; Bengio, 2011;</ref><ref type="bibr" target="#b18">Mhaskar &amp; Poggio, 2016;</ref><ref type="bibr" target="#b10">Eldan &amp; Shamir, 2016;</ref><ref type="bibr" target="#b28">Telgarsky, 2016;</ref><ref type="bibr" target="#b6">Cohen &amp; Shashua, 2016)</ref>. All of these results are at the population level characterizing which mathematical functions certain families of neural networks can express over the entire domain. We instead study the representational power of neural networks for a finite sample of size n. This leads to a very simple proof that even O(n)-sized two-layer perceptrons have universal finite-sample expressivity.</p><p>Bartlett (1998) proved bounds on the fat shattering dimension of multilayer perceptrons with sigmoid activations in terms of the 1 -norm of the weights at each node. This important result gives a generalization bound for neural nets that is independent of the network size. However, for RELU networks the 1 -norm is no longer informative. This leads to the question of whether there is a different form of capacity control that bounds generalization error for large neural nets. This question was raised in a thought-provoking work by <ref type="bibr" target="#b21">Neyshabur et al. (2014)</ref>, who argued through experiments that network size is not the main form of capacity control for neural networks. An analogy to matrix factorization illustrated the importance of implicit regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EFFECTIVE CAPACITY OF NEURAL NETWORKS</head><p>Our goal is to understand the effective model capacity of feed-forward neural networks. Toward this goal, we choose a methodology inspired by non-parametric randomization tests. Specifically, we take a candidate architecture and train it both on the true data and on a copy of the data in which the true labels were replaced by random labels. In the second case, there is no longer any relationship between the instances and the class labels. As a result, learning is impossible. Intuition suggests that this impossibility should manifest itself clearly during training, e.g., by training not converging or slowing down substantially. To our surprise, several properties of the training process for multiple standard achitectures is largely unaffected by this transformation of the labels. This poses a conceptual challenge. Whatever justification we had for expecting a small generalization error to begin with must no longer apply to the case of random labels. To gain further insight into this phenomenon, we experiment with different levels of randomization exploring the continuum between no label noise and completely corrupted labels. We also try out different randomizations of the inputs (rather than labels), arriving at the same general conclusion.</p><p>The experiments are run on two image classification datasets, the CIFAR10 dataset <ref type="bibr" target="#b14">(Krizhevsky &amp; Hinton, 2009)</ref> and the ImageNet <ref type="bibr" target="#b24">(Russakovsky et al., 2015)</ref> ILSVRC 2012 dataset. We test the Inception V3 <ref type="bibr" target="#b27">(Szegedy et al., 2016)</ref> architecture on ImageNet and a smaller version of Inception, Alexnet <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref>, and MLPs on CIFAR10. Please see Section A in the appendix for more details of the experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FITTING RANDOM LABELS AND PIXELS</head><p>We run our experiments with the following modifications of the labels and input images:</p><p>• True labels: the original dataset without modification.</p><p>• Partially corrupted labels: independently with probability p, the label of each image is corrupted as a uniform random class.</p><p>• Random labels: all the labels are replaced with random ones.</p><p>• Shuffled pixels: a random permutation of the pixels is chosen and then the same permutation is applied to all the images in both training and test set.</p><p>• Random pixels: a different random permutation is applied to each image independently.</p><p>• Gaussian: A Gaussian distribution (with matching mean and variance to the original image dataset) is used to generate random pixels for each image.</p><p>Surprisingly, stochastic gradient descent with unchanged hyperparameter settings can optimize the weights to fit to random labels perfectly, even though the random labels completely destroy the relationship between images and labels. We further break the structure of the images by shuffling the image pixels, and even completely re-sampling random pixels from a Gaussian distribution. But the networks we tested are still able to fit.</p><p>Figure <ref type="figure" target="#fig_1">1a</ref> shows the learning curves of the Inception model on the CIFAR10 dataset under various settings. We expect the objective function to take longer to start decreasing on random labels because initially the label assignments for every training sample is uncorrelated. Therefore, large predictions errors are back-propagated to make large gradients for parameter updates. However, since the random labels are fixed and consistent across epochs, the network starts fitting after going through the training set multiple times. We find the following observations for fitting random labels very interesting: a) we do not need to change the learning rate schedule; b) once the fitting starts, it converges quickly; c) it converges to (over)fit the training set perfectly. Also note that "random pixels" and "Gaussian" start converging faster than "random labels". This might be because with random pixels, the inputs are more separated from each other than natural images that originally belong to the same category, therefore, easier to build a network for arbitrary label assignments.</p><p>On the CIFAR10 dataset, Alexnet and MLPs all converge to zero loss on the training set. The shaded rows in Table <ref type="table" target="#tab_1">1</ref> show the exact numbers and experimental setup. We also tested random labels on the ImageNet dataset. As shown in the last three rows of Table <ref type="table" target="#tab_3">2</ref> in the appendix, although it does not reach the perfect 100% top-1 accuracy, 95.20% accuracy is still very surprising for a million random labels from 1000 categories. Note that we did not do any hyperparameter tuning when switching from the true labels to random labels. It is likely that with some modification of the hyperparameters, perfect accuracy could be achieved on random labels. The network also manages to reach ∼90% top-1 accuracy even with explicit regularizers turned on.</p><p>Partially corrupted labels We further inspect the behavior of neural network training with a varying level of label corruptions from 0 (no corruption) to 1 (complete random labels) on the CIFAR10 dataset. The networks fit the corrupted training set perfectly for all the cases. Figure <ref type="figure" target="#fig_1">1b</ref> shows the slowdown of the convergence time with increasing level of label noises. Figure <ref type="figure" target="#fig_1">1c</ref> depicts the test errors after convergence. Since the training errors are always zero, the test errors are the same as generalization errors. As the noise level approaches 1, the generalization errors converge to 90%the performance of random guessing on CIFAR10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">IMPLICATIONS</head><p>In light of our randomization experiments, we discuss how our findings pose a challenge for several traditional approaches for reasoning about generalization.</p><p>Rademacher complexity and VC-dimension. Rademacher complexity is commonly used and flexible complexity measure of a hypothesis class. The empirical Rademacher complexity of a hypothesis class H on a dataset {x 1 , . . . , x n } is defined as</p><formula xml:id="formula_0">Rn (H) = E σ sup h∈H 1 n n i=1 σ i h(x i )<label>(1)</label></formula><p>where σ 1 , .  <ref type="formula">2015</ref>), but even these do not seem to explain the generalization behavior that we observe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uniform stability.</head><p>Stepping away from complexity measures of the hypothesis class, we can instead consider properties of the algorithm used for training. This is commonly done with some notion of stability, such as uniform stability <ref type="bibr" target="#b3">(Bousquet &amp; Elisseeff, 2002)</ref>. Uniform stability of an algorithm A measures how sensitive the algorithm is to the replacement of a single example. However, it is solely a property of the algorithm, which does not take into account specifics of the data or the distribution of the labels. It is possible to define weaker notions of stability <ref type="bibr" target="#b20">(Mukherjee et al., 2002;</ref><ref type="bibr" target="#b23">Poggio et al., 2004;</ref><ref type="bibr">Shalev-Shwartz et al., 2010)</ref>. The weakest stability measure is directly equivalent to bounding generalization error and does take the data into account. However, it has been difficult to utilize this weaker stability notion effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE ROLE OF REGULARIZATION</head><p>Most of our randomization tests are performed with explicit regularization turned off. Regularizers are the standard tool in theory and practice to mitigate overfitting in the regime when there are more  <ref type="bibr" target="#b29">(Vapnik, 1998)</ref>. The basic idea is that although the original hypothesis is too large to generalize well, regularizers help confine learning to a subset of the hypothesis space with manageable complexity. By adding an explicit regularizer, say by penalizing the norm of the optimal solution, the effective Rademacher complexity of the possible solutions is dramatically reduced.</p><p>As we will see, in deep learning, explicit regularization seems to play a rather different role. As the bottom rows of Table <ref type="table" target="#tab_3">2</ref> in the appendix show, even with dropout and weight decay, InceptionV3 is still able to fit the random training set extremely well if not perfectly. Although not shown explicitly, on CIFAR10, both Inception and MLPs still fit perfectly the random training set with weight decay turned on. However, AlexNet with weight decay turned on fails to converge on random labels. To investigate the role of regularization in deep learning, we explicitly compare behavior of deep nets learning with and without regularizers.</p><p>Instead of doing a full survey of all kinds of regularization techniques introduced for deep learning, we simply take several commonly used network architectures, and compare the behavior when turning off the equipped regularizers. The following regularizers are covered:</p><p>• Data augmentation: augment the training set via domain-specific transformations. For image data, commonly used transformations include random cropping, random perturbation of brightness, saturation, hue and contrast.</p><p>• Weight decay: equivalent to a 2 regularizer on the weights; also equivalent to a hard constrain of the weights to an Euclidean ball, with the radius decided by the amount of weight decay.</p><p>• Dropout <ref type="bibr" target="#b26">(Srivastava et al., 2014)</ref>: mask out each element of a layer output randomly with a given dropout probability. Only the Inception V3 for ImageNet uses dropout in our experiments. performance, but even with all of the regularizers turned off, all of the models still generalize very well.</p><p>Table <ref type="table" target="#tab_3">2</ref> in the appendix shows a similar experiment on the ImageNet dataset. A 18% top-1 accuracy drop is observed when we turn off all the regularizers. Specifically, the top-1 accuracy without regularization is 59.80%, while random guessing only achieves 0.1% top-1 accuracy on ImageNet. More strikingly, with data-augmentation on but other explicit regularizers off, Inception is able to achieve a top-1 accuracy of 72.95%. Indeed, it seems like the ability to augment the data using known symmetries is significantly more powerful than just tuning weight decay or preventing low training error.</p><p>Inception achieves 80.38% top-5 accuracy without regularization, while the reported number of the winner of ILSVRC 2012 <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref> achieved 83.6%. So while regularization is important, bigger gains can be achieved by simply changing the model architecture. It is difficult to say that the regularizers count as a fundamental phase change in the generalization capability of deep nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IMPLICIT REGULARIZATIONS</head><p>Early stopping was shown to implicitly regularize on some convex learning problems <ref type="bibr" target="#b30">(Yao et al., 2007;</ref><ref type="bibr" target="#b16">Lin et al., 2016)</ref>. In Table <ref type="table" target="#tab_3">2</ref> in the appendix, we show in parentheses the best test accuracy along the training process. It confirms that early stopping could potentially<ref type="foot" target="#foot_0">1</ref> improve the generalization performance. Figure <ref type="figure">2a</ref> shows the training and testing accuracy on ImageNet. The shaded area indicate the accumulative best test accuracy, as a reference of potential performance gain for early stopping. However, on the CIFAR10 dataset, we do not observe any potential benefit of early stopping.</p><p>Batch normalization <ref type="bibr" target="#b13">(Ioffe &amp; Szegedy, 2015)</ref> is an operator that normalizes the layer responses within each mini-batch. It has been widely adopted in many modern neural network architectures such as Inception <ref type="bibr" target="#b27">(Szegedy et al., 2016)</ref> and Residual Networks <ref type="bibr" target="#b12">(He et al., 2016)</ref>. Although not explicitly designed for regularization, batch normalization is usually found to improve the generalization performance. The Inception architecture uses a lot of batch normalization layers. To test the impact of batch normalization, we create a "Inception w/o BatchNorm" architecture that is exactly the same as Inception in Figure <ref type="figure">3</ref>, except with all the batch normalization layers removed. Figure <ref type="figure">2b</ref> compares the learning curves of the two variants of Inception on CIFAR10, with all the explicit regularizers turned off. The normalization operator helps stablize the learning dynamics, but the impact on the generalization performance is only 3∼4%. The exact accuracy is also listed in the section "Inception w/o BatchNorm" of Table <ref type="table" target="#tab_1">1</ref>.</p><p>In summary, our observations on both explicit and implicit regularizers are consistently suggesting that regularizers, when properly tuned, could help to improve the generalization performance. However, it is unlikely that the regularizers are the fundamental reason for generalization, as the networks continue to perform well after all the regularizers removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FINITE-SAMPLE EXPRESSIVITY</head><p>Much effort has gone into characterizing the expressivity of neural networks, e.g, <ref type="bibr" target="#b7">Cybenko (1989)</ref>; <ref type="bibr" target="#b19">Mhaskar (1993)</ref>; <ref type="bibr" target="#b8">Delalleau &amp; Bengio (2011)</ref>; <ref type="bibr" target="#b18">Mhaskar &amp; Poggio (2016)</ref>; <ref type="bibr" target="#b10">Eldan &amp; Shamir (2016)</ref>; <ref type="bibr" target="#b28">Telgarsky (2016)</ref>; <ref type="bibr" target="#b6">Cohen &amp; Shashua (2016)</ref>. Almost all of these results are at the "population level" showing what functions of the entire domain can and cannot be represented by certain classes of neural networks with the same number of parameters. For example, it is known that at the population level depth k is generically more powerful than depth k − 1.</p><p>We argue that what is more relevant in practice is the expressive power of neural networks on a finite sample of size n. It is possible to transfer population level results to finite sample results using uniform convergence theorems. However, such uniform convergence bounds would require the sample size to be polynomially large in the dimension of the input and exponential in the depth of the network, posing a clearly unrealistic requirement in practice.</p><p>We instead directly analyze the finite-sample expressivity of neural networks, noting that this dramatically simplifies the picture. Specifically, as soon as the number of parameters p of a networks is greater than n, even simple two-layer neural networks can represent any function of the input sample. We say that a neural network C can represent any function of a sample of size n in d dimensions if for every sample S ⊆ R d with |S| = n and every function f : S → R, there exists a setting of the weights of C such that C(x) = f (x) for every x ∈ S. Theorem 1. There exists a two-layer neural network with ReLU activations and 2n + d weights that can represent any function on a sample of size n in d dimensions.</p><p>The proof is given in Section C in the appendix, where we also discuss how to achieve width O(n/k) with depth k. We remark that it's a simple exercise to give bounds on the weights of the coefficient vectors in our construction. Lemma 1 gives a bound on the smallest eigenvalue of the matrix A. This can be used to give reasonable bounds on the weight of the solution w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLICIT REGULARIZATION: AN APPEAL TO LINEAR MODELS</head><p>Although deep neural nets remain mysterious for many reasons, we note in this section that it is not necessarily easy to understand the source of generalization for linear models either. Indeed, it is useful to appeal to the simple case of linear models to see if there are parallel insights that can help us better understand neural networks.</p><p>Suppose we collect n distinct data points {(x i , y i )} where x i are d-dimensional feature vectors and y i are labels. Letting loss denote a nonnegative loss function with loss(y, y) = 0, consider the empirical risk minimization (ERM) problem</p><formula xml:id="formula_1">min w∈R d 1 n n i=1 loss(w T x i , y i )<label>(2)</label></formula><p>If d ≥ n, then we can fit any labeling. But is it then possible to generalize with such a rich model class and no explicit regularization?</p><p>Let X denote the n × d data matrix whose i-th row is x T i . If X has rank n, then the system of equations Xw = y has an infinite number of solutions regardless of the right hand side. We can find a global minimum in the ERM problem (2) by simply solving this linear system. But do all global minima generalize equally well? Is there a way to determine when one global minimum will generalize whereas another will not? One popular way to understand quality of minima is the curvature of the loss function at the solution. But in the linear case, the curvature of all optimal solutions is the same <ref type="bibr" target="#b4">(Choromanska et al., 2015)</ref>. To see this, note that in the case when y i is a scalar,</p><formula xml:id="formula_2">∇ 2 1 n n i=1 loss(w T x i , y i ) = 1 n X T diag(β)X, β i := ∂ 2 loss(z,yi) ∂z 2 z=yi , ∀i</formula><p>A similar formula can be found when y is vector valued. In particular, the Hessian is not a function of the choice of w. Moreover, the Hessian is degenerate at all global optimal solutions.</p><p>If curvature doesn't distinguish global minima, what does? A promising direction is to consider the workhorse algorithm, stochastic gradient descent (SGD), and inspect which solution SGD converges to. Since the SGD update takes the form w t+1 = w t − η t e t x it where η t is the step size and e t is the prediction error loss. If w 0 = 0, we must have that the solution has the form w = n i=1 α i x i for some coefficients α. Hence, if we run SGD we have that w = X T α lies in the span of the data points. If we also perfectly interpolate the labels we have Xw = y. Enforcing both of these identities, this reduces to the single equation</p><formula xml:id="formula_3">XX T α = y<label>(3)</label></formula><p>which has a unique solution. Note that this equation only depends on the dot-products between the data points x i . We have thus derived the "kernel trick" <ref type="bibr" target="#b25">(Schölkopf et al., 2001)</ref>-albeit in a roundabout fashion.</p><p>We can therefore perfectly fit any set of labels by forming the Gram matrix (aka the kernel matrix) on the data K = XX T and solving the linear system Kα = y for α. This is an n × n linear system that can be solved on standard workstations whenever n is less than a hundred thousand, as is the case for small benchmarks like CIFAR10 and MNIST.</p><p>Quite surprisingly, fitting the training labels exactly yields excellent performance for convex models.</p><p>On MNIST with no preprocessing, we are able to achieve a test error of 1.2% by simply solving (3). Note that this is not exactly simple as the kernel matrix requires 30GB to store in memory. Nonetheless, this system can be solved in under 3 minutes in on a commodity workstation with 24 cores and 256 GB of RAM with a conventional LAPACK call. By first applying a Gabor wavelet transform to the data and then solving (3), the error on MNIST drops to 0.6%. Surprisingly, adding regularization does not improve either model's performance! Similar results follow for CIFAR10. Simply applying a Gaussian kernel on pixels and using no regularization achieves 46% test error. By preprocessing with a random convolutional neural net with 32,000 random filters, this test error drops to 17% error<ref type="foot" target="#foot_1">2</ref> . Adding 2 regularization further reduces this number to 15% error. Note that this is without any data augmentation.</p><p>Note that this kernel solution has an appealing interpretation in terms of implicit regularization.</p><p>Simple algebra reveals that it is equivalent to the minimum 2 -norm solution of Xw = y. That is, out of all models that exactly fit the data, SGD will often converge to the solution with minimum norm. It is very easy to construct solutions of Xw = y that don't generalize: for example, one could fit a Gaussian kernel to data and place the centers at random points. Another simple example would be to force the data to fit random labels on the test data. In both cases, the norm of the solution is significantly larger than the minimum norm solution.</p><p>Unfortunately, this notion of minimum norm is not predictive of generalization performance. For example, returning to the MNIST example, the 2 -norm of the minimum norm solution with no preprocessing is approximately 220. With wavelet preprocessing, the norm jumps to 390. Yet the test error drops by a factor of 2. So while this minimum-norm intuition may provide some guidance to new algorithm design, it is only a very small piece of the generalization story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work we presented a simple experimental framework for defining and understanding a notion of effective capacity of machine learning models. The experiments we conducted emphasize that the effective capacity of several successful neural network architectures is large enough to shatter the training data. Consequently, these models are in principle rich enough to memorize the training data. This situation poses a conceptual challenge to statistical learning theory as traditional measures of model complexity struggle to explain the generalization ability of large artificial neural networks.</p><p>We argue that we have yet to discover a precise formal measure under which these enormous models are simple. Another insight resulting from our experiments is that optimization continues to be empirically easy even if the resulting model does not generalize. This shows that the reasons for why optimization is empirically easy must be different from the true cause of generalization.</p><p>The ImageNet dataset contains 1,281,167 training and 50,000 validation images, split into 1000 classes. Each image is resized to 299x299 with 3 color channels. In the experiment on ImageNet, we use the Inception V3 <ref type="bibr" target="#b27">(Szegedy et al., 2016)</ref> architecture and reuse the data preprocessing and experimental setup from the TENSORFLOW package. The data pipeline is extended to allow disabling of data augmentation and feeding random labels that are consistent across epochs. We run the ImageNet experiment in a distributed asynchronized SGD system with 50 workers.</p><p>B DETAILED RESULTS ON IMAGENET Table <ref type="table" target="#tab_3">2</ref> shows the performance on Imagenet with true labels and random labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF OF THEOREM 1</head><p>Lemma 1. For any two interleaving sequences of n real numbers b</p><formula xml:id="formula_4">1 &lt; x 1 &lt; b 2 &lt; x 2 • • • &lt; b n &lt; x n , the n × n matrix A = [max{x i − b j , 0}] ij has full rank. Its smallest eigenvalue is min i x i − b i .</formula><p>Proof. By its definition, the matrix A is lower triangular, that is, all entries with i &lt; j vanish. A basic linear algebra fact states that a lower-triangular matrix has full rank if and only if all of the entries on the diagional are nonzero. Since, x i &gt; b i , we have that max{x i − b i , 0} &gt; 0. Hence, A is invertible. The second claim follows directly from the fact that a lower-triangular matrix has all its eigenvalues on the main diagonal. This in turn follows from the first fact, since A − λI can have lower rank only if λ equals one of the diagonal values. It is easy to see that c can be expressed by a depth 2 network with ReLU activations. Now, fix a sample S = {z 1 , . . . , z n } of size n and a target vector y ∈ R n . To prove the theorem, we need to find weights a, b, w so that y i = c(z i ) for all i ∈ {1, . . . , n} First, choose a and b such that with x i = a, z i we have the interleaving property b</p><formula xml:id="formula_5">1 &lt; x 1 &lt; b 2 &lt; • • • &lt; b n &lt; x n</formula><p>. This is possible since all z i 's are distinct. Next, consider the set of n equations in the n unknowns w, y i = c(z i ) , i ∈ {1, . . . , n} .</p><p>Published as a conference paper at ICLR 2017</p><p>We have c(z i ) = Aw, where A = [max{x i − b i , 0}] ij is the matrix we encountered in Lemma 1. We chose a and b so that the lemma applies and hence A has full rank. We can now solve the linear system y = Aw to find suitable weights w.</p><p>While the construction in the previous proof has inevitably high width given that the depth is 2, it is possible to trade width for depth. The construction is as follows. With the notation from the proof and assuming w.l.o.g. that x 1 , . .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RESULTS OF IMPLICIT REGULARIZATION FOR LINEAR MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E FITTING RANDOM LABELS WITH EXPLICIT REGULARIZATION</head><p>In Section 3, we showed that it is difficult to say that commonly used explicit regularizers count as a fundamental phase change in the generalization capability of deep nets. In this appendix, we add some experiments to investigate how explicit regularizers affect the ability to fit random labels. 99.28%</p><p>From Table <ref type="table" target="#tab_6">4</ref>, we can see that for weight decay using the default coefficient for each model, except Alexnet, all other models are still able to fit random labels. We also tested random cropping and data augmentation with the Inception architecture. By changing the default weight decay factor from 0.95 to 0.999, and running for more epochs, we observe overfitting to random labels in both cases. It is expected to take longer to converge because data augmentation explodes the training set size (though many samples are not i.i.d. any more).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fitting random labels and random pixels on CIFAR10. (a) shows the training loss of various experiment settings decaying with the training steps. (b) shows the relative convergence time with different label corruption ratio. (c) shows the test error (also the generalization error since training error is 0) under different label corruptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proof of Theorem 1 .</head><label>1</label><figDesc>For weight vectors w, b ∈ R n and a ∈ R d , consider the function c : R n → R, c(x) = j=1 w j max{ a, x − b j , 0}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. , x n ∈ [0, 1], partition the interval [0, 1] into b disjoint intervals I 1 , . . . , I b so that each interval I j contains n/b points. At layer j, apply the construction from the proof to all points in I j . This requires O(n/b) nodes at level j. This construction results in a circuit of width O(n/b) and depth b + 1 which so far has b outputs (one from each layer). It remains to implement a multiplexer which selects one of the b outputs based on which interval a given input x falls into. This boils down to implementing one (approximate) indicator function f j for each interval I j and outputting b j=1 f j (x)o j , where o j is the output of layer j. This results in a single output circuit. Implementing a single indicator function requires constant size and depth with ReLU activiations. Hence, the final size of the construction is O(n) and the depth is b+c for some constant c. Setting k = b − c gives the next corollary. Corollary 1. For every k ≥ 2, there exists neural network with ReLU activations of depth k, width O(n/k) and O(n + d) weights that can represent any function on a sample of size n in d dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This is, of course, a trivial upper bound on the Rademacher complexity that does not lead to useful generalization bounds in realistic settings. A similar reasoning applies to VC-dimension and its continuous analog fat-shattering dimension, unless we further restrict the network. While Bartlett (1998) proves a bound on the fat-shattering dimension in terms of 1 norm bounds on the weights of the network, this bound does not apply to the ReLU networks that we consider here. This result was generalized to other norms by Neyshabur et al. (</figDesc><table /><note>. . , σ n ∈ {±1} are i.i.d. uniform random variables. This definition closely resembles our randomization test. Specifically, Rn (H) measures ability of H to fit random ±1 binary label assignments. While we consider multiclass problems, it is straightforward to consider related binary classification problems for which the same experimental observations hold. Since our randomization tests suggest that many neural networks fit the training set with random labels perfectly, we expect that Rn (H) ≈ 1 for the corresponding model class H.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The training and test accuracy (in percentage) of various models on the CIFAR10 dataset. Performance with and without data augmentation and weight decay are compared. The results of fitting random labels are also included.</figDesc><table><row><cell>model</cell><cell cols="5"># params random crop weight decay train accuracy test accuracy</cell></row><row><cell></cell><cell></cell><cell>yes</cell><cell>yes</cell><cell>100.0</cell><cell>89.05</cell></row><row><cell>Inception</cell><cell>1,649,402</cell><cell>yes no</cell><cell>no yes</cell><cell>100.0 100.0</cell><cell>89.31 86.03</cell></row><row><cell></cell><cell></cell><cell>no</cell><cell>no</cell><cell>100.0</cell><cell>85.75</cell></row><row><cell cols="2">(fitting random labels)</cell><cell>no</cell><cell>no</cell><cell>100.0</cell><cell>9.78</cell></row><row><cell>Inception w/o BatchNorm</cell><cell>1,649,402</cell><cell>no no</cell><cell>yes no</cell><cell>100.0 100.0</cell><cell>83.00 82.00</cell></row><row><cell cols="2">(fitting random labels)</cell><cell>no</cell><cell>no</cell><cell>100.0</cell><cell>10.12</cell></row><row><cell></cell><cell></cell><cell>yes</cell><cell>yes</cell><cell>99.90</cell><cell>81.22</cell></row><row><cell>Alexnet</cell><cell>1,387,786</cell><cell>yes no</cell><cell>no yes</cell><cell>99.82 100.0</cell><cell>79.66 77.36</cell></row><row><cell></cell><cell></cell><cell>no</cell><cell>no</cell><cell>100.0</cell><cell>76.07</cell></row><row><cell cols="2">(fitting random labels)</cell><cell>no</cell><cell>no</cell><cell>99.82</cell><cell>9.86</cell></row><row><cell>MLP 3x512</cell><cell>1,735,178</cell><cell>no no</cell><cell>yes no</cell><cell>100.0 100.0</cell><cell>53.35 52.39</cell></row><row><cell cols="2">(fitting random labels)</cell><cell>no</cell><cell>no</cell><cell>100.0</cell><cell>10.48</cell></row><row><cell>MLP 1x512</cell><cell>1,209,866</cell><cell>no no</cell><cell>yes no</cell><cell>99.80 100.0</cell><cell>50.39 50.51</cell></row><row><cell cols="2">(fitting random labels)</cell><cell>no</cell><cell>no</cell><cell>99.34</cell><cell>10.61</cell></row><row><cell cols="2">parameters than data points</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Effects of implicit regularizers on generalization performance. aug is data augmentation, wd is weight decay, BN is batch normalization. The shaded areas are the cumulative best test accuracy, as an indicator of potential performance gain of early stopping. (a) early stopping could potentially improve generalization when other regularizers are absent. (b) early stopping is not necessarily helpful on CIFAR10, but batch normalization stablize the training process and improves generalization.</figDesc><table><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>0 0.0 0.2 0.4 0.6 0.8</cell><cell>2000 4000 6000 8000 10000 thousand training steps test(w/ aug, wd, dropout) train(w/ aug, wd, dropout) test(w/o aug, dropout) train(w/o aug, dropout) test(w/o aug, wd, dropout) train(w/o aug, wd, dropout)</cell><cell>accuracy</cell><cell>0 0.6 0.7 0.8 0.9</cell><cell>5 thousand training steps 10 15 test(Inception) train(Inception) test(Inception w/o BN) train(Inception w/o BN)</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell>(a) Inception on ImageNet</cell><cell></cell><cell></cell><cell>(b) Inception on CIFAR10</cell><cell></cell></row><row><cell cols="2">Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>shows the results of Inception, Alexnet and MLPs on CIFAR10, toggling the use of data augmentation and weight decay. Both regularization techniques help to improve the generalization</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The top-1 and top-5 accuracy (in percentage) of the Inception v3 model on the ImageNet dataset. We compare the training and test accuracy with various regularization turned on and off, for both true labels and random labels. The original reported top-5 accuracy of the Alexnet on ILSVRC 2012 is also listed for reference. The numbers in parentheses are the best test accuracy during training, as a reference for potential performance gain of early stopping.</figDesc><table><row><cell>data aug</cell><cell>dropout</cell><cell>weight decay</cell><cell cols="2">top-1 train top-5 train</cell><cell>top-1 test</cell><cell>top-5 test</cell></row><row><cell cols="4">ImageNet 1000 classes with the original labels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>92.18</cell><cell>99.21</cell><cell>77.84</cell><cell>93.92</cell></row><row><cell>yes</cell><cell>no</cell><cell>no</cell><cell>92.33</cell><cell>99.17</cell><cell>72.95</cell><cell>90.43</cell></row><row><cell>no</cell><cell>no</cell><cell>yes</cell><cell>90.60</cell><cell>100.0</cell><cell cols="2">67.18 (72.57) 86.44 (91.31)</cell></row><row><cell>no</cell><cell>no</cell><cell>no</cell><cell>99.53</cell><cell>100.0</cell><cell cols="2">59.80 (63.16) 80.38 (84.49)</cell></row><row><cell cols="3">Alexnet (Krizhevsky et al., 2012)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.6</cell></row><row><cell cols="4">ImageNet 1000 classes with random labels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>no</cell><cell>yes</cell><cell>yes</cell><cell>91.18</cell><cell>97.95</cell><cell>0.09</cell><cell>0.49</cell></row><row><cell>no</cell><cell>no</cell><cell>yes</cell><cell>87.81</cell><cell>96.15</cell><cell>0.12</cell><cell>0.50</cell></row><row><cell>no</cell><cell>no</cell><cell>no</cell><cell>95.20</cell><cell>99.14</cell><cell>0.11</cell><cell>0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Generalizing with kernels. The test error associated with solving the kernel equation (3) on small benchmarks. Note that changing the preprocessing can significantly change the resulting test error.</figDesc><table><row><cell>data set</cell><cell>pre-processing</cell><cell>test error</cell></row><row><cell>MNIST</cell><cell>none</cell><cell>1.2%</cell></row><row><cell>MNIST</cell><cell>gabor filters</cell><cell>0.6%</cell></row><row><cell>CIFAR10</cell><cell>none</cell><cell>46%</cell></row><row><cell cols="2">CIFAR10 random conv-net</cell><cell>17%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>list the experiment results of linear models described in Section 5.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on fitting random labels on the CIFAR10 dataset with weight decay and data augmentation.</figDesc><table><row><cell>Model</cell><cell>Regularizer</cell><cell>Training Accuracy</cell></row><row><cell>Inception</cell><cell></cell><cell>100%</cell></row><row><cell>Alexnet MLP 3x512</cell><cell>Weight decay</cell><cell>Failed to converge 100%</cell></row><row><cell>MLP 1x512</cell><cell></cell><cell>99.21%</cell></row><row><cell>Inception</cell><cell>Random Cropping 1 Augmentation 2</cell><cell>99.93%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We say "potentially" because to make this statement rigorous, we need to have another isolated test set and test the performance there when we choose early stopping point on the first test set (acting like a validation set).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">This conv-net is the<ref type="bibr" target="#b5">Coates &amp; Ng (2012)</ref> net, but with the filters selected at random instead of with k-means.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2">In random cropping and augmentation, a new randomly modified image is used in each epoch, but the (randomly assigned) labels are kept consistent for all the epochs. The "training accuracy" means a slightly different thing here as the training set is different in each epoch. The global average of the online accuracy at each mini-batch on the augmented samples is reported here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">Data augmentation includes random left-right flipping and random rotation up to 25 degrees.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL SETUP</head><p>We focus on two image classification datasets, the CIFAR10 dataset <ref type="bibr" target="#b14">(Krizhevsky &amp; Hinton, 2009)</ref> and the ImageNet <ref type="bibr" target="#b24">(Russakovsky et al., 2015)</ref> ILSVRC 2012 dataset.</p><p>The CIFAR10 dataset contains 50,000 training and 10,000 validation images, split into 10 classes. Each image is of size 32x32, with 3 color channels. We divide the pixel values by 255 to scale them into [0, 1], crop from the center to get 28x28 inputs, and then normalize them by subtracting the mean and dividing the adjusted standard deviation independently for each image with the per_image_whitening function in TENSORFLOW <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>.</p><p>For the experiment on CIFAR10, we test a simplified Inception <ref type="bibr" target="#b27">(Szegedy et al., 2016)</ref> and Alexnet <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref> by adapting the architectures to smaller input image sizes. We also test standard multi-layer perceptrons (MLPs) with various number of hidden layers.</p><p>The small Inception model uses a combination of 1x1 and 3x3 convolution pathways. The detailed architecture is illustrated in Figure <ref type="figure">3</ref>. The small Alexnet is constructed by two (convolution 5x5 → max-pool 3x3 → local-response-normalization) modules followed by two fully connected layers with 384 and 192 hidden units, respectively. Finally a 10-way linear layer is used for prediction. The MLPs use fully connected layers. MLP 1x512 means one hidden layer with 512 hidden units. All of the architectures use standard rectified linear activation functions (ReLU).</p><p>For all experiments on CIFAR10, we train using SGD with a momentum parameter of 0.9. An initial learning rate of 0.1 (for small Inception) or 0.01 (for small Alexnet and MLPs) are used, with a decay factor of 0.95 per training epoch. Unless otherwise specified, for the experiments with randomized labels or pixels, we train the networks without weight decay, dropout, or other forms of explicit regularization. Section 3 discusses the effects of various regularizers on fitting the networks and generalization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<ptr target="http://tensorfow.org/.Softwareavailablefromten-sorflow.org" />
		<editor>Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Vincent Vanhoucke, Vijay Vasudevan</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Sample Complexity of Pattern Classification with Neural Networks -The Size of the Weights is More Important than the Size of the Network</title>
		<author>
			<persName><forename type="first">Bartlett</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: risk bounds and structural results</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stability and generalization</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="499" to="526" />
			<date type="published" when="2002-03">March 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning feature representations with k-means</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, Reloaded</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional Rectifier Networks as Generalized Tensor Decompositions</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximation by superposition of sigmoidal functions</title>
		<author>
			<persName><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shallow vs. Deep Sum-Product Networks</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randomization Tests. Statistics: A Series of Textbooks and Monographs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Edgington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Onghena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Taylor &amp; Francis</publisher>
		</imprint>
	</monogr>
	<note>ISBN 9781584885894</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Power of Depth for Feedforward Neural Networks</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalization Properties and Implicit Regularization for Multiple Passes SGM</title>
		<author>
			<persName><forename type="first">Junhong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaello</forename><surname>Camoriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep vs. shallow networks : An approximation theory perspective</title>
		<author>
			<persName><forename type="first">Hrushikesh</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<idno>CoRR, abs/1608.03287</idno>
		<ptr target="http://arxiv.org/abs/1608.03287" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximation properties of a multilayered feedforward artificial neural network</title>
		<author>
			<persName><forename type="first">Hrushikesh</forename><surname>Narhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mhaskar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Statistical learning: Stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization</title>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rifkin</surname></persName>
		</author>
		<idno>AI Memo 2002-024</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">In search of the real inductive bias: On the role of implicit regularization in deep learning</title>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6614</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Norm-Based Capacity Control in Neural Networks</title>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1376" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">General conditions for predictivity in learning theory</title>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="issue">6981</biblScope>
			<biblScope unit="page" from="419" to="422" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<idno type="ISSN">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2001-10">2001. October 2010</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2635" to="2670" />
		</imprint>
	</monogr>
	<note>A generalized representer theorem</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benefits of depth in neural networks</title>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Statistical Learning Theory. Adaptive and learning systems for signal processing, communications, and control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On early stopping in gradient descent learning</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Caponnetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="315" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
