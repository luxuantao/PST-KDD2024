<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data mining for hypertext: A tutorial survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
							<email>soumen@cse@iitb.ernet</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay t</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data mining for hypertext: A tutorial survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2001A66660B867E9F2DB386DEEC9D2D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With over 800 million pages covering most areas of human endeavor, the World-wide Web is a fertile ground for data mining research to make a difference to the effectiveness of information search. Today, Web surfers access the Web through two dominant interfaces: clicking on hyperlinks and searching via keyword queries. This process is often tentative and unsatisfactory. Better support is needed for expressing one's information ,heed and dealing with a search result in more structured ways than available now. Data mining and machine learning have significant roles to play towards this end.</p><p>In this paper we will survey recent advances in learning and mining problems related to hypertext in general and the Web in particular. We will review the continuum of supervised to semi-supervised to unsupervised learning problems, highlight the specific challenges which distinguish data mining in the hypertext domain from data mining in the context of data warehouses, and summarize the key areas of recent and ongoing research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The volume of unstructured text and hypertext data exceeds that of structured data. Text and hypertext are used for digital libraries, product catalogs, reviews, newsgroups, medical reports, customer service reports, and homepages for individuals, orgamzatxons, and projects. Hypertext has been widely used long before the popularization of the Web.</p><p>Communities such as the ACM t Special Interest Groups on Information Retrieval (SIGIR) 2, Hypertext, Hypermedia and Web (SIGLINK/SIGWEB) 3 and Digital Libraries 4 have been engaged in research on effective search and retrieval from text and hypertext databases. The spectacular ascent in the size and popularity of the Web has subjected traditional information retrieval (IR) techniques to an intense stress-test. The Web exceeds 800 million HTML pages, or six terabytes of data on about three million servers. Almost a million pages are added daily, a typical page changes in a few months, and several hundred gigabytes change every month. Even the largest search enlhttp://~ncw, acm. org 2http://www. acm. org/sigir 3http://www. acm. org/siglink 4http ://www. acm. org/dl gines, such as Alta Vista 5 and HotBot 6 index less than 18% of the accessible Web as of February 1999 <ref type="bibr" target="#b51">[49]</ref>, down from 35% in late 1997 <ref type="bibr" target="#b5">[6]</ref>.</p><p>Apart from sheer size and flux, the Web and its users differ significantly from traditional IR. corpora and workloads. The Web is populist hypermedia at its best: there is no consistent standard or style of authorship dictated centrally; content is created autonomously by diverse people. Misrepresentation of content by 'spamming' the page with meaningless terms is rampant so that keyword indexing search engines rank pages highly for many queries. Hyperlinks are created for navigation, endorsement, citation, criticism, or plain whim.</p><p>Search technology inherited from the world of IR is evolving slowly to meet these new challenges. As mentioned before, search engines no longer attempt to index the whole Web. But apart from this deficiency of scale, search engines are also known for poor accuracy: they have both low recall (fraction of relevant documents that are retrieved) and low precision (fraction of retrieved documents that are relevant). The usual problems of text search, such as synonymy, polysemy (a word with more than one meaning), and context sensitivity become especially severe on the Web. Moreover, a new dimension is added because of extant partial structure: whereas IR research deals predominantly with documents, the Web offers semistructured data in the form of directed graphs where nodes are primitive or compound objects and edges are field labels• Apart from the IR community, documents and the use of natural language (NL) have been studied also by a large body of linguists and computational linguists. NL techniques can now parse relatively well-formed sentences in many languages <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b74">71,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b71">68]</ref>, disambiguate polysemous words with high accuracy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b65">62,</ref><ref type="bibr">11]</ref>, tag words in running text with part-of-speech information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32]</ref>, represent NL documents in a canonical machine-usable form <ref type="bibr" target="#b70">[67,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b68">65]</ref>, and perform NL translation <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>. A combination of NL and IR techniques have been used for creating hyperlinks automatically <ref type="bibr">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b12">12]</ref> and expanding brief queries with related words for enhanced search. For reasons unclear to us, popular Web search engines have been slow to embrace these capabilities. Although we see the progression towards better semantic understanding as inevitable, NL techniques are outside the scope of this survey. 5http://www.altavista.com 6http://wT,~w.hotbot.com <ref type="bibr">Outline:</ref> In this survey we will concentrate on statistical techniques for learning structure in various forms from text, hypertext and semistructured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models:</head><p>In §2 we describe some of the models used to represent hypertext and semistructured data.</p><p>Supervised learning: In §3 we discuss techniques for supervised learning or classification.</p><p>Unsupervised learning: The other end of the spectrum, unsupervised learning or clustering is discussed in §4.</p><p>Semi-supervlsed learning: Real-life applications fit somewhere in between fully supervised or unsupervised learning, and are discussed in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social network analysis:</head><p>A key feature which distinguishes hypertext mining from warehouse mining and much of IR is the analysis of structural information in hyperlinks. This is discussed in §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic models</head><p>In its full generality, a model for text must build machine representations of world knowledge, and must therefore involve a NL grammar. Since we restrict our scope to statistical analyses, we need to find suitable representations for text, hypertext, and semistructured data. which will suffice for our learning applications. We discuss some such models in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models for text</head><p>In the IR domain, documents have been traditionally represented in the vector space model <ref type="bibr" target="#b66">[63,</ref><ref type="bibr" target="#b29">29]</ref>. Documents are tokenized using simple syntactic rules (such as whitespace delimiters in English) and tokens stemmed to canonical form (e.g., 'reading' to 'read,' 'is,' 'was,' 'are' to 'be' </p><formula xml:id="formula_0">Ildll~ = maxt n(d, t).</formula><p>These representations do not capture the fact that some terms (like 'algebra') are more important than others (like 'the' and 'is') in determining document content. If t occurs in Nt out of N documents, Nt/N gives a sense of the rarity, hence, importance, of the term. The "inverse document frequency" IDF(t) = 1 + log -~-is used to stretch the axes of the vector space differentially. (Many variants of this formula exist; our form is merely illustrative.) Thus the t-th coordinate of d may be chosen as ~</p><p>x IDF(t), popularly Itdlh called the 'TFIDF' (term frequency times inverse document frequency) weighted vector space model.</p><p>Alternatively, one may construct probabilstic models for document generation. Once again, we should start with the disclaimer that these models have no bearing on grammar and semantic coherence. The simplest statistical model is the binary model. In this model, a document is a set of terms, which is a subset of the lexicon (the universe of possible terms). Word counts are not significant. In the multinomial model, one imagines a die with as many faces as there are words in the lexicon, each face t having an associated probability 0t of showing up when tossed. To compose a document the author fixes a total word count (arbitrarily or by generating it from a distribution), and then tosses the die as many times, each time writing down the term corresponding to the face that shows up.</p><p>In spite of being extremely crude and not capturing any aspect of language or semantics, these models often perform well for their intended purpose. In spite of minor variations all these models regard documents as multisets of terms, without paying attention to ordering between terms. Therefore they are collectively called bag-of-words models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models for hypertext</head><p>Hypertext has hyperlinks in addition to text. These are modeled with varying levels of detail, depending on the application. In the simplest model, hypertext can be regarded as a directed graph (D, L) where D is the set of nodes, documents, or pages, and L is the set of links. Crude models may not need to include the text models at the nodes. More refined models will characterize some sort of joint distribution between the term distribution of a node with those in a certain neighborhood. One may also wish to recognize that the source document is in fact a sequence of terms interspersed with outbound hyperlinks. This may be used to establish specific relations between certain links and terms ( §6).</p><p>On occasion we will regard documents as being generated from topic-specific term distributions. For example, the term distribution of documents related to 'bicycling' is quite different from that of documents related to 'archaeology.' Unlike journals on archaeology and magazines on bicycling, the Web is not isolated: nodes related to diverse topics link to each other. (We have found recreational bicycling pages to link to pages about first-aid significantly more often than a typical page.) If the application so warrants (as in §6.2) one needs to model such coupling among topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Models for semistructured data</head><p>Apart from hyperlinks, other structures exist on the Web~ both across and within documents. One prominent kind of inter-document structure are topic directories like the Open Directory Project 7 and Yahoo! s. Such services have constructed, through human effort, a giant taxonomy of topic directories. Each directory has a collection of hyperlinks to relevant (and often popular or" authoritative) sites related to the specific topic. One may model tree-structured hierarchies with an is-a(specific-topic, general-topic) relation, and an example (topic, url) relation to assign URLs to topics. Although topic taxonomies are a special case of semistructured data, it is important and frequent enough to mention separately. Semistructured data is a point of convergence <ref type="bibr" target="#b25">[25]</ref> for the Web 9 and database 1° communities: the former deals with documents, the latter with data. The form of that data is evolving from rigidly structured relational tables with numbers and strings to enable the natural representation of complex real-world objects like books, papers, movies, jet engine 7http ://dmoz. org Shttp://www. yahoo, com 9http://wm~9. org l°http ://~n~. acm. org/sigmod components, and chip designs without sending the application writer into contortions. Emergent representations for semistructured data (such as X M L 11) are variations on the O b j e c t E x c h a n g e M o d e l ( O E M ) 12 <ref type="bibr" target="#b57">[54,</ref><ref type="bibr" target="#b33">33]</ref>. In OEM, data is in the form of atomic or compound objects: atomic objects may be integers or strings; compound objects refer to other objects through labeled edges. HTML is a special case of such 'intra-document' structure.</p><p>The above forms of irregular structures naturally encourage data mining techniques from the domain of 'standard' structured warehouses to be applied, adapted, and extended to discover useful patterns from semistructured sources as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supervised learning</head><p>In supervised learning, also called classification, the learner first receives training data in which each item is marked with a label or class from a discrete finite set. The algorithm is trained using this data, after which it is given unlabeled data and has to guess the label.</p><p>Classification has numerous applications in the hypertext and semistructured data domains. Surfers use topic directories because they help structure and restrict keyword searches, making it easy, for example, to ask for documents with the word socks which are about garments, not the see u r l t y proxy protocol 13. (socks AND NOT n e t v o r k will drop garment sites boasting of a large distribution network.) Robust hypertext classification is useful for email and newsgroup management and maintining Web directories.</p><p>As another example, a campus search engine that can categorize faculty, student and project pages can answer queries that select on these attributes in addition to keywords (find faculty interested in cycling). Furthermore, learning relations between these types of pages (advised-by, investigatorof) can enable powerful searches such as "find faculty supervising more than the departmental average of Masters students."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistie models for text learning</head><p>Suppose there are m classes or topics c l , . . . , cm, with some training documents Dc associated with each class c. The prior probability of a class is usually estimated as the fraction of documents in it, i.e., IDol/~o IDol. Let T be the universe of terms, or the vocabulary, in all the training documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Naive Bayes classification</head><p>The bag-of-words models readily enable naive Bayes classification <ref type="bibr" target="#b13">[13]</ref> ('naive' indicates the assumption of independence between terms).</p><p>One may assume that for each class c, there is a binary text generator model. The model parameters are ~b~,t which indicates the probability that a document in class c will mention term t at least once. With this definition, Pr(dlc) = H ¢~,t r I (1-the,t).</p><p>( In the multinomial model, each class has an associated die with T faces. The ¢ parameters are replaced with 0c,t, the probability of the face t E T turning up on tossing the die ( §2.1). The conditional probability of document d being generated from a class c is</p><formula xml:id="formula_1">Vr(dlc ) _--( Ildll 1 ~ l-l'n=(d, t) \{n(d, t ) } ] x • ' o , , ' (:") ted where [ IIdlh ~ Ildl[~! is the multinomial coef- ~{n(d,t)}) ~-n(d,tl)!n(d,t2)!'" ficient.</formula><p>Note that short documents are encouraged, in particular, the empty document is most likely. Not only is inter-term correlation ignored, but each occurrence of term t results in a multiplicative O~,t 'surprise' factor. The multinomial model is usually prefered to the binary model because the additional term count information usually leads to higher accuracy <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Both models are surprisingly effective given their crudeness; although their estimate of Pr(dlc ) must be terrible, their real task is to pick argmaxe Pr(cld), and the poor estimate of Pr(dlc) appears to matter less than one would expect <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b26">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Parameter smoothing and feature selection</head><p>Many of the terms t E T will occur in only a few classes. The maximum likelihood (ML) estimates of 0c,t (defined as ~deDe n(d, t ) / ~ ~dEDe n(d, ~-)) will be zero for all classes c where t does not occur, and so Pr(cld ) will also be zero even if a test document d contains even one such term. 'Ib avoid this in practice, the ML estimate is often replaced by the Laplace corrected estimate <ref type="bibr" target="#b49">[47,</ref><ref type="bibr" target="#b64">61]</ref>:</p><formula xml:id="formula_2">1 + ~dEDe n(d,t) Oc,t = ITI + E , EdeD, n(d,v)" (3)</formula><p>Since in the binary case there are two outcomes instead of T outcomes, the corresponding correction is</p><formula xml:id="formula_3">~b~,~ = l + [ { d E D ~: t E d } [<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">+ IDol</head><p>Unfortunately the second form often leads to a gross overestimation of Pr(tlc) for sparse classes (small D~). Other techniques for parameter smoothing, which use information from related classes (e.g., parent or sibling in a topic hierarchy) are discussed in §3.1.5.</p><p>Many terms in T are quite useless and even potentially harmful for classification. 'Stopwords' such as 'a,' 'an,' 'the,' are likely candidates, although one should exercise caution in such judgement: 'can' (verb) is often regarded as a stopword, but 'can' (noun) is hardly a stopword in the context of h t t p : //dmoz. org/Science/Environment/Pollut ion_Prevention_ and_Recycling/, which may specialize into subtopics re- lated to cans, bottles, paper, etc. (and therefore 'can' might be an excellent feature at this level). Thus, stopwords are best determined by statistical testing on the corpus at hand w.r.t, the current topic context.</p><p>Various techniques can be used to grade terms on their "discriminating power" <ref type="bibr" target="#b75">[72]</ref>. One option is to use mutual information <ref type="bibr" target="#b19">[19]</ref>. Another option is to use class-preserving projections such as the Fisher index <ref type="bibr" target="#b27">[27]</ref>.</p><p>Ranking the terms gives some guidance on which to retain in T and which to throw out as 'noise.' One idea commonly used is to order the terms best first and retain a prefix. This option can be made near-linear in the number of initial features and I / O efficient <ref type="bibr" target="#b14">[14]</ref>. However this static ranking does not recognize that selecting (or discarding) a term may lead to a change in the desirability score of another term. More sophisticated (and more time-consuming) feature selection techniques based on finding Markov blankets in Bayesian networks <ref type="bibr" target="#b38">[38]</ref> have been proposed <ref type="bibr" target="#b44">[44]</ref>.</p><p>Experiments suggest that feature selection yields a significant improvement for the binary naive Bayes classifier and a moderate improvement for the multinomial naive Bayes classifier <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b75">72,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b53">51]</ref>. Performing some form of feature selection is important; the results are not too sensitive to the exact term scoring measures used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Limited dependence modeling</head><p>The most general model of term dependence that can be used in this context is a Bayesian network <ref type="bibr" target="#b38">[38]</ref> of the following form: there is a single node that encodes the class of the document; edges from this node go to 'feature' terms chosen from T, these term nodes are arbitrarily connected with each other (but there are no directed cycles); and there are no other edges in the Bayesian network. Deriving the edges among T from training data is made difficult by the enormous dimensionality of the problem space. IT I is usually large, in the range of tens of thousands of terms. One approach to cut down the complexity (to slightly worse than IT] 2) is to insist that each node in T has at most d ancestors, and pick them greedily based on pre-estimated pairwise correlation between terms <ref type="bibr" target="#b44">[44]</ref>. As in simple greedy approaches, it is possible to discard a large fraction (say over 2/3) of T and yet maintain classification accuracy; in some cases, discarding noisy features improved accuracy by 5%. However, further experimental analyses on larger text data sets are needed to assess the impact of modeling term dependence on classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Other advanced models</head><p>Two other techniques have emerged in recent research that capture term dependence. The maximum entropy technique regards individual term occurrence rates as marginal probabilities that constrain the cell probabilities of a gigantic contingency table pg~tentiaily involving any subset of terms. Operationally there are similarities with Bayesian networks in the sense that one has to choose which regions of thiS rather large contingency table to estimate for subsequent use in classification. In experiments, improvements in accuracy over naive Bayes have been varied and dependent on specific data sets <ref type="bibr" target="#b60">[57]</ref>.</p><p>Thus far we have been discussing distribution-based classifters which characterize class-conditional term distributions to estimate likely generators for test documents. Another approach is to use separator-based classifiers that learn separating surfaces between the classes. Decision trees and perceptrons are examples of separator-based classifiers. Support vector machines (SVMs) <ref type="bibr" target="#b72">[69]</ref> have yielded some of the best accuracy to date. SVMs seek to find a separator surface between classes such that a band of maximum possible thickness of the region (also called the margin) around the separator is empty, i.e., has no training points. This leads to better generalization <ref type="bibr" target="#b62">[59,</ref><ref type="bibr" target="#b42">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Hierarchies over class labels</head><p>In the traditional warehouse mining domain, class labels are drawn from a small discrete set, e.g., "fraudulent credit card transaction" vs. "normal transaction." Sometimes a mild ordering between the labels is possible, e.g., "low risk," "medium risk" and "high risk" patients. Topic directories such as Yahoo! provide a more complex scenario: the class labels have a large hierarchy defined on them, with the common interpretation that if c i s -a co, all training documents belonging to c are also examples of co. (The Yahoo! is a directed acyclic graph, not a tree; but we restrict our discussion to trees only for simplicity.) The role of the classifier in this setting is open to some discussion; a common goal is to find the leaf node in the class hierarchy which has the highest posterior probability.</p><p>The common formulation in this setting is that Pr(rootld ) ----1 for any document d, and if co is the parent of c l , . . . , cm, then Pr(cold) = ~i Pr(c~ Id). Using the chain rule, Pr(cild) --Pr(cold)Pr(cilco,d), one starts at the root and computes posterior probabilities for all classes in the hierarchy via a best-first search, always expanding that c with the highest Pr(cld ) and collecting unexpanded nodes in a priority queue until sufficiently many leaf classes are obtained. An approximation to this is the 'Pachinko machine' <ref type="bibr" target="#b46">[45]</ref> which, starting at the root, selects in each step the most likely child of the current class until it reaches a leaf.</p><p>Another issue that arises with a taxonomy is that of contextdependent feature selection. As mentioned before in §3.1.2, the set of features useful for discrimination between classes may vary significantly, depending on co. This has been observed in a set of experiments <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b46">45]</ref> across different data sets. Surprisingly few terms (tens to hundreds) suffice at each node in the topic hierarchy to build models which are as good (or slightly better) as using the whole lexicon. Bayesian classifiers in this domain tend to have high bias and rarely overfit the data; they are quite robust to moderate excesses in parameters. Therefore the major benefit is in storage cost and speed <ref type="bibr" target="#b14">[14]</ref>. For a data set such as Yahoo!, feature selection may make the difference between being able to hold the classifier model data in memory or not.</p><p>Yet another utility of a class hierarchy is in improved model estimation. Sparsity of data is always a problem in the text domain, especially for rare classes. Information gleaned indirectly from the class models of better populated ancestors and siblings in the taxonomy may be used to 'patch' local term distributions. One such approach is s h r i n k a g e <ref type="bibr" target="#b56">[53]</ref>. Given a topic tree, hierarchy, its root is made the only child of a special 'class' co where all terms are equally likely (Oco,~ = 1/T for all t E T). Consider a path co,c1,... ,ck up to a leaf Ck and suppose we need to estimate the 0-parameters of the classes on this path. The set of training documents Dc~ for leaf ck is used as is. For ck-1, the training set used is Dc~_l \ Dc~, i.e., documents in Dck are discarded so that the estimates become independent. This process is continued up the path. From the training data, the maximum likelihood estimates of 0c,t are computed; this is similar to equation (3), after dropping the ' 1 + ' and 'IT[+' terms. Shrinkage involves estimating a weight vector A = (Ao,... , Ak) such that the 'corrected' a parameters for Ck are</p><formula xml:id="formula_4">= (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>o_&lt;i&lt;k</head><p>A is estimated empirically by maximizing the likelihood of a held-out portion of the training data using an Expectation Maximization (EM) framework <ref type="bibr" target="#b24">[24]</ref> (also see §5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning relations</head><p>Classifying documents into topics can be extended into learning relations between pages. For example, in trying to classify pages at a college into faculty, student, project, and course pages, one may wish to learn relations like teaches(faculty, course), advises(faculty, student), enrolled(student, course), etc. In turn, learning such relations may improve the accuracy of node classification.</p><p>Learning relations between hypertext pages involve a combination of statistical and relational learning. The former component can analyze textual features while the latter can exploit hyperlinks. The idea is to augment FOIL-based learner <ref type="bibr" target="#b52">[50]</ref> with the ability to invent predicates, using a naive Bayes text classifier. Due to its relational component, it can represent hyperlink graph structure and the word statistics of the neighbouring documents. At the same time, using a statistical method for text implies that the learned rules will not be dependent on the presence or absence of specific keywords as would be the case with a purely relational method <ref type="bibr" target="#b52">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unsupervised learning</head><p>In unsupervised learning <ref type="bibr" target="#b41">[41]</ref> of hypertext, the learner is given a set of hypertext documents, and is expected to discover a hierarchy among the documents based on some notion of similarity, and organize the documents along that hierarchy. A good clustering will collect similar documents together near the leaf levels of the hierarchy and defer merging dissimilar subsets until near the root of the hierarchy. Clustering is used to enhance search, browsing, and visualization <ref type="bibr" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic clustering techniques</head><p>Clustering is a fundamental operation in structured data domains, and has been intensely studied. Some of the existing techniques, such as k-means <ref type="bibr" target="#b41">[41]</ref> and hierarchical agglomerative clustering <ref type="bibr" target="#b21">[21]</ref> can be applied to documents. Typically, documents are represented ill unweighted or TFIDF vector space, and the similarity between two documents is the cosine of the angle between their corresponding vectors, or the distance between the vectors, provided their lengths are normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">k-means clustering</head><p>The number k of clusters desired is input to the k-means algorithm, which then picks k 'seed documents' whose coordinates in vector space are initially set arbitrarily. Iteratively, each input document is 'assigned to' the most similar seed. The coordinate of the seed in the vector space is recomputed to be the centroid of all the documents assigned to that seed. This process is repeated until the seed coordinates stabilize.</p><p>The extreme high dimensionality of text creates two problems for the top-down k-meaus approach. Even if each of 30000 dimensions has only two possible values, the number of input documents will always be too small to populate each of the 230000 possible cells in the vector space. Hence most dimensions are unreliable for similarity computations. But since this is not a supervised problem, it is hard to detect them a priori. There exist bottom-up techniques to determine orthogonal subspaces of the original space (obtained by projecting out other dimensions) such that the clustering in those subspaces can be characterized as 'strong' <ref type="bibr" target="#b0">[1]</ref>. However these techniques cannot deal with the tens of thousands of dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Agglomerative clustering</head><p>In agglomerative or bottom-up clustering, documents are continually merged into super-documents or groups until only one group is left; the merge sequence generates a hierarchy based on similarity. In one variant <ref type="bibr" target="#b21">[21]</ref>, the selfsimilarity of a group F is defined as</p><formula xml:id="formula_5">1 s(r) = irl(ir I_ 1) ~ s(dl,d2),<label>(6)</label></formula><p>dl ,d2 E F,dl ~d 2</p><p>where s(dl, d2) is the similarity between documents dl and d2, often defined as the cosine of the angle between the vectors corresponding to these documents in TFIDF vector space. (Many other measures of similarity have been used.) The algorithm initially places each document into a group by itself. While there is more than one group the algorithm looks for a F and a A such as s(F U •) is maximized, and merges these two groups. This algorithm takes time that is worse than quadratic in the number of initial documents. To scale it up to large collections, a variety of sampling techniques exist <ref type="bibr" target="#b20">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Techniques from linear algebra</head><p>The vector space model and related representations suggest that linear transformations to documents and terms, regarded as vectors in Euclidean space, may expose interesting structure. We now discuss some applications of linear algebra to text analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Latent semantic indexing</head><p>The similarity between two documents in the vector space model is a syntactic definition involving those two documents alone. (In TFIDF weighted vector space other documents do exert some influence through IDF.) However, indirect evidence often lets us build semantic connections between documents that may not even share terms. For example, 'car' and 'auto' co-occurring in a document may lead us to believe they are related. This may help us relate a document mentioning 'car' and 'gearbox' with another document mentioning 'auto' and 'transmission,' which may in turn lead us to believe 'gearbox' and 'transmission' are related. Latent Semantic Indexing (LSI) <ref type="bibr" target="#b23">[23]</ref> formalizes this intuition using notions from linear algebra. Consider the termby-document matrix A where aij is 0 if term i does not occur in document j, and 1 otherwise (word counts and TFIDF weighting may also be used). Now if 'car' and 'auto' are related, we would expect them to occur in similar sets of documents, hence the rows corresponding to these words should have some similarity. Extending this intuition we might suspect that A may have a rank r far lower than its dimensions; many rows and/or columns may be somewhat 'redundant.'</p><p>Let the singular values <ref type="bibr" target="#b34">[34]</ref> of A (the eigenvalues of AA T) be al,... ,at, where Jail &gt; .--&gt; ]ar[. The Singular Value Decomposition (SVD) <ref type="bibr" target="#b34">[34]</ref> factorizes A into three matrices UDV T, where D = diag(al,... ,at), and U and V are column-orthogonal matrices. LSI retains only some first k singular values together with the corresponding rows of U and V, which induce an approximation to A, denoted Ak = UkDkV[ (k is a tuned parameter). Each row of Uk represents a term as a k-dimensional vector; similarly each row of Vk represents a document as a k-dimensional vector.</p><p>Among many things, this modified representation makes for a new search method: map a keyword query to a kdimensional vector, and find documents which are nearby. (For multi-word queries one may take the centroid of the word vectors as the query.) Heuristically speaking, the hope is that the rows in U corresponding to 'car' and 'auto' will look very similar, and a query on 'car' will have a nonzero match with a document containing the word 'auto.' Improvement in query performance has indeed been observed <ref type="bibr" target="#b23">[23]</ref>.</p><p>In the Signal Processing community, SVD and truncation has been used for decades for robust model fitting and noise reduction <ref type="bibr" target="#b59">[56]</ref>. Analogously for text, it has been shown that LSI extracts semantic clusters in spite of noisy mappings from clusters to terms <ref type="bibr" target="#b61">[58]</ref>. Apart from search, LSI can also be used to preprocess the data (eliminating noise) for clustering and visualization of document collection in very low-dimensional spaces (2-or 3-d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Random projections</head><p>In either k-means or agglomerative clustering, a significant fraction of the running time is spent in computing similarities between documents and document groups. Although individual documents can be expected to have bounded length, cluster centroids and document groups become dense as they collect contributions from more and more documents during the execution of these algorithms.</p><p>There is a theorem which establishes that a projection of a set of points to a randomly oriented subspace induces small distortions in most inter-point distances with high probability. More precisely, let v E I:L '~ be a unit vector and H a randomly oriented e-dimensional subspace through the origin, and let X be the square of the length of the projection of v on to ~ (X is thus a random variable).</p><p>Then E[X] = £/n and ff £ is chosen between fl(log n) and O(v'B), Pr(IXe/n[ &gt; ee/n) &lt; 2v/~exp(-(e-1)e2/4), where 0 &lt; e &lt; 1/2 <ref type="bibr" target="#b30">[30]</ref>. It is easy to see that this implies small distortion in inter-point distances and inner products.</p><p>This theorem can be used to develop a technique for reducing the dimensionality of the points so as to speed up the distance computation inherent in clustering programs. It has been proven <ref type="bibr" target="#b61">[58]</ref> that the quality of the resulting clustering is not significantly worse than a clustering derived from the points in the original space. In practice, simpler methods such as truncating the document or cluster centroid vectors to retain only the most frequent terms (i.e., the largest orthogonal components) perform almost as well as projection <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b69">66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Semi-supervised learning</head><p>Supervised learning is a goal-directed activity which can be precisely evaluated, whereas unsupervised learning is open to interpretation. On the other hand, supervised learning needs training data which must be obtained through human effort. In real life, most often one has a relatively small collection of labeled training documents, hut a larger pool of unlabeled documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning from labeled and unlabeled documents</head><p>Clearly, term sharing and similarity between labeled and unlabeled documents is a source of information that may lead to increased classification accuracy. This has indeed been verified using a simple algorithm patterned after Expectation Maximization (EM) <ref type="bibr" target="#b24">[24]</ref>. The algorithm first trains a naive Bayes classifier using only labeled data. Thereafter each EM iteration consists of the E-step and M-step. We restrict the discussion to naive Bayesian classifiers. In the E-step one estimates 0c,t using a variant of equation ( <ref type="formula">3</ref>):</p><formula xml:id="formula_6">\ 1 + Ed n(d, t) Pr(c[d) Oe,t = [T[ + ~dPr(c[d) ~.n(d,~-)" (7)</formula><p>The modification is that documents do not belong deterministically to a single class, but may belong probabilistically to many classes. (This distribution will be degenerate for the initial labeled documents.) In the M-step, the O¢,t estimates are used to assign class probabilities Pr(cld ) to all documents not labeled as part of th input. These EM iterations continue until (near-) convergence. Results are mostly favorable compared to naive Bayes alone: error is reduced by a third in the best cases, but care needs to be taken in modeling classes as mixtures of term distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relaxation labeling</head><p>In supervised topic learning from text, distribution-based classifiers posit a class-conditional term distribution Pr(t]c) and use this to estimate the posterior class probabilities Pr(c]d) for a given document. Consider now the hypertext model in §2.2, where training and testing documents are nodes in a hypertext graph. It seems obvious that apart from terms in the training and testing documents, there are other sources of information induced by the links, but it is unclear how to exploit them.</p><p>One can adopt the notion, motivated by spreading activation techniques, that a term 'diffuses' to documents in the local link neighborhood of the document where it belongs; this has been proved useful in hypertext IR <ref type="bibr" target="#b67">[64]</ref>. In our context, it means upgrading our model to a class-conditional distribution over not only terms in the current document d, but also neighboring documents N(d) (~ome suitable radius can be chosen to determine N(d)), viz., Pr(t(d), ~N(d))lc ).</p><p>Operationally, it means that terms in neighbors of a training document dl contribute to the model for the class of dl, and terms in neighbors of testing document d2 may be used to estimate the class probabilities for d2. (One may apply a heuristic damping function which limits the radius of influence of a term.) Interestingly, this does not lead to better accuracy in experiments <ref type="bibr" target="#b16">[16]</ref>. The reason, in hindsight, is that content expansion is not the only purpose of citation; in other words, it is not always the case that with respect to a set of top-its, hypertext forms very tightly isolated clusters. In the US Patents database, for example, one often finds citations from patents from the class 'amplifiers' to the class 'oscillators' (but none to 'pesticides'). Thus one is led to refine the model of linkages to posit a class conditional joint distribution on terms in a document and the classes of neighbors, denoted by Pr({(d),c~N(d))[c). Note that there is a circulaxity involving class labels.</p><p>One approach to resolve the circularity is to initially use a text-only classifier to assign initial class probabilities Pr(o)(c[d) to all documents in a suitable neighborhood of the test document, then iteratively compute, for each node at step r + 1,</p><formula xml:id="formula_7">Pr (~+1)(c[d, N(d)) = Pr(~)c~N(d))Pr@)(c[d, ff(Y(d))), (8) eC~(d))</formula><p>where the sum is over all possible neighborhood class assignments. (In practice one truncates this sum.)</p><p>In the case where some of the nodes in N(d) are preclassified, this becomes a partially supervised learning problem. Relaxation labeling is able to smoothly improve its accuracy as this happens <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Social network analysis</head><p>The web is an example of a social network. Social networks have been extensively researched <ref type="bibr" target="#b73">[70]</ref>. Social networks are formed between academics by co-authoring, advising, serving on committees; between movie personnel by directing and acting; between musicians, football stars, friends and relatives; between people by making phone calls and transmitting infection; between papers through citation, and between web pages by hyperlinking to other web pages.</p><p>Social network theory is concerned with properties related to connectivity and distances in graphs, with diverse applications like epidemiology, espionage, citation indexing, etc. In the first two examples, one might be interested in identifying a few nodes to be removed to significantly increase average path length between pairs of nodes. In citation analysis, one may wish to identify influential or central papers; this turns out to be quite symmetric to finding good survey papers; this symmetry has been explored by Mizruchi and others <ref type="bibr" target="#b58">[55]</ref>. IR literature includes insightful studies of citation, co-citation, and influence of academic publication <ref type="bibr" target="#b50">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Applying social network analysis to the Web</head><p>Starting in 1996, a series of applications of social network analysis were made to the web graph, with the purpose of identifying the most authoritative pages related to a user query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Google</head><p>If one wanders on the Web fi)r infinite time, following a random link out of each page with probability 1 -p and jumps to a random Web page with probability p, then different pages will be visited at different rates; popular pages with many in-links will tend to be visited more often. This measure of popularity is called PageRank where '--+' means "links to" and N is the total number of nodes in the Web graph. (The artifice ofp is needed because the Web is not connected or known to be aperiodic, therefore the simpler eigenequation is not guaranteed to have a fixed point.) The G o o g l e 14 search engine simulates such a random walk on the web graph in order to estimate PageRank, which is used as a score of popularity. Given a keyword query, matching documents are ordered by this score. Note that the popularity score is precomputed independent of the query, hence Google can be potentially as fast as any relevance-ranking search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Hyperlink induced topic search (HITS)</head><p>Hyperlink induced topic search <ref type="bibr" target="#b43">[43]</ref> is slightly different: it does not crawl or pre-process the web, but depends on a search engine. A query to HITS is forwarded to a search engine such as Alta Vista, which retrieves a subgraph of the web whose nodes (pages) match the query. Pages citing or cited by these pages are also included. Each node u in this expanded graph has two associated scores h~ and an, initialized to 1. HITS then iteratively assigns</p><formula xml:id="formula_8">:= a n d ho := (10) u -:'+ v u--+v</formula><p>where ~, hu and ~v av are normalized to 1 after each iteration. The a and h scores converge respectively to the measure of a page being an authority, and the measure of' a page being a hub (a compilation of links to authorities, or a "survey paper" in bibliometric terms).</p><p>Because of the query-dependent graph construction, HITS is slower than Google. A variant of this technique has been used by Dean and Henzinger to find similar pages on the Web using link-based analysis alone <ref type="bibr" target="#b22">[22]</ref>. They improve speed by fetching the Web graph from a connectivity server which has pre-crawled substantial portions of the Web [7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Adding text information to link-based popularity</head><p>HITS's graph expansion sometimes leads to topic contamination or drift. E.g., the community of movie awards pages on the web is closely knit with highly cited (and to some extent relevant) home pages of movie companies. Although movie awards is a finer topic than movies, the top movie companies emerge as the victors upon running HITS. This is partly because in HITS (and Google) all edges in the graph have the same importance 15. Contamination can be reduced by recognizing that hyperlinks that contain award or awards near the anchor text are more relevant for this query them other edges. The Automatic Resource Compilation (ARC) and Clever systems incorporate such query-dependent modification of edge weights <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">17]</ref>. Query results are significantly improved. In user studies, the results compared favorably with lists compiled by humans, such as Yahoo! and Infoseek 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Outlier filtering</head><p>Bharat and Henzinger have invented another way to integrate textual content and thereby avoid contamination of 14http : I/google. com 15Google might be using edge-weighting strategies which are not published. 16http: I/~n~. infoseek, tom the graph to be distilled. They model each page according to the "vector space" model <ref type="bibr" target="#b66">[63]</ref>. During the graph expansion step, unlike HITS, they do not include all nodes at distance one from the preliminary query result. Instead they prune the graph expansion at nodes whose corresponding term vectors are outliers with respect to the set of vectors corresponding to documents directly retrieved from the search engine [8]. In the example above, one would hope that the response to the query movie award from the initial keyword search would contain a majority of pages related to awards and not companies; thus the distribution of keywords on these pages will enable the Bharat and Henzinger algorithm to effectively prune away as outliers nodes in the neighborhood that are about movie companies. Apart from speeding up their system by using the Connectivity Server [7], they describe several heuristics that cut down the query time substantially. It is possible to fabricate queries that demonstrate the strengths and weaknesses of each of these systems. ARC, Clever, and Outlier Filtering have been shown to be better (as judged by testers) than HITS. There has not been a systematic comparison between Google and the HITS family. This would be of great interest given the basic difference in graph construction and consequent greater speed of Google.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Resource discovery</head><p>There is short-range topic locality on the Web, in at least two prominent forms. If the reader finds this paper interesting, there is reasonable chance that the reader will also find some significant fraction of citations in this paper to be interesting too. If this paper cites a paper that the reader thinks is interesting, that makes it more likely that another paper cited by this one is interesting (i.e., this paper is a suitable hub).</p><p>Coupling the ability to find good hubs with the ability to judge how likely a given page is to be generated from a given topic of interest, one can devise crawlers that crawl the Web for selected topics <ref type="bibr" target="#b18">[18]</ref>. The goal is to crawl as many relevant pages as fast as possible while crawling as few irrelevant pages as possible. The crawler can be guided by a text or hypertext classifier. Alternatively, one may use reinforcement learning techniques <ref type="bibr" target="#b63">[60,</ref><ref type="bibr" target="#b55">52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this survey we have reviewed recent research on the application of techniques from machine learning, statistical pattern recognition, and data mining to analyzing hypertext. Starting from simple bulk models for documents and hyperlinks, researchers have progressed towards document substructure and the interplay between link and content. Mining of semistructured data has followed a slightly different path, but it seems clear that a confluence will be valuable. Commercial search products and services are slowly adopting the results of recent investigations. It seems inevitable that knowledge bases and robust algorithms from computational linguistics will play an increasingly important role in hypertext content management and mining in the future. We will also likely see increased systems and tool building as the winning research ideas become more firmly established.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[10], defined recursively as PageRank(u) Pageaank(v) = p/N + (1 -p) ~ OutDegree(u)' (9) u --t ~</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Since typicallyIT[ &gt;&gt; [d[, short  documents are discouraged by this model. Also, the second product makes strong independence assumptions and is likely to greatly distort the estimate of Pr(dlc).</figDesc><table /><note><p>1) t6d tET, t~d l l h t t p : / / ~. wS. org/XHI.,/ 12http://m~-db. s t a n f o r d , edu/-widem/xml-wh:i.tepaper. html 13hl;tp : / / ~. socks.nec, com/</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Thanks to Pushpak Bhattacharya and Yusuf Batterywala for helpful discussions and references.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic subspace clustering of high dimensional data for data mining applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<ptr target="http://~,almaden.ibm.com/cs/quest/papers/sis~nod98_cliqne.pdf" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference on Management of Data</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic hypertext link typing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th ACM Conference on Hypertext, Hypertext &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<title level="m">Natural Language Understanding. Benjamin/Cummings</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Balkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sadler</surname></persName>
		</author>
		<ptr target="http://cl~rw.essex,ac.uk/-doug/book/book,html" />
		<title level="m">Machine translation: An introductory guide</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://www.altavista" />
		<title level="m">Babelfish Language Translation Service</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A technique for measuring the relative size and overlap of public web search engines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<ptr target="http://www.research,digital,corn/SRC/whatsnew/sem.html" />
	</analytic>
	<monogr>
		<title level="m">7th World-Wide Web Conference (WWWT)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Connectivity Server: Fast access to linkage information on the Web</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brsder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<ptr target="http://decweb.ethz.ch/~/W7/1938/eom1938,him" />
	</analytic>
	<monogr>
		<title level="m">7th World Wide Web Conference</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved algorithms for topic distillation in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
		<ptr target="ftp://ftp.digital,com/pub/DEC/SRC/publications/monika/sigir98,pdf" />
	</analytic>
	<monogr>
		<title level="m">21st International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1998-08">Aug. 1998</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hypertext versions of journal articles: Computer aided linking and realistic human evaluation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Bluestein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Western Ontario</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The anatomy a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<ptr target="http://decweb.ethz.ch/W~/7/1921/com1921,him" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th World-Wide Web Conference (WWWT)</title>
		<meeting>the 7th World-Wide Web Conference (WWWT)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The role of lexicalization and pruning for base noun phrase grammars</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAA1 99</title>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">SIGKDD Explorations</title>
		<imprint>
			<date type="published" when="2000-01">Jan 2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The publishing process: the hyperbook approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Catenazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gibb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="172" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using taxonomy, discriminants, and signatures to navigate in text databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable feature selection, classification and signature generation for organizing large text databases into hierarchical topic taxonomies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<ptr target="//~r~w.cs.berkeley,edu/~soumen/VLDB54_3.PDF" />
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<date type="published" when="1998-08">Aug. 1998</date>
		</imprint>
	</monogr>
	<note>Invited paper, online at h t t p</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic resource compilation by analyzing hyperlink structure and associated text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<ptr target="http://wv~r7,scu.edu.au/programme/fullpapers/1898/com1898,html" />
	</analytic>
	<monogr>
		<title level="m">7th World-wide web conference (WWWT)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced hypertext categorization using hypertinks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<ptr target="www.cs.berkeley,edu/-soumen/sigmod98,ps" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Online at hgtp ://</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining the Web&apos;s link structure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Dora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="1999-08">Aug. 1999</date>
		</imprint>
	</monogr>
	<note>Feature article</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Focused crawling: a new approach to topic-specific web resource discovery. Computer Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Dora</surname></persName>
		</author>
		<ptr target="http://www8.org/w8-papers/5a-search-query/crawling/index,html" />
		<imprint>
			<date type="published" when="1999-05">1999. May 1999</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1623" to="1640" />
			<pubPlace>Toronto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>John Wiley and Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constant interaction-time scatter/gather browsing of very large document collections</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scatter/gather: A cluster-based approach to browsing large document collections</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding related pages in the world wide web</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Henzinger</surname></persName>
		</author>
		<ptr target="http://~w8.org" />
	</analytic>
	<monogr>
		<title level="m">8th World Wide Web Conference</title>
		<meeting><address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
		<ptr target="http://superbook.telcordia,com/&apos;remde/isi/papers/JASISgO.ps" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, B</title>
		<imprint>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What do those~weird XML types want, anyway? Keynote address, VLDB 1999</title>
		<author>
			<persName><forename type="first">S</forename><surname>Derose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-09">Sept. 1999</date>
			<pubPlace>Edinburgh, Scotland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the optimality of the simple Bayesian classifier under zero-one loss</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Dude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Parsing with Principles and Parameters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berwick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<title level="m">Information retrieval: Data structures and algorithms</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Johnson-Lindenstrauss lemma and the sphericity of some graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frankl</surname></persName>
		</author>
		<author>
			<persName><surname>H~maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, B</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="355" to="362" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On bias, variance, 0/1 loss, and the curse of dimensionality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="77" />
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>online at f t p : / / p l a y : f a i r . stanford, edu/pub/friedman/cur se. ps. Z</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Natural Language Processing in LISP</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gazder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mellish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From semistructured data to XML: Migrating the Lore data model and query language</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<ptr target="http://www-db.stanford,edu/pub/papers/xml,ps" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on the Web and Databases (WebDB &apos;99)</title>
		<meeting>the 2nd International Workshop on the Web and Databases (WebDB &apos;99)<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building newspaper links in newspaper articles using semantic similarity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language and Data Bases Conference, Vancouver, NLDB&apos;97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Introduction to Government and Binding Theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Haegeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Basil Blackwell Ltd</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cat-a-Cone: An interactive interface for specifying searches and viewing retrieval results using a large category hierarchy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Karadi</surname></persName>
		</author>
		<ptr target="ftp://parcftp.xerox,eom/pub/hearst/sigir97,ps" />
	</analytic>
	<monogr>
		<title level="m">Proceeding s of the 20th Annual International ACM/SIGIR Conference</title>
		<meeting>eeding s of the 20th Annual International ACM/SIGIR Conference<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian networds for data mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<ptr target="//ftp,research.microsoft,com/pub/dtg/david/tutorial.PSandftp://ftp,research.microsoft,com/pub/tr/TR-95-06.PS" />
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Also see URLs ftp</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An Introduction to Machine Translation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Somers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Institute of Advanced Studies. The universal networking language: Specification document</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">N U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internal Technical Document</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jaln</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Making large-scale SVM learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="http://www-ai.informatik,uni-dortmund,de/FORSCHUNG/VER/~AHREN/SVM_LIGHT/7.sv~light,eng.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods: Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schslkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<ptr target="http://~.cs.cornell,edu/home/kleinber/auth,ps" />
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Toward optimal feature selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kouer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Saitta</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Morgan-Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchically classifying documents using very few words</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kouer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Morgan-Kaufmann</surname></persName>
		</author>
		<ptr target="http://robotics.stanford,edu/users/sahami/papers-dir/m197-hier,ps" />
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic hypertext creation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kumarvel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering Department, IIT Bombay</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M. Teeh Thesis</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Philosophical Essays on Probabilities</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Laplace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Translated by A. I. Dale from the 5th French edition of 1825</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bibliometrics of the world wide web: An exploratory analysis of the intellectual structure of cyberspace</title>
		<author>
			<persName><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
		<ptr target="http://sherlock.berkeley.edu/asis96/asis96.h%ml" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the American Society for Information Science</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Accessibility of information on the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="107" to="109" />
			<date type="published" when="1999-07">July 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">First order learning for web mining</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mark Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">lOth European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive Bayes text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<idno>WS-98-05</idno>
		<ptr target="http://www.cs.cmu.edu/-knigam/papers/multinomial-aaaiws98.pdf" />
	</analytic>
	<monogr>
		<title level="m">AAAI/ICML-98 Workshop on Learning for Text Categorization</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Also technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m">SIGKDD Explorations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Building domain-specific search engines with machine learning techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
		<ptr target="http://ww,cs.cmu.edu/&apos;mccallum/papers/cora-aaaiss99,ps.gz" />
	</analytic>
	<monogr>
		<title level="m">AAAI-99 Spring Symposium</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving text classification by shrinkage in a hierarchy of classes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://w~.cs.cmu.edu/-mccallum/papers/hier-icm198,ps.gz" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Lore: A database management system for semistructured data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>: Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<ptr target="http://www-db.stanford,edu/pub/papers/lore97,ps" />
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Techniques for disaggregating centrality scores in social networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Mizruchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mariolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methodology</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Tuma</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="26" to="48" />
			<date type="published" when="1986">1986</date>
			<publisher>Jossey-Bass</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Mathematical Methods and Algorithms for Signal Processing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Sterling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-08">Aug. 1999</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/-mccallum/papers/maxent-ijcaiws99,ps.gz" />
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;99 Workshop on Infor~nation Filtering</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Latent sematic indexing: A probabilistic analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Sequential minimal optimization: A fast algorithm for training support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno>MSR-TR-98-14</idno>
		<ptr target="http://~,research.microsoft.com/users/jplatt/smoTIt,pdf" />
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Using reinforcement learning to spider the web efficiently</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://~,cs.crau.edu/&apos;mccallum/papers/rlspider-icm199s,ps.gz" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A natural law of succession</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ristad</surname></persName>
		</author>
		<idno>CS-TR-495-95</idno>
		<imprint>
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research report</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Automatic Text Processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An extended vector processing scheme for searching information in hypertext systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="170" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Inference and computer understanding of natural language</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rieger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Knowledge Representation</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Braehman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A comparison of projections for efficient document clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schiitze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Silw~rstein</surname></persName>
		</author>
		<ptr target="http://www-cs-students,stanford,edu/-csilvers/papers/metrics-sigir,ps" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Annual ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Twentieth Annual ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sowa</surname></persName>
		</author>
		<title level="m">Conceptual Structures: Information Processing in Mind and Machines</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">An introduction to link grammar parser</title>
		<author>
			<persName><forename type="first">D</forename><surname>Temperley</surname></persName>
		</author>
		<ptr target="http://www.link.cs.cmu.edu/link/dict/introduction,html" />
		<imprint>
			<date type="published" when="1999-04">Apr. 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Support vector method for function approximation, regression estimation, and signal processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faust</surname></persName>
		</author>
		<title level="m">Social Network Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Language as a Cognitive Process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">He was a Research Staff Member at IBM Almaden Research Center between 1996 and 1999. At IBM he worked on hypertext analysis and information retrieval. He designed the Focused Crawler 19 and part of the Clever ~° search engine. He is currently an Assistant Professor in the Department of Computer Science and Engineering at the Indian Institute of Technology, Bombay</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
		<ptr target="IShttp://~.cse.iitb.ernet.in/-soumen/19http://~.cs.berkeley.edu/~soumen/focus2°http://~.almaden.ibm.com/cs/k53/clever.html" />
	</analytic>
	<monogr>
		<title level="m">Kharagpur, in 1991 and his M.S. and Ph.D. in Computer Science from the University of California, Berkeley in 1992 and 1996</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
		<respStmt>
			<orgName>Tech in Computer Science from the Indian Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include hypertext information retrieval, web analysis and data mining</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
