<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIFFORMER: SCALABLE (GRAPH) TRANSFORMERS INDUCED BY ENERGY CONSTRAINED DIFFUSION</title>
				<funder ref="#_9TaygcP">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_QtETHh3">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_v2fqYdF">
					<orgName type="full">STCSM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-04">4 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixuan</forename><surname>He</surname></persName>
							<email>yixuan.he@stats.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
							<email>davidwipf@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of CSE &amp; MoE Lab of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="department" key="dep3">Amazon Web Service</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DIFFORMER: SCALABLE (GRAPH) TRANSFORMERS INDUCED BY ENERGY CONSTRAINED DIFFUSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-04">4 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.09474v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t. a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFORMER (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction. The codes are available at https://github.com/qitianwu/DIFFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Real-world data are generated from a convoluted interactive process whose underlying physical principles are often unknown. Such a nature violates the common hypothesis of standard representation learning paradigms assuming that data are IID sampled. The challenge, however, is that due to the absence of prior knowledge about ground-truth data generation, it can be practically prohibitive to build feasible methodology for uncovering data dependencies, despite the acknowledged significance. To address this issue, prior works, e.g., <ref type="bibr" target="#b43">Wang et al. (2019)</ref>; <ref type="bibr" target="#b17">Franceschi et al. (2019)</ref>; <ref type="bibr" target="#b22">Jiang et al. (2019)</ref>; <ref type="bibr" target="#b59">Zhang et al. (2019)</ref>, consider encoding the potential interactions between instance pairs, but this requires sufficient degrees of freedom that significantly increases learning difficulty from limited labels <ref type="bibr" target="#b16">(Fatemi et al., 2021)</ref> and hinders the scalability to large systems <ref type="bibr">(Wu et al., 2022b)</ref>.</p><p>Turning to a simpler problem setting where putative instance relations are instantiated as an observed graph, remarkable progress has been made in designing expressive architectures such as graph neural networks (GNNs) <ref type="bibr" target="#b35">(Scarselli et al., 2008;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b41">Velickovic et al., 2018;</ref><ref type="bibr" target="#b46">Wu et al., 2019;</ref><ref type="bibr">Chen et al., 2020a;</ref><ref type="bibr" target="#b52">Yang et al., 2021)</ref> for harnessing inter-connections between instances as a geometric prior <ref type="bibr" target="#b3">(Bronstein et al., 2017)</ref>. However, the observed relations can be incomplete/noisy, due to error-prone data collection, or generated by an artificial construction independent from downstream targets. The potential inconsistency between observation and the underlying data geometry would presumably elicit systematic bias between structured representation of graph-based learning and true Figure <ref type="figure">1</ref>: An illustration of the general idea behind DIFFORMER which takes a whole dataset (or a batch) of instances as input and encodes them into hidden states through a diffusion process aimed at minimizing a regularized energy. This design allows feature propagation among arbitrary instance pairs at each layer with optimal inter-connecting structures for informed prediction on each instance. data dependencies. While a plausible remedy is to learn more useful structures from the data, this unfortunately brings the previously-mentioned obstacles to the fore.</p><p>To resolve the dilemma, we propose a novel general-purpose encoder framework that uncovers data dependencies from observations (a dataset of partially labeled instances), proceeding via two-fold inspiration from physics as illustrated in Fig. <ref type="figure">1</ref>. Our model is defined through feed-forward continuous dynamics (i.e., a PDE) involving all the instances of a dataset as locations on Riemannian manifolds with latent structures, upon which the features of instances act as heat flowing over the underlying geometry <ref type="bibr" target="#b19">(Hamzi &amp; Owhadi, 2021)</ref>. Such a diffusion model serves an important inductive bias for leveraging global information from other instances to obtain more informative representations. Its major advantage lies in the flexibility for the diffusivity function, i.e., a measure of the rate at which information spreads <ref type="bibr" target="#b33">(Rosenberg &amp; Steven, 1997)</ref>: we allow for feature propagation between arbitrary instance pairs at each layer, and adaptively navigate this process by pairwise connectivity weights. Moreover, for guiding the instance representations towards some ideal constraints of internal consistency, we introduce a principled energy function that enforces layer-wise regularization on the evolutionary directions. The energy function provides another view (from a macroscopic standpoint) into the desired instance representations with low energy that are produced, i.e., soliciting a steady state that gives rise to informed predictions on unlabeled data.</p><p>As a justification for the tractability of above general methodology, our theory reveals the underlying equivalence between finite-difference iterations of the diffusion process and unfolding the minimization dynamics for an associated regularized energy. This result further suggests a closed-form optimal solution for the diffusivity function that updates instance representations by the ones of all the other instances towards giving a rigorous decrease of the global energy. Based on this, we also show that the energy constrained diffusion model can serve as a principled perspective for unifying popular models like MLP, GCN and GAT which can be viewed as special cases of our framework.</p><p>On top of the theory, we propose a new class of neural encoders, Diffusion-based Transformers (DIFFORMER), and its two practical instantiations: one is a simple version with O(N ) complexity (N for instance number) for computing all-pair interactions among instances; the other is a more expressive version that can learn complex latent structures. We empirically demonstrate the success of DIFFORMER on a diverse set of tasks. It outperforms SOTA approaches on semi-supervised node classification benchmarks and performs competitively on large-scale graphs. It also shows promising power for image/text classification with low label rates and predicting spatial-temporal dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph-based Semi-supervised Learning. Graph-based SSL <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> aims to learn from partially labeled data, where instances are treated as nodes and their relations are given by a graph. The observed structure can be leveraged as regularization for learning representations <ref type="bibr" target="#b1">(Belkin et al., 2006;</ref><ref type="bibr" target="#b45">Weston et al., 2012;</ref><ref type="bibr" target="#b53">Yang et al., 2016)</ref> or as an inductive bias of modern GNN architectures <ref type="bibr" target="#b35">(Scarselli et al., 2008)</ref>. However, there frequently exist situations where the observed structure is unavailable or unreliable <ref type="bibr" target="#b17">(Franceschi et al., 2019;</ref><ref type="bibr" target="#b22">Jiang et al., 2019;</ref><ref type="bibr">Chen et al., 2020c;</ref><ref type="bibr" target="#b16">Fatemi et al., 2021;</ref><ref type="bibr" target="#b27">Lao et al., 2022)</ref>, in which case the challenge remains how to uncover the underlying relations. This paper explores a new Transformer-like encoder for discovering data geometry to promote learning through the inter-dependence among instances (either labeled or unlabeled).</p><p>Neural Diffusion Models. Several recent efforts explore diffusion-based learning where continuous dynamics serve as an inductive bias for representation learning <ref type="bibr" target="#b19">(Hamzi &amp; Owhadi, 2021)</ref>. These works can be generally grouped into PDE-based learning, where the model itself is a continuous diffusion process described by a differential equation (e.g., <ref type="bibr">Chamberlain et al. (2021a)</ref>; <ref type="bibr">Eliasof et al. (2021)</ref>; <ref type="bibr">Thorpe et al. (2022)</ref>), and PDE-inspired learning, where the diffusion perspective is adopted to develop new models <ref type="bibr" target="#b0">(Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b25">Klicpera et al., 2019)</ref> or characterize graph geometric properties <ref type="bibr" target="#b50">(Yang et al., 2022)</ref>. Our work leans on the later category and the key originality lies in two aspects. First, we introduce a novel diffusion model whose dynamics are implicitly defined by optimizing a regularized energy. Second, our theory establishes an equivalence between the numerical iterations of diffusion process and unfolding the optimization of the energy, based on which we develop a new class of neural encoders for uncovering latent structures among a large number of instances.</p><p>Transformers. Transformers serve as a model class of wide research interest showing competitive efficacy for modeling the dependency among tokens of inputs through all-pair attention mechanism. While the original architecture <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> is motivated by the inter-dependence among words of a sentence in NLP tasks, from different aspects, this work targets a Transformer-like architecture for modeling the inter-dependence among instances in a dataset. In the terminology of Transformers in NLPs, one can treat the whole dataset as an extremely long 'sequence' (with the length equaling to the dataset size), and each instance (with a label to predict) acts as the 'token' in such a sequence. In the context of graph machine learning, our goal can be framed as learning latent interaction graphs among nodes beyond the observed graph (if available), and this can be generally viewed as an embodiment of node-level prediction <ref type="bibr" target="#b20">(Hu et al., 2020)</ref>. For the latter case, critically though, it remains under-explored how to build a scalable and expressive Transformer for learning node-pair interactions given the prohibitively large number of node instances <ref type="bibr">(Wu et al., 2022b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ENERGY CONSTRAINED GEOMETRIC DIFFUSION TRANSFORMERS</head><p>Consider a set of partially labeled instances {x i } N i=1 , whose labeled portion is {(x j , y j )} M j=1 (often M N ). In some cases there exist relational structures that connect instances as a graph G = (V, E), where the node set V contains all the instances and the edge set E = {e ij } consists of observed relations. Without loss of generality, the main body of this section does not assume graph structures as input, but we will later discuss how to trivially incorporate them if they are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GEOMETRIC DIFFUSION MODEL</head><p>The starting point of our model is a diffusion process that treats a dataset of instances as a whole and produces instance representations through information flows characterized by an anisotropic diffusion process, which is inspired by an analogy with heat diffusion on a Riemannian manifold <ref type="bibr" target="#b33">(Rosenberg &amp; Steven, 1997)</ref>. We use a vector-valued function z i (t) : [0, ?) ? R d to define an instance's state at time t and location i. The anisotropic diffusion process describes the evolution of instance states (i.e., representations) via a PDE with boundary conditions <ref type="bibr" target="#b18">(Freidlin &amp; Wentzell, 1993;</ref><ref type="bibr" target="#b29">Medvedev, 2014)</ref>:</p><formula xml:id="formula_0">?Z(t) ?t = ? * (S(Z(t), t) ?Z(t)) , s. t. Z(0) = [x i ] N i=1 , t ? 0,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">Z(t) = [z i (t)] N i=1 ? R N ?d</formula><p>, denotes the Hadamard product, and the function S(Z(t), t) : R N ?d ? [0, ?) ? [0, 1] N ?N defines the diffusivity coefficient controlling the diffusion strength between any pair at time t. The diffusivity is specified to be dependent on instances' states. The gradient operator ? measures the difference between source and target states, i.e., (?Z(t)) ij = z j (t) -z i (t), and the divergence operator ? * sums up information flows at a point, i.e., (? * ) i = N j=1 S ij (Z(t), t) (?Z(t)) ij . Note that both operators are defined over a discrete space consisting of N locations. The physical implication of Eq. 1 is that the temporal change of heat at location i equals to the heat flux that spatially enters into the point. Eq. 1 can be explicitly written as</p><formula xml:id="formula_2">?z i (t) ?t = N j=1 S ij (Z(t), t)(z j (t) -z i (t)).<label>(2)</label></formula><p>Such a diffusion process can serve as an inductive bias that guides the model to use other instances' information at every layer for learning informative instance representations. We can use numerical methods to solve the continuous dynamics in Eq. 2, e.g., the explicit Euler scheme involving finite differences with step size ? , which after some re-arranging gives:</p><formula xml:id="formula_3">z (k+1) i = ? ? 1 -? N j=1 S (k) ij ? ? z (k) i + ? N j=1 S (k) ij z (k) j .<label>(3)</label></formula><p>The numerical iteration can stably converge for ? ? (0, 1). We can adopt the state after a finite number K of propagation steps and use it for final predictions, i.e., ?i = MLP(z</p><formula xml:id="formula_4">(K) i</formula><p>).</p><p>Remark. The diffusivity coefficient in Eq. 1 is a measure of the rate at which heat can spread over the space <ref type="bibr" target="#b33">(Rosenberg &amp; Steven, 1997)</ref>. Particularly in Eq. 2, S(Z(t), t) determines how information flows over instances and the evolutionary direction of instance states. Much flexibility remains for its specification. For example, a basic choice is to fix S(Z(t), t) as an identity matrix which constrains the feature propagation to self-loops and the model degrades to an MLP that treats all the instances independently. One could also specify S(Z(t), t) as the observed graph structure if available in some scenarios. In such a case, however, the information flows are restricted by neighboring nodes in a graph. An ideal case could be to allow S(Z(t), t) to have non-zero values for arbitrary (i, j) and evolve with time, i.e., the instance states at each layer can efficiently and adaptively propagate to all the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DIFFUSION CONSTRAINED BY A LAYER-WISE ENERGY</head><p>As mentioned previously, the crux is how to define a proper diffusivity function to induce a desired diffusion process that can maximize the information utility and accord with some inherent consistency. Since we have no prior knowledge for the explicit form or the inner structure of S (k) , we consider the diffusivity as a time-dependent latent variable and introduce an energy function that measures the presumed quality of instance states at a given step k:</p><formula xml:id="formula_5">E(Z, k; ?) = Z -Z (k) 2 F + ? i,j ?( z i -z j 2 2 ),<label>(4)</label></formula><p>where ? : R + ? R is defined as a function that is non-decreasing and concave on a particular interval of our interest, and promotes robustness against large differences <ref type="bibr" target="#b52">(Yang et al., 2021)</ref> among any pair of instances. Eq. 4 assigns each state in R d with an energy scalar which can be leveraged to regularize the updated states (towards lower energy desired). The weight ? trades two effects: 1) for each instance i, the states not far from the current one z (k) i have low energy; 2) for all instances, the smaller differences their states have, the lower energy is produced.</p><p>Remark. Eq. 4 can essentially be seen as a robust version of the energy introduced by <ref type="bibr" target="#b61">Zhou et al. (2004)</ref>, inheriting the spirit of regularizing the global and local consistency of representations. "Robust" here particularly implies that the ? adds uncertainty to each pair of the instances and could implicitly filter the information of noisy links (potentially reflected by proximity in the latent space).</p><p>Energy Constrained Diffusion. The diffusion process describes the microscopic behavior of instance states through evolution, while the energy function provides a macroscopic view for quantifying the consistency. In general, we expect that the final states could yield a low energy, which suggests that the physical system arrives at a steady point wherein the yielded instance representations have absorbed enough global information under a certain guiding principle. Thereby, we unify two schools of thoughts into a new diffusive system where instance states would evolve towards producing lower energy, e.g., by finding a valid diffusivity function. Formally, we aim to find a series of S (k) 's whose dynamics and constraints are given by</p><formula xml:id="formula_6">z (k+1) i = ? ? 1 -? N j=1 S (k) ij ? ? z (k) i + ? N j=1 S (k) ij z (k) j s. t. z (0) i = x i , E(Z (k+1) , k; ?) ? E(Z (k) , k -1; ?), k ? 1.</formula><p>(5)</p><p>The formulation induces a new class of geometric flows on latent manifolds whose dynamics are implicitly defined by optimizing a time-varying energy function (see Fig. <ref type="figure">1</ref> for an illustration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRACTABILITY OF SOLVING DIFFUSION PROCESS WITH ENERGY MINIMIZATION</head><p>Unfortunately, Eq. 5 is hard to solve since we need to infer the value for a series of coupled S (k) 's that need to satisfy K inequalities by the energy minimization constraint. The key result of this paper is the following theorem that reveals the underlying connection between the geometric diffusion model and iterative minimization of the energy, which further suggests an explicit closed-form solution for S (k) based on the current states Z (k) that yields a rigorous decrease of the energy. Theorem 1. For any regularized energy defined by Eq. 4 with a given ?, there exists 0 &lt; ? &lt; 1 such that the diffusion process of Eq. 3 with the diffusivity between pair (i, j) at the k-th step given by</p><formula xml:id="formula_7">?(k) ij = ? (k) ij N l=1 ? (k) il , ? (k) ij = ??(z 2 ) ?z 2 z 2 = z (k) i -z (k) j 2 2 , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>yields a descent step on the energy, i.e., E(Z (k+1) , k; ?) ? E(Z (k) , k -1; ?) for any k ? 1.</p><p>Theorem 1 suggests the existence for the optimal diffusivity in the form of a function over the l 2 distance between states at the current step, i.e., z</p><formula xml:id="formula_9">(k) i -z (k) j 2 .</formula><p>The result enables us to unfold the implicit process and compute S (k) in a feed-forward way from the initial states. We thus arrive at a new family of neural model architectures with layer-wise computation specified by: Diffusivity Inference:</p><formula xml:id="formula_10">?(k) ij = f ( z (k) i -z (k) j 2 2 ) N l=1 f ( z (k) i -z (k) l 2 2 ) , 1 ? i, j ? N, State Updating: z (k+1) i = 1 -? N j=1 ?(k) ij z (k) i state conservation + ? N j=1 ?(k) ij z (k) j state propagation , 1 ? i ? N.<label>(7)</label></formula><p>Remark. The choice of function f in above formulation is not arbitrary, but needs to be a non-negative and decreasing function of z 2 , so that the associated ? in Eq. 4 is guaranteed to be non-decreasing and concave w.r.t. z 2 . Critically though, there remains much room for us to properly design the specific f , so as to provide adequate capacity and scalability. Also, in our model presented by Eq. 7 we only have one hyper-parameter ? in practice, noting that the weight ? in the regularized energy is implicitly determined through ? by Theorem 1, which reduces the cost of hyper-parameter searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INSTANTIATIONS OF DIFFORMER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MODEL INSTANTIATIONS</head><p>We next go into model instantiations based on the above theory, with two specified f 's as practical versions of our model. Due to space limits, we describe the key ideas concerning the model design in this subsection, and defer the details of model architectures to the self-contained Appendix E. First, because</p><formula xml:id="formula_11">z i -z j 2 2 = z i 2 2 + z j 2 2 -2z i z j , we can convert f ( z i -z j<label>2</label></formula><p>2 ) into the form g(z i z j ) using a change of variables on the condition that z i 2 remains constant. And we add layer normalization to each layer to loosely enforce such a property in practice.</p><p>Simple Diffusivity Model. A straightforward design is to adopt the linear function g</p><formula xml:id="formula_12">(x) = 1 + x: ? (k) ij = f ( z(k) i - z(k) j 2 2 ) = 1 + z (k) i z (k) i 2 z (k) j z (k) j 2 , (<label>8</label></formula><formula xml:id="formula_13">) Assuming z(k) i = z (k) i z (k) i 2 , z(k) j = z (k) j z (k) j 2 and z = z(k) i - z(k) j 2 ,</formula><p>Eq. 8 can be written as f (z 2 ) = 2 -1 2 z 2 , which yields a non-negative result and is decreasing on the interval [0, 2] in which z 2 lies. One scalability concern for the model Eq. 7 arises because of the need to compute pairwise diffusivity and propagation for each individual, inducing O(N 2 ) complexity. Remarkably, the simple diffusivity model allows a significant acceleration by noting that the state propagation can be re-arranged via</p><formula xml:id="formula_14">N j=1 S (k) ij z (k) j = N j=1 1 + (z (k) i ) z(k) j N l=1 1 + (z (k) i ) z(k) l z (k) j = N j=1 z (k) j + N j=1 z(k) j ? (z (k) j ) ? z(k) i N + (z (k) i ) N l=1 z(k) l .<label>(9)</label></formula><p>Table <ref type="table">1</ref>: A unified view for MLP, GCN and GAT from our energy-driven geometric diffusion framework regarding energy function forms, diffusivity specifications and algorithmic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Energy Function E(Z, k; ?)</p><formula xml:id="formula_15">Diffusivity S (k) Complexity MLP Z -Z (k) 2 2 S (k) ij = 1, if i = j 0, otherwise O(N Kd 2 ) GCN (i,j)?E zi -zj 2 2 S (k) ij = ? ? ? 1 didj , if (i, j) ? E 0, otherwise O(|E|Kd 2 ) GAT (i,j)?E ?( zi -zj 2 2 ) S (k) ij = ? ? ? ? ? f ( z (k) i -z (k) j 2 2 ) l:(i,l)?E f ( z (k) i -z (k) l 2 2 ) , if (i, j) ? E 0, otherwise O(|E|Kd 2 ) DIFFORMER Z -Z (k) 2 2 + ? i,j ?( zi -zj 2 2 ) S (k) ij = f ( z (k) i -z (k) j 2 2 ) N l=1 f ( z (k) i -z (k) l 2 2 ) , 1 ? i, j ? N DIFFORMER-s : O(N Kd 2 ) DIFFORMER-a : O(N 2 Kd 2 )</formula><p>The two summation terms above can be computed once and shared to every instance i, reducing the complexity in each iteration to O(N ). We refer to this implementation as DIFFORMER-s.</p><p>Advanced Diffusivity Model. The simple model facilitates efficiency/scalability, yet may sacrifice the capacity for complex latent geometry. We thus propose an advanced version with g</p><formula xml:id="formula_16">(x) = 1 1+exp(-x) : ? (k) ij = f ( z(k) i - z(k) j 2 2 ) = 1 1 + exp -(z (k) i ) (z (k) j ) ,<label>(10)</label></formula><p>which corresponds with f (z 2 ) = 1 1+e z 2 /2-1 guaranteeing monotonic decrease and non-negativity. We dub this version as DIFFORMER-a. Appendix D further compares the two models (i.e., different f 's and ?'s) through synthetic results. Real-world empirical comparisons are in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MODEL EXTENSIONS AND FURTHER DISCUSSION</head><p>Incorporating Layer-wise Transformations. Eq. 7 does not use feature transformations for each layer. To further improve the representation capacity, we can add such transformations after the updating, i.e., z</p><formula xml:id="formula_17">(k) i ? h (k) (z (k) i )</formula><p>where h (k) can be a fully-connected layer (see Appendix E for details). In this way, each iteration of the diffusion yields a descent of a particular energy</p><formula xml:id="formula_18">E(Z, k; ?, h (k) ) = Z -h (k) (Z (k) ) 2 2 + i,j ?( z i -z j<label>2</label></formula><p>2 ) dependent on k. The trainable transformation h (k) can be optimized w.r.t. the supervised loss to map the instance representations into a proper latent space. Our experiments find that the layer-wise transformation is not necessary for small datasets, but contributes to positive effects for datasets with larger sizes. Furthermore, one can consider non-linear activations in the layer-wise transformation h (k) though we empirically found that using a linear model already performs well. We also note that Theorem 1 can be extended to hold even when incorporating such a non-linearity in each layer (see Appendix C for detailed discussions).</p><p>Incorporating Input Graphs. For the model presented so far, we do not assume an input graph for the model formulation. For situations with observed structures as available input, we have G = (V, E) that can be leveraged as a geometric prior. We can thus modify the updating rule as:</p><formula xml:id="formula_19">z (k+1) i = ? ? 1 - ? 2 N j=1 ?(k) ij + ?ij ? ? z (k) i + ? 2 N j=1 ?(k) ij + ?ij z (k) j ,<label>(11)</label></formula><p>where </p><formula xml:id="formula_20">?ij = 1 ? didj if (i, j) ? E</formula><formula xml:id="formula_21">(k) i -z (k) j<label>2</label></formula><p>2 (see Appendix D for details). Scaling to Large Datasets. Another advantage of DIFFORMER over GNNs is the flexibility for mini-batch training. For datasets with prohibitive instance numbers that make it hard for full-batch training on a single GPU, the common practice for GNNs resorts to subgraph sampling <ref type="bibr" target="#b56">Zeng et al. (2020)</ref> or graph clustering <ref type="bibr" target="#b11">Chiang et al. (2019)</ref> to reduce the overhead. These strategies, however, require extra time and tricky designs to preserve the internal structures. In contrast, thanks to the less reliance on input graphs, for training DIFFORMER, we can naturally partition the dataset into Table <ref type="table">2</ref>: Mean and standard deviation of testing accuracy on node classification (with five different random initializations). All the models are split into groups with a comparison of non-linearity (whether the model requires activation for layer-wise transformations), PDE-solver (whether the model requires PDE-solver) and Input-G (whether the propagation purely relies on input graphs). random mini-batches and feed one mini-batch for one feed-forward and backward computation. The flexibility for mini-batch training also brings up convenience for parallel acceleration and federated learning. In fact, such a computational advantage is shared by Transformer-like models for cases where instances are inter-dependent, as demonstrated by the NodeFormer <ref type="bibr">(Wu et al., 2022b)</ref>.</p><p>Connection with Existing Models. Our theory in fact gives rise to a general diffusion framework that unifies some existing models as special cases of ours. As a non-exhaustive summary and highlevel comparison, Table <ref type="table">1</ref> presents the relationships with MLP, GCN and GAT (see more elaboration in Appendix F). Specifically, the multi-layer perceptrons (MLP) can be seen as only considering the local consistency regularization in the energy function and only allowing non-zero diffusivity for self-loops. For graph convolution networks (GCN) <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref>, it only regularizes the global consistency term constrained within 1-hop neighbors of the observed structure and simplifies ? as an identity function. The diffusivity of GCN induces information flows through observed edges.</p><p>For graph attention networks (GAT) <ref type="bibr" target="#b41">(Velickovic et al., 2018)</ref>, the energy function is similar to GCN's except that the non-linearity ? remains (as a certain specific form), and the diffusivity is computed by attention over edges. In contrast with the above models, DIFFORMER is derived from the general diffusion model that enables non-zero diffusivity values between arbitrary instance pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We apply DIFFORMER to various tasks for evaluation: 1) graph-based node classification where an input graph is given as observation; 2) image and text classification without input graphs; 3) spatial temporal dynamics prediction. In each case, we compare a different set of competing models closely associated with DIFFORMER and specifically designed for the particular task. Unless otherwise stated, for datasets where input graphs are available, we incorporate them for feature propagation as is defined by Eq. 11. Due to space limit, we defer details of datasets to Appendix G and the implementations to Appendix H. Also, we provide additional empirical results in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SEMI-SUPERVISED NODE CLASSIFICATION BENCHMARKS</head><p>We test DIFFORMER on three citation networks Cora, Citeseer and Pubmed. Table <ref type="table">2</ref> reports the testing accuracy. We compare with several sets of baselines linked with our model from different aspects. 1) Basic models: MLP and two classical graph-based SSL models Label Propagation (LP) <ref type="bibr" target="#b62">(Zhu et al., 2003)</ref> and ManiReg <ref type="bibr" target="#b1">(Belkin et al., 2006)</ref>. 2) GNN models: SGC <ref type="bibr" target="#b46">(Wu et al., 2019)</ref>, GCN <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b41">(Velickovic et al., 2018)</ref>, their variants GCN-kNN, GAT-kNN (operating on kNN graphs constructed from input features) and Dense GAT (with a densely connected graph replacing the input one), and two strong structure learning models LDS <ref type="bibr" target="#b17">(Franceschi et al., 2019)</ref>    and GLCN <ref type="bibr" target="#b22">(Jiang et al., 2019)</ref>. 3) PDE graph models: the SOTA models GRAND <ref type="bibr">(Chamberlain et al., 2021a)</ref> (with its linear variant GRAND-l) and GRAND++ <ref type="bibr">(Thorpe et al., 2022)</ref>. 4) Diffusion-inspired GNN models: GDC <ref type="bibr" target="#b25">(Klicpera et al., 2019)</ref>, GraphHeat <ref type="bibr" target="#b49">(Xu et al., 2020)</ref> and a recent work DGC-Euler <ref type="bibr" target="#b42">(Wang et al., 2021)</ref>. Table <ref type="table">2</ref> shows that DIFFORMER achieves the best results on three datasets with significant improvements. Also, we notice that the simple diffusivity model DIFFORMER-s significantly exceeds the counterparts without non-linearity (SGC, GRAND-l and DGC-Euler) and even comes to the first on Cora and Pubmed. These results suggest that DIFFORMER can serve as a very competitive encoder backbone for node-level prediction that learns inter-instance interactions for generating informative representations and boosting downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LARGE-SCALE NODE CLASSIFICATION GRAPHS</head><p>We also consider two large-scale graph datasets ogbn-Proteins, a multi-task protein-protein interaction network, and Pokec, a social network. Table <ref type="table" target="#tab_2">3</ref> presents the results. Due to the dataset size (0.13M/1.63M nodes for two graphs) and scalability issues that many of the competitors in Table <ref type="table">2</ref> as well as DIFFORMER-a would potentially experience, we only compare DIFFORMER-s with standard GNNs. In particular, we found GCN/GAT/DIFFORMER-s are still hard for full-graph training on a single V100 GPU with 16GM memory. We thus consider mini-batch training with batch size 10K/100K for Proteins/Pokec. We found that DIFFORMER outperforms common GNNs by a large margin, which suggests its desired efficacy on large datasets. As mentioned previously, we prioritize the efficacy of DIFFORMER as a general encoder backbone for solving node-level prediction tasks on large graphs. While there are quite a few practical tricks shown to be effective for training GNNs for this purpose, e.g., hop-wise attention <ref type="bibr" target="#b37">(Sun et al., 2022)</ref> or various label re-use strategies, these efforts are largely orthogonal to our contribution here and can be applied to most any model to further boost performance. For further investigation, we supplement more results using different mini-batch sizes for training and study its impact on testing performance in Appendix I.4. Furthermore, we compare the training time and memory costs in Appendix I.5 where we found that DIFFORMER-s is about 6 times faster than GAT and 39 times faster than DenseGAT on Pokec, which suggests superior scalability and efficiency of DIFFORMER-s on large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">IMAGE AND TEXT CLASSIFICATION WITH LOW LABEL RATES</head><p>We next conduct experiments on CIFAR-10, STL-10 and 20News-Group datasets to test DIF-FORMER for standard classification tasks with limited label rates. For 20News provided by <ref type="bibr" target="#b30">Pedregosa et al. (2011)</ref>, we take 10 topics and use words with TF-IDF more than 5 as features. For CIFAR and STL, two public image datasets, we first use the self-supervised approach SimCLR (Chen et al., 2020b) (that does not use labels for training) to train a ResNet-18 for extracting the feature maps as input features of instances. These datasets contain no graph structure, so we use kNN to construct a graph over input features for GNN competitors and do not use input graphs for DIFFORMER.</p><p>Table <ref type="table" target="#tab_3">4</ref> reports the testing accuracy of DIFFORMER and competitors including MLP, ManiReg, GCN-kNN, GAT-kNN, DenseGAT and GLCN. Two DIFFORMER models perform much better than MLP in nearly all cases, suggesting the effectiveness of learning the inter-dependence over instances. Besides, DIFFORMER yields large improvements over GCN and GAT which are in some sense limited by the handcrafted graph that leads to sub-optimal propagation. Moreover, DIFFORMER significantly outperforms GLCN, a strong baseline that learns new (static) graph structures, which demonstrates the superiority of our evolving diffusivity that can adapt to different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">SPATIAL-TEMPORAL DYNAMICS PREDICTION</head><p>We consider three spatial-temporal datasets with details in Appendix G. Each dataset consists of a series of graph snapshots where nodes are treated as instances and each of them has a integer label (e.g., reported cases for Chickenpox or Covid). The task is to predict the labels of one snapshot based on the previous ones. Table <ref type="table" target="#tab_4">5</ref> compares testing MSE of four DIFFORMER variants (here DIFFORMER-s w/o g denotes the model DIFFORMER-s without using input graphs) against baselines. We can see that two DIFFORMER variants without input graphs even outperform the counterparts using input structures in four out of six cases. This implies that our diffusivity estimation module could learn useful structures for informed prediction, and the input structure might not always contribute to positive effect. In fact, for temporal dynamics, the underlying relations that truly influence the trajectory evolution can be much complex and the observed relations could be unreliable with missing or noisy links, in which case GNN models relying on input graphs may perform undesirably. Compared to the competitors, our models rank the first with significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">FURTHER RESULTS AND DISCUSSIONS</head><p>How do different diffusivity functions perform? Figure <ref type="figure">2</ref>(a) compares DIFFORMER with four variants using other diffusivity functions that have no essential connection with energy minimization: 1) Identity sets S (k) as a fixed identity matrix; 2) Constant fixes S (k) as all-one constant matrix; 3) Full Attn parameterizes S (k) by attention networks <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>; 4) Kernel adopts Gaussian kernel for computing S (k) . More results on other datasets are in Appendix I.1 and they consistently show that our adopted diffusivity forms produce superior performance, which verifies the effectiveness of our diffusivity designs derived from minimization of a principled energy.</p><p>How do model depth and step size impact the performance? We discuss the influence of model depth K and step size ? on Cora in Fig. <ref type="figure">2(b</ref>). More results on Citeseer and Pubmed are generally consistent with Fig. <ref type="figure">2</ref>(b) and deferred to Appendix I.2. The curves indicate that the performance of GCN and GAT models exhibit significant degradation with deeper layers, while DIFFORMER maintains its superiority and performs stably with large K. Furthermore, when K is not large enough (less than 32), there is a clear performance improvement of DIFFORMER as K increases, and larger ? contributes to a steeper increase. When K continues increasing (more than 32), the model performance still goes up with small ? (0.1 and 0.25) yet exhibits a slight drop with large ? (0.5 and 0.9). The reason could be that larger (smaller) ? contributes to more (less) concentration on global information from other instances in each iteration, which brings up more (less) benefits with increasing propagation layers yet could lead to instability when the step size is too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>This paper proposes an energy-driven geometric diffusion model with latent diffusivity function for data representations. The model encodes all the instances as a whole into evolving states aimed at minimizing a principled energy as implicit regularization. We further design two practical implemen-tations with enough scalability and capacity for learning complex interactions over the underlying data geometry. Extensive experiments demonstrate the effectiveness and superiority of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDE-inspired Learning.</head><p>Another category is PDE-inspired learning using the diffusion perspective as principled guidelines on top of which new (discrete) neural network-based approaches are designed for node classification <ref type="bibr" target="#b0">(Atwood &amp; Towsley, 2016;</ref><ref type="bibr" target="#b25">Klicpera et al., 2019;</ref><ref type="bibr" target="#b49">Xu et al., 2020)</ref>, graph comparison <ref type="bibr" target="#b39">(Tsitsulin et al., 2018)</ref>, and geometric knowledge distillation <ref type="bibr" target="#b50">(Yang et al., 2022)</ref>. Our work leans on PDE-inspired learning and introduce a new diffusion model that is implicitly defined as minimizing a regularized energy. Our theory also reveals the underlying equivalence between the numerical iterations of the diffusion process and unfolding the minimization dynamics of a corresponding energy. The new results can serve to extend the class of diffusion process and illuminate its fundamental connection with energy optimization systems. More importantly, as we will in the maintext, such a new perspective brings up a unifying view for existing models like MLP and GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 THEORETICAL INTERPRETATIONS OF GNNS</head><p>Optimization-induced Models. As GNNs have proven to be powerful encoders for modeling data with geometry, there arise quite a few studies that aim at understanding the efficacy of GNNs with respect to node representations yielded by the propagation. For instance, <ref type="bibr" target="#b52">Yang et al. (2021)</ref> founds that the layer-wise updates of common GNNs can be derived from a classical iterative algorithm solving an optimization problem defined over an energy. Furthermore, <ref type="bibr" target="#b28">Ma et al. (2021)</ref> points out the relationship between GNNs' message passing rules and the graph denoising problem. These works interpret GNNs as optimization-induced models. In our model, the diffusion process is implicitly defined by an optimization problem and our theory further reveals the fundamental equivalence between diffusion iterations and energy optimization dynamics, which can bridge two schools of thinking and facilitate the understandings from both perspectives.</p><p>Generalization of GNNs. Another line of works turn attention to GNNs' generalization capability.</p><p>As increasing evidence shows that GNNs are sensitive to distribution shifts <ref type="bibr">(Wu et al., 2022a)</ref>, it becomes urgent to investigate and figure out how to improve GNNs' generalization. A concurrent work <ref type="bibr" target="#b51">(Yang et al., 2023)</ref> identifies an intriguing phenomenon that GNNs without propagation (i.e., an MLP architecture) during training can still yield competitive or even close performance at inference time to standard GNNs (using feature propagation at both training and inference stage). This suggests that the propagation design contributes to better generalization instead of the model capacity, and the former plays as a critical factor that makes GNNs a more powerful encoder than MLP. This result can further interprets the success of DIFFORMER: the new propagation of DIFFORMER that aggregates other nodes' embeddings can enhance the generalization at inference time on testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 TRANSFORMERS</head><p>General Transformers. The key design of Transformers lies in the (all-pair) attention mechanism that captures the interactions among tokens of an input sequence. The attention design is originally motivated by the inter-dependence among words of a sentence in NLP tasks <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>. Furthermore, a surge of follow-up works extend the attention mechanisms for capturing the interdependence among patches of an image <ref type="bibr" target="#b13">(Dosovitskiy et al., 2021)</ref>, atoms of a molecule <ref type="bibr" target="#b14">(Dwivedi &amp; Bresson, 2020)</ref>, dimensions of multivariate time series <ref type="bibr" target="#b60">(Zhang &amp; Yan, 2023)</ref>, etc. From different aspects, our work explores a Transformer model that targets learning inter-dependence among instances in a dataset.</p><p>Efficient Transformers. When the length of inputs goes large, the quadratic complexity of all-pair attention becomes the computational bottleneck of Transformers. To address the long sequences in NLP tasks, quite a few recent works propose various strategies via, e.g., Softmax kernel <ref type="bibr" target="#b12">(Choromanski et al., 2021)</ref>, local-global attention <ref type="bibr" target="#b55">(Zaheer et al., 2020)</ref>, and low-rank approximation <ref type="bibr" target="#b23">(Katharopoulos et al., 2020)</ref>. In our problem setting, if one treats the whole dataset as an input 'sequence' for the Transformer model, then the length of such a sequence could go to even million-level, which poses demanding scalability challenges. Our model DIFFORMER-s achieves strictly linear complexity w.r.t. N through introducing a new attention function that keeps a simple form yet accommodates the influence among arbitrary node pairs.</p><p>Transformers on Graphs. There is also increasing research interest on building powerful Transformers for graph-structured data, due to the inherent good ability of Transformer models for capturing long-range dependence and potential node-pair interactions that are unobserved in input structures.</p><p>Recent works, e.g., Dwivedi &amp; Bresson (2020); <ref type="bibr" target="#b57">Zhang et al. (2020)</ref>; <ref type="bibr" target="#b54">Ying et al. (2021)</ref> explore various effective architectures for graph-level prediction (GP) tasks, e.g., molecular property prediction, whereby each graph itself generally has a label to predict and a dataset contains many graph instances.</p><p>Our target problem, as mentioned in Section 2, can be seen an embodiment of node-level prediction (NP) studied in graph learning community. These two problems are typically tackled separately in the literature <ref type="bibr" target="#b20">(Hu et al., 2020)</ref> with disparate technical considerations. This is because input instances are inter-dependent in NP (due to the instance interactions involved in the data-generating process), while in GP tasks the instances can be treated as IID samples. For node-level prediction, a recent work <ref type="bibr">(Wu et al., 2022b)</ref> proposes kernelized Gumbel-Softmax-based message passing to achieve all-pair feature propagation with linear complexity. Concurrently at the same conference, <ref type="bibr" target="#b6">(Chen et al., 2023)</ref> proposes to transform neighborhood features into tokens fed into Transformers to enable efficient training on large graphs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 GRAPH NEURAL NETWORKS</head><p>Scalable Graph Neural Networks. Due to the large graph sizes that could be frequently encountered in graph-based predictive tasks (particularly NP), many recent works focus on designing scalable GNNs. From different technical aspects, these works can be generally grouped into graph samplingbased partition <ref type="bibr" target="#b56">(Zeng et al., 2020;</ref><ref type="bibr" target="#b11">Chiang et al., 2019)</ref>, approximation-based propagation <ref type="bibr" target="#b2">(Bojchevski et al., 2020)</ref> and simplified architectures <ref type="bibr" target="#b46">(Wu et al., 2019;</ref><ref type="bibr" target="#b58">Zhang et al., 2022)</ref>. In contrast with them, DIFFORMER can scale to medium-sized graphs with the linear complexity, and to large-scale graphs with mini-batch training that is simple, stable and also flexible for balancing efficiency and precision with proper mini-batch sizes. The later is allowed by the all-pair message passing schemes that do not rely on input structures (the input graphs play an auxiliary role in the model).</p><p>Graph Structure Learning. Learning latent graph structures is a well-established research problem in graph learning community. Motivated by the observation that input graphs can often be unreliable or unavailable, based on which the message passing of GNNs could yield undesired results, graph structure learning aims at learning adaptive structures that can boost GNNs towards better representations and downstream prediction <ref type="bibr" target="#b17">(Franceschi et al., 2019;</ref><ref type="bibr">Chen et al., 2020c;</ref><ref type="bibr" target="#b22">Jiang et al., 2019;</ref><ref type="bibr" target="#b16">Fatemi et al., 2021;</ref><ref type="bibr" target="#b27">Lao et al., 2022)</ref>. Since graph structure learning requires estimation for N 2 potential edges that connect all node pairs in a dataset, the time and space complexity of most existing models are at least quadratic w.r.t. node numbers, which is prohibitive for large graphs. The recent work <ref type="bibr">(Wu et al., 2022b)</ref> proposes a Transformer model with linear complexity that can scale graph structure learning to graphs with millions of nodes. The proposed model DIFFORMER-s can be another powerful approach for learning latent structures in large-scale systems, without sacrificing the accommodation of all-pair interactions at each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF FOR THEOREM 1</head><p>First of all, we can convert the minimization of Eq. 4 into a minimization of its variational upper bound, shown in the following proposition. Proposition 1. The energy function E(Z, k; ?) is upper bounded by</p><formula xml:id="formula_22">?(Z, k; {? ij }, ?) = Z -Z (k) 2 F + ? ? ? i,j ? ij z i -z j 2 2 -?(? ij ) ? ? , (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>where ? is the concave conjugate of ?, and the equality holds if and only if the variational parameters satisfy</p><formula xml:id="formula_24">? ij = ??(z 2 ) ?z 2 z= zi-zj 2 . (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>Proof. The proof of the proposition follows the principles of convex analysis and Fenchel duality <ref type="bibr" target="#b32">(Rockafellar, 1970)</ref>. For any concave and non-decreasing function ? : R + ? R, one can express it as the variational decomposition</p><formula xml:id="formula_26">?(z 2 ) = min ??0 [?z 2 -?(?)] ? ?z 2 -?(?),<label>(14)</label></formula><formula xml:id="formula_27">0 1 2 z 2 0 1 2 3 ?(z 2 ) Penalty -1 0 1 zx -1 0 1 zy Divergence -1 0 1 zx -1 0 1 zy Energy 1.0 1.5 2.0 f(z 2 ) (a) DIFFORMER-s: ?(z 2 ) = 2z 2 -1 4 z 4 0 1 2 z 2 -1 0 1 ?(z 2 ) Penalty -1 0 1 zx -1 0 1 zy Divergence -1 0 1 zx -1 0 1 zy Energy 0.5 0.6 0.7 f(z 2 ) (b) DIFFORMER-a: ?(z 2 ) = z 2 -2 log(e z 2 /2-1 + 1)</formula><p>Figure <ref type="figure">3</ref>: Plot of penalty curves ?(z 2 ) and f (z 2 ) = ??(z 2 ) ?z 2 , divergence field (produced by 10 randomly generated instances marked as red stars) and cross-section energy field of an individual.</p><p>where ? is a variational parameter and ? is the concave conjugate of ?. Eq. 14 essentially defines ?(z 2 ) as the minimal envelope of a series of quadratic bounds ?z 2 -?(?) defined by a different values of ? ? 0 and the upper bound is given for a fixed ? when removing the minimization operator. Based on this, we obtain the result of Eq. 12. In terms of the sufficient and necessary condition for equality, we note that for any optimal ? * we have</p><formula xml:id="formula_28">? * z 2 -?(? * ) = ?(z 2 ),<label>(15)</label></formula><p>which is tangent to ? at z 2 and ? * = ??(z 2 ) ?z 2 . We thus obtain the result of Eq. 13.</p><p>We next continue to prove the main result of Theorem 1. According to Proposition 1, we can minimize the upper bound surrogate Eq. 12 and it becomes equivalent to a minimization of the original energy on condition that the variational parameters are given by Eq. 13. Then with a one-step gradient decent of Eq. 12, the instance states could be updated via (assuming l as the index of steps and ? as step size)</p><formula xml:id="formula_29">Z (l+1) = Z (l) -? ? ?(Z; ?) ?Z Z=Z (l) = Z (l) -? ?(D (l) -? (l) )Z (l) + Z (l) -Z (l) = Z (l) -? (D (l) -? (l) )Z (l)<label>(16)</label></formula><p>where l) denotes the diagonal degree matrix associated with ? (l) and we introduce ? = ?? to combine two parameters as one. Common practice to accelerate convergence adopts a positive definite preconditioner term, e.g., (D (l) ) -1 , to re-scale the updating gradient and the final updating form becomes</p><formula xml:id="formula_30">? (l) = {? (l) ij } N ?N , D<label>(</label></formula><formula xml:id="formula_31">Z (l+1) = (1 -? )Z (l) + ? (D (l) ) -1 ? (l) Z (l) .<label>(17)</label></formula><p>One can notice that Eq. 17 shares similar forms as the numerical iteration Eq. 3 for the PDE diffusion system, in particular if we write Eq. 3 as a matrix form:</p><formula xml:id="formula_32">Z (k+1) = 1 -? D(k) Z (k) + ? S (k) Z (k) . (<label>18</label></formula><formula xml:id="formula_33">)</formula><p>where D(k) is the degree matrix associated with S (k) . Pushing further, we can see that the effect of Eq. 18 is the same as 17 when we let ? = ? , k = l, S (k) = (D (l) ) -1 ? (l) and S (k) is row-normalized, i.e., v?V S (k) uv = 1 and D(k) = I. Thereby, we have proven by construction that a one-step numerical iteration by the explicit Euler scheme, specifically shown by Eq. 17 is equivalent to a one-step gradient descent on the surrogate Eq. 14 which further equals to the original energy Eq. 4. We thus have the result E(Z (k+1) , k; ?) ? E(Z (k) , k; ?). Besides, we notice that for a fixed Z, E(Z, k</p><formula xml:id="formula_34">; ?) = Z-Z (k) 2 F +? ij ?( z i -z j<label>2</label></formula><p>2 ) becomes a function of k and its optimum is achieved if and only if Z (k) = Z. Such a fact yields that E(Z (k) , k; ?) ? E(Z (k) , k -1; ?). The result of the main theorem follows by noting that E(Z (k+1) , k; ?) ? E(Z (k) , k; ?) ? E(Z (k) , k -1; ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXTENSION OF OUR THEORY TO INCORPORATE NON-LINEARITY</head><p>In Section 3, we mainly focus on the situation without non-linearity in each model layer to stay consistent with our implementation where we empirically found that omitting the non-linearity in the middle diffusion layers works smoothly in practice (see the pseudo code of Alg. 1 in appendix for details). Even so, our theory can also be extended to incorporate the layer-wise non-linearity in diffusion propagation. Specifically, the non-linear activation can be treated as a proximal operator (which projects the output into a feasible region) and the gradient descent used in our analysis Eqn. 16 and 17 can be modified to add a proximal operator:</p><formula xml:id="formula_35">Z (l+1) = Prox ? (1 -?)Z (l) + ?(D (l) ) -1 ? (l) Z (l) ,</formula><p>where Prox ? (z) = arg min x?? x -z 2 and ? defines a feasible region. The updating above corresponds to proximial gradient descent which also guarantees a strict minimization for the energy function and our Theorem 1 will still hold. In particular, if one uses ReLU activation, the proximal operator will be Prox ? (z) = max(0, z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DIFFERENT ENERGY FORMS</head><p>We present more detailed illustration for the choices of f and specific energy function forms in Eq. 4.</p><p>Simple Diffusivity Model. As discussed in Section 4, the simple model assumes f (z 2 ) = 2 -1 2 z 2 that corresponds to g(x) = 1 + x, where we define z = z i -z j 2 and x = z i z j . The corresponding penalty function ? whose first-order derivative is f would be ?(z 2 ) = 2z 2 -1 4 z 4 . We plot the penalty function curves in Fig. <ref type="figure">3</ref>(a). As we can see, the f is a non-negative, decreasing function of z 2 , which implies that the ? satisfies the non-decreasing and concavity properties to guarantee a valid regularized energy function. Also, in Fig. <ref type="figure">3</ref>(a) we present the divergence field produced by 10 randomly generated instances (marked as red stars) and the cross-section energy field of one instance.</p><p>Advanced Diffusivity Model. The diffusivity model defines f (z 2 ) = 1 1+e z 2 /2-1 with g(x) = 1 1+exp(-x) , and the corresponding penalty function ?(z 2 ) = z 2 -2 log(e z 2 /2-1 + 1). The penalty function curves, divergence field and energy field are shown in Fig. <ref type="figure">3(b)</ref>.</p><p>Incorporating Input Graphs. In Section 4, we present an extended version of our model for incorporating input graphs. Such an extension defines a new diffusion process whose iterations are equivalent (up to a re-scaling factor on the adjacency matrix) to a sequence of descending steps for the following regularized energy:</p><formula xml:id="formula_36">E(Z, k; ?) = Z -Z (k) 2 F + ? 2 i,j ?( z i -z j 2 2 ) + ? 2 (i,j)?E z i -z j 2 2 ,<label>(19)</label></formula><p>where the last term contributes to a penalty for observed edges in the input graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DETAILS OF DIFFORMER MODEL</head><p>In this section, we present the details for the feed-forward computation of DIFFORMER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 DIFFORMER'S FEED-FORWARD WITH A MATRIX VIEW</head><p>Input Layer. For input data X = {x i } N i=1 ? R N ?D where x i denotes the D-dimensional input features of the i-th instance, we first use a shallow fully-connected layer to convert it into a ddimensional embedding in the latent space:</p><formula xml:id="formula_37">Z = ? (LayerNorm(W I X + b I )) ,<label>(20)</label></formula><p>where W I ? R d?D and b I ? R d are trainable parameters, and ? is a non-linear activation (i.e., ReLU). Then the node embeddings Z will be used for feature propagation with our diffusion-induced Transformer model by letting Z (0) = Z as the initial states. instances' property. We follow the original splitting of <ref type="bibr" target="#b20">Hu et al. (2020)</ref> for evaluation. Pokec is a large-scale social network with features including profile information, such as geographical region, registration time, and age, for prediction on users' gender. For semi-supervised learning, we consider randomly splitting the instances into train/valid/test with 10%/10%/80% ratios. Table <ref type="table">6</ref> summarizes the statistics of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 IMAGE AND TEXT CLASSIFICATION DATASETS</head><p>We evaluate our model on two image classification datasets: STL-10 and CIFAR-10. We use all 13000 images from STL-10, each of which belongs to one of ten classes. We choose 1500 images from each of 10 classes of CIFAR-10 and obtain a total of 15,000 images. For STL-10 and CIFAR-10, we randomly select 10/50/100 instances per class as training set, 1000 instances for validation and the remaining instances for testing. We first use the self-supervised approach SimCLR <ref type="bibr">(Chen et al., 2020b)</ref> (that does not use labels for training) to train a ResNet-18 for extracting the feature maps as input features of instances. We also evaluate our model on 20Newsgroup, which is a text classification dataset consisting of 9607 instances. We follow <ref type="bibr" target="#b17">Franceschi et al. (2019)</ref> to take 10 classes from 20 Newsgroup and use words (TFIDF) with a frequency of more than 5% as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 SPATIAL-TEMPORAL DATASETS</head><p>The spatial-temporal datasets are from the open-source library PyTorch Geometric Temporal <ref type="bibr" target="#b34">(Rozemberczki et al., 2021)</ref>, with properties and summary statistics described in Table <ref type="table" target="#tab_7">7</ref>. Node features are evolving for all the datasets considered here, i.e., we have different node features for different snapshots. For each dataset, we split the snapshots into training, validation, and test sets according to a 2:2:6 ratio in order to make it more challenging and close to the real-world low-data learning setting. In details:</p><p>? Chickenpox describes weekly officially reported cases of chickenpox in Hungary from 2005 to 2015, whose nodes are counties and edges denote direct neighborhood relationships. Node features are lagged weekly counts of the chickenpox cases (we included 4 lags). The target is the weekly number of cases for the upcoming week (signed integers).  One can see that using small batch sizes would indeed sacrifice the performance yet large batch sizes can produce decent and low-variance results with acceptable memory costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.5 COMPARISON OF RUNNING TIME AND MEMORY COSTS</head><p>To further study the efficiency and scalability of our model, we provide more comparison regarding the training time per epoch and memory costs of two DIFFORMER's variants, GCN, GAT and DenseGAT in Table <ref type="table" target="#tab_9">9</ref>. One can see that compared to GAT, DIFFORMER-s costs comparable time on small datasets such as Cora and WikiMath, and is much faster on large dataset Pokec. As for memory consumption, DIFFORMER-s reduces the costs by several times over DenseGAT, which clearly shows the efficiency of our new diffusion function designs. Overall, DIFFORMER-s has nice scalability, decent efficiency and yield significantly better accuracy. In contrast, DIFFORMER-a costs much larger time and memory costs than DIFFORMER-s, due to its quadratic complexity induced by the explicit computation for the all-pair diffusivity. Still, DIFFORMER-a accommodates non-linearity for modeling the diffusion strengths which enables better capacity for learning complex layer-wise inter-interactions. As an initial verification of this claim, we use the Meta Pseudo Labels (MPL) <ref type="bibr" target="#b31">(Pham et al., 2021)</ref> as a plug-in module to boost DIFFORMER as well as our competitors GCN-kNN and GAT-kNN, and empirically compare the relative improvement. Specifically, we use DIFFORMER-s, DIFFORMER-a, GCN and GAT as the encoder backbone of teacher and student models, respectively, and use the MPL algorithm to generate pseudo labels for augmenting the training data used for computing the supervised loss. The results on CIFAR-10 and STL-10 are shown in Table <ref type="table">10</ref>. As we can see, the MPL contributes to some performance gains across all four encoders, while our two DIFFORMER variants still maintain the superiority over the competitors. Note also that as a proof-of-concept here we did not use an additional consistency loss that requires careful manual tuning. However, in practice this type of more sophisticated MPL implementation could in principle be applied to further improve performance (across all models).</p><p>Table 10: Comparison of using and not using Meta Pseudo Labels (MPL) as a plug-in module to boost different encoder backbones on CIFAR-10 and STL-10.  Node colors correspond to ground-truth labels (i.e., reported cases), varying from red to blue as the label increases. We visualize the edges with top 100 diffusion strength, where edge colors change from blue to red as ?(1) ij increases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: (a) Ablation studies w.r.t. different diffusivity function forms on CIFAR. (b) Impact of K and ? on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Visualization of instance representations and diffusivity strengths (we set a threshold and only plot the edges with weights more than the threshold) at different layers given by DIFFORMER-s on 20News.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Visualization of instance representations and diffusivity strengths (we set a threshold and only plot the edges with weights more than the threshold) at different layers given by DIFFORMER-s on STL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and 0 otherwise, and d i is instance i's degree in G.</figDesc><table><row><cell>The diffusion</cell></row><row><cell>iteration of Eq. 11 is essentially a descent step on a new energy additionally incorporating a graph-</cell></row><row><cell>based penalty (Ioannidis et al., 2017), i.e., (i,j)?E z</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Testing ROC-AUC for Proteins and Accuracy for Pokec on large-scale node classification datasets. * denotes using mini-batch training.</figDesc><table><row><cell>Models</cell><cell>Proteins</cell><cell>Pokec</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP LP SGC</cell><cell>72.41 ? 0.10 74.73 49.03 ? 0.93</cell><cell>60.15 ? 0.03 52.73 52.03 ? 0.84</cell><cell>100</cell><cell>500 Label Number</cell><cell>1000</cell></row><row><cell>GCN</cell><cell cols="2">74.22 ? 0.49  *  62.31 ? 1.13  *</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GAT</cell><cell cols="2">75.11 ? 1.45  *  65.57 ? 0.34  *</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NodeFormer</cell><cell cols="2">77.45 ? 1.15  *  68.32 ? 0.45  *</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DIFFORMER-s 79.49 ? 0.44  *  69.24 ? 0.76  *</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Testing accuracy for image (CIFAR and STL) and text (20News) classification.</figDesc><table><row><cell cols="2">Dataset</cell><cell>MLP</cell><cell>LP</cell><cell>ManiReg</cell><cell cols="2">GCN-kNN GAT-kNN</cell><cell>DenseGAT</cell><cell>GLCN</cell><cell>DIFFORMER-s DIFFORMER-a</cell></row><row><cell></cell><cell>100 labels</cell><cell cols="3">65.9 ? 1.3 66.2 67.0 ? 1.9</cell><cell>66.7 ? 1.5</cell><cell cols="3">66.0 ? 2.1 out-of-memory 66.6 ? 1.4</cell><cell>69.1 ? 1.1</cell><cell>69.3 ? 1.4</cell></row><row><cell>CIFAR</cell><cell>500 labels</cell><cell cols="3">73.2 ? 0.4 70.6 72.6 ? 1.2</cell><cell>72.9 ? 0.4</cell><cell cols="3">72.4 ? 0.5 out-of-memory 72.8 ? 0.5</cell><cell>74.8 ? 0.5</cell><cell>74.0 ? 0.6</cell></row><row><cell></cell><cell cols="4">1000 labels 75.4 ? 0.6 71.9 74.3 ? 0.4</cell><cell>74.7 ? 0.5</cell><cell cols="3">74.1 ? 0.5 out-of-memory 74.7 ? 0.3</cell><cell>76.6 ? 0.3</cell><cell>75.9 ? 0.3</cell></row><row><cell></cell><cell>100 labels</cell><cell cols="3">66.2 ? 1.4 65.2 66.5 ? 1.9</cell><cell>66.9 ? 0.5</cell><cell cols="3">66.5 ? 0.8 out-of-memory 66.4 ? 0.8</cell><cell>67.8 ? 1.1</cell><cell>66.8 ? 1.1</cell></row><row><cell>STL</cell><cell>500 labels</cell><cell cols="3">73.0 ? 0.8 71.8 72.5 ? 0.5</cell><cell>72.1 ? 0.8</cell><cell cols="3">72.0 ? 0.8 out-of-memory 72.4 ? 1.3</cell><cell>73.7 ? 0.6</cell><cell>72.9 ? 0.7</cell></row><row><cell></cell><cell cols="4">1000 labels 75.0 ? 0.8 72.7 74.2 ? 0.5</cell><cell>73.7 ? 0.4</cell><cell cols="3">73.9 ? 0.6 out-of-memory 74.3 ? 0.7</cell><cell>76.4 ? 0.5</cell><cell>75.3 ? 0.6</cell></row><row><cell></cell><cell cols="4">1000 labels 54.1 ? 0.9 55.9 56.3 ? 1.2</cell><cell>56.1 ? 0.6</cell><cell>55.2 ? 0.8</cell><cell>54.6 ? 0.2</cell><cell>56.2 ? 0.8</cell><cell>57.7 ? 0.3</cell><cell>57.9 ? 0.7</cell></row><row><cell>20News</cell><cell cols="4">2000 labels 57.8 ? 0.9 57.6 60.0 ? 0.8</cell><cell>60.6 ? 1.3</cell><cell>59.1 ? 2.2</cell><cell>59.3 ? 1.4</cell><cell>60.2 ? 0.7</cell><cell>61.2 ? 0.6</cell><cell>61.3 ? 1.0</cell></row><row><cell></cell><cell cols="4">4000 labels 62.4 ? 0.6 59.5 63.6 ? 0.7</cell><cell>64.3 ? 1.0</cell><cell>62.9 ? 0.7</cell><cell>62.4 ? 1.0</cell><cell>64.1 ? 0.8</cell><cell>65.9 ? 0.8</cell><cell>64.8 ? 1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Mean and standard deviation of MSE on spatial-temporal prediction datasets.</figDesc><table><row><cell>Dataset</cell><cell>MLP</cell><cell>GCN</cell><cell>GAT</cell><cell cols="6">Dense GAT GAT-kNN GCN-kNN DIFFORMER-s DIFFORMER-a DIFFORMER-s w/o g DIFFORMER-a w/o g</cell></row><row><cell>Chickenpox</cell><cell cols="3">0.924 (?0.001) (?0.001) (?0.002) 0.923 0.924</cell><cell>0.935 (?0.005)</cell><cell>0.926 (?0.004)</cell><cell>0.936 (?0.004)</cell><cell>0.914 (0.006)</cell><cell>0.915 (0.008)</cell><cell>0.916 (0.006)</cell><cell>0.916 (0.006)</cell></row><row><cell>Covid</cell><cell cols="3">0.956 (?0.198) (?0.162) (?0.336) 1.080 1.052</cell><cell>1.524 (?0.319)</cell><cell>0.861 (?0.123)</cell><cell>1.475 (?0.560)</cell><cell>0.779 (0.037)</cell><cell>0.757 (0.048)</cell><cell>0.779 (0.028)</cell><cell>0.741 (0.052)</cell></row><row><cell>WikiMath</cell><cell cols="3">1.073 (?0.042) (?0.125) (?0.073) 1.292 1.339</cell><cell>0.826 (?0.070)</cell><cell>0.882 (?0.015)</cell><cell>1.023 (?0.058)</cell><cell>0.731 (0.007)</cell><cell>0.763 (0.020)</cell><cell>0.727 (0.025)</cell><cell>0.716 (0.030)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>E.2 PSEUDO CODESWe provide the Pytorch-style pseudo codes for DIFFORMER class in Alg. 1 and the one-layer propagation of two model versions (shown in Alg. 2 for DIFFORMER-s and Alg. 3 for DIFFORMERa). The key design of our methodology lies in the model architectures which are shown in detail in Alg. 2 for DIFFORMER-s and Alg. 3 for DIFFORMER-a, where for each case, the model takes the data as input and outputs prediction for each individual instance. For more details concerning the implementation, please refer to our provided codes at the first page.</figDesc><table><row><cell>Algorithm 1 PyTorch-style Code for DIFFORMER</cell></row><row><cell># fcs: fully-connected layers</cell></row><row><cell># bns: layer normalization layers</cell></row><row><cell># convs: DIFFormer layers (see implementation in Alg. 2 and 3)</cell></row><row><cell># activation: activation function for the input layer</cell></row><row><cell># x: input data sized [N, D], N for instance number, D for input feature dimension</cell></row><row><cell># edge_index: input graph structure if available, None otherwise</cell></row><row><cell># tau: step size for each iteration update</cell></row><row><cell># use_act: whether to use activation for propagation layers</cell></row><row><cell>layer_ = []</cell></row><row><cell># input MLP layer</cell></row><row><cell>x = fcs[0](x)</cell></row><row><cell>x = bns[0](x)</cell></row><row><cell>x = activation(x)</cell></row><row><cell># store as residual link</cell></row><row><cell>layer_.append(x)</cell></row><row><cell>for i, conv in enumerate(convs):</cell></row><row><cell># graph convolution with global all-pair attention (specified by Alg. 2 for DIFFormer</cell></row><row><cell>-s and Alg. 3 for DIFFormer-a)</cell></row><row><cell>x = conv(x, x, edge_index)</cell></row><row><cell>x = tau * x + (1-tau) * layer_[i]</cell></row><row><cell>x = bns[i+1](x)</cell></row><row><cell>if use_act:</cell></row><row><cell>x = activation(x)</cell></row><row><cell>layer_.append(x)</cell></row><row><cell># output MLP layer</cell></row><row><cell>out = fcs[-1](x)</cell></row><row><cell># supervised loss calculation, negative log-likelihood</cell></row><row><cell>y_logp = F.log_softmax(out, dim=1)</cell></row><row><cell>loss = criterion(y_logp[train_idx], y_true[train_idx])</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>?</head><label></label><figDesc>Covid contains daily mobility graph between regions in England NUTS3 regions, with node features corresponding to the number of confirmed COVID-19 cases in the previous days from March to May 2020. The graph indicates how many people moved from one region to the other each day, based on Facebook Data For Good disease prevention maps. Node features correspond to the number of COVID-19 cases in the region in the past 8 days. The task is to predict the number of cases in each node after 1 day.? WikiMath is a dataset whose nodes describe Wikipedia pages on popular mathematics topics and edges denote the links from one page to another. Node features are provided by the number of daily visits between 2019 March and 2021 March. The graph is directed and weighted. Weights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page. The target is the daily user visits to the Wikipedia pages between March 16 th 2019 and March 15 th 2021 which results in 731 periods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Properties and summary statistics of the spatial-temporal datasets used in the experiments with information about whether the graph structure is dynamic or static, meaning of node features (the same as the prediction target) and the corresponding dimension (D), the number of snapshots (T ), the number of nodes (|V |), as well as the meaning of edges/edge weights.</figDesc><table><row><cell>Chickenpox</cell><cell>Static</cell><cell cols="5">Weekly Chickenpox Cases 4 Weekly 522 20 Direct Neighborhoods</cell></row><row><cell>Covid</cell><cell>Dynamic</cell><cell>Daily Covid Cases</cell><cell>8</cell><cell>Daily</cell><cell>61 129</cell><cell>Daily Mobility</cell></row><row><cell>WikiMath</cell><cell>Static</cell><cell>Daily User Visits</cell><cell cols="3">14 Daily 731 1,068</cell><cell>Page Links</cell></row></table><note><p>Dataset Graph structure Node features/ Target D Frequency T |V | Edges/ Edge weights</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Discussions on using different mini-batch sizes for training on Pokec. We report testing accuracy and training memory for comparison. ? 0.34 67.48 ? 0.81 68.53 ? 0.75 68.96 ? 0.63 69.24 ? 0.76 69.15 ? 0.52</figDesc><table><row><cell>Batch size</cell><cell>5000</cell><cell>10000</cell><cell>20000</cell><cell>50000</cell><cell>100000</cell><cell>200000</cell></row><row><cell cols="2">Test Acc (%) 65.24 GPU Memory (MB) 1244</cell><cell>1326</cell><cell>1539</cell><cell>2060</cell><cell>2928</cell><cell>4011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison of training time and memory of different models on Cora, Pokec, STL-10 and WikiMath. OOM refers to out-of-memory when training on a GPU with 16GB memory.For semi-supervised learning, there is a line of approaches that leverage pseudo labels to augment the training data. Our model DIFFORMER essentially has orthogonal technical aspects compared to this line of work in that we focus on building a new encoder backbone and only train the model with a standard supervised loss on the labeled data. This means that pseudo-label-based approaches are equally applicable for enhancing the training of our model as well as the competitors we used in our experiments.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">GCN GAT DenseGAT DIFFORMER-s DIFFORMER-a</cell></row><row><cell>Cora</cell><cell cols="3">Train time (s) Training memory (MB) 1168 1380 0.0584 0.0807 0.5165 8460</cell><cell>0.1438 1350</cell><cell>0.3292 3893</cell></row><row><cell>Pokec</cell><cell cols="2">Train time (s) Training memory (MB) 1812 2014 1.069 14.87</cell><cell>88.07 13174</cell><cell>2.206 2923</cell><cell>OOM OOM</cell></row><row><cell>STL</cell><cell cols="3">Train time (s) Training memory (MB) 1224 1980 0.0069 0.0424 OOM OOM</cell><cell>0.0323 1342</cell><cell>0.3298 7680</cell></row><row><cell>WikiMath</cell><cell cols="3">Train time (s) Training memory (MB) 1048 1054 0.0081 0.0261 0.0364 1316</cell><cell>0.0281 1046</cell><cell>0.0350 1142</cell></row><row><cell cols="3">I.6 INCORPORATION OF PSEUDO LABELS</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>The work was in part supported by <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2020AAA0107600</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62222607</rs>), <rs type="funder">STCSM</rs> (<rs type="grantNumber">22511105100</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QtETHh3">
					<idno type="grant-number">2020AAA0107600</idno>
				</org>
				<org type="funding" xml:id="_9TaygcP">
					<idno type="grant-number">62222607</idno>
				</org>
				<org type="funding" xml:id="_v2fqYdF">
					<idno type="grant-number">22511105100</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Propagation Layer. The initial embeddings Z (0) will be transformed into Z (1) , ? ? ? , Z (L) with L layers of propagation. We next illustrate one-layer propagation from Z (k) to Z (k+1) . We use the superscript (k, h) to denote the k-th layer and the h-th head:</p><p>where</p><p>? R d?d are trainable parameters of the h-th head at the k-th layer. Then the transformed embeddings will be fed into the all-pair propagation unit of DIFFORMER-s or DIFFORMER-a.</p><p>? For DIFFORMER-s: we adopt L2 normalization for key and query vectors</p><p>where</p><p>denotes the i-th row vector of K (k,h) ? R N ?d . Then the all-pair propagation of the h-th head is achieved by</p><p>The above computation only requires linear complexity w.r.t. N since the bottleneck computation lies in Q(k,h) ( K(k,h) ) V (k,h) where the two matrix products both require</p><p>? For DIFFORMER-a: we need to compute the all-pair similarity before aggregating the results If using input graphs, we add the updated embeddings of GCN-based propagation to the all-pair propagation's ones:</p><p>where A is the input graph and D denotes its corresponding diagonal degree matrix.</p><p>We then average the propagated results of multiple heads:</p><p>The next-layer embeddings will be updated by</p><p>where ? can be identity mapping or non-linear activation (e.g., ReLU).</p><p>Output Layer. After K layers of propagation, we then use a shallow fully-connected layer to output the predicted logits:</p><p>where W O ? R d?C and b O ? R C are trainable parameters, and C denotes the number of classes. And, the predicted logits ? will be used for computing a loss of the form l( ?, Y) where l can be cross-entropy for classification or mean square error for regression.</p><p>Algorithm 2 PyTorch-style Code for One-layer Feed-forward of DIFFORMER-s # x: data embeddings sized <ref type="bibr">[N, d]</ref>, N for instance number, d for hidden size # edge_index: input graph structure if available, None otherwise # H: head number # use_graph: whether to use input graph # use_weight: whether to use feature transformation for each layer # graph_conv: graph convolution operatior using the normalized adjacency matrix D^{-1/2} AD^{-1/2} # Wq, Wk, Wv: weight matrices for feature transformation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F CONNECTIONS WITH EXISTING MODELS</head><p>MLP. MLPs can be viewed as a simplified diffusion model with only non-zero diffusivity values on the diagonal line, i.e., S (k) ij = 1 if i = j and 0 otherwise. From a graph convolution perspective, MLP only considers propagation on the self-loop connection in each layer. Correspondlying, the energy function the feed-forward process of the model essentially optimizes would be</p><p>F , which only counts for the local consistency term and ignores the global information. <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017</ref>) define a convolution operator on a graph G = (V, E) by multiplying the node feature matrix with D -1 2 AD -1 2 where A denotes the adjacency matrix and D is its associated degree matrix. The layer-wise updating rule can be written as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN. Graph Convolution Networks</head><p>Eq. 32 generalizes the original message-passing rule of GCN by adding an additional self-loop links with adaptive weights for different nodes. The diffusivity matrix is defined with the observed adjacency, i.e., ?(k) = D -1 2 AD -1 2 . Though such a design leverages the geometric information from input structures as a guidance for feature propagation, it constrains the efficiency of layer-wise information flows within the receptive field of local neighbors and could only exploit partial global information with a finite number of iterations.</p><p>GAT. Graph Attention Networks <ref type="bibr" target="#b41">(Velickovic et al., 2018)</ref> extend the GCN architecture to incorporate attention mechanisms as a learnable function producing adaptive weights for each observed edge. From our diffusion perspective, the attention matrix for layer-wise convolution can be treated as the diffusivity in our updating rule:</p><p>We notice that</p><p>?(k) ij = 1 due to the normalization in the denominator. Therefore, the layer-wise updating can be viewed as an attentive aggregation over graph structures and a subsequent residual link. In fact, the original implementation for the GAT model <ref type="bibr" target="#b41">(Velickovic et al., 2018)</ref> specifies the function f as a particular form, i.e., exp(LeakyReLU(a [Wz</p><p>)), which can be viewed as a generalized similarity function given trainable W, a towards optimizing a supervised loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G DATASET INFORMATION</head><p>In this section, we present the detailed information for all the experimental datasets, the pre-processing and evaluation protocol used in Section 5. We use feature transformation for each layer on two large datasets and omit it for citation networks. The head number is set as 1. We set ? = 0.5 and incorporate the input graphs for DIFFORMER. For other hyper-paramters, we adopt grid search for all the models with learning rate from {0.0001, 0.001, 0.01, 0.1}, weight decay for the Adam optimizer from {0, 0.0001, 0.001, 0.01, 0.1, 1.0}, dropout rate from {0, 0.2, 0.5}, hidden size from {16, 32, 64}, number of layers from {2, 4, 8, 16}. For evaluation, we compute the mean and standard deviation of the results with five repeating runs with different initializations. For each run, we run for a maximum of 1000 epochs and report the testing performance achieved by the epoch yielding the best performance on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 IMAGE AND TEXT CLASSIFICATION EXPERIMENT</head><p>For image and text datasets, we consider feature transformation for layer-wise updating. The head number is set as 1. We set ? = 0.5. These datasets do not have input graphs so we only consider learning new structures for the diffusion model. For hyper-parameter settings, we conduct grid search for all the models with learning rate from {0.0001, 0.0005, 0.005, 0.01, 0.05}, weight decay for the Adam optimizer from {0.0001, 0.001, 0.01, 0.1}, dropout rate from {0, 0.2, 0.5}, hidden size from {32, 64, 100, 200, 300, 400}, number of layers from {1, 2, 4, 6, 8, 10, 12}. We average the results for five repeating runs and report as well the standard deviation. For each run, we run for a maximum of 600 epochs and report the testing accuracy achieved by the epoch yielding the highest accuracy on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 SPATIAL-TEMPORAL PREDICTION EXPERIMENT</head><p>We do not use feature transformation for these datasets due to their small sizes and also set ? = 0.5. The head number is set as 1. These spatial-temporal dynamics prediction datasets contain available graph structures, we consider both cases, using the input graphs and not, in our experiments and discuss their impact on the performance. For other hyper-parameters, we also consider grid search for all models here with learning rate from {0.01, 0.05, 0.005}, weight decay for the Adam optimizer from {0, 0.005}, dropout rate from {0, 0.2, 0.5}, and report the test mean squared error (MSE) based on the lowest validation MSE. We average the results for five repeating runs and report as well the standard deviation for each MSE result. For each run, we run for a maximum of 200 epochs in total and stop the training process with 20-epoch early stopping on the validation performance. The data split is done in time order, and hence is deterministic. We report the results using the same hidden size (4) and number of layers (2) for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I MORE EXPERIMENT RESULTS</head><p>We supplement more experiment results including extra ablation studies, hyper-parameter studies and visualization results on more datasets that are not presented in Section 5 due to the limit of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 ABLATION STUDIES</head><p>In Fig. <ref type="figure">4</ref> we present more experiment results for ablation studies w.r.t. the energy function forms used by DIFFORMER. See discussions and analysis in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 HYPER-PARAMETER ANALYSIS</head><p>We plot the testing performance of several baselines and DIFFORMER with different step size ? as the model size K increases in Fig. <ref type="figure">5</ref>. See discussions and analysis in Section 5.     <ref type="figure">6</ref> and 7 plot the produced instance-level representations and diffusion strength estimates by the model on 20News and STL, respectively. We observe that the diffusivity estimates tend to connect nodes with different classes, which contribute to increasing the global connectivity and facilitate absorbing other instances' information for informative representations. The node embeddings produced by our model have small intra-class distance and large inter-class distance, making it easier for the classifier to distinguish. Fig. <ref type="figure">8</ref> visualizes the diffusivity estimates on Chickenpox. We conclude that large diffusion strength usually exists between nodes with similar ground-truth labels. DIFFORMER-s has more concentrated large weights while DIFFORMER-a tends to have large diffusivity spreading out more. DIFFORMERa indeed learns more complex underlying structures than DIFFORMER-s due to its better capacity for diffusivity modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.4 IMPACT OF MINI-BATCH SIZES ON LARGE GRAPHS</head><p>The randomness of mini-batch partition on large graphs has negligible effect on the performance since we use large batch sizes for training, which is facilitated by the linear complexity of DIFFORMER-s. Even setting the batch size to be 100000, our model only costs 3GB GPU memory on Pokec. As a further investigation on this, we add more experiments using different batch sizes on Pokec and the results are shown in Table <ref type="table">8</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GRAND: graph neural diffusion</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">I</forename><surname>Gorinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1407" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beltrami flow and neural diffusion on graphs</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NAGphormer: A tokenized graph transformer for node classification in large graphs</title>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam?s</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR, abs/2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PDE-GCN: novel architectures for graph neural networks motivated by partial differential equations</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Eliasof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Treister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slaps: Self-supervision improves structure learning for graph neural networks</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Diffusion processes on graphs and the averaging principle. The Annals of probability</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">D</forename><surname>Freidlin</surname></persName>
		</author>
		<author>
			<persName><surname>Wentzell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="2215" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning dynamical systems from data: A simple crossvalidation perspective, part i: Parametric kernel flows</title>
		<author>
			<persName><forename type="first">Boumediene</forename><surname>Hamzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houman</forename><surname>Owhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">421</biblScope>
			<biblScope unit="page">132817</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kernelbased inference of functions over graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vassilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><forename type="middle">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Nikolakopoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Giannakis</surname></persName>
		</author>
		<idno>CoRR, abs/1711.10353</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Artificial neural networks for solving ordinary and partial differential equations</title>
		<author>
			<persName><forename type="first">Aristidis</forename><surname>Isaac E Lagaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><surname>Fotiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="987" to="1000" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational inference for training graph neural networks in low-data regime through joint structure-label estimation</title>
		<author>
			<persName><forename type="first">Danning</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="824" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified view on graph neural networks as graph signal denoising</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The nonlinear heat equation on dense graphs and graph limits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Georgi</surname></persName>
		</author>
		<author>
			<persName><surname>Medvedev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2743" to="2766" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11557" to="11568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex analysis</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Laplacian on a Riemannian manifold: an introduction to analysis on manifolds</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosenberg</forename><surname>Steven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch geometric temporal: Spatiotemporal signal processing with neural machine learning models</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Astefanoaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guzm?n</forename><surname>Beres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName><surname>Collignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4564" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adaptive graph diffusion networks</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012.15024, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GRAND++: graph neural diffusion with a source term</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Strohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Netlsd: Hearing the shape of a graph</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2347" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dissecting the diffusion process in linear graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ACMP: Allen-cahn message passing with attractive and repulsive forces for graph neural networks</title>
		<author>
			<persName><forename type="first">Yuelin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?d?ric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nodeformer: A scalable graph structure learning transformer for node classification</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keting</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16002</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Geometric knowledge distillation: Topology compression for graph neural networks</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph neural networks are inherently good generalizers: Insights by bridging GNNs and MLPs</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph neural networks inspired by classical iterative algorithms</title>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11773" to="11783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Do transformers really perform bad for graph representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scalegcn: Efficient and effective graph convolution via channel-wise scale transformation</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyasundar</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>?stebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5829" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Navin Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A FURTHER RELATED WORKS AND CONNECTION WITH OURS We discuss more related works that associate with ours from different aspects to properly position this paper with different areas. Based on this, we further shed more lights on the technical contributions of our work and its potential impact in different communities</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">(2021a) and its follow-ups (Chamberlain et al., 2021b; Thorpe et al., 2022) reveal the analogy between the discretization of diffusion process and GNNs&apos; feedforward rules, and devise new (continuous) models on graphs whose training requires PDE-solving tools. A concurrent work (Wang et al., 2023) explores how to derive neural networks from gradient flows and proposes Allen-Cahn Message Passing that combines attractive and repulsive effects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pde-Based Learning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Lagaris</surname></persName>
		</author>
		<editor>e.g., Chamberlain et al.</editor>
		<imprint>
			<date type="published" when="1998">1998. 2018</date>
		</imprint>
	</monogr>
	<note>The diffusion-based learning has gained increasing research interests, as the continuous dynamics can serve as an inductive bias incorporated with prior knowledge of the tasks at hand. One category directly solves a continuous process of differential equations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
