<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Pyramid Co-occurrence for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yyang6@ucmerced.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of California at Merced</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatial Pyramid Co-occurrence for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73FB84C309D9116E5C3730C90FDCD99F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a novel image representation termed spatial pyramid co-occurrence which characterizes both the photometric and geometric aspects of an image. Specifically, the co-occurrences of visual words are computed with respect to spatial predicates over a hierarchical spatial partitioning of an image. The representation captures both the absolute and relative spatial arrangement of the words and, through the choice and combination of the predicates, can characterize a variety of spatial relationships.</p><p>Our representation is motivated by the analysis of overhead imagery such as from satellites or aircraft. This imagery generally does not have an absolute reference frame and thus the relative spatial arrangement of the image elements often becomes the key discriminating feature. We validate this hypothesis using a challenging ground truth image dataset of 21 land-use classes manually extracted from high-resolution aerial imagery. Our approach is shown to result in higher classification rates than a non-spatial bagof-visual-words approach as well as a popular approach for characterizing the absolute spatial arrangement of visual words, the spatial pyramid representation of Lazebnik et  al. [7]. While our primary objective is analyzing overhead imagery, we demonstrate that our approach achieves stateof-the-art performance on the Graz-01 object class dataset and performs competitively on the 15 Scene dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Local invariant features have proven effective for a range of computer vision problems over the last decade. These features characterize the photometric aspects of an image while allowing for robustness against variations in illumination and noise. The geometric aspects of an image can further be characterized by considering the spatial arrangement of the local features.</p><p>This paper proposes a novel image representation termed spatial pyramid co-occurrence which characterizes both the photometric and geometric aspects of an image. Specif-  ically, the co-occurrences of visual words-quantized local invariant features-are computed with respect to spatial predicates over a hierarchical spatial partitioning of an image. The local co-occurrences combined with the global partitioning allows the proposed approach to capture both the relative and absolute layout of an image. This is one of the salient aspects of spatial pyramid co-occurrence.</p><p>Another salient aspect of the proposed approach is that it is general enough to characterize a variety of spatial arrangements. We give examples of spatial predicates which constrain the distances between pairs of visual words, the relative orientation between pairs of words, or both.</p><p>We are motivated by the problem of analyzing overhead imagery such as from satellites or aircraft. This imagery generally does not have an absolute reference frame and thus the relative spatial arrangement of the image elements often becomes the key discriminating feature. See, for example, the images of different land-use classes in figure <ref type="figure" target="#fig_1">1</ref>.</p><p>We evaluate our approach using a novel ground truth image dataset of 21 land-use classes manually extracted from publicly available high-resolution overhead imagery. This dataset is one of the first of its kind and will be made available for other researchers <ref type="foot" target="#foot_0">1</ref> . Our approach is shown to result in higher classification rates on the land-use dataset than a non-spatial bag-of-visual-words approach as well as a popu-lar approach for characterizing the absolute spatial arrangement of visual words, the spatial pyramid representation of Lazebnik et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>We perform a thorough evaluation of the effects of different configurations of our approach such as the size of the visual dictionary and the specificity of the spatial predicate. We report interesting findings like the fact that smaller visual dictionaries become preferable for the co-occurrence component of our representation as the spatial predicate becomes more specialized. This somewhat counter-intuitive result has important implications for the computational complexity of our representation.</p><p>Finally, even though our primary objective is analyzing overhead imagery, we demonstrate that our approach achieves state-of-the-art performance on the Graz object class evaluation dataset and performs competitively on the 15 Scene evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The broader context of our work is bag-of-visual-words (BOVW) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> approaches to image classification. These approaches quantize local invariant image descriptors using a visual dictionary typically constructed through k-means clustering. The set of visual words is then used to represent an image regardless of their spatial arrangement similar to how documents can be represented as an unordered set of words in text analysis. The quantization of the often high-dimensional local descriptors provides two important benefits: it provides further invariance to photometric image transformations, and it allows compact representation of the image such as through a histogram of visual word counts and/or efficient indexing through inverted files. The size of the visual dictionary used to quantize the descriptors controls the tradeoff between invariance/efficiency and discriminability.</p><p>Lazebnik et al. <ref type="bibr" target="#b6">[7]</ref> was one of the first works to address the lack of spatial information in the BOVW representation. Their spatial pyramid representation was motivated by earlier work termed pyramid matching by Grauman and Darrell <ref type="bibr" target="#b3">[4]</ref> on finding approximate correspondences between sets of points in high-dimensional feature spaces. The fundamental idea behind pyramid matching is to partition the feature space into a sequence of increasingly coarser grids and then compute a weighted sum over the number of matches that occur at each level of resolution. Two points are considered to match if they fall into the same grid cell and matched points at finer resolutions are given more weight than those at coarser resolutions. The spatial pyramid representation of Lazebnik et al. applies this approach in the two-dimensional image space instead of the feature space; that is, it finds approximate spatial correspondences between sets of visual words in two images.</p><p>The spatial pyramid representation characterizes the ab-solute location of the visual words in an image. Saverese et al. <ref type="bibr" target="#b11">[12]</ref> propose a model which instead characterizes the relative locations. Motivated by earlier work on using correlograms of quantized colors for indexing and classifying images <ref type="bibr" target="#b5">[6]</ref>, they use correlograms of visual words to model the spatial correlations between quantized local descriptors. The correlograms are three dimensional structures which in essence record the number of times two visual words appear at a particular distance from each other. Correlogram elements corresponding to a particular pair of words are quantized to form correlations. Finally, images are represented as histograms of correlations and classified using nearest neighbor search against exemplar images. One challenge of this approach is that the quantization of correlograms to correlations can discard the identities of associated visual word pairs and thus may diminish the discriminability of the local image features. Ling and Soatto <ref type="bibr" target="#b7">[8]</ref> also characterize the relative locations of visual words. Their proximity distribution representation is a three dimensional structure which records the number of times a visual word appears within a particular number of nearest neighbors of another word. It thus captures the distances between words based on ranking and not absolute units. A corresponding proximity distribution kernel is used for classification in a support vector machine (SVM) framework. However, since proximity kernels are applied to the whole image, distinctive local spatial distributions of visual words may be overshadowed by global distributions.</p><p>Liu et al. <ref type="bibr" target="#b8">[9]</ref> extend the BOVW framework by calculating spatial histograms where the co-occurrences of local features are calculated in circular regions of varying distances. However, the spatial histograms are only extracted for select visual words determined through an additional feature selection algorithm. Also, the spatial histograms are generated by averaging the counts of co-occurrences throughout the entire image and thus may also fail to capture distinctive local spatial arrangements.</p><p>Our proposed spatial pyramid co-occurrence differs from the above approaches in the following ways:</p><p>• It characterizes both the absolute and relative spatial layout of an image. • It can characterize a greater variety of local spatial arrangements through the underlying spatial predicate. For example, combined proximity and orientation predicates can capture the general spatial distribution of visual words as well as the shape of local regions. • The approach is simple in that it does not require learning a generative or other form of model. • The representation can be easily combined with other representations such as a non-spatial bag-of-visualwords. And, since the representations are fused late, visual dictionaries of different sizes can be used for the spatial (co-occurrence) and non-spatial components of the combined representation. This allows the nonspatial component to leverage the increased discriminability of the larger dictionary while limiting the computational costs associated with storing and comparing the co-occurrence structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Spatial pyramid co-occurrence characterizes both the absolute and relative spatial arrangement of visual words in an image. After stating our assumptions and describing the non-spatial BOVW representation, we review the spatial pyramid representation of Lazebnik et al. <ref type="bibr" target="#b6">[7]</ref> since the proposed method uses the same hierarchical decomposition of an image. We then describe the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Assumptions</head><p>We assume each image I contains a set of N visual words c i at pixel locations (x i , y i ) where each word has been assigned a discrete label c i ∈ [1...M ] from a visual dictionary containing M entries. The locations of the visual words could either be determined using a (dense) grid or an interest-point/saliency detector. We use Lowe's scale invariant feature transform (SIFT) detector <ref type="bibr" target="#b9">[10]</ref> in the experiments below. Local invariant features are extracted at these locations and quantized into a discrete set of labels using a codebook typically generated by applying k-means clustering to a large, random set of features. We also use Lowe's SIFT descriptor <ref type="bibr" target="#b9">[10]</ref> in the experiments below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BOVW Representation</head><p>The non-spatial BOVW representation simply records the visual word occurrences in an image. It is typically represented as a histogram</p><formula xml:id="formula_0">BOV W = [t 1 , t 2 , . . . , t M ] ,</formula><p>where t m is the number of occurrences of visual word m. To account for the difference in the number of visual words between images, the BOVW histogram is typically normalized to have unit L1 norm.</p><p>A BOVW representation can be used in kernel based learning algorithms, such as non-linear support vector machines, by computing the intersection between histograms. Given BOV W 1 and BOV W 2 corresponding to two images, the BOVW kernel is computed as:</p><formula xml:id="formula_1">K BOV W (BOV W 1, BOV W 2) = M m=1 min (BOV W 1(m), BOV W 2(m)).</formula><p>The intersection kernel is a Mercer kernel which guarantees an optimal solution to kernel-based algorithms based on convex optimization such as nonlinear SVMs.</p><formula xml:id="formula_2">             1/2 level 2              1/4 level 1              1/4 level 0 Figure 2.</formula><p>Toy example of a three-level spatial pyramid (adapted from <ref type="bibr" target="#b6">[7]</ref>).</p><p>The image has three visual words and is divided at three different levels of resolution. For each level, the number of words in each grid cell is counted. Finally, the spatial histogram is weighted according to equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatial Pyramid</head><p>The spatial pyramid representation of Lazebnik et al. <ref type="bibr" target="#b6">[7]</ref> partitions an image into a sequence of spatial grids at resolutions 0, . . . , L such that the grid at level l has 2 l cells along each dimension for a total of D = 4 l cells. A BOVW histogram is then computed separately for each cell in the multiresolution grid. Specifically, H l (k, m) is the count of visual word m contained in grid cell k at level l. This representation is summarized in figure <ref type="figure">2</ref>.</p><p>A spatial pyramid match kernel (SPMK) is derived as follows. Let H1 l and H2 l be the histograms of two images at resolution l. Then, the number of matches at level l is computed as the histogram intersection:</p><formula xml:id="formula_3">I (H1 l , H2 l ) = D k=1 M m=1 min (H1 l (k, m) , H2 l (k, m)) .</formula><p>Abbreviate I (H1 l , H2 l ) to I l . Since the number of matches at level l includes all matches at the finer level l+1, the number of new matches found at level l is I l -I l+1 for l = 0, . . . , L -1. Further, the weight associated with level l is set to 1  2 L-l which is inversely proportional to the cell size and thus penalizes matches found in larger cells. Finally, the SPMK for two images is given by:</p><formula xml:id="formula_4">K SP MK = I L + L-1 l=0 1 2 L-l (I l -I l+1 ) . (<label>1</label></formula><formula xml:id="formula_5">)</formula><p>The SPMK is also a Mercer kernel <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spatial Co-occurrence</head><p>Spatial co-occurrence of visual words is motivated by Haralick et al.'s seminal work <ref type="bibr" target="#b4">[5]</ref> on gray level cooccurrence matrices (GLCM) which is some of the earliest work on image texture. A GLCM provides a straightforward way to characterize the spatial dependence of pixel values in an image. We extend this to the spatial dependence of visual words.</p><p>Formally, given an image I containing a set of N visual words c i at pixel locations (x i , y i ) and a binary spatial pred-icate ρ where c i ρc j ∈ {T, F }, we define the visual word co-occurrence matrix (VWCM) as</p><formula xml:id="formula_6">V W CM ρ (u, v) = (c i , c j )| (c i = u)∧(c j = v)∧(c i ρc j ) .</formula><p>That is, the VWCM is a count of the number of times two visual words satisfy the spatial predicate. The choice of the predicate ρ determines the nature of the spatial dependencies. This framework can support a variety of dependencies such as the two visual words needing to be within a certain distance of each other, to have the same orientation, etc. We describe a number of predicates in the experiments section.</p><p>We derive a spatial co-occurrence kernel (SCK) as follows. Given two visual co-occurrence matrices V W CM1 ρ and V W CM2 ρ corresponding to two images, the SCK is computed as the intersection between the matrices</p><formula xml:id="formula_7">K SCKρ (V W CM1 ρ , V W CM2 ρ ) = u,v∈M min(V W CM1 ρ (u, v), V W CM2 ρ (u, v)).</formula><p>To account for differences in the number of pairs of codewords satisfying the spatial predicate between images, the matrices are normalized to have an L1 norm of one. The SCK, as an intersection of two multidimensional counts, is also Mercer kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Combining Multiple Spatial Predicates</head><p>Multiple binary spatial predicates can be combined as follows. Given co-occurrence matrices V W CM1 ρA (u, v) and V W CM2 ρA (u, v) corresponding to predicate ρ A for two images, and co-occurrence matrices V W CM1 ρB (u, v) and V W CM2 ρB (u, v) corresponding to predicate ρ B for the same two images, a single SCK is computed as the sum of the individual SCKs</p><formula xml:id="formula_8">KSCK ρ A +ρ B = KSCK ρ A (V W CM1ρ A , V W CM2ρ A ) + KSCK ρ B (V W CM1ρ B , V W CM2ρ B ).</formula><p>This too is a Mercer kernel. While it is possible to weight the components corresponding to the two predicates differently, we have so far not considered this and leave it for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Spatial Pyramid Co-occurrence</head><p>We now describe the main contribution of this paper, spatial pyramid co-occurrence. Again, an image is partitioned into a sequence of spatial grids at resolutions 0, . . . , L such that the grid at level l has 2 l cells along each dimension for a total of D = 4 l cells. The spatial cooccurrence of visual words is then computed separately for each cell in the multiresolution grid. Specifically, given a binary spatial predicate ρ, compute</p><formula xml:id="formula_9">V W CM l ρ (k, u, v) = (c i , c j )| (c i = u)∧(c j = v)∧(c i ρc j )</formula><p>where the visual words c i are restricted to those in grid cell k at pyramid level l.</p><p>A spatial pyramid co-occurrence kernel (SPCK) corresponding to the spatial pyramid co-occurrences for two images V W CM1 ρ and V W CM2 ρ is then computed as</p><formula xml:id="formula_10">KSP CK (V W CM1ρ, V W CM2ρ) = L X l=0 w l D X k=1 X u,v∈M min(V W CM1 l ρ (k, u, v), V W CM2 l ρ (k, u, v))</formula><p>where the weights w l are chosen so that the sum of intersections has the same maximum achievable value for each level; e.g., w l = 1/4 l . As a sum of intersections, the SPCK is a Mercer kernel.</p><p>Note that the spatial pyramid co-occurrence representation captures both the absolute and relative spatial arrangements of the visual words. The pyramid decomposition characterizes the absolute locations through the hierarchical gridding of the image and the VLCMs characterize the relative arrangements within the individual grid cells.</p><p>Multiple binary spatial predicates can again be combined by summing the SPCKs corresponding to the individual predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Extended SPCK</head><p>The SPCK and the non-spatial BOVW representations are complementary and so it is natural to consider combining them. We thus form an extended SPCK representation, termed SPCK+, as the sum of the individual kernels:</p><formula xml:id="formula_11">KSP CK+ ({V W CM1ρ, BOV W 1}, {V W CM2ρ, BOV W 2}) = KSP CK (V W CM1ρ, V W CM2ρ)+KBOV W (BOV W 1, BOV W 2).</formula><p>This sum is a Mercer kernel. We have not considered different weights for the the spatial and non-spatial components of the combined kernel and leave this too for future work.</p><p>Note that since the spatial and non-spatial components of our representation are fused late, the visual dictionary used to derive the spatial co-occurrence matrices need not be the same as that used to derive the BOVW histograms. Indeed, the experiments below show that smaller co-occurrence dictionaries are preferable for SPCK+ as the spatial predicates become more specialized. This helps reduce the computational complexity of the proposed approach.</p><p>Since SPCK and SPMK are also complementary in how they characterize spatial dependencies, we also consider a second extended SPCK representation, termed SPCK++, as the sum of the SPCK and SPMK kernels:</p><formula xml:id="formula_12">K SP CK++ = K SP CK + K SP MK .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Computational Complexity</head><p>We compare the computational costs of BOVW, SMPK, and SPCK in terms of the sizes of the different representations and the operations required to evaluate the kernels.</p><p>For a dictionary of size M , the BOVW representation has size M and evaluating the BOVW kernel requires M min computations (plus M -1 additions). For the same sized dictionary, an SPMK representation with levels 0, . . . , L has size</p><formula xml:id="formula_13">S SP MK = L l=0 l 4 k=1 M</formula><p>and evaluating the SPMK kernel requires the same number of min computations. For L = 2, S SP MK = 21M .</p><p>A VLCM corresponding to a co-occurrence dictionary of size N has N 2 entries (this reduces to N (N + 1)/2 unique entries for symmetric spatial predicates such as those used in the experiments below). So, a SPCK representation with levels 0, . . . , L has size</p><formula xml:id="formula_14">S SP CK = L l=0 l 4 k=1 N 2</formula><p>and evaluating the SPCK kernel requires the same number of min computations. For L = 2, S SP CK = 21N 2 . In the case where N ≤ √ M , the computational complexity of SPCK in terms of storage and kernel-evaluation is O(M ), the same as for BOVW and SPMK. This remains true when combining multiple spatial predicates. This is significant with respect to the finding in the experiments below that greatly reduced co-occurrence dictionaries are sufficient or even optimal for the extended SPCK representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We evaluate our proposed spatial pyramid co-occurrence representation on three datasets: 1) a novel dataset of landuse classes in high-resolution overhead imagery, 2) the publicly available Graz-01 object class evaluation dataset, and 3) the publicly available 15 Scene evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spatial Predicates</head><p>We consider two types of spatial predicates: proximity predicates which characterize the distance between pairs of visual words, and orientation predicates which characterize the relative orientations of pairs of visual words. Proximity Since our primary goal is to analyze overhead imagery, and, according to Tobler's first law of geography, all things on the surface of the earth are related but nearby things are more related than distant things <ref type="bibr" target="#b13">[14]</ref>, we define a proximity predicate ρ prox to be true when two visual words are within r pixels of each other. That is, given visual words c i and c j at locations (x i , y i ) and (x j , y j ),</p><formula xml:id="formula_15">c i ρ prox c j = T, if (x i -x j ) 2 + (y i -y j ) 2 ≤ r; F, otherwise.</formula><p>(3) Thus, the VWCM corresponding to ρ prox indicates the number of times pairs of codewords appear within r pixels of each other in a given image or region. Figure <ref type="figure" target="#fig_2">3(a)</ref> shows an example of where ρ prox evaluates to F for two words. Orientation The SIFT detector provides the orientation of the interest points used to derive the visual words. We postulate that these orientations are indicative of the local shape of image regions and thus derive orientation predicates ρ orien which consider the relative orientations of pairs of visual words.</p><p>Given visual words c i and c j with (absolute) orientations θ i and θ j with respect to some canonical direction such as the x-axis, we define a pair of orientation predicates, one which evaluates to true when the visual words are in-phase (pointing in the same direction) and another which evaluates to true when the visual words are out-of-phase (pointing in opposite directions):</p><formula xml:id="formula_16">c i ρ orien21 c j = T, if cos(θ i -θ j ) ≥ 0; F, otherwise<label>and</label></formula><formula xml:id="formula_17">c i ρ orien22 c j = T, if cos(θ i -θ j ) &lt; 0; F, otherwise where -π &lt; θ i , θ j ≤ π. Figure 3(b)</formula><p>shows an example of where ρ orien21 evaluates to T and ρ orien22 evaluates to F for two words. We also define a set of four orientation predicates ρ orien41,...,4 which partition the phase space into four bins. That is, the four predicates separately evaluate to true for</p><formula xml:id="formula_18">{ √ 2/2 ≤ cos(θ i -θ j )}, {0 ≤ cos(θ i -θ j ) &lt; √ 2/2}, {- √ 2/2 ≤ cos(θ i -θ j ) &lt; 0}, and {cos(θ i -θ j ) &lt; - √ 2/2}.</formula><p>We characterize the relative instead of absolute orientation of pairs of visual words since overhead imagery generally does not have an absolute reference frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Land-Use Dataset</head><p>We evaluate SPCK using a ground truth image dataset of 21 land-use classes. This dataset was manually extracted from aerial orthoimagery downloaded from the United States Geological Survey (USGS) National Map. The images have a resolution of one foot per pixel. 100 images measuring 256×256 pixels were manually selected for each of the following 21 classes: agricultural, airplane, baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection, medium density residential, mobile home park, overpass, parking lot, river, runway, sparse residential, storage tanks, and tennis courts. Note that we use the term land-use to refer to this set of classes even though they contain some land-cover and possibly object classes. This particular set of classes was selected because it contains a variety of spatial patterns.</p><p>To the best of our knowledge, this is one of the first ground truth datasets derived from publicly available highresolution overhead imagery. This allows us to make it available to other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Land-Use Dataset Experiments</head><p>We construct visual dictionaries of varying size by applying k-means clustering to over a million SIFT features randomly sampled from images disjoint from the ground truth images. These dictionaries are then used to label SIFT features extracted from the 2,100 ground truth images.</p><p>We use an SVM classification framework to compare the different representations and their associated kernels. Multi-class classification is implemented using a set of binary classifiers and taking the majority vote. Non-linear SVMs incorporating the kernels described above are trained using grid-search for model selection. The only parameter that needs to be estimated is the penalty parameter of the error term. Five-fold cross-validation is performed in which the ground truth dataset is randomly split into five equal sized sets. The classifier is then trained on four of the sets and evaluated on the held-out set. The classification rate is the average over the five evaluations. The results presented below are the average rates over all 21 classes. The SVMs are implemented using the LIBSVM package <ref type="bibr" target="#b0">[1]</ref>.</p><p>We compare the following approaches:</p><p>• The "baseline" non-spatial BOVW kernel (sec. 3.2).</p><p>• The spatial pyramid match kernel (SPMK) <ref type="bibr" target="#b6">[7]</ref> (sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3).</head><p>• The proposed spatial pyramid co-occurrence kernel (SPCK) (sec. 3.6). • The extended SPCK+ and SPCK++ representations (sec. 3.7).</p><p>We also compare the following configurations:</p><p>• A proximity predicate alone. This is referred to as SP1 below. We consider distances of r = 20, 50, 100, and 150 pixels.</p><p>• A proximity predicate combined with orientation predicates corresponding to a two-bin phase space. This is referred to as SP2 below.</p><p>• A proximity predicate combined with orientation predicates corresponding to a four-bin phase space. This is referred to as SP3 below.</p><p>• Visual dictionary sizes of 10, 50, and 100 for the cooccurrence component of the SPCK.</p><p>• Different numbers of pyramid levels in the SPCK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Land-Use Dataset Results</head><p>Table <ref type="table" target="#tab_0">1</ref> compares the best classification rates of the different approaches for the land-use dataset. A visual dictionary size of 100 is used for the BOVW and SPMK approaches as well as the BOVW and SPMK extensions to SPCK. Visual dictionary sizes of 100, 10, and 50 are used for the co-occurrence components of SPCK, SPCK+, and SPCK++. Combined proximity plus 4-bin orientation predicates (SP3) are used for SPCK, SPCK+, and SPCK++.</p><p>The land-use dataset is challenging and so the improvement that the proposed SPCK+ and SPCK++ provide over SPMK is significant. In particular, SPCK++ improves performance over SPMK by more than what SPMK itself improves over the non-spatial BOVW. And, SPCK+ provides about the same improvement. Note that since SMPK includes BOVW by construction, SPCK+ is a suitable comparison since it is simply SPMK combined with BOVW.</p><p>We pick a relatively small BOVW and SPMK dictionary size of 100 for the sake of comparison. SPCK+ and SPCK++ provide a similar improvement over BOVW and SPMK for larger dictionary sizes.</p><p>The remainder of this section provides further analysis of the proposed SPCK. Co-occurrence Dictionary Size Table <ref type="table" target="#tab_1">2</ref> and figure <ref type="figure" target="#fig_3">4</ref> show the effect of co-occurrence dictionary size on SPCK and SPCK+. The significant result here is that smaller cooccurrence dictionaries become sufficient or even optimal as the SPCK+ spatial predicates become more specialized. In particular, a co-occurrence dictionary of just 10 codewords provides better SP3 (SPCK+) performance than one with 50 or 100 codewords. This reduces the computational complexity of SPCK+ to be of the same order as SPMK.    Results are shown for two different spatial predicate configurations as well as for different co-occurrence dictionary sizes.</p><p>Proximity Distance Table <ref type="table" target="#tab_2">3</ref> and figure <ref type="figure" target="#fig_4">5</ref> show the effect of the spatial predicate proximity distance (r in Eq. 3) on SPCK. Results are shown for the three different spatial predicate configurations as well as for different cooccurrence dictionary sizes. The clear trend is that larger distances improve performance. This indicates that even  Results are shown for two different spatial predicate configurations as well as for different co-occurrence dictionary sizes.</p><p>long range spatial interactions between visual words is important for characterizing the land-use classes.</p><p>Pyramid Levels Table <ref type="table" target="#tab_3">4</ref> and figure <ref type="figure" target="#fig_5">6</ref> shows the effect of the number of pyramid levels on SPCK. Results are shown for just level 0, just level 1, just level 2, and for all three levels combined. While combining all three levels usually performs best, the interesting trend is that the order of the individual levels depends on the size of the co-occurrence dictionary. In particular, level 0 performs best for a cooccurrence dictionary of size 100 while level 2 performs best for a dictionary of size 10. We will investigate this further in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Graz-01 Dataset</head><p>We also apply our approach to the publicly available dataset Graz-01 <ref type="bibr" target="#b10">[11]</ref>. This dataset contains 373 images of category bike, 460 images of category person, and 270 background images as category "counter-class". All the images measure 640x480 pixels and the objects come in different scales, poses, and orientations. This dataset is challenging due to high intra-class variation and have been broadly used as an evaluation dataset in the computer vision community. We evaluate our approach using the same experimental set up as in <ref type="bibr" target="#b10">[11]</ref>. In particular, our training set contains 100 positive images (bike or person) and 100 negative images from the other two categories, where half are from the background and half are from the other object category. Our test set consists of 100 images with a similar distribution to the training set. We report equal error rates averaged over ten runs. Table <ref type="table" target="#tab_4">5</ref> compares our technique with other approaches that characterize the spatial arrangement of visual words, namely the Boosting+SIFT approach of Opelt et al. <ref type="bibr" target="#b10">[11]</ref>, the SPMK approach of Lazebnik et al. <ref type="bibr" target="#b6">[7]</ref>, the proximity distribution kernel (PDK) approach of Ling and Soatto <ref type="bibr" target="#b7">[8]</ref>, and the naive Bayes nearest neighbor (NBNN) approach of Boiman et al. <ref type="bibr" target="#b1">[2]</ref> (while NBNN is not a spatial based approach, we include it here for completeness). Our SPCK+ is shown to perform better than the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">15 Scene Dataset</head><p>Finally, we apply our approach to the publicly available 15 Scene dataset <ref type="bibr" target="#b6">[7]</ref>. This dataset contains a total of 4485 images in 15 categories varying from indoor scenes such as store, bedroom, and kitchen, to outdoor scenes such as coast, city, and forest. Each category has between 200 to 400 images and each image measures approximately 300x300 pixels. Following the same experiment setup as <ref type="bibr" target="#b6">[7]</ref>, we randomly pick 100 images per category for training and use the rest for testing. Table <ref type="table" target="#tab_5">6</ref> compares our results with those of SPMK. We see again that our approach SPCK++ improves over SPMK.</p><p>The images in the 15 Scene dataset tend to be strongly aligned so that local spatial arrangement tends to be less important than global layout. The proposed approach thus results in only a modest improvement over SPMK (and does not beat the best published results) on this dataset since it is designed to distinguish between image classes that possibly differ only in their relative spatial arrangements such as the land-use dataset above. The global alignment of the 15 Scene dataset is a much stronger signal for discriminating between classes than relative spatial arrangement. It is for these same reasons that SPCK is not appropriate for strongly aligned object class datasets such as Caltech-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed spatial pyramid co-occurrence, a novel approach to characterizing the photometric and geometric aspects of an image. The representation captures both the absolute and relative spatial arrangements of visual words and can characterize a wide variety of spatial relationships through the choice of the underlying spatial predicates.</p><p>We performed a thorough evaluation using a challenging 21 land-use class dataset which can be made pub-licly available since it was derived from royalty free imagery. The proposed approach was shown to perform better on this dataset than a non-spatial bag-of-visual-words approach as well as a popular approach for characterizing the absolute spatial arrangement of visual words. And, while our primary objective is analyzing overhead imagery, we also demonstrated that our approach achieves state-of-theart performance on the Graz-01 object class dataset and performs competitively on the 15 Scene dataset.</p><p>We noted several salient aspects of our approach. In particular, we demonstrated small visual dictionaries become optimal as the spatial predicates become more cialized. This tradeoff is an interesting result which we will investigate further future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our primary focus is on analyzing overhead imagery which generally does not have an absolute reference frame. The relative spatial arrangement of the image elements often becomes the key discriminating feature as demonstrated in the four land-use classes above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. We consider spatial predicates which characterize (a) the dis- tance between pairs of visual words, and (b) the relative orientation of pairs of visual words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The effect of co-occurrence dictionary size on SPCK and SPCK+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The effect of the spatial predicate proximity distance on SPCK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The performance of the individual pyramid levels on SPCK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification rates for the land-use dataset. See text for details.</figDesc><table><row><cell>BOVW</cell><cell>SPMK [7]</cell><cell>SPCK</cell><cell>SPCK+</cell><cell>SPCK++</cell></row><row><cell>71.86</cell><cell>74.00</cell><cell>73.14</cell><cell>76.05</cell><cell>77.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The effect of co-occurrence dictionary size (rows) on SPCK and SPCK+. Results are shown for the three different spatial predicate configurations (SP1=proximity only, SP2=proximity+2-bin orientation, SP3=proximity+4-bin orientation), and for the baseline SPCK and extended SPCK+.</figDesc><table><row><cell></cell><cell></cell><cell>SP1</cell><cell></cell><cell></cell><cell></cell><cell>SP2</cell><cell></cell><cell></cell><cell cols="2">SP3</cell></row><row><cell cols="2">SPCK</cell><cell cols="2">SPCK+</cell><cell></cell><cell>SPCK</cell><cell cols="2">SPCK+</cell><cell cols="2">SPCK</cell><cell>SPCK+</cell></row><row><cell>10</cell><cell>58.86</cell><cell></cell><cell>72.33</cell><cell></cell><cell>61.48</cell><cell></cell><cell>74.76</cell><cell></cell><cell>66.43</cell><cell>76.05</cell></row><row><cell>50</cell><cell>71.57</cell><cell></cell><cell>74.43</cell><cell></cell><cell>70.90</cell><cell></cell><cell>74.90</cell><cell></cell><cell>73.00</cell><cell>76.00</cell></row><row><cell>100</cell><cell>72.62</cell><cell></cell><cell>72.86</cell><cell></cell><cell>72.57</cell><cell></cell><cell>73.14</cell><cell></cell><cell>73.14</cell><cell>74.05</cell></row><row><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>classification rate</cell><cell>64 66 68 70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SP3(SPCK+)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SP2(SPCK+)</cell></row><row><cell></cell><cell>62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SP1(SPCK+)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP3(SPCK)</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP2(SPCK)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP1(SPCK)</cell></row><row><cell></cell><cell>10 58</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">co-occurrence dictionary size</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The effect of the spatial predicate proximity distance (rows) on SPCK. Results are shown for different spatial predicate configurations and different co-occurrence dictionary sizes (columns).</figDesc><table><row><cell></cell><cell>SP1</cell><cell></cell><cell></cell><cell>SP2</cell><cell></cell><cell></cell><cell>SP3</cell><cell></cell></row><row><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell cols="9">20 58.00 66.52 67.19 60.67 66.38 66.38 62.19 65.71 64.76</cell></row><row><cell cols="9">50 58.76 69.24 70.81 60.43 69.76 69.81 65.57 70.14 69.14</cell></row><row><cell cols="9">100 58.62 70.76 72.00 61.38 70.48 72.38 66.29 72.62 72.43</cell></row><row><cell cols="9">150 58.86 71.57 72.62 61.48 70.90 72.57 66.43 73.00 73.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The effect of the number of pyramid levels on SPCK. The rows indicate just level 0, just level 1, just level 2, and all three levels combined. The columns indicate different spatial predicate configurations and co-occurrence dictionary sizes.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SP1</cell><cell></cell><cell></cell><cell>SP2</cell><cell></cell><cell></cell><cell></cell><cell>SP3</cell></row><row><cell></cell><cell>10</cell><cell>50</cell><cell></cell><cell>100</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell></cell><cell>50</cell><cell>100</cell></row><row><cell>0</cell><cell cols="10">52.05 69.81 72.52 55.10 70.19 72.52 60.57 72.57 74.19</cell></row><row><cell>1</cell><cell cols="10">51.81 66.05 68.00 52.86 65.86 67.57 58.23 67.52 68.33</cell></row><row><cell>2</cell><cell cols="10">55.01 66.05 66.00 58.52 63.52 62.00 61.19 62.48 59.00</cell></row><row><cell cols="11">0+1+2 58.86 71.57 72.62 61.48 70.90 72.57 66.43 73.00 73.14</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>classification rate</cell><cell>60 65</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP3 level 0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP3 level 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP3 level 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP1 level 0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP1 level 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SP1 level 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">co-occurrence dictionary size</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Evaluation using the Graz-01 dataset. Comparison of the proposed SPCK+ approach with Boosting+SIFT<ref type="bibr" target="#b10">[11]</ref>, SPMK<ref type="bibr" target="#b6">[7]</ref>, PDK<ref type="bibr" target="#b7">[8]</ref>, and NBNN<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell></cell><cell>[11]</cell><cell>SPMK [7]</cell><cell>PDK [8]</cell><cell>NBNN [2]</cell><cell>SPCK+</cell></row><row><cell>Bike</cell><cell>86.5</cell><cell>86.3±2.5</cell><cell>90.2±2.6</cell><cell>90.0±4.3</cell><cell>91.0±4.8</cell></row><row><cell>Person</cell><cell>80.8</cell><cell>82.3±3.1</cell><cell>87.0±3.8</cell><cell>87.0±4.6</cell><cell>87.2±3.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results on the 15 Scene dataset.</figDesc><table><row><cell></cell><cell>SPMK [7]</cell><cell>SPCK++</cell></row><row><cell>Classification Rate</cell><cell>81.40±0.50</cell><cell>82.51±0.43</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The dataset is available at http://vision.ucmerced.edu/datasets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2011" xml:id="foot_1"><p>IEEE International Conference on Computer Vision 978-1-4577-1102-2/11/$26.00 c 2011 IEEE</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work funded in part by NSF grant IIS-0917069 and a Department of Energy Early Career Scientist and Engineer/PECASE award. Any opinions, findings, and conclusions or recommendations expressed in this work are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The authors would like to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm/" />
		<title level="m">LIBSVM-A library for support vector machines</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In defense of nearestneighbor based image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Texture features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image indexing using color correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proximity distribution kernels for geometric context in category recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integrated feature selection and higher-order spatial feature extraction for object categorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weak hypotheses and boosting for generic object detection and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fussenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative object class models of appearance and shape by correlatons</title>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A computer movie simulating urban growth in the Detroit region</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Geography</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="240" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
