<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-03-30">March 30, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Stutz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Image Understanding</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Image Understanding</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Image Understanding</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-30">March 30, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">1AD57055E1220520997E382E8BB24598</idno>
					<idno type="DOI">10.1016/j.cviu.2017.03.007</idno>
					<note type="submission">Received date: 5 December 2016 Revised date: 26 February 2017 Accepted date: 29 March 2017 Preprint submitted to Journal of L A T E X Templates</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>superpixels</term>
					<term>superpixel segmentation</term>
					<term>image segmentation</term>
					<term>perceptual grouping</term>
					<term>benchmark</term>
					<term>evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Superpixels group perceptually similar pixels to create visually meaningful entities while heavily reducing the number of primitives for subsequent processing steps. As of these properties, superpixel algorithms have received much attention since their naming in 2003 <ref type="bibr" target="#b1">[1]</ref>. By today, publicly available superpixel algorithms have turned into standard tools in low-level vision. As such, and due to their quick adoption in a wide range of applications, appropriate benchmarks are crucial for algorithm selection and comparison. Until now, the rapidly growing number of algorithms as well as varying experimental setups hindered the development of a unifying benchmark. We present a comprehensive evaluation of 28 state-of-the-art superpixel algorithms utilizing a benchmark focussing on fair comparison and designed to provide new insights relevant for applications. To this end, we explicitly discuss parameter optimization and the importance of strictly enforcing connectivity. Furthermore, by extending well-known metrics, we are able to summarize algorithm performance independent of the number of generated superpixels, thereby overcoming a major limitation of available benchmarks. Furthermore, we discuss runtime, robustness against noise, blur and affine transformations, implementation details as well as aspects of visual quality. Finally, we present an overall ranking of superpixel algorithms which redefines the stateof-the-art and enables researchers to easily select appropriate algorithms and the corresponding implementations which themselves are made publicly available as part of our benchmark at davidstutz.de/projects/superpixel-benchmark/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T Highlights • An extensive evaluation of 28 superpixel algorithms on 5 datasets. • Explicit discussion of parameter optimization, including superpixel connectivity. • Presentation of visual quality, algorithm runtime, and a performance-based ranking. • The evaluated implementations as well as the benchmark are publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Introduced by Ren and Malik in 2003 <ref type="bibr" target="#b1">[1]</ref>, superpixels group pixels similar in color and other low-level properties. In this respect, superpixels address two problems inherent to the processing of digital images <ref type="bibr" target="#b1">[1]</ref>: firstly, pixels are merely a result of discretization; and secondly, the high number of pixels in large images prevents many algorithms from being computationally feasible. Ren and Malik introduce superpixels as more natural entities -grouping pixels which perceptually belong together while heavily reducing the number of primitives for subsequent algorithms.</p><p>Superpixels have been been used in a wide range of applications -even before the term "superpixel" was coined. As early as 1988, Mester and Franke <ref type="bibr" target="#b2">[2]</ref> present segmentation results similar to superpixels. Later, in 1997, early versions of the watershed algorithm were known to produce superpixel-like segments <ref type="bibr" target="#b3">[3]</ref>. In the early 2000s, Hoiem et al. <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref> used the segmentation algorithms of <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b7">[7]</ref> to generate oversegmentations for 3D reconstruction and occlusion boundaries. Similarly, the normalized cuts algorithm was early adopted for oversegmentation <ref type="bibr" target="#b1">[1]</ref> and semantic segmentation <ref type="bibr" target="#b8">[8]</ref>. In <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref> and <ref type="bibr" target="#b9">[9]</ref>, superpixels have been used to extract meaningful features for subsequent tasks -extensive lists of used features are included.</p><p>Since the introduction of the first superpixel algorithms around 2009, they have been applied to many important problems in computer vision: tracking <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>, stereo and occlusion <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>, 3D-reconstruction <ref type="bibr" target="#b14">[14]</ref>, saliency <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>, object detection <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> and object proposal detection 30 <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>, depth recovery <ref type="bibr" target="#b21">[21]</ref> and depth estimation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, semantic segmentation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b24">24]</ref>, indoor scene understanding <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>, optical flow <ref type="bibr" target="#b28">[28]</ref>, scene flow <ref type="bibr" target="#b29">[29]</ref>, clothes parsing <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref> and as basis for convolutional neural networks <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b16">16]</ref> to name just a few. Superpixels have also 35 been adopted in domain specific applications such as medical image segmentation <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref> or medical image retrieval <ref type="bibr" target="#b36">[36]</ref>. Moreover, superpixels have been found useful for dataset annotation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b37">37]</ref>. Finally, several superpixel algorithms (among others <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b40">[40]</ref>) have been <ref type="bibr" target="#b40">40</ref> adapted to videos and image volumes -a survey and comparison of some of these so-called supervoxel algorithms can be found in <ref type="bibr" target="#b41">[41]</ref>.</p><p>In view of this background, most authors do not make an explicit difference between superpixel algorithms and 45 oversegmentation algorithms, i.e. superpixel algorithms are usually compared with oversegmentation algorithms and the terms have been used interchangeably (e.g. <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>). Veksler et al. <ref type="bibr" target="#b45">[45]</ref> distinguish superpixel algorithms from segmentation algorithms running in "oversegmenta-50 tion mode". More recently, Neubert and Protzel <ref type="bibr" target="#b46">[46]</ref> distinguish superpixel algorithms from oversegmentation al-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>gorithms with respect to their behavior on video sequences. In general, it is very difficult to draw a clear line between superpixel algorithms and oversegmentation algorithms. Several oversegmentation algorithms were not intended to generate superpixels, nevertheless, some of them share many characteristics with superpixel algorithms. We use the convention that superpixel algorithms offer control over the number of generated superpixels while segmentation algorithms running in "oversegmentation mode" do not. This covers the observations made by <ref type="bibr">Veksler et al. and Neubert and Protzel.</ref> In general, most authors (e.g. <ref type="bibr" target="#b42">[42,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b43">43]</ref>) agree on the following requirements for superpixels:</p><p>-Partition. Superpixels should define a partition of the image, i.e. superpixels should be disjoint and assign a label to every pixel. -Connectivity. Superpixels are expected to represent connected sets of pixels.</p><p>-Boundary Adherence. Superpixels should preserve image boundaries. Here, the appropriate definition of image boundaries may depend on the application. -Compactness, Regularity and Smoothness. In the absence of image boundaries, superpixels should be compact, placed regularly and exhibit smooth boundaries. -Efficiency. Superpixels should be generated efficiently.</p><p>-Controllable Number of Superpixels. The number of generated superpixels should be controllable. Some of these requirements may be formulated implicitly, e.g. Liu et al. <ref type="bibr">[47]</ref> require that superpixels may not lower the achievable performance of subsequent processing steps. Achanta et al. <ref type="bibr" target="#b39">[39]</ref> even require superpixels to increase the performance of subsequent processing steps. Furthermore, the above requirements should be fulfilled with as few superpixels as possible <ref type="bibr">[47]</ref>.</p><p>Contributions. We present an extensive evaluation of 28 algorithms on 5 datasets regarding visual quality, performance, runtime, implementation details and robustness to noise, blur and affine transformations. In particular, we demonstrate the applicability of superpixel algorithms to indoor, outdoor and person images. To ensure a fair comparison, parameters have been optimized on separate training sets; as the number of generated superpixels heavily influences parameter optimization, we additionally enforced connectivity. Furthermore, to evaluate superpixel algorithms independent of the number of superpixels, we propose to integrate over commonly used metrics such as Boundary Recall <ref type="bibr" target="#b48">[48]</ref>, Undersegmentation Error <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44]</ref> and Explained Variation <ref type="bibr" target="#b49">[49]</ref>. Finally, we present a ranking of the superpixel algorithms considering multiple metrics and independent of the number of generated superpixels.</p><p>Outline. In Section 2 we discuss important related work regarding the comparison of superpixel algorithms and subsequently, in Section 3, we present the evaluated superpixel algorithms. In Section 4 we discuss relevant datasets and introduce the used metrics in Section 5. Then, Section 6 briefly discusses problems related to parameter optimization before we present experimental results in Sec-110 tion 7. We conclude with a short summary in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our efforts towards a comprehensive comparison of available superpixel algorithms is motivated by the lack thereof within the literature. Notable publications in this regard 115 are <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b44">[44]</ref>, and <ref type="bibr" target="#b46">[46]</ref>. Schick et al. <ref type="bibr" target="#b43">[43]</ref> introduce a metric for evaluating the compactness of superpixels, while Achanta et al. <ref type="bibr" target="#b39">[39]</ref> as well as Neubert and Protzel <ref type="bibr" target="#b44">[44]</ref> concentrate on using known metrics. Furthermore, Neubert and Protzel evaluate the robustness of superpixel 120 algorithms with respect to affine transformations such as scaling, rotation, shear and translation. However, they do not consider ground truth for evaluating robustness. More recently, Neubert and Protzel <ref type="bibr" target="#b46">[46]</ref> used the Sintel dataset <ref type="bibr" target="#b50">[50]</ref> to evaluate superpixel algorithms based on optical flow 125 in order to assess the stability of superpixel algorithms in video sequences.</p><p>Instead of relying on an application independent evaluation of superpixel algorithms, some authors compared the use of superpixel algorithms for specific computer vi-130 sion tasks. Achanta et al. <ref type="bibr" target="#b39">[39]</ref> use the approaches of <ref type="bibr" target="#b8">[8]</ref> and <ref type="bibr" target="#b51">[51]</ref> to assess superpixel algorithms as pre-processing step for semantic segmentation. Similarly, Strassburg et al. <ref type="bibr" target="#b52">[52]</ref> evaluate superpixel algorithms based on the semantic segmentation approach described in <ref type="bibr" target="#b9">[9]</ref>. <ref type="bibr">Weikersdorfer et al.</ref> 135 <ref type="bibr" target="#b53">[53]</ref> use superpixels as basis for the normalized cuts algorithm <ref type="bibr" target="#b54">[54]</ref> applied to classical segmentation and compare the results with the well-known segmentation algorithm by Arbeláez et al. <ref type="bibr" target="#b55">[55]</ref>. Koniusz and Mikolajczyk <ref type="bibr" target="#b56">[56]</ref>, in contrast, evaluate superpixel algorithms for interest point 140 extraction.</p><p>In addition to the above publications, authors of superpixel algorithms usually compare their proposed approaches to existing superpixel algorithms. Usually, the goal is to demonstrate superiority with regard to specific 145 aspects. However, used parameter settings are usually not reported, or default parameters are used, and implementations of metrics differ. Therefore, these experiments are not comparable across publications.</p><p>Complementing the discussion of superpixel algorithms 150 in the literature so far, and similar to <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b44">[44]</ref>, we concentrate on known metrics to give a general, application independent evaluation of superpixel algorithms. However, we consider minimum/maximum as well as standard deviation in addition to metric averages in order as-155 sess the stability of superpixel algorithms as also considered by Neubert and Protzel <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b46">46]</ref>. Furthermore, we explicitly document parameter optimization and strictly enforce connectivity to ensure fair comparison. In contrast to <ref type="bibr" target="#b44">[44]</ref>, our robustness experiments additionally consider 160 noise and blur and make use of ground truth for evaluation. Finally, we render three well-known metrics independent of the number of generated superpixels allowing us to present a final ranking of superpixel algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithms</head><p>In our comparison, we aim to discuss popular algorithms with publicly available implementations alongside less-popular and more recent algorithms for which implementations were partly provided by the authors. To address the large number of superpixel algorithms, we find a rough categorization of the discussed algorithms helpful. Based on the categorization by Achanta et al. <ref type="bibr" target="#b39">[39]</ref> -who presented (to the best of our knowledge) the first and only categorization of superpixel algorithms -we categorized algorithms according to their high-level approach.</p><p>We found that this categorization provides an adequate abstraction of algorithm details, allowing to give the reader a rough understanding of the different approaches, while being specific enough to relate categories to experimental results, as done in Section 7. For each algorithm, we present the used acronym, the reference and its number of citations 1 . In addition, we provide implementation details such as the programming language, the used color space, the number of parameters as well as whether the number of superpixels, the compactness and the number of iterations (if applicable) are controllable.</p><p>Watershed-based. These algorithms are based on the waterhed algorithm (W) and usually differ in how the image is pre-processed and how markers are set. The number of superpixels is determined by the number of markers, and some watershed-based superpixel algorithms offer control over the compactness, for example WP or CW. control over the number of superpixels or their compactness and are, therefore, also categorized as oversegmentation algorithms. Clustering-based. These superpixel algorithms are inspired by clustering algorithms such as k-means initialized by seed pixels and using color information, spatial information and additional information such as depth (as for example done by DASP). Intuitively, the number of generated superpixels and their compactness is controllable. Although these algorithms are iterative, post-processing is required in order to enforce connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SLIC -Simple Linear Iterative Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference (Google Scholar Citations) Color</head><p>Achanta et al. <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b39">39]</ref>, 2010 (438 + 1843) We note that VCCS directly operates within a point cloud and we, therefore, backproject the generated supervoxels onto the image plane. Thus, the number of generated superpixels is harder to control.</p><p>Energy optimization. These algorithms iteratively 230 optimize a formulated energy. The image is partitioned into a regular grid as initial superpixel segmentation, and pixels are exchanged between neighboring superpixels with regard to the energy. The number of superpixels is controllable, compactness can be controlled and the iterations 235 can usually be aborted at any point. Wavelet-based. We found that Superpixels from Edge-Avoiding Wavelets (SEAW) <ref type="bibr" target="#b52">[52]</ref> is not yet captured in the discussed categories. In particular, it is not comparable to the algorithms discussed so far. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Further Algorithms</head><p>While the above algorithms represent a large part of the proposed superpixel algorithms, some algorithms are not included due to missing, unnoticed or only recently published implementations<ref type="foot" target="#foot_1">2</ref> . These include <ref type="bibr" target="#b85">[85,</ref><ref type="bibr" target="#b86">86,</ref><ref type="bibr" target="#b87">87,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b89">89,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b93">93,</ref><ref type="bibr" target="#b94">94,</ref><ref type="bibr" target="#b95">95,</ref><ref type="bibr" target="#b96">96,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b98">98,</ref><ref type="bibr" target="#b95">95,</ref><ref type="bibr" target="#b99">99,</ref><ref type="bibr" target="#b100">100]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>We chose five different datasets to evaluate superpixel algorithms: two indoor datasets, two outdoor datasets and one person dataset. We found that these datasets realisticly reflect the setting of common applications ( <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b26">26]</ref> to mention just a few applications on the used datasets), while leveraging the availability of large, pixel-level annotated datasets. However, we also note that by focusing on natural images some application domains might not be represented well -these include for example specialized research areas such as medical imaging where superpixels are also commonly used <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35]</ref>. Still, we believe that the experiments conducted on the chosen datasets will aid algorithm selection in these cases, as well. Furthermore, we expect the experiments to be useful for similar but larger datasets (such as PASCAL VOC <ref type="bibr" target="#b101">[101]</ref>, ImageNet <ref type="bibr" target="#b102">[102]</ref> or MS COCO <ref type="bibr" target="#b103">[103]</ref> to name a few prominent ones). In addition, the selected datasets enable us to draw a more complete picture of algorithm performance going beyond the datasets commonly used within the literature. Furthermore, both indoor datasets provide depth information, allowing us to evaluate superpixel algorithms requiring depth information as additional cue. In the following we briefly discuss the main aspects of these datasets; Figure <ref type="figure" target="#fig_4">1</ref> shows example images and Table <ref type="table">1</ref> summarizes key statistics.</p><p>BSDS500 <ref type="bibr" target="#b55">[55]</ref>. The Berkeley Segmentation Dataset 500 (BSDS500) was the first to be used for superpixel algorithm evaluation (e.g. <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b42">42]</ref>). It contains 500 images and provides at least 5 high-quality ground truth segmentations per image. Therefore, we evaluate algorithms on all ground truth segmentations and, for each image and a given metric, choose the ground truth segmentation resulting in the worst score for averaging. The images represent simple outdoor scenes, showing landscape, buildings, animals and humans, where foreground and background are usually easily identified. Nevertheless, natural scenes where segment boundaries are not clearly identifiable contribute to the difficulty of the dataset. SBD <ref type="bibr" target="#b104">[104]</ref>. The Stanford Background Dataset (SBD) combines 715 images from several datasets <ref type="bibr" target="#b105">[105,</ref><ref type="bibr" target="#b106">106,</ref><ref type="bibr" target="#b101">101,</ref><ref type="bibr" target="#b107">107]</ref>. As result, the dataset contains images of varying size, quality and scenes. The images show outdoor scenes such as landscape, animals or street scenes. In contrast to the 290 BSDS500 dataset the scenes tend to be more complex, often containing multiple foreground objects or scenes without clearly identifiable foreground. The semantic ground truth has been pre-processed to ensure connected segments.</p><p>NYUV2 <ref type="bibr" target="#b108">[108]</ref>. The NYU Depth Dataset V2 (NYUV2) 295 contains 1449 images including pre-processed depth. Silberman et al. provide instance labels which are used to ensure connected segments. Furthermore, following Ren and Bo <ref type="bibr" target="#b109">[109]</ref>, we pre-processed the ground truth to remove small unlabeled regions. The provided ground truth 300 is of lower quality compared to the BSDS500 dataset. The images show varying indoor scenes of private apartments and commercial accomodations which are often cluttered and badly lit. The images were taken using Microsoft's Kinect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>305</head><p>SUNRGBD <ref type="bibr" target="#b110">[110]</ref>. The Sun RGB-D dataset (SUN-RGBD) contains 10335 images including pre-processed depth. The dataset combines images from the NYUV2 dataset and other datasets <ref type="bibr" target="#b111">[111,</ref><ref type="bibr" target="#b112">112]</ref> with newly acquired images. In contrast to the NYUV2 dataset, the SUNRGBD data-310 set combines images from the following devices: Intel Re-alSense, Asus Xtion and Microsoft Kinect v1 and v2 -we refer to <ref type="bibr" target="#b110">[110]</ref> for details. We removed the images taken from the NYUV2 dataset. The images show cluttered indoor scenes with bad lighting taken from private apart-315 ments as well as commercial accomodations. The provided semantic ground truth has been pre-processed similarly to the NYUV2 dataset.</p><p>Fash <ref type="bibr" target="#b30">[30]</ref>. The Fashionista dataset (Fash) contains 685 images which have previously been used for clothes pars-320 ing. The images show the full body of fashion bloggers in front of various backgrounds. Yamaguchi et al. leveraged Amazon Mechanical Turk to acquire semantic ground truth based on pre-computed segments ( <ref type="bibr" target="#b30">[30]</ref> suggests that the algorithm in <ref type="bibr" target="#b55">[55]</ref> has been used). The ground truth 325 has been pre-processed to ensure connected segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_0">M A N U S C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Benchmark</head><p>Our benchmark aims to score the requirements for superpixels discussed in Section 1; in particular boundary adherence and compactness (note that connectivity is enforced during parameter optimization, see Section 6.2). As these metrics inherently depend on the number of generated superpixels, we further extend these metrics to allow the assessment of superpixel algorithms independent of the number of generated superpixels. Therefore, let</p><formula xml:id="formula_1">S = {S j } K j=1 and G = {G i } be partitions of the same im- age I : x n → I(x n ), 1 ≤ n ≤ N ,</formula><p>where S represents a superpixel segmentation and G a ground truth segmentation.</p><p>Boundary Recall (Rec) <ref type="bibr" target="#b48">[48]</ref> is the most commonly used metric to asses boundary adherence given ground truth.</p><p>Let FN(G, S) and TP(G, S) be the number of false negative and true positive boundary pixels in S with respect to G. Then Rec is defined as</p><formula xml:id="formula_2">Rec(G, S) = TP(G, S) TP(G, S) + FN(G, S) .<label>(1)</label></formula><p>Overall, high Rec represents better boundary adherence with respect to to the ground truth boundaries, i.e. higher is better. In practice, a boundary pixel in S is matched to an arbitrary boundary pixel in G within a local neighborhood of size (2r + 1) × (2r + 1), with r being 0.0025 times the image diagonal rounded to the next integer (e.g. r = 1 for the BSDS500 dataset). Undersegmentation Error (UE) <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44]</ref> measures the "leakage" of superpixels with respect to G and, therefore, implicitly also measures boundary adherence. Here, "leakage" refers to the overlap of superpixels with multiple, nearby ground truth segments. The original formulation by Levinshtein et al. <ref type="bibr" target="#b42">[42]</ref> can be written as</p><formula xml:id="formula_3">UE Levin (G, S) = 1 |G| Gi Sj ∩Gi =∅ |S j | -|G i | |G i |<label>(2)</label></formula><p>where the inner term represents the "leakage" of superpixel S j with respect to G. However, some authors <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b44">44]</ref> argue that Equation (2) penalizes superpixels overlapping only slightly with neighboring ground truth segments and is not constrained to lie in [0, 1]. Achanta et al. <ref type="bibr" target="#b39">[39]</ref> suggest to threshold the "leakage" term of Equation ( <ref type="formula" target="#formula_3">2</ref>) and only consider those superpixels S j with a minimum overlap of  and Protzel <ref type="bibr" target="#b44">[44]</ref> propose formulations not suffering from the above drawbacks. In the former,</p><formula xml:id="formula_4">UE Bergh (G, S) = 1 N Sj |S j -arg max Gi |S j ∩ G i ||,<label>(3)</label></formula><p>each superpixel is assigned to the ground truth segment with the largest overlap, and only the "leakage" with respect to other ground truth segments is considered. There-370 fore, UE Bergh corresponds to (1 -ASA) -with ASA being Achievable Segmentation Accuracy as described below. The latter,</p><formula xml:id="formula_5">UE NP (G, S) = 1 N Gi Sj ∩Gi =∅ min{|S j ∩ G i |, |S j -G i |},<label>(4)</label></formula><p>is not directly equivalent to (1 -ASA), however, UE NP and ASA are still strongly correlated as we will see later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>375</head><p>All formulations have in common that lower UE refers to less "leakage" with respect to the ground truth, i.e. lower is better. In the following we use UE ≡ UE NP .</p><p>Explained Variation (EV) <ref type="bibr" target="#b49">[49]</ref> quantifies the quality of a superpixel segmentation without relying on ground 380 truth. As image boundaries tend to exhibit strong change in color and structure, EV assesses boundary adherence independent of human annotions. EV is defined as</p><formula xml:id="formula_6">EV(S) = Sj |S j |(µ(S j ) -µ(I)) 2 xn (I(x n ) -µ(I)) 2<label>(5)</label></formula><p>where µ(S j ) and µ(I) are the mean color of superpixel S j and the image I, respectively. As result, EV quantifies 385 the variation of the image explained by the superpixels, i.e. higher is better.</p><p>Compactness (CO) <ref type="bibr" target="#b43">[43]</ref> has been introduced by Schick et al. <ref type="bibr" target="#b43">[43]</ref> to evaluate the compactness of superpixels:</p><formula xml:id="formula_7">CO(G, S) = 1 N Sj |S j | 4πA(S j ) P (S j ) .<label>(6)</label></formula><p>CO compares the area A(S j ) of each superpixel S j 390 with the area of a circle (the most compact 2-dimensional shape) with same perimeter P (S j ), i.e. higher is better. While we will focus on Rec, UE, EV and CO, further notable metrics are briefly discussed in the following. Achievable Segmentation Accuracy (ASA) <ref type="bibr">[47]</ref> quantifies 395 the achievable accuracy for segmentation using superpixels as pre-processing step:</p><formula xml:id="formula_8">ASA(G, S) = 1 N Sj max Gi {|S j ∩ G i |};<label>(7)</label></formula><p>Intra-Cluster Variation (ICV) <ref type="bibr" target="#b58">[58]</ref> computes the average variation within each superpixel:</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T ICV(S) = 1 |S| Sj xn∈Sj (I(x n ) -µ(S j )) 2 |S j | ;<label>(8)</label></formula><p>Mean Distance to Edge (MDE) <ref type="bibr" target="#b58">[58]</ref> refines Rec by also considering the distance to the nearest boundary pixel within the ground truth segmentation:</p><formula xml:id="formula_10">MDE(G, S) = 1 N xn∈B(G) dist S (x n )<label>(9)</label></formula><p>where B(G) is the set of boundary pixels in G, and dist S is a distance transform of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Expressiveness and Chosen Metrics</head><p>Due to the large number of available metrics, we examined their expressiveness in order to systematically concentrate on few relevant metrics. We found that UE tends to correlate strongly with ASA which can be explained by Equations ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_8">7</ref>), respectively. In particular, simple calculation shows that ASA strongly resembles (1 -UE). Surprisingly, UE NP does not correlate with UE Levin suggesting that either both metrics reflect different aspects of superpixels, or UE Levin unfairly penalizes some superpixels as suggested in <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b44">[44]</ref>. Unsurprisingly, MDE correlates strongly with Rec which can also be explained by their respective definitions. In this sense, MDE does not provide additional information. Finally, ICV does not correlate with EV which may be attributed to the missing normalization in Equation ( <ref type="formula" target="#formula_9">8</ref>) when compared to EV. This also results in ICV not begin comparable across images as the intra-cluster variation is not related to the overall variation within the image. As of these considerations, we concentrate on Rec, UE and EV for the presented experiments. Details can be found in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Average Miss Rate, Average Undersegmentation Error and Average Unexplained Variation</head><p>As the chosen metrics inherently depend on the number of superpixels, we seek a way of quantifying the performance with respect to Rec, UE and EV independent of K and in a single plot per dataset. In order to summarize performance over a given interval [K min , K max ], we consider MR = (1 -Rec), UE and UV = (1 -EV). Here, the first corresponds to the Boundary Miss Rate(MR) and the last, Unexplained Variation (UV), quantifies the variation in the image not explained by the superpixels. We use the area below these curves in [K min , K max ] = [200, 5200] to quantify performance independent of K. In Section 7.2, we will see that these metrics appropriately summarize the performance of superpixel algorithms. We denote these metrics by Average Miss Rate (AMR), Average Undersegmentation Error (AUE) and Average Unexplained Variation (AUV) -note that this refers to an average over K. By construction (and in contrast to Rec and EV), lower AMR, AUE and AUV is better, making side-by-side 445 comparison across datasets easy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Parameter Optimization</head><p>For the sake of fair comparison, we optimized parameters on the training sets depicted in Table <ref type="table">1</ref>. Unfortunately, parameter optimization is not explicitly discussed 450 in related work (e.g. <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b43">43]</ref>) and used parameters are not reported in most publications. In addition, varying runtimes as well as categorical and integer parameters render parameter optimization difficult such that we had to rely on discrete grid search, jointly optimizing Rec 455 and UE, i.e. minimizing (1 -Rec) + UE. As Rec and UE operate on different representations (boundary pixels and superpixel segmentations, respectively), the additive formulation ensures that algorithms balance both metrics. For example, we observed that using a multiplicative for-460 mulation allows superpixel algorithms to drive (1 -Rec) towards zero while disregarding UE. We optimized parameters for K ∈ {400, 1200, 3600} and interpolated linearly in between (however, we found that for many algorithms, parameters are consistent across different values of K). Op-465 timized parameters also include compactness parameters and the number of iterations as well as the color space. We made sure that all algorithms at least support RGB color space for fairness. In the following, we briefly discuss the main difficulties encountered during parameter 470 optimization, namely controlling the number of generated superpixels and ensuring connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Controlling the Number of Generated Superpixels</head><p>As discussed in Section 1, superpixel algorithms are expected to offer control over the number of generated super-475 pixels. We further expect the algorithms to meet the desired number of superpixels within acceptable bounds. For several algorithms, however, the number of generated superpixels is strongly dependent on other parameters. Figure <ref type="figure" target="#fig_7">2</ref> demonstrates the influence of specific parameters on 480 the number of generated superpixels (before ensuring connectivity as in Section 6.2) for LSC, CIS, VC, CRS and PB. For some of the algorithms, such parameters needed to be constrained to an appropriate value range even after enforcing connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>485</head><p>For oversegmentation algorithms such as FH, EAMS and QS not providing control over the number of generated superpixels, we attempted to exploit simple relationships between the provided parameters and the number of generated superpixels. For EAMS and QS this allows 490 to control the number of generated superpixels at least roughly. FH, in contrast, does not allow to control the number of generated superpixels as easily. Therefore, we evaluated FH for a large set of parameter combinations and chose the parameters resulting in approximately the 495 desired number of superpixels.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ensuring Connectivity</head><p>Unfortunately, many implementations (note the difference between implementation and algorithm) cannot ensure the connectivity of the generated superpixels as re-500 quired in Section 1. Therefore, we decided to strictly enforce connectivity using a connected components algorithm, i.e. after computing superpixels, each connected component is relabeled as separate superpixel. For some implementations, this results in many unintended super-505 pixels comprising few pixels. In these cases we additionally merge the newly generated superpixels into larger neighboring ones. However, even with these post-processing steps, the evaluated implementations of CIS, CRS, PB, DASP, VC, VCCS or LSC generate highly varying num-510 bers of superpixels across different images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Common Trade-Offs: Runtime and Compactness</head><p>Two other types of parameters deserve detailed discussion: the number of iterations and the compactness parameter. The former controls the trade-off between runtime 515 and performance, exemplarily demonstrated in Figure <ref type="figure">3</ref> showing that more iterations usually result in higher Rec and higher runtime in seconds t. The latter controls the trade-off between compactness and performance and Figure <ref type="figure">4</ref> shows that higher CO usually results in lower Rec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>520</head><p>Overall, parameter optimization with respect to Rec and UE results in higher runtime and lower compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>Our experiments include visual quality, performance with respect to Rec, UE and EV as well as runtime. In 525 contrast to existing work <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b43">43]</ref>, we consider minimum/maximum and standard deviation of Rec, UE and EV (in relation to the number of generated superpixels K) and present results for the introduced metrics AMR, AUE and AUV. Furthermore, we present experiments regard-530 ing implementation details as well as robustness against noise, blur and affine transformations. Finally, we give an overall ranking based on AMR and AUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Qualitative</head><p>Visual quality is best determined by considering com-535 pactness, regularity and smoothness on the one hand and boundary adherence on the other. Here, compactness refers to the area covered by individual superpixels (as captured in Equation ( <ref type="formula" target="#formula_7">6</ref>)); regularity corresponds to both the superpixels' sizes and their arrangement; and smootness refers 540 to the superpixels' boundaries. Figures <ref type="figure">5</ref> and<ref type="figure">6</ref> show results on all datasets. We begin by discussing boundary adherence, in particular with regard to the difference between superpixel and oversegmentation algorithms, before considering compactness, smoothness and regularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>545</head><p>The majority of algorithms provides solid adherence to important image boundaries, especially for large K. We consider the woman image -in particular, the background We also find that depth information, as used in DASP and VCCS, may help resemble the underlying 3D-structure. Best viewed in color.  We find that compactness does not necessarily induce regularity and smoothness; some algorithms, however, are able to unite compactness, regularity and smoothness. Considering the sea image in Figure <ref type="figure">5</ref> for CIS and TP, we observe that compact superpixels are not necessarily arranged regularly. Similarly, compact superpixels do not need to exhibit smooth boundaries, as can be seen for PB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SLIC CRS</head><p>On the other hand, compact superpixels are often generated in a regular fashion, as can be seen for many algorithms providing a compactness parameter such as SLIC, VC and CCS. In such cases, compactness also induces smoother and more regular superpixels. We also observe that many algorithms exhibiting excellent boundary adherence such as CRS, SEEDS or ETPS generate highly irregular and non-smooth superpixels. These observations also justify the separate consideration of compactness, reg- ularity and smoothness to judge visual quality. While the importance of compactness, regularity and smoothness may depend on the application at hand, these properties represent the trade-off between abstraction from and sensitivity to low-level image content which is inherent to all 610 superpixel algorithms.</p><p>In conclusion, we find that the evaluated path-based and density-based algorithms as well as oversegmentation algorithms show inferior visual quality. On the other hand, clustering-based, contour evolution and iterative energy 615 optimization algorithms mostly demonstrate good boundary adherence and some provide a compactness parameter, e.g. SLIC, ERGC and ETPS. Graph-based algorithms show mixed results -algorithms such as FH, CIS and PB show inferior boundary adherence, while ERS, RW, NC 620 and POISE exhibit better boundary adherence. However, good boundary adherence, especially regarding details in the image, often comes at the price of lower compactness, regularity and/or smoothness as can be seen for ETPS and SEEDS. Furthermore, compactness, smoothness and 625 regularity are not necessarily linked and should be discussed separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">Compactness</head><p>CO measures compactness, however, does not reflect regularity or smoothness; therefore, CO is not sufficient 630 to objectively judge visual quality. We consider Figure <ref type="figure">8</ref>, showing CO on the BSDS500 and NYUV2 datasets, and we observe that CO correctly measures compactness. For example, WP, TP and CIS, exhibiting high CO, also present very compact superpixels in Figures <ref type="figure">5</ref> and<ref type="figure">6</ref>. How-635 ever, these superpixels do not necessarily have to be visually appealing, i.e. may lack regularity and/or smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>This can exemplarily be seen for TPS, exhibiting high compactness bur poor regularity, or PB showing high compactness but inferior smoothness. Overall, we find that CO should not be considered isolated from a qualitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Depth</head><p>Depth information helps superpixels resemble the 3Dstructure within the image. Considering Figure <ref type="figure">6</ref>, in particular both images for DASP and VCCS, we deduce that depth information may be beneficial for superpixels to resemble the 3D-structure of a scene. For example, when considering planar surfaces (e.g. the table) in both images from Figure <ref type="figure">6</ref> for DASP, we clearly see that the superpixels easily align with the surface in a way perceived as 3-dimensional. For VCCS, this effect is less observable which may be due to the compactness parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Quantitative</head><p>Performance is determined by Rec, UE and EV. In contrast to most authors, we will look beyond metric averages. In particular, we consider the minimum/maximum as well as the standard deviation to get an impression of the behavior of superpixel algorithms. Furthermore, this allows us to quantify the stability of superpixel algorithms as also considered by Neubert and Protzel in <ref type="bibr" target="#b46">[46]</ref>.</p><p>Rec and UE offer a ground truth dependent overview to assess the performance of superpixel algorithms. We consider Figures <ref type="figure">9a</ref> and<ref type="figure">9b</ref>, showing Rec and UE on the BSDS500 dataset. With respect to Rec, we can easily identify top performing algorithms, such as ETPS and SEEDS, as well as low performing algorithms, such as FH, QS and PF. However, the remaining algorithms lie closely together in between these two extremes, showing (apart from some exceptions) similar performance especially for large K. Still, some algorithms perform consistently better than others, as for example ERGC, SLIC, ERS and CRS. For UE, low performing algorithms, such as PF or QS, are still easily identified while the remaining algorithms tend to lie more closely together. Nevertheless, we can identify algorithms consistently demonstrating good performance, such as ERGC, ETPS, CRS, SLIC and ERS. On the NYUV2 dataset, considering Figures <ref type="figure" target="#fig_4">10a</ref> and<ref type="figure" target="#fig_4">10b</ref>, these observations can be confirmed except for minor differences as for example the excellent performance of ERS regarding UE or the better performance of QS regarding UE. Overall, Rec and UE provide a quick overview of superpixel algorithm performance but might not be sufficient to reliably discriminate superpixel algorithms.</p><p>In contrast to Rec and UE, EV offers a ground truth independent assessment of superpixel algorithms. Considering Figure <ref type="figure">9c</ref>, showing EV on the BSDS500 dataset, we observe that algorithms are dragged apart and even for large K significantly different EV values are attained. This suggests, that considering ground truth independent metrics may be beneficial for comparison. However, EV cannot replace Rec or UE, as we can observe when comparing to Figures <ref type="figure">9a</ref> and<ref type="figure">9b</ref>, showing Rec and UE on the BSDS500 dataset; in particular QS, FH and CIS are per-695 forming significantly better with respect to EV than regarding Rec and UE. This suggests that EV may be used to identify poorly performing algorithms, such as TPS, PF, PB or NC. On the other hand, EV is not necessarily suited to identify well-performing algorithms due to the 700 lack of underlying ground truth. Overall, EV is suitable to complement the view provided by Rec and UE, however, should not be considered in isolation.</p><p>The stability of superpixel algorithms can be quantified by min Rec, max UE and min EV considering the behavior 705 for increasing K. We consider Figures 9d, 9e and 9f, showing min Rec, max UE and min EV on the BSDS500 dataset. We define the stability of superpixel algorithms as follows: an algorithm is considered stable if performance monotonically increases with K (i.e. monotonically increasing 710 Rec and EV and monotonically decreasing UE). Furthermore, these experiments can be interpreted as empirical bounds on the performance. For example algorithms such as ETPS, ERGC, ERS, CRS and SLIC can be considered stable and provide good bounds. In contrast, algo-715 rithms such as EAMS, FH, VC or POISE are punished by considering min Rec, max UE and min EV and cannot be described as stable. Especially oversegmentation algorithms show poor stability. Most strikingly, EAMS seems to perform especially poorly on at least one image from the 720 BSDS500 dataset. Overall, we find that min Rec, max UE and min EV appropriately reflect the stability of superpixel algorithms.</p><p>The minimum/maximum of Rec, UE and EV captures lower/upper bounds on performance. In contrast, the cor-725 responding standard deviation can be thought of as the expected deviation from the average performance. We consider Figures <ref type="figure">9g, 9h</ref> and<ref type="figure">9i</ref> showing the standard deviation of Rec, UE and EV on the BSDS500 dataset. We can observe that in many cases good performing algorithms 730 such as ETPS, CRS, SLIC or ERS also demonstrate low standard deviation. Oversegmentation algorithms, on the other hand, show higher standard deviation -together with algorithms such as PF, TPS, VC, CIS and SEAW. In this sense, stable algorithms can also be identified by 735 low and monotonically decreasing standard deviation.</p><p>The variation in the number of generated superpixels is an important aspect for many superpixel algorithms. In particular, high standard deviation in the number of generated superpixels can be related to poor performance 740 regarding Rec, UE and EV. We find that superpixel algorithms ensuring that the desired number of superpixels is met within appropriate bounds are preferrable. We consider Figures <ref type="figure">9j</ref> and<ref type="figure">9k</ref>, showing max K and std K for K ≈ 400 on the BSDS500 dataset. Even after enforcing 745 connectivity as described in Section 6.2, we observe that several implementations are not always able to meet the desired number of superpixels within acceptable bounds. Figure <ref type="figure">9</ref>: Quantitative experiments on the BSDS500 dataset; remember that K denotes the number of generated superpixels. Rec (higher is better) and UE (lower is better) give a concise overview of the performance with respect to ground truth. In contrast, EV (higher is better) gives a ground truth independent view on performance. While top-performers as well as poorly performing algorithms are easily identified, we provide more find-grained experimental results by considering min Rec, max UE and min EV. These statistics additionally can be used to quantity the stability of superpixel algorithms. In particular, stable algorithms are expected to exhibit monotonically improving min Rec, max UE and min EV. The corresponding std Rec, std UE and std EV as well as max K and std K help to identify stable algorithms. Best viewed in color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Among these algorithms are QS, VC, FH, CIS and LSC. Except for the latter case, this can be related to poor performance with respect to Rec, UE and EV. Conversely, considering algorithms such as ETPS, ERGC or ERS which guarantee that the desired number of superpixels is met exactly, this can be related to good performance regarding these metrics. To draw similar conclusions for algorithms utilizing depth information, i.e. DASP and VCCS, the reader is encouraged to consider Figures <ref type="figure" target="#fig_4">10j</ref> and<ref type="figure" target="#fig_4">10k</ref>, showing max K and std K for K ≈ 400 on the NYUV2 dataset. We can conclude that superpixel algorithms with low standard deviation in the number of generated superpixels are showing better performance in many cases.</p><p>Finally, we discuss the proposed metrics AMR, AUE and AUV (computed as the area below the MR = (1 -Rec), UE and UV = (1 -EV) curves within the interval [K min , K max ] = [200, 5200], i.e. lower is better). We find that these metrics appropriately reflect and summarize the performance of superpixel algorithms independent of K. As can be seen in Figure <ref type="figure" target="#fig_4">11a</ref>, showing AMR, AUE and AUV on the BSDS500 dataset, most of the previous observations can be confirmed. For example, we exemplarily consider SEEDS and observe low AMR and AUV which is confirmed by Figures <ref type="figure">9a</ref> and<ref type="figure">9c</ref>, showing Rec and EV on the BSDS500 dataset, where SEEDS consistently outperforms all algorithms except for ETPS.</p><p>However, we can also observe higher AUE compared to algorithms such as ETPS, ERS or CRS wich is also consistent with Figure <ref type="figure">9b</ref>, showing UE on the BSDS500 dataset. We conclude, that AMR, AUE and AUV give an easy-tounderstand summary of algorithm performance. Furthermore, AMR, AUE and AUV can be used to rank the different algorithms according to the corresponding metrics; we will follow up on this idea in Section 7.7.</p><p>The observed AMR, AUE and AUV also properly reflect the difficulty of the different datasets. We consider Figure <ref type="figure" target="#fig_4">11</ref> showing AMR, AUE and AUV for all five datasets. Concentrating on SEEDS and ETPS, we see that the relative performance (i.e. the performance of SEEDS compared to ETPS) is consistent across datasets; SEEDS usually showing higher AUE while AMR and AUV are usually similar. Therefore, we observe that these metrics can be used to characterize the difficulty and ground truth of the datasets. For example, considering the Fash dataset, we observe very high AUV compared to the other datasets, while AMR and AUE are usually very low. This can be explained by the ground truth shown in Figure <ref type="figure">B</ref>.17e, i.e. the ground truth is limited to the foreground (in the case of Figure B.17e, the woman), leaving even complicated background unannotated. Similar arguments can be developed for the consistently lower AMR, AUE and AUV for the NYUV2 and SUNRGBD datasets compared to the BSDS500 dataset. For the SBD dataset, lower AMR, AUE and AUV can also be explained by the smaller average image size.</p><p>In conclusion, AMR, AUE and AUV accurately reflect the performance of superpixel algorithms and can be used to judge datasets. Across the different datasets, pathbased and density-based algorithms perform poorly, while the remaining classes show mixed performance. However, some iterative energy optimization, clustering-based and 810 graph-based algorithms such as ETPS, SEEDS, CRS, ERS and SLIC show favorable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Depth</head><p>Depth information does not necessarily improve performance regarding Rec, UE and EV. We consider Fig- <ref type="figure" target="#fig_4">815</ref> ures 10a, 10b and 10c presenting Rec, UE and EV on the NYUV2 dataset. In particular, we consider DASP and VCCS. We observe, that DASP consistently outperforms VCCS. Therefore, we consider the performance of DASP and investigate whether depth information improves per-820 formance. Note that DASP performs similar to SLIC, exhibiting slightly worse Rec and slightly better UE and EV for large K. However, DASP does not clearly outperform SLIC. As indicated in Section 3, DASP and SLIC are both clustering-based algorithms. In particular, both 825 algorithms are based on k-means using color and spatial information and DASP additionally utilizes depth information. This suggests that the clustering approach does not benefit from depth information. We note that a similar line of thought can be applied to VCCS except that 830 VCCS directly operates within a point cloud, rendering the comparison problematic. Still we conclude that depth information used in the form of DASP does not improve performance. This might be in contrast to experiments with different superpixel algorithms, e.g. a SLIC variant 835 using depth information as in <ref type="bibr" target="#b92">[92]</ref>. We suspect that regarding the used metrics, the number of superpixels (K = 200) and the used superpixel algorithm, the effect of depth information might be more pronounced in the experiments presented in <ref type="bibr" target="#b92">[92]</ref> compared to ours. Furthermore, it should 840 be noted that our evaluation is carried out in the 2D image plane, which does not directly reflect the segmentation of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Runtime</head><p>Considering runtime, we report CPU time<ref type="foot" target="#foot_2">3</ref> excluding 845 connected components but including color space conversions if applicable. We made sure that no multi-threading or GPU computation were used. We begin by considering runtime in general, with a glimpse on realtime applications, before considering iterative algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>850</head><p>We find that some well performing algorithms can be run at (near) realtime. We consider Figure <ref type="figure" target="#fig_12">12</ref> showing runtime in seconds t on the BSDS500 (image size 481 × 321) and NYUV2 (image size 608 × 448) datasets. Concretely, considering the watershed-based algorithms W and CW,  In the light of realtime applications, CW, W and PF even provide runtimes below 10ms. However, independent of the application at hand, we find runtimes below 1s beneficial for using superpixel algorithms as pre-processing tools.</p><formula xml:id="formula_11">A C C E P T E D M A N U S C</formula><p>Best viewed in color.   we can report runtimes below 10ms on both datasets, corresponding to roughly 100fps. Similarly, PF runs at below 10ms. Furthermore, several algorithms, such as SLIC, ERGC, FH, PB, MSS and preSLIC provide runtimes below 80ms and some of them are iterative, i.e. reducing the number of iterations may further reduce runtime. However, using the convention that realtime corresponds to roughly 30fps, this leaves preSLIC and MSS on the larger images of the NYUV2 dataset. However, even without explicit runtime requirements, we find runtimes below 1s per image to be beneficial for using superpixel algorithms as pre-processing tool, ruling out TPS, CIS, SEAW, RW and NC. Overall, several superpixel algorithms provide runtimes appropriate for pre-processing tools; realtime applications are still limited to a few fast algorithms.</p><p>Iterative algorithms offer to reduce runtime while gradually lowering performance. Considering Figure <ref type="figure" target="#fig_4">13</ref>, show-ing Rec, UE and runtime in seconds t for all iterative algorithms on the BSDS500 dataset, we observe that the 875 number of iterations can safely be reduced to decrease runtime while lowering Rec and increasing UE only slightly. In the best case, for example considering ETPS, reducing the number of iterations from 25 to 1 reduces the runtime from 680ms to 58ms, while keeping Rec und UE nearly 880 constant. For other cases, such as SEEDS<ref type="foot" target="#foot_3">4</ref> , Rec decreases abruptly when using less than 5 iterations. Still, runtime can be reduced from 920ms to 220ms. For CRS and CIS, runtime reduction is similarly significant, but both algorithms still exhibit higher runtimes. If post-processing is 885 necessary, for example for SLIC and preSLIC, the number of iterations has to be fixed in advance. However, for other iterative algorithms, the number of iterations may be adapted at runtime depending on the available time. Overall, iterative algorithms are beneficial as they are able 890 to gradually trade performance for runtime.</p><p>We conclude that watershed-based as well as some pathbased and clustering-based algorithms are candidates for realtime applications. Iterative algorithms, in particular many clustering-based and iterative energy optimization 895 algorithms, may further be speeded up by reducing the number of iterations and trading performance for runtime. On a final note, we want to remind the reader that the image sizes of all used datasets may be relatively small compared to today's applications. However, the relative 900 runtime comparison is still valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Influence of Implementations</head><p>We discuss the influence of implementation details on performance and runtime for different implementations of SEEDS, SLIC and FH. In particular, reSEEDS and 905 reFH are our revised implementations of SEEDS and FH, respectively. Both implementations follow the algorithm as described in the corresponding publications, provide exactly the same parameters and are also implemented in C/C++. Still, we tried to optimize the im-910 plementations with respect to connectivity and kept them as efficient as possible. reFH additionally uses a slightly different graph data structure -we refer to the implementations for details; these will be made publicly available together with the benchmark. Furthermore, we include vl-915 SLIC, an implementation of SLIC as part of the VLFeat library <ref type="bibr" target="#b114">[114]</ref>, and preSLIC <ref type="bibr" target="#b57">[57]</ref>, an accelerated version of SLIC based on the original implementation. Both vl-SLIC and preSLIC are also implemented in C/C++ as their original counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>920</head><p>We find that revisiting implementation details may be beneficial for both performance and runtime. We consider Figure <ref type="figure" target="#fig_15">14</ref> showing Rec, UE and runtime in seconds t for the introduced implementations of SLIC, SEEDS  and FH on the BSDS500 dataset. For reSEEDS and reFH, we observe improved performance which can be related to the improved connectivity. However, even very similar implementations such as SLIC and vlSLIC differ slightly in performance; note the lower Rec and higher UE of vlSLIC compared to SLIC. Overall, the difference in runtime is most striking, for example reSEEDS and preSLIC show significantly lower runtime compared to SEEDS and SLIC. reFH, in contrast, shows higher runtime compared to FH due to a more complex data structure.</p><p>As expected, implementation details effect runtime, however, in the presented cases, i.e. for SLIC, SEEDS and FH, performance is also affected. Nevertheless, it still needs to be examined whether this holds true for the remaining algorithms, as well. Furthermore, the experiments suggest that improving connectivity helps performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Robustness</head><p>Similar to Neubert and Protzel <ref type="bibr" target="#b44">[44]</ref>, we investigate the influence of noise, blur and affine transformations. We evaluated all algorithms for K ≈ 400 on the BSDS500 dataset. In the following we exemplarily discuss salt and pepper noise and average blurring.</p><p>Most algorithms are robust to salt and pepper noise; blurring, in contrast, tends to reduce performance. We consider Figure <ref type="figure" target="#fig_4">15</ref> showing Rec, UE and K for p ∈ {0, 0.04, 0.08, 0.12, 0.16} being the probability of a pixel being salt or pepper. Note that Figure <ref type="figure" target="#fig_4">15</ref> shows the number of superpixels K before enforcing connectivity as described in Section 6.2. As we can deduce, salt and pepper noise only slightly reduces Rec and UE for most algorithms. Some algorithms compensate the noise by generating more superpixels such as VC or SEAW while only slightly reducing performance. In contrast, for QS the performance even increases -a result of the strongly increasing number As can be seen, blurring gradually reduces performance -which may be explained by vanishing image boundaries. In addition, for algorithms such as VC and QS, blurring also leads to fewer superpixels being generated. Best viewed in color. of superpixels. Similar results can be obtained for Gaussian additive noise. Turning to Figure <ref type="figure" target="#fig_4">16</ref> showing Rec, UE and K for k ∈ {0, 5, 9, 13, 17} being the size of a box filter used for average blurring. As expected, blurring leads to reduced performance with respect to both Rec and UE.</p><p>965 Furthermore, it leads to a reduced number of generated superpixels for algorithms such as QS or VC. Similar observations can be made for motion blur as well as Gaussian blur.</p><p>Overall, most superpixel algorithms are robust to the 970 considered noise models, while blurring tends to reduce performance. Although the corresponding experiments are omitted for brevity, we found that affine transformations do not influence performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">More Superpixels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>975</head><p>Up to now, we used AMR, AUE and AUV to summarize experimental results for K ∈ [200, 5200]. However, for some applications, generating K 5200 superpixels may be interesting.</p><p>For K ≈ 20000, superpixel algorithms can be used to 980 dramatically reduce the number of primitives for subsequent processing steps with negligible loss of information. We consider  seconds t and K for K ≈ 20, 000 on the BSDS500 dataset. We note that some algorithms were not able to generate K 5200 superpixels and are, therefore, excluded. Similarly, we excluded algorithms not offering control over the number of generated superpixels. We observe that except for VC and PF all algorithms achive Rec ≥ 0.99, UE ≈ 0.03, and EV &gt; 0.9. Furthermore, the runtime of many algorithms is preserved. For example W and CW still run in below 10ms and the runtime for preSLIC and SLIC increases only slightly. Obviously, the number of generated superpixels varies more strongly for large K. Overall, most algorithms are able to capture the image content nearly perfectly while reducing 321×481 = 154401 pixels to K ≈ 20000 superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.">Ranking</head><p>We conclude the experimental part of this paper with a ranking with respect to AMR and AUE-reflecting the objective used for parameter optimization. Unfortunately, the high number of algorithms as well as the low number of datasets prohibits using statistical tests to extract rankings, as done in other benchmarks (e.g. <ref type="bibr" target="#b115">[115,</ref><ref type="bibr" target="#b116">116]</ref>). Therefore, Table <ref type="table">3</ref> presents average AMR and AUE, average ranks as well as the corresponding rank matrix. On each dataset, the algorithms were ranked according to AMR + AUE where lowest AMR + AUE corresponds to the best rank, i.e. rank one. The corresponding rank matrix represents the rank distribution (i.e. the frequencies of the attained ranks) for each algorithm. We find that the presented average ranks provide a founded overview of the evaluated algorithms, summarizing many of the observations discussed before. In the absence of additional constraints, Table <ref type="table">3</ref> may be used to select suitable superpixel algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we presented a large-scale comparison of superpixel algorithms taking into account visual quality, ground truth dependent and independent metrics, run-1020 time, implementation details as well as robustness to noise, blur and affine transformations. For fairness, we systematically optimized parameters while strictly enforcing connectivity. Based on the obtained parameters, we presented experiments based on five different datasets includ-1025 ing indoor and outdoor scenes as well as persons. In contrast to existing work <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref>, we considered minimum/maximum as well as the standard deviation in addition to simple metric averages. We further proposed Average Miss Rate (AMR), Average Undersegmentation Error 1030 (AUE) and Average Unexplained Variation (AUV) to summarize algorithm performance independent of the number of generated superpixels. This enabled us to present an overall ranking of superpixel algorithms aimed to simplify and guide algorithm selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1035</head><p>Regarding the mentioned aspects of superpixel algorithms, we made several observations relevant for applications and future research. Considering visual quality, we found that the majority of algorithms provides good boundary adherence; some algorithms are able to capture 1040 even small details. However, better boundary adherence may influence compactness, regularity and smoothness. While regularity and smoothness strongly depends on the individual algorithms, a compactness parameter is beneficial to trade-off boundary adherence for compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1045</head><p>Regarding performance, Boundary Recall (Rec) <ref type="bibr" target="#b48">[48]</ref>, Undersegmentation Error (UE) <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44]</ref> and Explained Variation (EV) <ref type="bibr" target="#b49">[49]</ref> provide a good overview but are not sufficient to discriminate algorithms reliably. Therefore, we used the minimum/maximum as well as the standard 1050 deviation of these metrics to identify stable algorithms, i.e. algoritms providing monotonically increasing performance with regard to the number of generated superpixels. Furthermore, we were able to relate poor performance to a high standard deviation in the number of generated super-1055 pixels, justifying the need to strictly control connectivity. Concerning runtime, we identified several algorithms providing realtime capabilities, i.e. roughly 30fps, and showed that iterative algorithms allow to reduce runtime while only gradually reducing performance. Implementation de-1060 tails are rarely discussed in the literature; on three examples, we highlighted the advantage of ensuring connectivity and showed that revisiting implementations may benefit performance and runtime. We further demonstrated that generating a higher number of superpixels, e.g. roughly ETPS 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.57 7.98 SEEDS 3.8 0 2 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.38 6.28 ERS 3.8 0 1 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.16 6.29 CRS 4.8 0 0 2 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.18 6.77 EAMS 5.4 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.55 6.34 ERGC 5.8 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.91 6.50 SLIC 6.2 0 0 0 0 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5.40 7.24 LSC 9.2 0 0 0 1 0 1 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 5.49 7.02 preSLIC 9.2 0 0 0 0 0 0 0 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.60 6.41 CCS 10.6 0 0 0 0 0 0 0 2 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 6. <ref type="bibr">49 7.19</ref> CW 11 0 0 0 0 0 0 0 0 0 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8.24 7.83 DASP 12 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.51 7.44 W 12.8 0 0 0 0 0 0 0 0 0 0 2 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 7.44 6.97 WP 13.4 0 0 0 0 0 0 0 0 0 0 0 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 7.85 7.58 POISE 13.8 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 8.13 6.90 NC 15.25 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 7.66 7.43 MSS 15.8 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 2 1 0 0 0 0 0 0 0 0 0 0 8.90 7.67 VC 17.8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 1 0 0 0 1 0 0 0 0 0 8.31 7.85 PB 18.4 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 8.53 8.15 FH 19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3 0 1 0 0 0 0 0 0 0 8. <ref type="bibr">35 7.32</ref> RW 19 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 11.45 6.70 CIS 20.8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 2 0 0 0 1 0 0 0 11. <ref type="bibr">45 7.19</ref> TPS 21 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 11.05 7.93 TP 21.6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 14.22 13.54 VCCS 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 11.97 8.36 SEAW 24 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 1 0 0 15.72 10.75 QS 24.8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 0 1 0 24.64 14.91 PF 26.2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0 1</p><p>Table <ref type="table">3</ref>: Average AMR and AUE, average ranks and rank distribution for each evaluated algorithm. To compute average ranks, the algorithms were ranked according to AMR + AUE (where lowest AMR + AUE corresponds to the best rank, i.e. 1) on each dataset separately. For all algorithms (rows), the rank distribution (columns 1 through 28) illustrates the frequency a particular rank was attained over all considered datasets. We note that we could not evaluate RW, NC and SEAW on the SUNRGBD dataset and DASP and VCCS cannot be evaluated on the BSDS500, SBD and Fash datasets.</p><p>From the ranking in Table <ref type="table">3</ref>, we recommend 6 algorithms for use in practice, thereby covering a wide range of application scenarios: ETPS <ref type="bibr" target="#b84">[84]</ref>, SEEDS <ref type="bibr" target="#b80">[80]</ref>, ERS a compactness parameter (except for SEEDS and ERS). Except for ERS and CRS, they provide runtimes below 100ms -depending on the implementation -and preSLIC <ref type="bibr" target="#b57">[57]</ref>, which we see as a variant of SLIC, provides realtime capabilities. Finally, the algorithms provide control over 1085 the number of generated superpixels (therefore, EAMS, ranked 5th in Table <ref type="table">3</ref>, is not recommended), are able to generate mostly connected superpixels and exhibit a very low standard deviation in the number of generated superpixels. ----"--------"----C/C++ -----3 -----RW <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b65">65]</ref>  First of all we present the used acronym (in parts consistent with <ref type="bibr" target="#b39">[39]</ref> and <ref type="bibr" target="#b44">[44]</ref>), the corresponding publication, the year of publication and the number of Google Scholar citations as of October 13, 2016. We present a coarse categorization which is discussed in Section 3. We additionally present implementation details such as the programming language, supported color spaces and provided parameters. The color space used for evaluation is marked by a square. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>within the interval [K min , K max ] = [200, 5200]. As introduced before, the first corresponds to the Boundary Miss Rate (MR) and the last is referred to as Unexplained Variation (UV). In particular, we use the trapezoidal rule for integration. As the algorithms do not necessarily generate the desired number of superpixels, we additionally considered the following two cases for special treatment. First, if an algorithm generates more that K max superpixels (or less than K min ), we interpolate linearly to determine the value for K max (K min ). Second, if an algorithm consistently generates less that K max (or more than K min ) superpixels, we take the value lower or equal (greater or equal) and closest to K max (K min ). In the second case, a superpixel algorithm is penalized if it is not able to generate very few (i.e. K min ) or very many (i.e. K max ) superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Parameter Optimization</head><p>We discuss the following two topics concerning parameter optimization in more detail: color spaces and controlling the number of superpixels in a consistent manner.</p><p>Overall, we find that together with Section 6, the described parameter optimization procedure ensures fair comparison 1570 as far as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D.1. Color Spaces</head><p>The used color space inherently influences the performance of superpixel algorithms as the majority of superpixel algorithms depend on comparing pixels within this 1575 color space. To ensure fair comparison, we included the color space in parameter optimization. In particular, we ensured that all algorithms support RGB color space and considered different color spaces only if reported in the corresponding publications or supported by the respective 1580 implementation. While some algorithms may benefit from different color spaces not mentioned in the corresponding publications, we decided to not consider additional color spaces for simplicity and to avoid additional overhead during parameter optimization. Parameter optimization 1585 yielded the color spaces highlighted in Table A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Appendix D.2. Controlling the Number of Generated Superpixels Some implementations, for example ERS and POISE, control the number of superpixels directly -for example by stopping the merging of pixels as soon as the desired number of superpixels is met. In contrast, clustering-based algorithms (except for DASP), contour evolution algorithms, watershed-based algorithms as well as path-based algorithms utilize a regular grid to initialize superpixels. Some algorithms allow to adapt the grid in both horizontal and vertical direction, while others require a Cartesian grid. We expected this difference to be reflected in the experimental results, however, this is not the case. We standardized initialization in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Experiments</head><p>We complement Section 7 with additional experimental results. In particular, we provide additional qualitative results to better judge the visual quality of individual superpixel algorithms. Furthermore, we explicitly present ASA and UE Levin on the BSDS500 and NYUV2 datasets as well as Rec, UE and EV on the SBD, SUNRGBD and Fash datasets. Most algorithms exhibit good boundary adherence, especially for large K. In contrast to the discussion in Section 7.1 focussing on qualitative results with K ≈ 400 and K ≈ 1200, Figures E.18 and E.19 also show results for K ≈ 3600. We observe that with rising K, most algorithms exhibit better boundary adherence. Exceptions are, again, easily identified: FH, QS, CIS, PF, PB, TPS and SEAW. Still, due to higher K, the effect of missed image boundaries is not as serious as with less superpixels. Overall, the remaining algorithms show good boundary adherence, especially for high K.</p><p>Compactness increases with higher K; still, a compactness parameter is beneficial. While for higher K, superpixels tend to be more compact in general, the influence of parameter optimization with respect to Rec and UE is still visible -also for algorithms providing a compactness parameter. For example, ERGC or ETPS exhibit more irregular superpixels compared to SLIC or CCS. Complementing this discussion, Figure <ref type="figure">E</ref>.20 shows the influence of the compactness parameter for the algorithms with compactness parameter not discussed in detail in Section 7.1. It can be seen, that a compactness parameter allows to gradually trade boundary adherence for compactness in all of the presented cases. However, higher K also induces higher compactness for algorithms not providing a compactness parameter such as CIS, RW, W or MSS to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T name only a few examples. Overall, compactness benefits from higher K. Overall, higher K induces both better boundary adherence and higher compactness independent of whether a compactness parameter is involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E.2. Quantitative</head><p>The following experiments complement the discussion in Section 7.2 in two regards. First, we present additional experiments considering both ASA and UE Levin on the BSDS500 and NYUV2 datasets. Then, we consider Rec, UE and EV in more details for the remaining datasets, i.e. the SBD, SUNRGBD and Fash datasets. We begin by discussing ASA and UE Levin , also in regard to the observations made in Sections 5.1 and Appendix C.</p><p>As observed on the BSDS500 and NYUV2 datasets in Section 7.2, Rec and UE can be used to roughly asses superpixel algorithms based on ground truth. However, for large K, these metrics are not necessarily sufficient to discriminate between the superpixel algorithms. Considering Figure E.21, in particular with regard to Rec, we can identify algorithms showing above-average performance such as ETPS and SEEDS. These algorithms perform well on all three datasets. Similarly, PF, QS, SEAW and TPS perform poorly on all three datasets. Regarding UE, in contrast, top-performer across all three algorithms are not identified as easily. For example, POISE demonstrates low UE on the SBD and Fash datasets, while performing poorly on the SUNRGBD dataset. Similarly, ERS shows excellent performance on the SUNRGBD dataset, while being outperformed by POISE as well as ETPS on the SBD and Fash datasets. Overall, Rec and UE do not necessarily give a consistent view on the performance of the superpixel algorithms across datasets. This may also be explained by the ground truth quality as already discussed in Section 7.2.</p><p>The above observations also justify the use of EV to judge superpixel algorithms independent of ground truth.</p><p>Considering Figure E.21, in particular, with regard to EV, we can observe a more consistent view across the datasets. Both, top-performing algorithms such as ETPS and SEEDS, as well as poorly performing algorithms such as PF, PB or TPS can easily be identified. In between these two extremes, superpixel algorithms are easier to discriminate compared to Rec and UE. Furthermore, some superpixel algorithms such as QS, FH or CIS are performing better compared to Rec or UE. This confirms the observations that ground truth independent assessment is beneficial but cannot replace Rec or UE. We find that ASA closely mimicks the behavior of (1 -UE) while UE Levin may complement our discussion with an additional viewpoint which is, however, hard to interpret.</p><p>We consider Figure E.23 showing UE, ASA and UE Levin for both the BSDS500 and NYUV2 datasets. Focussing on UE and ASA, we easily see that ASA nearly reflects (1 -UE) while being a small constant off. In particular, all algorithms exhibit nearly the same behavior, while absolutely the algorithms show higher ASA compared to (1 -UE). This demonstrates that ASA does not give new insights with respect to the quantitative comparison of superpixel algorithms. In contrast, the algorithms show different behavior considering UE Levin . This is mainly 1700 due to the unconstrained range of UE Levin (compared to UE ∈ [0, 1]). In particular, for algorithms such as EAMS and FH, UE Levin reflects the behavior of max UE as shown in Figure <ref type="figure">9e</ref>. The remaining algorithms lie more closely together. Still, algorithms such as ERS, SEEDS or PB 1705 show better UE Levin than UE (seen relatively to the remaining algorithms). In the case of EAMS and FH, high UE Levin may indeed be explained by the considerations of Neubert and Protzel <ref type="bibr" target="#b44">[44]</ref> arguing that UE Levin unjustly penalizes large superpixels slightly overlapping with multi-1710 ple ground truth segments. For the remaining algorithms, the same argument can only be applied in smaller scale as these algorithms usually do not generate large superpixels. In this line of throught, the excellent performance of ERS may be explained by the employed regularizer for 1715 enforcing uniform superpixel size. Overall, ASA does not contribute to an insightful discussion, while UE Levin may be considered in addition to UE to complete the picture of algorithm performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E.3. Runtime 1720</head><p>We briefly discuss runtime on the SBD, SUNRGBD and Fash datasets allowing to get more insights on how the algorithms scale with respect to image size and the number of generated superpixels.</p><p>We find that the runtime of most algorithms scales 1725 roughly linear in the input size, while the number of generated superpixels has little influence. We first remember that the average image size of the SBD, SUNRGBD and Fash datasets is: 314 × 242 = 75988, 660 × 488 = 322080 and 400 × 600 = 240000. For K ≈ 400, W runs in roughly 1730 1.9ms and 7.9ms on the SBD and SUNRGBD datasets, respectively. As the input size for the SUNRGBD dataset is roughly 4.24 times larger compared to the SBD dataset, this results in roughly linear scaling of runtime with respect to the input size. Similar reasoning can be ap-1735 plied to most of the remaining algorithms, especially fast algorithms such as CW, PF, preSLIC, MSS or SLIC. Except for RW, QS and SEAW we also notice that the number of generated superpixels does not influence runtime significantly. Overall, the results confirm the claim 1740 of many authors that algorithms scale linear in the input size, while the number of generated superpixels has little influence.  , Rec and UE give a roguh overview of algorithm performance with respect to ground truth. Concerning Rec, we observe similar performance across the three datasets, while algorithms may show different behavior with respect to UE. Similarly, EV gives a ground truth independent overview of algorithm performance where algorithms show similar performance across datasets. Best viewed in color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. Popular density-based algorithms are Edge-Augmented Mean Shift (EAMS) and Quick Shift (QS). Both perform mode-seeking in a computed density image; each pixel is assigned to the corresponding mode it falls into. Density-based algorithms usually cannot offer 1 Google Scholar citations as of October 13, 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. Graph-based algorithms treat the image as undirected graph and partition this graph based on edge-weights which are often computed as color differences or similarities. The algorithms differ in the partitioning algorithm, for example FH, ERS and POISE exhibit a 205 bottom-up merging of pixels into superpixels, while NC and CIS use cuts and PB uses elimination [63]. Name NC -Normalized Cuts Reference (Google Scholar Citations) Color Ren and Malik [1], 2002 (996) for Objects from Improved Seeds and Energies Reference (Google Scholar Citations) Color Humayun et al. [67], 2015 (3) Implementation Superpixels Compactness Iterations MatLab/C; RGB; 5 Parameters --A C C E P T E D M A N U S C R I P T Contour evolution. These algorithms represent superpixels as evolving contours starting from inital seed pixels. Name TP -Turbo Pixels Reference (Google Scholar Citations) ColorLevinshtein et al.<ref type="bibr" target="#b42">[42]</ref>, 2009 (559) . Path-based approaches partition an image into superpixels by connecting seed points through pixel paths following specific criteria. The number of superpixels is easily controllable, however, compactness usually is not. Often, these algorithms use edge information:PF uses discrete image gradients and TPS uses edge detection as proposed in<ref type="bibr" target="#b70">[70]</ref>.NamePF -Path FinderReference (Google Scholar Citations) ColorDrucker et al.<ref type="bibr" target="#b71">[71]</ref>, 2009<ref type="bibr" target="#b18">(18)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Scholar Citations) Color Van den Bergh et al. [80], 2012 (98) Implementation Superpixels Compactness Iterations C/C++; Lab; 6 Parameters -Name CCS -Convexity Constrained Superpixels Reference (Google Scholar Citations) ColorTasli et al.<ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b83">83]</ref>, 2013(6 + 4)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example images from the used datasets. From left to right: BSDS500, SBD, NYUV2, SUNRGBD, and Fash. Black contours represent ground truth and red rectangles indicate excerpts used for qualitative comparison in Figures 5 and 6. Best viewed in color.</figDesc><graphic coords="7,464.44,75.23,69.56,104.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 100 •</head><label>100</label><figDesc>|S j |. Both van den Bergh et al. [81] and Neubert</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>365</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: K and Rec on the training set of the BSDS500 dataset when varying parameters strongly influencing the number of generated superpixels of: LSC; CIS; VC; CRS; and PB. The parameters have been omitted and scaled for clarity. A higher number of superpixels results in increased Rec. Therefore, unnoticed superpixels inherently complicate fair comparison. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure</head><label></label><figDesc>Figure 3: Rec and runtime in seconds t on the training set of the BSDS500 dataset when varying the number of iterations of: SLIC; CRS; SEEDS; preSLIC; LSC; and ETPS. Most algorithms achieve reasonable Rec with about 3 -10 iterations. Still, parameter optimization with respect to Rec and UE favors more iterations. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure5: Qualitative results on the BSDS500, SBD and Fash datasets. Excerpts from the images in Figure1are shown for K ≈ 400 in the upper left corner and K ≈ 1200 in the lower right corner. Superpixel boundaries are depicted in black; best viewed in color. We judge visual quality on the basis of boundary adherence, compactness, smoothness and regularity. Boundary adherence can be judged both on the caterpillar image as well as on the woman image -the caterpillar's boundaries are hard to detect and the woman's face exhibits small details. In contrast, compactness, regularity and smoothness can be evaluated considering the background in the caterpillar and see images. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The influence of a low, on the left, and high, on the right, compactness parameter demonstrated on the caterpillar image from the BSDS500 datasets using SLIC and CRS for K ≈ 400. Superpixel boundaries are depicted in black; best viewed in color. Superpixel algorithms providing a compactness parameter allow to trade boundary adherence for compactness. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Runtime in seconds on the BSDS500 and NYUV2 datasets. Watershed-based, some clustering-based algorithms as well as PF offer runtimes below 100ms. In the light of realtime applications, CW, W and PF even provide runtimes below 10ms. However, independent of the application at hand, we find runtimes below 1s beneficial for using superpixel algorithms as pre-processing tools. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure</head><label></label><figDesc>Figure13: Rec, UE and runtime in seconds t for iterative algorithms with K ≈ 400 on the BSDS500 dataset. Some algorithms allow to gradually trade performance for runtime, reducing runtime by several 100ms in some cases. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure14: Rec, UE and runtime in seconds t on the BSDS500 dataset for different implementations of SLIC, SEEDS and FH. In particular, reSEEDS and reFH show slightly better performance which may be explained by improved connectivity. vlSLIC, in contrast, shows similar performance to SLIC and, indeed, the implementations are very similar. Finally, preSLIC reduces runtime by reducing the number of iterations spend on individual superpixels. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>3 KFigure 15 : 3 KFigure 16 :</head><label>315316</label><figDesc>Figure 15:  The influence of salt and pepper noise for p ∈ {0, 0.04, 0.08, 0.12, 0.16} being the probability of salt or pepper. Regarding Rec and UE, most algorithms are not significantly influence by salt and pepper noise. Algorithms such as QS and VC compensate the noise by generating additional superpixels. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>1065 20000 ,</head><label>20000</label><figDesc>results in nearly no loss of information while reducing the high number of pixels to only ∼ 20000 superpixels. Finally, we experimentally argued that superpixel algorithms are robust against noise and affine transformations before providing a final ranking of the algorithms based 1070 on the proposed metrics Average Miss Rate and Average Undersegmentation Error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Appendix E.1. Qualitative We briefly discuss visual quality on additional examples provided in Figures E.18 and E.19. Additionally, Figure E.20 shows the influence of the compactness parameter on superpixel algorithms not discussed in Section 7.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure E. 20 :Figure E. 18 :Figure E. 19 :</head><label>201819</label><figDesc>Figure E.20: The influence of a low, on the left, and high, on the right, compactness parameter demonstrated on the caterpillar image from the BSDS500 dataset for K ≈ 400. Superpixel boundaries are depicted in black; best viewed in color. For all shown algorithms, the compactness parameter allows to gradually trade boundary adherence for compactness. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure E. 21 :</head><label>21</label><figDesc>Figure E.21: Rec, UE and EV on the SBD, SUNRGBD and Fash datasets. Similar to the results presented for the BSDS500 and NYUV2 datasets (compare Figures9 and 10), Rec and UE give a roguh overview of algorithm performance with respect to ground truth. Concerning Rec, we observe similar performance across the three datasets, while algorithms may show different behavior with respect to UE. Similarly, EV gives a ground truth independent overview of algorithm performance where algorithms show similar performance across datasets. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Figure E.21: Rec, UE and EV on the SBD, SUNRGBD and Fash datasets. Similar to the results presented for the BSDS500 and NYUV2 datasets (compare Figures9 and 10), Rec and UE give a roguh overview of algorithm performance with respect to ground truth. Concerning Rec, we observe similar performance across the three datasets, while algorithms may show different behavior with respect to UE. Similarly, EV gives a ground truth independent overview of algorithm performance where algorithms show similar performance across datasets. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>with inferior boundary adherence are easily identified as those not capturing the pattern in the background or the silhouette of the caterpillar: FH, QS, CIS, PF, PB, TPS, TP and SEAW. The remaining algorithms do not necessarily capture all image details, as for example the woman's face, but important image boundaries are consistently captured.We note that of the three evaluated oversegmentation algorithms, i.e. EAMS, FH and QS, only EAMS demonstrates adequate boundary adherence. Furthermore, we observe that increasing K results in more details being captured by all algorithms. Notable algorithms regarding boundary adherence include CRS, ERS, SEEDS, ERGC and ETPS. These algorithms are able to capture even smaller details such as the coloring of the caterpillar or elements of the woman's face.Compactness strongly varies across algorithms and a compactness parameter is beneficial to control the degree of compactness as it allows to gradually trade boundary adherence for compactness. We consider the caterpillar image in Figure5. TP, RW, W, and PF are examples for algorithms not providing a compactness parameter. While</figDesc><table /><note><p><p><p><p><p>TP generates very compact superpixels and RW tends to resemble grid-like superpixels, W and PF generate highly non-compact superpixels. In this regard, compactness depends on algorithm and implementation details (e.g. gridlike initialization) and varies across algorithms. For algorithms providing control over the compactness of the generated superpixels, we find that parameter optimization has strong impact on compactness. Examples are CRS, LSC, ETPS and ERGC showing highly irregular superpixels, while SLIC, CCS, VC and WP generate more compact superpixels. For DASP and VCCS, requiring depth information, similar observations can be made on the kitchen image in Figure</p>6</p>. Inspite of the influence of parameter optimization, we find that a compactness parameter is beneficial. This can best be observed in Fig-</p>ure 7</p>, showing superpixels generated by SLIC and CRS for different degrees of compactness. We observe that compactness can be increased while only gradually sacrificing boundary adherence.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CO on the BSDS500 and NYUV2 datasets. Considering Figures5 and 6, CO appropriately reflects compactness. However, it does not take into account other aspects of visual quality such as regularity and smoothness. Therefore, we find that CO is of limited use in a quantitative assessment of visual quality. Best viewed in color.</figDesc><table><row><cell></cell><cell cols="2">BSDS500</cell><cell cols="2">NYUV2</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell>0.6</cell></row><row><cell>CO</cell><cell>0.4</cell><cell>CO</cell><cell>0.4</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell>0</cell><cell>1,000</cell><cell>0</cell><cell>1,000</cell></row><row><cell></cell><cell></cell><cell>log K</cell><cell></cell><cell>log K</cell></row><row><cell cols="2">Figure 8: W</cell><cell>EAMS</cell><cell>NC</cell><cell>FH</cell></row><row><cell></cell><cell>RW</cell><cell>QS</cell><cell>PF</cell><cell>TP</cell></row><row><cell></cell><cell>CIS</cell><cell>SLIC</cell><cell>CRS</cell><cell>ERS</cell></row><row><cell></cell><cell>PB</cell><cell>SEEDS</cell><cell>TPS</cell><cell>VC</cell></row><row><cell></cell><cell>CCS</cell><cell>CW</cell><cell cols="2">ERGC</cell><cell>MSS</cell></row><row><cell></cell><cell>preSLIC</cell><cell>WP</cell><cell cols="2">ETPS</cell><cell>LSC</cell></row><row><cell></cell><cell>POISE</cell><cell>SEAW</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>R I P T</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) BSDS500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell>AMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 20</cell><cell></cell><cell>AUV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W</cell><cell></cell><cell>EAMS</cell><cell>NC</cell><cell>FH</cell><cell>RW</cell><cell>QS</cell><cell>PF</cell><cell>TP</cell><cell>CIS</cell><cell>SLIC</cell><cell>CRS</cell><cell>ERS</cell><cell>PB</cell><cell>(DASP)</cell><cell>SEEDS</cell><cell>TPS</cell><cell>VC</cell><cell>CCS</cell><cell>(VCCS)</cell><cell>CW</cell><cell>ERGC</cell><cell>MSS</cell><cell>preSLIC</cell><cell>WP</cell><cell>ETPS</cell><cell>LSC</cell><cell>POISE</cell><cell>SEAW</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) NYUV2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell>AUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AUV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">W</cell><cell>EAMS</cell><cell>NC</cell><cell>FH</cell><cell>RW</cell><cell>QS</cell><cell>PF</cell><cell>TP</cell><cell>CIS</cell><cell>SLIC</cell><cell>CRS</cell><cell>ERS</cell><cell>PB</cell><cell>DASP</cell><cell>SEEDS</cell><cell>TPS</cell><cell>VC</cell><cell>CCS</cell><cell>VCCS</cell><cell>CW</cell><cell>ERGC</cell><cell>MSS</cell><cell>preSLIC</cell><cell>WP</cell><cell>ETPS</cell><cell>LSC</cell><cell>POISE</cell><cell>SEAW</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) SBD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell>AMR AUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell>AUV</cell><cell>failed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W</cell><cell></cell><cell>EAMS</cell><cell>(NC)</cell><cell>FH</cell><cell>RW</cell><cell>QS</cell><cell>PF</cell><cell>TP</cell><cell>CIS</cell><cell>SLIC</cell><cell>CRS</cell><cell>ERS</cell><cell>PB</cell><cell>(DASP)</cell><cell>SEEDS</cell><cell>TPS</cell><cell>VC</cell><cell>CCS</cell><cell>(VCCS)</cell><cell>CW</cell><cell>ERGC</cell><cell>MSS</cell><cell>preSLIC</cell><cell>WP</cell><cell>ETPS</cell><cell>LSC</cell><cell>POISE</cell><cell>SEAW</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(d) SUNRGBD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell>AUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AUV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell>failed</cell><cell></cell><cell>failed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>failed</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W</cell><cell></cell><cell>EAMS</cell><cell>(NC)</cell><cell>FH</cell><cell>(RW)</cell><cell>QS</cell><cell>PF</cell><cell>TP</cell><cell>CIS</cell><cell>SLIC</cell><cell>CRS</cell><cell>ERS</cell><cell>PB</cell><cell>DASP</cell><cell>SEEDS</cell><cell>TPS</cell><cell>VC</cell><cell>CCS</cell><cell>VCCS</cell><cell>CW</cell><cell>ERGC</cell><cell>MSS</cell><cell>preSLIC</cell><cell>WP</cell><cell>ETPS</cell><cell>LSC</cell><cell>POISE</cell><cell>(SEAW)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(e) Fash</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">AMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15</cell><cell cols="2">AUE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 10</cell><cell cols="2">AUV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>no depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W</cell><cell></cell><cell>EAMS</cell><cell>NC</cell><cell>FH</cell><cell>RW</cell><cell>QS</cell><cell>PF</cell><cell>TP</cell><cell>CIS</cell><cell>SLIC</cell><cell>CRS</cell><cell>ERS</cell><cell>PB</cell><cell>(DASP)</cell><cell>SEEDS</cell><cell>TPS</cell><cell>VC</cell><cell>CCS</cell><cell>(VCCS)</cell><cell>CW</cell><cell>ERGC</cell><cell>MSS</cell><cell>preSLIC</cell><cell>WP</cell><cell>ETPS</cell><cell>LSC</cell><cell>POISE</cell><cell>SEAW</cell></row></table><note><p><p>Figure 11</p>: AMR, AUE and AUV (lower is better) on the used datasets. We find that AMR, AUE and AUV appropriately summarize performance independent of the number of generated superpixels. Plausible examples to consider are top-performing algorithms such as ETPS, ERS, SLIC or CRS as well as poorly performing ones such as QS and PF. Best viewed in color.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>presenting Rec, UE, EV, runtime in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>Rec, UE, EV, K and runtime in seconds t for K ≈ 20000 on the BSDS500 dataset including all algorithms able to generate K 5000 superpixels. The experiments demonstrate that nearly all superpixel algorithms are able to capture the image content without loss of information -with Rec ≈ 0.99 and UE ≈ 0.03 -while reducing the number of primitives from 481 • 321 = 154401 to K ≈ 20000.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>FashFigure E.22: Runtime in seconds t on the SBD, SUNRGBD and Fash datasets. The results allow to get an impression of how runtime of individual algorithms scales with the size of the image. In particular, we deduce that most algorithm's runtime scales linear in the input size, while the number of generated superpixels does have little influence. Best viewed in color. UE, ASA and UE Levin on the BSDS500 and NYUV2 datasets. We find that ASA does not provide new insights compared to UE, as it closely reflects (1 -UE) except for a minor absolute offset. UE Levin , in contrast, provides a different point view compared to UE. However, UE Levin is harder to interpret and strongly varies across datasets. Best viewed in color.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SBD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SUNRGBD</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell>10</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>1</cell></row><row><cell>t</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t</cell><cell></cell><cell></cell><cell>t</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell>0.1</cell></row><row><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.1 0.001 0.2 0.15 UE</cell><cell>1,000</cell><cell>2,000 K</cell><cell>3,000</cell><cell>4,000</cell><cell>ASA</cell><cell>0.9 0.01 0.96 0.92 0.94</cell><cell cols="2">50 0.01 150 A N U S C R I P T M 1,000 2,000 3,000 4,000 K 1,000 2,000 3,000 K BSDS500 100 UELevin</cell><cell>4,000</cell></row><row><cell>UE</cell><cell cols="8">500 1,000 log K 500 1,000 log K A C C E P T E D 3,000 6,000 0.05 0.88 3,000 6,000 0.05 0.1 0.15 0.2 0.88 0.9 0.96 0.98 0.92 0.94 ASA</cell><cell>500 1,000 log K 500 1,000 K NYUV2</cell><cell>3,000 6,000 3,000 6,000</cell><cell>UELevin</cell><cell>0 2 8 10 4 6</cell><cell>500 1,000 log K 500 1,000 K</cell><cell>3,000 6,000 3,000 6,000</cell></row><row><cell cols="2">W SLIC Figure E.23: W</cell><cell></cell><cell>EAMS CRS EAMS</cell><cell></cell><cell>NC ERS NC</cell><cell></cell><cell>FH PB FH</cell><cell></cell><cell>RW DASP RW</cell><cell>QS SEEDS QS</cell><cell>PF TPS PF</cell><cell>TP VC TP</cell><cell>CIS CCS CIS</cell></row><row><cell></cell><cell>VCCS SLIC</cell><cell></cell><cell>CW CRS</cell><cell></cell><cell>ERGC ERS</cell><cell></cell><cell>MSS PB</cell><cell></cell><cell>preSLIC DASP</cell><cell>WP SEEDS</cell><cell>ETPS TPS</cell><cell>LSC VC</cell><cell>POISE CCS</cell></row><row><cell></cell><cell>SEAW VCCS</cell><cell></cell><cell>CW</cell><cell></cell><cell>ERGC</cell><cell></cell><cell>MSS</cell><cell></cell><cell>preSLIC</cell><cell>WP</cell><cell>ETPS</cell><cell>LSC</cell><cell>POISE</cell></row><row><cell></cell><cell>SEAW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We note that, as in<ref type="bibr" target="#b81">[81]</ref>, SEEDS actually provides a compactness parameter. But the compactness parameter is not implemented in the publicly available implementation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Visit davidstutz.de/projects/superpixel-benchmark/ to integrate your algorithm into the comparison.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Runtimes have been taken on an Intel ® Core™ i7-3770 @ 3.4GHz, 64bit with 32GB RAM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We note that we use the original SEEDS implementation instead of the OpenCV<ref type="bibr" target="#b113">[113]</ref> implementation which is reported to be more efficient, see http://docs.opencv.org/3.0-last-rst/ modules/ximgproc/doc/superpixels.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1090" xml:id="foot_4"><p>Software. The individual implementations, together with the used benchmark, are made publicly available at davidstutz.de/projects/superpixel-benchmark/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The work in this paper was funded by the EU project STRANDS (ICT-2011-600623).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are also grateful for the implementations provided by many authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T A C C E P T E D M A N U S C R I P T Appendix A. Algorithms</head><p>Complementing the information presented in Section 3, Table <ref type="table">A</ref>.4 gives a complete overview of all algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Datasets</head><p>The BSDS500 dataset is the only dataset providing several ground truth segmentations per image. Therefore, we briefly discuss evaluation on the BSDS500 dataset in detail. Furthermore, additional example images from all used datasets are shown in Figure <ref type="figure">B</ref>.17 and are used for qualitative results in Appendix E.1.</p><p>Assuming at least two ground truth segmentations per image, Arbeláez et al. <ref type="bibr" target="#b55">[55]</ref> consider two methodologies of computing average Rec: computing the average Rec over all ground truth segmentations per image, and subsequently taking the worst average (i.e. the lowest Rec); or taking the lowest Rec over all ground truth segmentations per image and averaging these. We follow Arbeláez et al. and pick the latter approach. The same methodology is then applied to UE, EV, ASA and UE Levin . In this sense, we never overestimate performance with respect to the provided ground truth segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Benchmark</head><p>In the following section we discuss the expressiveness of the used metrics and details regarding the computation of AMR, AUE and AUV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C.1. Expressiveness and Correlation</head><p>Complementing Section 5.1, we exemplarily discuss the correlation computed for SEEDS with K ≈ 400 on the BSDS500 dataset as shown in Table <ref type="table">C</ref>. <ref type="bibr" target="#b5">5</ref>. We note that the following observations can be confirmed when considering different algorithms. Still, SEEDS has the advantage of showing good overall performance (see the ranking in Section 7.7) and low standard deviation in Rec, UE and EV. We observe a correlation of -0.47 between Rec and UE reflecting that SEEDS exhibits high Rec but comparably lower UE on the BSDS500 dataset. This justifies the choice of using both Rec and UE for quantitative comparison.</p><p>The correlation of UE and ASA is 1, which we explained with the respective definitions. More interestingly, the correlation of UE and UE Levin is -0.7. Therefore, we decided to discuss results regarding UE Levin in more detail in Appendix E. Nevertheless, this may also confirm the observations by Neubert and Protzel <ref type="bibr" target="#b44">[44]</ref> as well as Achanta et al. <ref type="bibr" target="#b39">[39]</ref> that UE Levin unjustly penalizes large superpixels. The high correlation of -0.97 between MDE and Rec has also been explained using the respective definitions. Interestingly, the correlation decreases with increased K. This can, however, be explained by the implementation of Rec, allowing a fixed tolerance of 0.0025 times the image diagonal. The correlation of -0.42 between ICV and EV  As introduced in Section 5.2, AMR, AUE and AUV are intended to summarize algorithm performance independent of K. To this end, we compute the area below the MR = (1 -Rec), UE and UV = (1 -EV) curves</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ERGC [68] and SLIC [74]. These algorithms show superior performance regarding Boundary Recall, Undersegmentation Error and Explained Variation and can be considered stable. Furthermore, they are iterative (except for ERGC and ERS) and provide References References</title>
		<author>
			<persName><surname>Crs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>78, 79</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmenta-1100 tion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical model based image segmentation using region growing, contour relaxation and classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Symposium on Visual Communications and 1105 Image Processing</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up segmentation of image sequences for coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales des Télécommunications</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="397" to="407" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="1110">1110. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recovering occlusion boundaries from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient graph-based 1115 image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenswalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing and its Applications</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="303" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiclass segmentation with relative location prior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SuperParsing: Scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on 1125 Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Superpixel tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1323" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust superpixel tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Superpixels, occlusion and stereo</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mashford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Image Computing Techniques and Applications</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="756" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Superpixel meshes for fast edge-preserving surface reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bódis-Szomorú</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supercnn: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving an object detector and extracting regions using superpixels</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3721" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection by labeling superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating object segmentation proposals using global and local search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rantalankila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2417" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth SEEDS: recovering incomplete depth data using superpixels</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="363" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1411.6387</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic segmentation with heterogeneous sensor coverages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D C</forename><surname>Lerma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2639" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with RGBD cameras</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indoor scene understanding with RGB-D images: Bottom-up segmentation, object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Joint 3d object and layout inference from a single RGB-D image</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
	<note>German Conference on Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Patch match filter: Efficient edge-aware filtering meets randomized search for fast correspondence field estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1854" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A deformable mixture parsing model with parselets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3408" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<idno>abs/1511.06739</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ham-1205 precht, Segmentation of SBFSEM volume data of neural tissue by hierarchical classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Helmstaedter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM Annual Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="142" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fully automated approach to segmentation of irregularly shaped cel-1210 lular structures in EM images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference onMedical Image Computing and Computer Assisted Interventions</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="463" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervoxel-based segmentation of mitochondria in EM image 1215 stacks with learned shape features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="474" to="486" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Superpixel-based interest points for effective bags of visual words medical image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference 1220 onMedical Image Computing and Computer Assisted Interventions</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fashion parsing with weak color-category labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="265" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online video seeds for temporal window objectness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art 1230 superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2281" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">2148</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluation of super-voxel methods for early video processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1202" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">TurboPixels: Fast superpixels using geometric flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2290" to="2297" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Measuring and evaluating the compactness of superpixels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Con-1245 ference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="930" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Superpixel benchmark and comparison</title>
		<author>
			<persName><forename type="first">P</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forum Bildverarbeitung</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Superpixels and supervoxels in an energy optimization framework</title>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mehrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Con-1250 ference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6315</biblScope>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evaluating superpixels in video: Metrics beyond figure-ground segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Entropy 1255 rate superpixel segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2097" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelli-1260 gence</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="530" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Superpixel lattices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J D</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A naturalistic 1265 open source movie for optical flow evaluation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Harmony potentials for joint classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer 1270 Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the influence of superpixel methods for image parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Strassburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Application</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="518" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Depth-adaptive superpixels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weikersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gossow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2087" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segmentation based interest points and evaluation of unsupervised image segmentation methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Protzel, Compact watershed and preemptive SLIC: on improving trade-offs of superpixel segmentation algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="996" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast superpixel segmentation using morphological processing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Benesova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kottman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Vision and Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Waterpixels: Superpixels based on the watershed transformation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Machairas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4343" to="4347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Machairas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cardenas-Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chabardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waterpixels</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3707" to="3716" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Quick shift and kernel methods for mode seeking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5305</biblScope>
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Minimizing energy functions on 4-connected lattices using elimination</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2042" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-label image segmentation for medical applications based on graph-theoretic electrical potentials</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Funka-Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops on Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="230" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Superpixels via pseudo-boolean optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mashford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1387" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The middle child problem: Revisiting parametric min-cut and seeds for object proposals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M R A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1600" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Eikonal based region growing for superpixels generation: Application to semi-supervised real time organ segmentation in CT images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buyssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Innovation and Research in BioMedical Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="26" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Eikonalbased vertices growing and iterative seeding for efficient graphbased segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buyssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toutain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lézoray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4368" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1841" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fast superpixels for video analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Motion and Video Computing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Topology preserved regular superpixel</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Regularity preserved superpixels and supervoxels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1165" to="1175" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<author>
			<persName><surname>Superpixels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>École Polytechnique Fédérale de Lausanne</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">VCells: Simple and efficient superpixels using edge-weighted centroidal voronoi tessellations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 1350 Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1241" to="1247" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Voxel cloud connectivity segmentation -supervoxels for point clouds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recogni-1355 tion</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2027" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Superpixel segmentation using linear spectral clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1356" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multichannel segmenta-1360 tion using contour relaxation: Fast super-pixels and temporal propagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference Image Analysis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="250" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Contour-relaxed superpixels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and 1365 Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="280" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">SEEDS: Superpixels extracted via energy-driven sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Capitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7578</biblScope>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">SEEDS: Superpixels extracted via energy-driven sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Capitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1309.3848</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Super pixel extraction via convexity induced boundary adaptation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Tasli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>¸igla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 1375 International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Convexity constrained efficient superpixel and supervoxel extraction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Tasli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cigla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Real-time coarse-tofine topologically preserving segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2947" to="2955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Efficient image segmentation using pair-1385 wise pixel similarities</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rohkohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM Annual Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Curio</surname></persName>
		</author>
		<title level="m">IAPR International Conference on Machine Vision Applica-1390 tions</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="252" />
		</imprint>
	</monogr>
	<note>Medial features for superpixel segmentation</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Hypergraph coarsening for image superpixelization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ducournau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rital</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Laget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on I/V Communications and Mobile Network</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Structuresensitive superpixels via geodesic distance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Homogeneous superpixels from random walks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications, Conference on</title>
		<imprint>
			<date type="published" when="1400">1400 2011</date>
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Superpixels using random walker</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE Global High Tech Congress on Electronics</publisher>
			<biblScope unit="page" from="62" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<title level="m">3-d geometry enhanced 1405 superpixels for RGB-D data</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
	<note>Pacific-Rim Conference on Multimedia</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Estimating the 3d layout of indoor scenes and its clutter from depth sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1410" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Image segmentation by cascaded region agglomeration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich ; A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T sion and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="2011" to="2018" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Improved simple linear iterative clustering superpixels</title>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Consumer Electronics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="259" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Lazy random walks for superpixel segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1451" to="1462" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A generative superpixel method</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Fusion</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Grid seams: A fast superpixel algorithm for real-time applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer and Robot Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Superpixels using morphology for rock image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R S P</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Southwest Symposium on Image Analysis and Interpretation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A fast method for inferring high-quality simply-connected superpixels</title>
		<author>
			<persName><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2184" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An improved slic superpixels using reciprocal nearest neighbor clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="239" to="248" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Image Processing and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context, Computing Research Repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Labelme: A database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/en-us/projects/objectclassrecognition/" />
		<title level="m">Microsoft research cambridge object recognition image database</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Rgb-D</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">A category-level 3-d object dataset: Putting the kinect to work</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1168" to="1174" />
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision Workshops</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<ptr target="Toolshttp://opencv.org/" />
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
