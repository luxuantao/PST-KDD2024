<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-30">30 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shady</forename><forename type="middle">Abu</forename><surname>Hussein</surname></persName>
							<email>shadya@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Tirer</surname></persName>
							<email>tomtirer@mail.tau.ac.il</email>
							<affiliation key="aff1">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-30">30 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.00157v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The single image super-resolution task is one of the most examined inverse problems in the past decade. In the recent years, Deep Neural Networks (DNNs) have shown superior performance over alternative methods when the acquisition process uses a fixed known downsampling kernel-typically a bicubic kernel. However, several recent works have shown that in practical scenarios, where the test data mismatch the training data (e.g. when the downsampling kernel is not the bicubic kernel or is not available at training), the leading DNN methods suffer from a huge performance drop. Inspired by the literature on generalized sampling, in this work we propose a method for improving the performance of DNNs that have been trained with a fixed kernel on observations acquired by other kernels. For a known kernel, we design a closed-form correction filter that modifies the low-resolution image to match one which is obtained by another kernel (e.g. bicubic), and thus improves the results of existing pre-trained DNNs. For an unknown kernel, we extend this idea and propose an algorithm for blind estimation of the required correction filter. We show that our approach outperforms other super-resolution methods, which are designed for general downsampling kernels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of Single Image Super-Resolution (SISR) is one of the most examined inverse problems in the past decade <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7]</ref>. In this problem, the goal is to reconstruct a latent high-resolution (HR) image from its low-resolution (LR) version, obtained by an acquisition process that includes low-pass filtering and sub-sampling. In the recent years, along with the developments in deep learning, many SISR methods that are based on Deep Neural Networks (DNNs) have been proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Typically, the performance of SISR approaches is evaluated on test sets with a fixed known acquisition process, e.g. a bicubic downsampling kernel. This evaluation methodol-ogy allows to prepare large training data, which are based on ground truth HR images and their LR counterparts synthetically obtained through the known observation model. DNNs, which have been exhaustively trained on such training data, clearly outperform other alternative algorithms, e.g. methods that are based on hand-crafted prior models such as sparsity or non-local similarity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Recently, several works have shown that in practical scenarios where the test data mismatch the training data, the leading DNN methods suffer from a huge performance drop <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Such scenarios include a downsampling kernel which is not the bicubic kernel and is not available at the training phase. A primary example is an unknown kernel that needs to be estimated from the LR image at test time.</p><p>Several recent SISR approaches have proposed different strategies for enjoying the advantages of deep learning while mitigating the restriction of DNNs to the fixed kernel assumption made in the training phase. These strategies include: modifying the training phase such that it covers a predefined set of downsampling kernels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13]</ref>; using DNNs to capture only a natural-image prior which is decoupled from the SISR task <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>; or completely avoid any offline training and instead train a CNN super-resolver from scratch at test time <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Contribution. In this work we take a different strategy, inspired by the generalized sampling literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, for handling LR images obtained by arbitrary downsampling kernels. Instead of completely ignoring the state-of-the-art DNNs that have been trained for the bicubic model, as done by other prior works, we propose a method that transforms the LR image to match one which is obtained by the bicubic kernel. The modified LR can then be inserted into existing leading super-resolvers, such as DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and proSR <ref type="bibr" target="#b32">[33]</ref>, thus, improving their performance significantly on kernels they have not been trained on. The proposed transformation is performed using a correction filter, which has a closed-form expression when the true (nonbicubic) kernel is given.</p><p>In the "blind" setting, where the kernel is unknown, we From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b25">[26]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR with correction, RCAN with correction, DBPN with correction. Note that our correction filter improves significantly the performance on neural networks trained on another SR kernel.</p><p>extend our approach and propose an algorithm that estimates the required correction. The proposed approach outperforms other super-resolution methods in various practical scenarios. See example results in Figure <ref type="figure" target="#fig_0">1</ref> in the case of a known kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the past five years many works have employed DNNs for the SISR task, showing a great advance in performance with respect to the reconstruction error <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> and the perceptual quality <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. However, one main disadvantage of DNN-based SISR methods is their sensitivity to the LR image formation model. A network performance tends to drop significantly if it has been trained for one acquisition model and then been tested on another <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Recently, different SISR strategies has been proposed with the goal of enjoying the advantages of deep learning while mitigating the restriction of DNNs to the fixed kernel assumption made in the training phase. One strategy is to train a CNN super-resolver that gets as inputs both the LR image and the degradation model and assumes that the downsampling kernels belong to a certain set of Gaussian filters <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>. Another approach builds on the structural prior of CNNs, which promotes signals with spatially recurring patterns (e.g. natural images) and thus allows to train a super-resolver CNN from scratch at test time <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Another line of work recovers the latent HR image by minimizing a cost function, composed of fidelity and prior terms, where only the prior term is handled by a pre-trained deep denoiser or GAN <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">39]</ref>. Recently, the two last approaches have been incorporated <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref>. In all these meth- ods the downsampling kernel is given as an input. In a blind setting (where the kernel is unknown) it is still possible to apply these methods after an initial kernel estimation phase.</p><p>Our approach is inspired by the literature on generalized sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8]</ref>, which generalizes the classical WhittakerNyquistKotelnikovShannon sampling theorem <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>, which considers signals that are bandlimited in the frequency domain and sinc interpolations.</p><p>The generalized theory provides a framework and conditions under which a signal that is sampled by a certain basis can be reconstructed by a different basis. In this framework, the sampled signal is reconstructed using a linear operator that can be decoupled into two phases, the first applies a digital correction filter and the second include processing with a reconstruction kernel. The role of the correction filter is to transform the sampling coefficients, associated with the sampling kernel, to coefficients which fit the reconstruction kernel.</p><p>Several works have used the correction filter approach for image processing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. These works typically propose linear interpolation methods, i.e. the correction filter is followed by a linear reconstruction operation, and do not use a strong natural-image prior. As a result, the recovery of fine details is lacking.</p><p>In this work, we plan to use (very) non-linear reconstruction methods, namely-DNNs, whose training is difficult, computationally expensive, storage demanding, and cannot be done when the observation model is not known in advance. To tackle these difficulties, we revive the correction filter approach and show how it can be used with deep super-resolvers which have been already trained.</p><p>The required correction filter depends on the kernel which is used for sampling. Therefore, in the blind setting, it needs to be estimated from the LR image. To this end, we propose an iterative optimization algorithm for estimating the correction filter. In general, only a few works have considered the blind SISR setting and developed kernel estimation methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Finally, we would like to highlight major differences be-tween this paper and the work in <ref type="bibr" target="#b12">[13]</ref>, whose "kernel correction" approach may be misunderstood as our "correction filter". In <ref type="bibr" target="#b12">[13]</ref>, three different DNNs (super-resolver, kernel estimator, and kernel corrector) are offline trained under the assumption that the downsampling kernel belongs to a certain family of Gaussian filters (similarly to <ref type="bibr" target="#b37">[38]</ref>), and the CNN super-resolver gets the estimated kernel as an input. So, the first major difference is that contrary to our approach, no pre-trained existing DNN methods (other than SRMD <ref type="bibr" target="#b37">[38]</ref>) can be used in <ref type="bibr" target="#b12">[13]</ref>. Secondly, their approach is restricted by the offline training assumptions to very certain type of downsampling kernels, contrary to our approach. Thirdly, the concepts of these works are very different: The (iterative) correction in <ref type="bibr" target="#b12">[13]</ref> modifies the estimated downsampling kernel, while our correction filter modifies the LR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>The single image super resolution (SISR) acquisition model, can be formulated as</p><formula xml:id="formula_0">y = (x * k) ↓ α ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">x ∈ R n represents the latent HR image, y ∈ R m represents the observed LR image, k ∈ R d (d ≪ n)</formula><p>is the (anti-aliasing) blur kernel, * denotes the linear convolution operator, and ↓ α denotes sub-sampling operator with stride of α. Under the common fashion of dropping the edges of x * k, such that it is in R n , we have that m = ⌈n/α⌉. Note that Equation ( <ref type="formula" target="#formula_0">1</ref>) can be written in a more elegant way as</p><formula xml:id="formula_2">y = S * x,<label>(2)</label></formula><p>where S * : R n → R m is a linear operator that encapsulates the entire sampling operation, i.e. S * is a composition of blurring followed by sub-sampling. The sampling operator S * is presented in Figure <ref type="figure" target="#fig_1">2</ref>(a). Most SISR deep learning methods, e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>, assume that the observations are obtained using the bicubic kernel. Let us denote by R * the associated sampling operator (essentially, R * coincides with the previously defined S * if k is the bicubic kernel k bicub ). The core idea of our approach is to modify the observations y = S * x, obtained for an arbitrary downsampling kernel k, such that they mimic the (unknown) "ideal observations" y bicub = R * x, which can be fed into pre-trained DNN models.</p><p>In what follows, we present a method to (approximately) achieve this goal using the correction filter tool, adopted from the generalized sampling literature. First, we consider the non-blind setting, where the downsampling kernel is known, and thus S * is known. In this case, we obtain a closed-form expression for the required correction filter, which depends on k (and on k bicub ). Later, we extend the approach to the blind setting, where k is unknown. In this case, we propose a technique for estimating the correction filter from the LR image y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The non-blind setting</head><p>In the non-blind setting, both the downsampling kernel k and the target kernel k bicub are known. Therefore, the sampling operators S * and R * are known as well. Using common notations from generalized sampling literature <ref type="bibr" target="#b7">[8]</ref>, let us denote by S and R the adjoint operators of S * and R * , respectively. The operator R : R m → R n can be interpreted as a reconstruction operator, that restores a signal in R n from m samples, associated with the sampling operator R * . In the context of our work, when R is applied on a vector it pads it with n − m zeros (α − 1 zeros between each two entries) and convolves it with a flipped version of k bicub . The reconstruction operator R * is presented in Figure <ref type="figure" target="#fig_1">2(c</ref>). A similar interpretation goes for S : R m → R n with the kernel k.</p><p>The key goal of generalized sampling is to identify signal models and sampling systems (i.e. sampling and reconstruction operators) that allow for perfect recovery. Therefore, to proceed, let us make the following assumption.</p><p>Assumption 1. The signal x sampled by R * , can be perfectly recovered by R, i.e.</p><formula xml:id="formula_3">x = RR * x.<label>(3)</label></formula><p>Even though Assumption 1 does not hold for natural images, it is motivated by the fact that downsampling with a bicubic kernel produces a naturaly looking LR image (typically better than the LR images obtained using other kernels), and we have many DNN methods that can handle observations of the form R * x quite well.</p><p>Suppose that we have been given the observations y bicub = R * x and that Assumption 1 holds, then the estimator x = Ry bicub would yield perfect reconstruction. However, since we are given different observations, y = S * x, let us propose a different estimator x = RHy, where H : R m → R m is a correction operation. This recovery procedure is presented in Figures <ref type="figure" target="#fig_1">2(b</ref>)+2(c). The following theorem presents a condition and a formula for H under which perfect recovery is possible.</p><p>Theorem 2. Let y = S * x, x = RHy, and assume that Assumption 1 hold. Then, if</p><formula xml:id="formula_4">null(S * ) ∩ range(R) = {0},<label>(4)</label></formula><p>we have that x = x for</p><formula xml:id="formula_5">H = (S * R) −1 : R m → R m . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Proof. Note that</p><formula xml:id="formula_7">x = RHy = RHS * x = RHS * RR * x,<label>(6)</label></formula><p>where the last equality follows from Assumption 1. Next, (4) implies that the operator (S * R) is invertible. Thus, setting H according to ( <ref type="formula" target="#formula_5">5</ref>) is possible, and we get</p><formula xml:id="formula_8">x = R (S * R) −1 S * RR * x = RR * x = x,<label>(7)</label></formula><p>where the last equality follows from Assumption 1.</p><p>Theorem 2 is presented in operator notations to simplify the derivation. In the context of SISR (i.e. with the previous definitions of S and R), the operator H = (S * R)</p><p>−1 can be applied simply as a convolution with a correction filter h, whose Discrete Fourier Transform (DFT) is given by</p><formula xml:id="formula_9">h = IDFT 1 DFT {(k * k bicub ) ↓ α } ,<label>(8)</label></formula><p>where DFT(•) and IDFT(•) denote the Discrete Fourier Transform and its inverse respectively. To ensure numerical stability, we slightly modify <ref type="bibr" target="#b8">(9)</ref>, and compute h using</p><formula xml:id="formula_10">h = IDFT DFT {(k * k bicub ) ↓ α } * |DFT {(k * k bicub ) ↓ α }| 2 + ǫ ,<label>(9)</label></formula><p>where ǫ is a small regularization parameter (in all of our experiments we set it to 10 −4 ).</p><p>In practice, instead of using the weak estimator x = RHy that does not use any natural-image prior, we propose to recover the HR image by</p><formula xml:id="formula_11">x = f (h * y),<label>(10)</label></formula><p>where f (•) is a DNN super-resolver that has been trained under the assumption of bicubic kernel (i.e. we replace the  <ref type="figure" target="#fig_1">2</ref>(c) with a DNN).</p><p>For the experiments in this paper we use DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and proSR <ref type="bibr" target="#b32">[33]</ref>, but in general any other method with state-of-the-art performance (for bicubic kernel) is expected to give good results. Note that the theoretical motivation for our strategy requires that the condition in (4) holds. This condition can be inspected by comparing the bandwidth of the kernels k and k bicub in the frequency domain. As k is commonly a lowpass filter (and so is k bicub ), the condition requires that the passband of k bicub is contained in the passband of k. Yet, as shown in the experiments section, our approach yields a significant improvement even when the passband of k is moderately smaller than the passband of k bicub .</p><p>Furthermore, we observe that even for very blurry LR images performance improvement can be obtained by increasing the regularization parameter in (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The blind setting</head><p>In the blind setting, the downsampling kernel k is unknown. Therefore, we cannot compute the correction filter h using (9), and extending our approach to this setting requires estimating h from the LR image y. To this end, we propose an iterative optimization algorithm that alternates between obtaining h that minimizes y − S * RHy 1 (note that H depends on h), and obtaining k that minimizes I − HS * R 1 + λ k 1 (note that S * depends on k). A detailed description of the algorithm is given in Algorithm 1. The first inner minimization is performed using Adam <ref type="bibr" target="#b15">[16]</ref> Algorithm 1: Correction filter estimation Input: y, k bicub , N iter Output: ĥ an estimate for h. Params.: k 0 = Gaussian kernel with σ = 1, i = 0, λ = 0.01 . R = DFT{k bicub }; while i &lt; N iter do i = i + 1;</p><formula xml:id="formula_12">h i = argmin h y − (k i−1 * k bicub * (( h * y) ↑ α )) ↓ α 1 ; H = DFT{h i } ↑ α ; k i = argmin k 1 − H • R • DFT{ k} 1 + λ k 1 ; end ĥ = h i ;</formula><p>with learning rate of 10 −5 for 50 iterations. The second inner minimization is performed using SGD with learning rate of 1.0 for 10 iterations at the beginning and then the iterations number is increased by 10 after each outer iteration (limited to 500). We also use N iter = 30 for scale factor of 2, and N iter = 50 for scale factor of 4.</p><p>After an estimator of h is obtained using Algorithm 1, the HR image can be reconstructed by <ref type="bibr" target="#b9">(10)</ref>, similarly to the non-blind setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we examine the performance and improvement due to our correction filter approach in the nonblind and blind settings, using three different off-the-shelf DNN super-resolvers that serve as f (•) in <ref type="bibr" target="#b9">(10)</ref>: DBPN <ref type="bibr" target="#b13">[14]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and proSR <ref type="bibr" target="#b32">[33]</ref>. We compare our approach to other methods that receive the downsampling kernel k (or its estimation in the blind setting) as an input: ZSSR <ref type="bibr" target="#b25">[26]</ref> and SRMD <ref type="bibr" target="#b37">[38]</ref>. We also compare our method to DPSR <ref type="bibr" target="#b38">[39]</ref>, however, since its results are extremely inferior to the other strategies (e.g. about 10 dB lower PSNR) they are deferred to Appendix A. All the experiments are performed with the official code of each method. Unfortunately, such code has not been available for <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The non-blind setting</head><p>In this section, we assume that the downsampling kernel k is known. Therefore, the correction filter h can be computed directly using <ref type="bibr" target="#b8">(9)</ref>. We examine scenarios with scale factors of 2 and 4. For scale factor of 2, we use Gaussian kernels with standard deviation σ = 1. The results are presented in Tables <ref type="table" target="#tab_1">1 and 2</ref> for the testsets Set14 and BSD100, respectively. Figures <ref type="figure" target="#fig_0">1 and 3</ref>, as well as Appendix A, present several visual results. It can be seen that the proposed filter correction approach significantly improves the results of DBPN, RCAN, and proSR, Figure <ref type="figure">3</ref>: Non-blind super-resolution of image 189080 from BSD100, for scale factor of 2 and Gaussian downsampling kernel with standard deviation 2.5/ √ 2. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b25">[26]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR with correction, RCAN with correction, DBPN with correction.</p><p>which have been trained for the (incorrect) bicubic kernel. Moreoever, note that the filter-corrected applications of DBPN, RCAN, and proSR, also outperform SRMD and ZSSR, while the plain applications of DBPN, RCAN, and proSR are inferior to SRMD.</p><p>Inference run-time. Computing the correction filter requires a negligible amount of time, so it does not change the run-time of an off-the-shelf DNN. Using NVIDIA RTX 2080ti GPU, the per image run-time of all the methods except ZSSR is smaller than 1 second (because no training is done in the test phase), while ZSSR requires approximately 2 minutes per image.</p><p>The spectrums of sampling kernels. In Figure <ref type="figure" target="#fig_3">4</ref> we present the frequency spectrums of Gaussian kernels that are used in this work and two bicubic kernels, one for scale factor of 2 and one for scale factor of 4. In order to ease the presentation, the curves are plotted for the one-dimensional version of these kernels. Our experiments show that for super-resolution factor of 2 the proposed filter correction approach yields very good results for Gaussian sampling kernels of 1.5/ √ 2 and 2.5/ √ 2 (see Tables <ref type="table" target="#tab_1">1 and 2</ref>), but not for 3.5/ √ 2 (not shown). This behavior correlates with the recovery guarantees in Theorem 2. The theory essentially requires that the passband of the bicubic kernel (associated with relevant scale factor) is contained in the passband of the downsampling kernels. Indeed, it can be seen in Figure 4 that the passband of the bicubic kernel for scale factor 2 is contained in the passband of the Gaussian with 1.5/ √ 2, but extremely larger than the passband of the Gaussian with 3.5/ √ 2. On the other hand, for super-resolution factor of 4 we do get good results for Gaussian sampling kernel of 3.5/ √ 2 (see Tables <ref type="table" target="#tab_1">1 and 2</ref>). This is aligned with the fact that the bicubic kernel for scale factor 4 is much narrower in frequency domain than the bicubic kernel for scale factor 2, as shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>We note that the deficiency of the correction filter approach for the setting with Gaussian kernel of σ = 3.5/ √ 2 and scale factor of 2 can be resolved by increasing the regularization parameter ǫ used in <ref type="bibr" target="#b8">(9)</ref>, which helps compensating the null space in S * R and stabilizes its inverse. For example, plain application of DBPN on Set14 yields PSNR of 24.71 dB, and applying it after correction filter with ǫ = 0 yields PSNR of 10.34 dB. However, applying DBPN after correction filter with ǫ = 10 yields PSNR of 28.67 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The blind setting</head><p>In this section, we turn to test non-blind super-resolution, where the downsampling kernel k is unknown. Therefore, to apply our approach we first estimate the correction filter using Algorithm 1, and then use this estimation to restore the HR image by <ref type="bibr" target="#b9">(10)</ref>. In this setting we compare our method to kernelGAN <ref type="bibr" target="#b1">[2]</ref>, which estimates the downsampling kernel using adversarial training (in test-time) and then uses ZSSR to restore the HR image.</p><p>The results for Set14 are presented in Table <ref type="table" target="#tab_2">3</ref> and visual examples are shown in Figure <ref type="figure" target="#fig_4">5</ref> and in Appendix B. It can be seen that the proposed filter correction approach still improves the results of DBPN, RCAN, and proSR, compared to their plain applications (which are presented in Tables 1).</p><p>It also outperforms kernelGAN (despite our approach being a much simpler) in terms of reconstruction error and gains comparable results in terms of perceptual distance. Our proposed strategy has an advantage over kernelGAN also in term of run-time. While kernelGAN requires about 6.5 minutes per image, estimating the correction filter requires 2 minutes, which is followed by off-the-shelf DNNs that require less than 1 second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The SISR task has gained a lot from the developments in deep learning in the recent years. Yet, the leading DNN methods suffer from a huge performance drop when they are tested on images that do not fit the acquisition process assumption used in their training phase-which is, typically, that the downsampling kernel is bicubic. In this work, we addressed this issue by a signal processing approach: computing a correction filter that modifies the lowresolution observations such that they mimic observations that are obtained with a bicubic kernel. (Notice that our focus in this work on the bicubic kernel is for the sake of simplicity of the presentation and due to its popularity. Yet, it is possible to use our developed tools also for other reconstruction kernels). The modified LR is then fed into existing state-of-the-art DNNs that are trained only under the assumption of bicubic kernel. Various experiments have shown that the proposed approach significantly improves the performance of the pre-trained DNNs and outperforms other (much more sophisticated) methods that are specifically designed to be robust to different kernels. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, kernelGAN <ref type="bibr" target="#b1">[2]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR with estimated correction, RCAN with estimated correction and DBPN with estimated correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional results for the non-blind setting</head><p>Additional visual results of the different methods in the non-blind setting (which are not included in the paper due to space limitation) are presented in Figures <ref type="figure">7 and 8</ref>.</p><p>We also present in Figure <ref type="figure">6</ref>, the results obtained by the DPSR method <ref type="bibr" target="#b38">[39]</ref> (using its official implementation code). It can be seen that DPSR results have many artifacts. In fact, DPSR average PSNR as produced by its official code is lower by more than 10 dB than the other examined methods (SRMD <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b25">[26]</ref>, DBPN <ref type="bibr" target="#b13">[14]</ref>, proSR <ref type="bibr" target="#b32">[33]</ref>, RCAN <ref type="bibr" target="#b40">[41]</ref>, and our proposed approach). Therefore, it is not displayed in the tables in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional results for the blind setting</head><p>Additional visual results of the different methods in the blind setting (which are not included in the paper due to space limitation) are presented in Figures <ref type="figure" target="#fig_8">9 and 10</ref>.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Non-blind super-resolution of image 223061 from BSD100, for scale factor 4 and Gaussian downsampling kernel with std 4.5/ √ 2.From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD<ref type="bibr" target="#b37">[38]</ref>, ZSSR<ref type="bibr" target="#b25">[26]</ref>, proSR<ref type="bibr" target="#b32">[33]</ref>, RCAN<ref type="bibr" target="#b40">[41]</ref>, DBPN<ref type="bibr" target="#b13">[14]</ref>, proSR with correction, RCAN with correction, DBPN with correction. Note that our correction filter improves significantly the performance on neural networks trained on another SR kernel.</figDesc><graphic url="image-9.png" coords="2,50.75,286.02,221.43,147.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sampling, correction, and (naive) reconstruction operators for single image super-resolution: (a) Sampling operator composed of convolution with a kernel k and sub-sampling by factor of α; (b) Correction operator composed of convolution with a correction filter h; (c) Reconstruction operator composed of up-sampling by factor of α and convolution with a (flipped) kernel k bicub . In our approach, H is computed for S * and R, but then we replace R with a pre-trained DNN super-resolver.</figDesc><graphic url="image-12.png" coords="3,73.45,72.00,448.33,76.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 , 2 ,</head><label>22</label><figDesc>and box kernel of size 4 × 4. For scale factor of 4, we use Gaussian kernels with standard deviation σ = 3.5/ √ 2 and σ = 4.5/ √ and box kernel of size 8 × 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4:Frequency spectrums of different onedimensional kernels.</figDesc><graphic url="image-24.png" coords="8,50.11,72.00,249.06,194.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Blind super-resolution of baboon image from Set14, for scale factor of 2 and Gaussian downsampling kernel with standard deviation 1.5/ √ 2.From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, kernelGAN<ref type="bibr" target="#b1">[2]</ref>, proSR<ref type="bibr" target="#b32">[33]</ref>, RCAN<ref type="bibr" target="#b40">[41]</ref>, DBPN<ref type="bibr" target="#b13">[14]</ref>, proSR with estimated correction, RCAN with estimated correction and DBPN with estimated correction.</figDesc><graphic url="image-32.png" coords="9,-7.70,241.85,312.72,300.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 : 2 ,</head><label>62</label><figDesc>Figure 6: DPSR [39] results for non-blind super-resolution of baboon and monarch images from Set14, for scale factor of 2 and Gaussian downsampling kernels. Left is with filter of std 1.5/ √ 2, and right is with filter of std 2.5/ √ 2.</figDesc><graphic url="image-38.png" coords="12,259.91,260.06,221.43,147.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 : 2 .</head><label>72</label><figDesc>Figure 7: Non-blind super-resolution of image 19021 from BSD100, for scale factor 4 and Gaussian downsampling kernel with std 4.5/ √ 2. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD, ZSSR, proSR, RCAN, DBPN, proSR with correction, RCAN with correction, DBPN with correction.</figDesc><graphic url="image-45.png" coords="12,50.75,512.89,221.43,147.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 : 2 .</head><label>82</label><figDesc>Figure 8: Non-blind super-resolution of image 148026 from BSD100, for scale factor 4 and Gaussian downsampling kernel with std 3.5/ √ 2. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, SRMD, ZSSR, proSR, RCAN, DBPN, proSR with correction, RCAN with correction, DBPN with correction.</figDesc><graphic url="image-56.png" coords="13,37.80,315.00,258.08,387.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Blind super-resolution of comic from Set14, for scale factor 2 and box downsampling kernel of width 4. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, kernelGAN, proSR, RCAN, DBPN, proSR with correction, RCAN with correction, DBPN with correction.</figDesc><graphic url="image-66.png" coords="14,104.21,493.41,177.39,255.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 : 2 .</head><label>102</label><figDesc>Figure 10: Blind super-resolution of man image from Set14, for scale factor 4 and Gaussian downsampling kernel of width 3.5/ √ 2. From left to right and top to bottom: original image, cropped zoom of the original image, bicubic upsampling, kernelGAN, proSR, RCAN, DBPN, proSR with correction, RCAN with correction, DBPN with correction.</figDesc><graphic url="image-76.png" coords="15,83.28,508.63,202.54,202.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Non-blind super-resolution comparison on Set14. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance<ref type="bibr" target="#b39">[40]</ref> (right).</figDesc><table><row><cell></cell><cell cols="2">scale Gaussian std = 1.5/</cell><cell>√</cell><cell>2</cell><cell>Gaussian std = 2.5/</cell><cell>√</cell><cell>2</cell><cell>Box of width = 4</cell></row><row><cell>DBPN</cell><cell>2</cell><cell cols="2">30.078 / 0.85 / 0.109</cell><cell></cell><cell cols="2">26.366 / 0.734 / 0.219</cell><cell>28.444 / 0.803 / 0.153</cell></row><row><cell>DBPN + Correction</cell><cell>2</cell><cell cols="2">33.686 / 0.904 / 0.041</cell><cell></cell><cell cols="2">31.473 / 0.881 / 0.054</cell><cell>29.147 / 0.819 / 0.141</cell></row><row><cell>proSR</cell><cell>2</cell><cell cols="2">30.073 / 0.849 / 0.108</cell><cell></cell><cell cols="2">26.371 / 0.734 / 0.218</cell><cell>28.459 / 0.803 / 0.151</cell></row><row><cell>proSR + Correction</cell><cell>2</cell><cell cols="2">33.626 / 0.903 / 0.042</cell><cell></cell><cell cols="2">31.582 / 0.883 / 0.055</cell><cell>29.165 / 0.820 / 0.140</cell></row><row><cell>RCAN</cell><cell>2</cell><cell cols="2">30.118 / 0.851 / 0.107</cell><cell></cell><cell cols="2">26.389 / 0.736 / 0.219</cell><cell>28.469 / 0.804 / 0.152</cell></row><row><cell cols="2">RCAN + Correction 2</cell><cell cols="2">33.654 / 0.903 / 0.041</cell><cell></cell><cell cols="2">31.272 / 0.878 / 0.053</cell><cell>29.140 / 0.820 / 0.141</cell></row><row><cell>ZSSR</cell><cell>2</cell><cell cols="2">28.107 / 0.829 / 0.066</cell><cell></cell><cell cols="2">27.954 / 0.806 / 0.100</cell><cell>28.506 / 0.802 / 0.135</cell></row><row><cell>SRMD</cell><cell>2</cell><cell cols="2">32.493 / 0.878 / 0.078</cell><cell></cell><cell cols="2">29.923 / 0.812 / 0.133</cell><cell>25.944 / 0.757 / 0.112</cell></row><row><cell></cell><cell cols="2">scale Gaussian std = 3.5/</cell><cell>√</cell><cell>2</cell><cell>Gaussian std = 4.5/</cell><cell>√</cell><cell>2</cell><cell>Box of width = 8</cell></row><row><cell>DBPN</cell><cell>4</cell><cell cols="2">25.067 / 0.685 / 0.254</cell><cell></cell><cell cols="2">23.890 / 0.645 / 0.293</cell><cell>24.636 / 0.667 / 0.265</cell></row><row><cell>DBPN + Correction</cell><cell>4</cell><cell cols="2">28.385 / 0.773 / 0.141</cell><cell></cell><cell cols="2">27.235 / 0.760 / 0.155</cell><cell>25.181 / 0.666 / 0.284</cell></row><row><cell>proSR</cell><cell>4</cell><cell cols="2">25.033 / 0.683 / 0.254</cell><cell></cell><cell cols="2">23.882 / 0.645 / 0.292</cell><cell>24.685 / 0.667 / 0.265</cell></row><row><cell>proSR + Correction</cell><cell>4</cell><cell cols="2">28.369 / 0.772 / 0.143</cell><cell></cell><cell cols="2">27.445 / 0.757 / 0.158</cell><cell>25.205 / 0.670 / 0.284</cell></row><row><cell>RCAN</cell><cell>4</cell><cell cols="2">25.077 / 0.685 / 0.257</cell><cell></cell><cell cols="2">23.904 / 0.646 / 0.296</cell><cell>24.694 / 0.668 / 0.268</cell></row><row><cell cols="2">RCAN + Correction 4</cell><cell cols="2">28.168 / 0.769 / 0.144</cell><cell></cell><cell cols="2">27.076 / 0.757 / 0.155</cell><cell>25.236 / 0.671 / 0.289</cell></row><row><cell>ZSSR</cell><cell>4</cell><cell cols="2">25.642 / 0.701 / 0.181</cell><cell></cell><cell cols="2">25.361 / 0.683 / 0.198</cell><cell>24.549 / 0.653 / 0.203</cell></row><row><cell>SRMD</cell><cell>4</cell><cell cols="2">26.877 / 0.718 / 0.200</cell><cell></cell><cell cols="2">25.350 / 0.674 / 0.236</cell><cell>19.704 / 0.525 / 0.275</cell></row><row><cell cols="2">naive reconstruction operator in Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Non-blind super-resolution comparison on BSD100. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance<ref type="bibr" target="#b39">[40]</ref> (right).</figDesc><table><row><cell></cell><cell cols="2">scale Gaussian std = 1.5/</cell><cell>√</cell><cell>2</cell><cell>Gaussian std = 2.5/</cell><cell>√</cell><cell>2</cell></row><row><cell>DBPN</cell><cell>2</cell><cell cols="2">29.512 / 0.827 / 0.122</cell><cell></cell><cell cols="2">26.371 / 0.711 / 0.216</cell></row><row><cell>DBPN + correction</cell><cell>2</cell><cell cols="2">32.087 / 0.885 / 0.060</cell><cell></cell><cell cols="2">30.702 / 0.866 / 0.069</cell></row><row><cell>proSR</cell><cell>2</cell><cell cols="2">29.513 / 0.827 / 0.122</cell><cell></cell><cell cols="2">26.381 / 0.711 / 0.216</cell></row><row><cell>proSR + correction</cell><cell>2</cell><cell cols="2">32.080 / 0.884 / 0.061</cell><cell></cell><cell cols="2">30.773 / 0.867 / 0.069</cell></row><row><cell>RCAN</cell><cell>2</cell><cell cols="2">29.558 / 0.829 / 0.121</cell><cell></cell><cell cols="2">26.397 / 0.713 / 0.216</cell></row><row><cell cols="2">RCAN + correction 2</cell><cell cols="2">32.099 / 0.885 / 0.058</cell><cell></cell><cell cols="2">30.617 / 0.865 / 0.067</cell></row><row><cell>ZSSR</cell><cell>2</cell><cell cols="2">29.339 / 0.822 / 0.109</cell><cell></cell><cell cols="2">26.415 / 0.715 / 0.210</cell></row><row><cell>SRMD</cell><cell>2</cell><cell cols="2">26.591 / 0.803 / 0.113</cell><cell></cell><cell cols="2">29.294 / 0.838 / 0.088</cell></row><row><cell></cell><cell cols="2">scale Gaussian std = 3.5/</cell><cell>√</cell><cell>2</cell><cell>Gaussian std = 4.5/</cell><cell>√</cell><cell>2</cell></row><row><cell>DBPN</cell><cell>4</cell><cell cols="2">25.268 / 0.662 / 0.247</cell><cell></cell><cell cols="2">24.357 / 0.628 / 0.282</cell></row><row><cell>DBPN + correction</cell><cell>4</cell><cell cols="2">27.527 / 0.739 / 0.155</cell><cell></cell><cell cols="2">27.124 / 0.732 / 0.164</cell></row><row><cell>proSR</cell><cell>4</cell><cell cols="2">25.237 / 0.661 / 0.247</cell><cell></cell><cell cols="2">24.353 / 0.628 / 0.281</cell></row><row><cell>proSR + correction</cell><cell>4</cell><cell cols="2">27.513 / 0.738 / 0.157</cell><cell></cell><cell cols="2">27.148 / 0.731 / 0.167</cell></row><row><cell>RCAN</cell><cell>4</cell><cell cols="2">25.281 / 0.663 / 0.248</cell><cell></cell><cell cols="2">24.373 / 0.629 / 0.284</cell></row><row><cell cols="2">RCAN + correction 4</cell><cell cols="2">27.418 / 0.738 / 0.156</cell><cell></cell><cell cols="2">27.016 / 0.731 / 0.165</cell></row><row><cell>ZSSR</cell><cell>4</cell><cell cols="2">25.115 / 0.651 / 0.201</cell><cell></cell><cell cols="2">24.348 / 0.625 / 0.246</cell></row><row><cell>SRMD</cell><cell>4</cell><cell cols="2">25.735 / 0.704 / 0.187</cell><cell></cell><cell cols="2">26.432 / 0.707 / 0.189</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">scale Gaussian std = 1.5/</cell><cell>√</cell><cell>2</cell><cell>Gaussian std = 2.5/</cell><cell>√</cell><cell>2</cell><cell>Box of width = 4</cell></row><row><cell>DBPN + estimated correction</cell><cell>2</cell><cell cols="2">30.706 / 0.879 / 0.092</cell><cell></cell><cell cols="2">27.788 / 0.772 / 0.202</cell><cell>29.129 / 0.830 / 0.144</cell></row><row><cell>proSR + estimated correction</cell><cell>2</cell><cell cols="2">29.151 / 0.830 / 0.142</cell><cell></cell><cell cols="2">30.705 / 0.878 / 0.092</cell><cell>27.799 / 0.773 / 0.201</cell></row><row><cell cols="2">RCAN + estimated correction 2</cell><cell cols="2">29.130 / 0.831 / 0.143</cell><cell></cell><cell cols="2">30.714 / 0.879 / 0.091</cell><cell>27.804 / 0.774 / 0.203</cell></row><row><cell>kernelGAN</cell><cell>2</cell><cell cols="2">26.381 / 0.785 / 0.108</cell><cell></cell><cell cols="2">28.868 / 0.807 / 0.125</cell><cell>28.221 / 0.802 / 0.097</cell></row><row><cell></cell><cell cols="2">scale Gaussian std = 3.5/</cell><cell>√</cell><cell>2</cell><cell>Gaussian std = 4.5/</cell><cell>√</cell><cell>2</cell><cell>Box of width = 8</cell></row><row><cell>DBPN + estimated correction</cell><cell>4</cell><cell cols="2">26.803 / 0.729 / 0.239</cell><cell></cell><cell cols="2">26.803 / 0.729 / 0.239</cell><cell>25.577 / 0.698 / 0.258</cell></row><row><cell>proSR + estimated correction</cell><cell>4</cell><cell cols="2">25.384 / 0.689 / 0.267</cell><cell></cell><cell cols="2">26.428 / 0.719 / 0.245</cell><cell>25.069 / 0.671 / 0.287</cell></row><row><cell cols="2">RCAN + estimated correction 4</cell><cell cols="2">25.396 /0.690 / 0.268</cell><cell></cell><cell cols="2">26.464 / 0.721 / 0.246</cell><cell>25.095 / 0.672 / 0.292</cell></row><row><cell>kernelGAN</cell><cell>4</cell><cell cols="2">24.424 / 0.673 / 0.189</cell><cell></cell><cell cols="2">25.174 / 0.669 / 0.183</cell><cell>23.575 / 0.634 / 0.191</cell></row></table><note>Blind super-resolution comparison on Set14. Each cell displays PSNR [dB] (left), SSIM (middle) and perceptual distance [40] (right).</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by the NSF-BSF grant (No. 2017729) and the European research council (ERC StG 757497 PI Giryes).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Imageadaptive GAN based reconstruction</title>
		<author>
			<persName><forename type="first">Abu</forename><surname>Shady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName><surname>Giryes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05284</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Blind super-resolution kernel estimation using an internal-GAN. NeurIPS</title>
		<author>
			<persName><forename type="first">Sefi</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compressed sensing using generative models</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of the Whittaker-Shannon sampling theorem and some of its extensions</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Butzer</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Res. Expo</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sampling theory: Beyond bandlimited systems</title>
		<author>
			<persName><forename type="first">Yonina</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond bandlimited sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="48" to="68" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Example-based super-resolution</title>
		<author>
			<persName><forename type="first">Thouis</forename><forename type="middle">R</forename><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egon</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and Applications</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal linear interpolation of images with known point spread function</title>
		<author>
			<persName><surname>Ca Glasbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Scandinavian Conference on Image Analysis</title>
				<meeting>the Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Superresolution from a single image</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<idno>IEEE. 1</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th international conference on computer vision</title>
				<imprint>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2019. 1, 2, 3, 6</date>
			<biblScope unit="page" from="1604" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009">2018. 1, 2, 3, 5, 6, 7, 9</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the transmission capacity of the ether and of cables in electrical communications</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Aleksandrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kotelnikov</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Certain topics in telegraph transmission theory</title>
		<author>
			<persName><forename type="first">Harry</forename><surname>Nyquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the American Institute of Electrical Engineers</title>
				<imprint>
			<date type="published" when="1928">1928</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="617" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing the nonlocal-means to super-resolution reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-ideal sampling and adapted reconstruction using the stochastic matern model</title>
		<author>
			<persName><forename type="first">Sathish</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ville</surname></persName>
		</author>
		<author>
			<persName><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</title>
				<imprint>
			<publisher>II-II. IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Communication in the presence of noise</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
				<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1949">1949</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">zero-shot super-resolution using deep internal learning</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2018. 1, 2, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unified approach to superresolution and multichannel blind deconvolution</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Sroubek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Cristóbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Flusser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2322" to="2332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image restoration by iterative denoising and backward projections</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1220" to="1234" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Super-resolution via imageadapted denoising CNNs: Incorporating external and internal learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sampling50 years after shannon. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Unser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A fully progressive approach to single-image super-resolution</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2009">2018. 1, 2, 3, 5, 6, 7, 9</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the functions which are represented by the expansions of the interpolation-theory</title>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whittaker</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="181" to="194" />
			<date type="published" when="1915">1915</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Curves and Surfaces</title>
				<meeting>the 7th International Conference on Curves and Surfaces</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2018. 1, 2, 3, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep plug-andplay super-resolution for arbitrary blur kernels</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009">2018. 1, 2, 3, 5, 6, 7, 9</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
