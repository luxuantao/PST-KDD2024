<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ambiguity-Aware In-Context Learning with Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingyu</forename><surname>Gao</surname></persName>
							<email>lygao@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditi</forename><surname>Chaudhary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
							<email>krishnaps@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
							<email>kazumah@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karthik</forename><surname>Raman</surname></persName>
							<email>karthikraman@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ambiguity-Aware In-Context Learning with Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific finetuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work <ref type="bibr" target="#b24">(Min et al., 2022)</ref>, we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demonstrations but also to choose those demonstrations that help resolve the inherent label ambiguity surrounding the test example. Interestingly, we find that including demonstrations that the LLM previously mis-classified and also fall on the test example's decision boundary, brings the most performance gain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While the increasing size of large language models (LLMs) <ref type="bibr">(Brown et al., 2020b;</ref><ref type="bibr" target="#b9">Chowdhery et al., 2022;</ref><ref type="bibr">Thoppilan et al., 2022)</ref> have demonstrated strong downstream gains, fine-tuning these models is time and compute intensive <ref type="bibr">(Brown et al., 2020b;</ref><ref type="bibr" target="#b32">Wang et al., 2023)</ref>. Instead, leveraging these LLMs via in-context learning (ICL) has become popular wherein the model is able to perform a task by simply being conditioned on the task definition and/or few task demonstrations (input-output examples) * Work done as an intern at Google Research. <ref type="bibr">(Brown et al., 2020a;</ref><ref type="bibr" target="#b35">Xie et al., 2021)</ref>. Not only has this attained strong performance <ref type="bibr">(Zhao et al., 2021a;</ref><ref type="bibr" target="#b18">Liu et al., 2022)</ref>, but has also allowed easy and fast experimentation with LLMs as it does not require any model training.</p><p>As ICL gets increasingly adopted, it has brought to light <ref type="bibr" target="#b16">(Lester et al., 2021;</ref><ref type="bibr" target="#b18">Liu et al., 2022;</ref><ref type="bibr" target="#b38">Zhang et al., 2022;</ref><ref type="bibr" target="#b20">Lu et al., 2022)</ref> that LLMs are sensitive to the choice of prompts, making "prompt engineering" for different tasks challenging and time-consuming. However, prompt engineering does not have to be a complete guessing game rather it can be governed by some data-derived signals. For example, selecting demonstrations that are semantically similar to the a new input have shown to be more effective over randomly sampled demonstrations <ref type="bibr" target="#b10">(Das et al., 2021;</ref><ref type="bibr" target="#b18">Liu et al., 2022;</ref><ref type="bibr" target="#b23">Margatina et al., 2023)</ref>, wherein a text retriever is used to select the top-k training examples for each test example based on the input text. The motivation behind this approach is using information from existing similar situations to solve a new problem <ref type="bibr" target="#b0">(Aamodt and Plaza, 1994)</ref>.</p><p>However, the solely input-based selection does not explicitly capture key information about the task-specific label space, inter alia:</p><p>? How is the usefulness of each demonstration attributed to the LLMs' existing knowledge?</p><p>? What clues about a test input can be derived through the LLMs' existing knowledge?</p><p>These questions depend on the LLMs' existing knowledge or capability of solving the target task, and can be answered only by interacting with the LLM itself. For example, on a five-way sentiment classification task (SST-5 <ref type="bibr" target="#b28">(Socher et al., 2013</ref>)), we have observed that the Flan-PaLM 2 model <ref type="bibr">(Anil et al., 2023)</ref> is confused between two specific labels, 'Very Negative' and 'Negative,' a lot more than say between 'Neutral' and 'Very Negative' </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Add constraint</head><p>Figure <ref type="figure">1</ref>: Overview of our proposed method for selecting ICL demonstrations: For each test example, we first use a retriever to rank training data by semantic similarity. At the same time, we identify the ambiguous label set for each test example and also obtain the output predictions on the retrieved training data. Next, we apply three constraints on the top-ranked demonstrations which are: 1) select those demonstrations whose gold label is in the ambiguous label set, 2) select those which are also mis-classified by the model, and 3) select those mis-classified examples whose predicted label is in the ambiguous label set. Finally, we construct prompts with selected ICL demonstrations to get the final model predictions.</p><p>(Figure <ref type="figure" target="#fig_1">2</ref> in Appendix A.2). This motivates us to investigate whether the model's existing knowledge can also be leveraged to select even more effective demonstrations.</p><p>Specifically, we derive signals from the underlying LLM model about the output label space of both the new test example and the training data from which we select the demonstrations. As motivated above, the model's ambiguity around the new test example's output label will help us know what the model is most confused about which, in turn, can be used to select those demonstrations that help reduce this confusion. For selecting such demonstrations from the training data, we propose to not only consider the ground truth labels paired with these demonstrations but also to consider the usefulness by looking at their model prediction. Given that <ref type="bibr" target="#b23">Margatina et al. (2023)</ref>; <ref type="bibr" target="#b21">Luo et al. (2023)</ref> find demonstrations based on semantic similarity to be the best performing across sentence classification tasks, we build our methods on top of it. First, given a test example and pool of training data, for each test example we use an off-the-shelf retriever to retrieve top-k examples that have similar input text. For each test example, we identify the ambiguous label set which is set of two output labels that the model is most confused about. Next, we select those top-ranked demonstrations whose ground truth label lies in the above label set. To further find useful demonstrations, we identify those which are mis-classified by the model, with the intuition that showing the model a previously misclassified demonstration could force it to correct it <ref type="bibr" target="#b29">(Tan, 2006;</ref><ref type="bibr" target="#b31">Wang et al., 2020)</ref>. Finally, on top of the mis-classified demonstrations we add a constraint to select only those demonstrations whose model prediction falls within the ambiguous label set i.e. on the test example's decision boundary.</p><p>To test our hypothesis, we focus on multi-class classification tasks that have fine-grained nuance in the label space, We conduct extensive experimentation across three text classification tasks, namely SST <ref type="bibr" target="#b28">(Socher et al., 2013)</ref>, GoEmotions <ref type="bibr" target="#b11">(Demszky et al., 2020)</ref>, and EDOS (Task-B) <ref type="bibr" target="#b14">(Kirk et al., 2023)</ref>, all of which have fine-grained label space, making the model more likely to be confused across labels and our key observations are:</p><p>1. Incrementally adding constraints, i.e., 1) considering label ambiguity of test example, 2) limiting ICL demonstrations to mis-classified demonstrations, and 3) considering label ambiguity of training examples leads to +1.5%, +2.2%, +2.6% improvement in F1 macro scores over the retriever-based ICL, averaged across all datasets (Table <ref type="table" target="#tab_2">2</ref>).</p><p>2. We find that adding such label-based constraints helps more on a smaller model, i.e., on Flan-PaLM 2 (M) (+3.9% gain) compared to +1.4% gain on Flan-PaLM 2 (L).</p><p>3. We attribute this success of our proposed methods to the observation that the ambiguous label set acts as a good proxy to the gold test label, and as noted by <ref type="bibr" target="#b24">Min et al. (2022)</ref> labels in the ICL demonstrations bias the model predictions the most. Therefore, showing the models the 'likely' gold label guides the model to make the correct prediction (Table <ref type="table" target="#tab_5">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>Typically, in an ICL regimen, we assume access to a training data D train = {(x 0 , y 0 ), ? ? ? , (x T , y T )} from which the goal is to select d demonstrations to be used as prompt. As motivated in the introduction, we follow a three-step approach for selecting demonstrations, for each test example, we need to 1) extract semantically similar examples from D train , 2) identify the ambiguous label-set and 3) extract model predictions for D train to identify misclassified examples. Below, we describe each step in more detail and how they are used together to select the "best" demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extract Semantically Similar Demonstrations</head><p>Typically, in this approach, demonstrations are selected for each test example x t by finding those examples from the D train that are semantically similar to the test input. The motivation being that observing demonstrations that are similar to the new input text will act as a hint for the model <ref type="bibr" target="#b23">(Margatina et al., 2023)</ref>. This requires the use of a retriever R, either an off-the-shelf one such as <ref type="bibr" target="#b18">(Liu et al., 2022;</ref><ref type="bibr" target="#b1">Agrawal et al., 2023;</ref><ref type="bibr" target="#b23">Margatina et al., 2023;</ref><ref type="bibr" target="#b21">Luo et al., 2023)</ref> or a retriever trained specifically for that task <ref type="bibr" target="#b10">(Das et al., 2021;</ref><ref type="bibr" target="#b26">Rubin et al., 2022</ref>). For each test example x t , the retriever R is used to rank examples from D train based on semantic similarity of the text inputs. Top-k input-output pairs are then selected from the ranked D train to be used as ICL demonstrations.</p><p>Identify Ambiguous Label-Set As we can observe from the confusion matrix in Figure <ref type="figure" target="#fig_1">2</ref>, the model is often confused between two labels. We hypothesize that in addition to semantic similarity, by providing demonstrations that help the model resolve this ambiguity will help the model correct itself. Thus, as a next step, we construct a prompt  <ref type="formula">2023</ref>) has looked at training data label-space from the lens of ground-truth labels, i.e., whether to retain them in the ICL or not, we aim to look at label-space from the perspective of model predictions. Specifically, we are interested in identifying "hard" demonstrations i.e. examples on which the model makes mistakes. We hope that by showing the model such examples with their ground truth labels will force the model to correct itself. Prior work has underscored the potential value of leveraging mis-classified examples from the training set to enhance model performance <ref type="bibr" target="#b29">(Tan, 2006;</ref><ref type="bibr" target="#b31">Wang et al., 2020)</ref>, but they haven't tested it for ICL demonstration selection on text classification. In addition to the mis-classified examples, we further constrain the model prediction of these mis-classified examples to be one of the ambiguous labels, identified in the above step. Given that we already know which output labels the model is confused between for the test examples, showing the model those demonstrations (with their ground truth labels) which fall on the decision boundary will likely guide the model to choose the correct label for the test input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We experiment with the Flan-PaLM 2 model, an instruction-tuned model which is finetuned on the Flan dataset <ref type="bibr" target="#b9">(Chung et al., 2022;</ref><ref type="bibr" target="#b19">Longpre et al., 2023)</ref> based on PaLM-2 <ref type="bibr">(Anil et al., 2023)</ref>, a multilingual large language model pretrained on web documents, books, code, mathematics and conversational data. We chose these models as <ref type="bibr" target="#b21">Luo et al., 2023</ref> find that retrieved demonstration for ICL works better with instruction-tuned models over general LLMs (e.g. GPT). In particular, we experiment with two variants of the model, namely Flan-PaLM-2 (M) and Flan-PaLM-2 (L), where the latter is a larger parameter model. <ref type="foot" target="#foot_0">1</ref> The ICL demonstrations are selected using an off-the-shelf retriever which is finetuned on mT5-base <ref type="bibr">(Xue et al., 2021)</ref> using the unsupervised objective proposed by <ref type="bibr" target="#b13">Izacard et al. (2021)</ref>. Since the order of demonstrations may impact the model performance <ref type="bibr" target="#b15">(Kumar and Talukdar, 2021;</ref><ref type="bibr" target="#b20">Lu et al., 2022)</ref>, we randomly shuffle the order of demonstrations for three random seeds and report the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>As mentioned above the Flan-PaLM 2 models are finetuned on the Flan dataset which is a mixture of many supervised datasets. Specifically, we choose three text classification datasets that satisfy the following desiderata, 1) the output label space shows fine-grained nuance that spans multiple labels, and 2) these datasets are not part of the Flan mixture to avoid any inherent bias from the underlying model. We describe them below, with dataset statistics shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EDOS (Task-B) :</head><p>The Task B of Explainable Detection of Online Sexism <ref type="bibr" target="#b14">(Kirk et al., 2023)</ref>, which is a topic classification task where the sexist content is classified into four categories, i.e., 1) Threats, plans to harm &amp; incitement, 2) Derogation, 3) Animosity, and 4) Prejudiced Discussion.</p><p>SST : This is the Stanford Sentiment Treebank (SST, <ref type="bibr" target="#b28">Socher et al., 2013)</ref>, a 5-way sentiment classification dataset on movie reviews with labels: Very Negative, Negative, Neutral, Positive, and Very Positive.</p><p>GoEmotions : The GoEmotions <ref type="bibr" target="#b11">(Demszky et al., 2020)</ref> is a multi-class sentiment classification dataset with "neutral" and 27 emotional classes, e.g., "admiration" and "fear", collected from Reddit comments. As the label space is very large and we have limited sequence length, it becomes even more crucial to select a concise but effective prompt.<ref type="foot" target="#foot_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare our proposed method against the following baselines: </p><formula xml:id="formula_0">ICL(x t ) = (x i , y i ) if y i ? L ambig,t for (x i , y i ) ? R(x t ) +GOLD+MIS Select those examples from R(x t )</formula><p>as demonstrations where the ground truth lies in L ambig,t and they are mis-classified, denoted by:</p><formula xml:id="formula_1">ICL(x t ) = (x i , y i ) if y i ? L ambig,t , ?i ? = y i for (x i , y i ) ? R(x, t)</formula><p>Note that the model predictions (?) on the R(x t ) are obtained from the ZERO model.</p><p>+GOLD+MIS+PRED Select those examples from R(x t ) as demonstrations where the ground truth lies in L ambig,t , they are mis-classified with the additional constraint these mis-classified examples' model predictions also lie in L ambig,t , denoted by:</p><formula xml:id="formula_2">ICL(x t ) = (x i , y i ) if y i ? L ambig,t , ?i ? = y i , ?i ? L ambig,t for (x i , y i ) ? R(x t )</formula><p>As above, the model predictions on the training data are obtained from ZERO. For all our proposed model variants, we finally select n demonstrations where n = 4 and n = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>We report all our results in Table <ref type="table" target="#tab_2">2</ref>. Specifically, we use the F1 macro scores to compare the model performance, as all our tasks have unbalanced datasets. <ref type="foot" target="#foot_4">4</ref> First, we note across all three tasks, our proposed methods outperform the baselines.</p><p>We also note that the zero-shot model (ZERO) which only uses a task definition but no task demonstrations, already is a strong baseline for both the Flan-PaLM 2 models (M/L), but we do find that adding only a few demonstrations generally leads to a higher performance. In particular, comparing the average scores of the few-shot baselines and ZERO, we find that the larger model Flan-PaLM 2 (L) benefits more from the addition of ICL demonstrations (+1.4% gain), compared to Flan-PaLM 2 (M) where the gain is +0.4%. This is because largerparameter models make better use of in-context learning <ref type="bibr" target="#b6">(Chan et al., 2022;</ref><ref type="bibr" target="#b2">Aky?rek et al., 2023;</ref><ref type="bibr" target="#b34">Wei et al., 2023)</ref>. Interestingly, we also observe that for SST and GoEmotions, the Flan-PaLM 2 (L) model achieves higher performance with n = 4 over n = 8, which highlights that quantity does not necessarily lead to better performance.</p><p>Considering output label space is more important than semantic similarity. Within the few- shot methods, where we use task demonstrations along with the task definition, we find that our proposed methods AMBIG-* outperforms retrieverbased models (RETR-*) by +3.0% (avg.) for Flan-PaLM 2 (M), and by +1.2% (avg.) for Flan-PaLM 2 (L), suggesting the considering output label space for selecting demonstrations is as important as considering the input similarity. In particular, we find that especially considering mis-classified demonstrations that fall on the test example's decision boundary leads to the overall best performance. In Table <ref type="table" target="#tab_3">3</ref>, we show the demonstrations selected for the n = 4 setting for one example of the GoEmotions task.  <ref type="bibr" target="#b10">(Das et al., 2021;</ref><ref type="bibr" target="#b18">Liu et al., 2022;</ref><ref type="bibr" target="#b23">Margatina et al., 2023)</ref>, have indicated that greater semantic similarity can enhance model performance, we can see that our methods can still outperform the retriever-based baselines which prioritize it.</p><p>The ambiguous label set is a good proxy for the test gold label. While <ref type="bibr" target="#b24">Min et al. (2022)</ref> find that using pseudo-demonstrations i.e. demonstrations with random labels instead of the ground truth labels, does not affect the downstream performance, <ref type="bibr" target="#b22">Lyu et al. (2023)</ref> find that for demonstrations that are similar to the test input, such as those from a retriever, pseudo-demonstrations hurt the performance. They refer to this as the copying-effect hypothesis which says that the "model prediction is biased towards the labels paired with the inputs in the demonstrations, especially when the inputs are similar to the test inputs". This, in turn, suggests that the best performance could be achieved if the labels paired with the inputs are same as the gold label of the test example. Given that we do not know the gold label of the test example apriori, the question then becomes how do we approximate the gold label?. We find that our ambiguous label set acts as a close proxy. In Table <ref type="table" target="#tab_5">4</ref>, we compute how many times is the label paired with ICL demonstrations the same as the test example gold label. We find that 44.2% of our proposed methods' (AMBIG) demonstrations have the same gold label as the test example on average, compared to 30.9% from the RETR method. This is why including the ambiguous label set in the demonstration selection process leads to a higher performance. This analysis also sheds light on the effectiveness of retriever-based ICL. From Table <ref type="table" target="#tab_5">4</ref> we can see that the demonstrations selected solely based on input text similarity is only 13.3% points (avg.) behind our proposed methods. This confirms that finding demonstrations similar to the input text also leads to selecting demonstrations that have the 'likely' gold label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The performance of large language models (LLMs) is significantly influenced by the quality of ICL demonstrations, as demonstrated in multiple studies <ref type="bibr">(Zhao et al., 2021b;</ref><ref type="bibr" target="#b18">Liu et al., 2022;</ref><ref type="bibr" target="#b38">Zhang et al., 2022)</ref>. Consequently, the focus on retrieving superior demonstrations has increased. One prominent strategy is to finetune a retriever for specific tasks by similarity metrics <ref type="bibr" target="#b10">(Das et al., 2021;</ref><ref type="bibr" target="#b12">Hu et al., 2022;</ref><ref type="bibr" target="#b25">Poesia et al., 2022)</ref> or by scores derived from language models <ref type="bibr" target="#b26">(Rubin et al., 2022;</ref><ref type="bibr" target="#b27">Shi et al., 2022)</ref>. While some works introduce an unified retriever trained across various task <ref type="bibr" target="#b17">(Li et al., 2023;</ref><ref type="bibr" target="#b7">Cheng et al., 2023)</ref> for generalizabilty, another direction is to leverage off-the-shelf retrievers.  <ref type="bibr">et al., 2023</ref> focus on selecting diverse demonstrations as well as promoting n-gram overlap between demonstrations and test examples. In our work, we adopt the off-the-shelf retriever approach as our focus is to show the generalizability of our approach across different classification tasks. However, we expect that our method will also be benefited from a task-specific retriever. Additionally, to the best of our knowledge, we are the first ones to leverage the LLM's existing knowledge surrounding the test example for selecting demonstrations. Prior works have typically explored only the LLM's existing knowledge i.e. model prediction for the training data.</p><p>Below we discuss some works which are orthogonal to our work but can be used in combination with our proposed methods. For example, <ref type="bibr" target="#b21">Luo et al., 2023</ref> use the LLM prediction score on the training data to train a task-specific retriever, and also use Chain-of-Thought prompting <ref type="bibr" target="#b33">(Wei et al., 2022)</ref> to improve model performance; Some works <ref type="bibr" target="#b15">(Kumar and Talukdar, 2021;</ref><ref type="bibr" target="#b20">Lu et al., 2022)</ref> have found that ordering of the ICL demonstrations also affects the downstream performance, that is why in Table <ref type="table" target="#tab_2">2</ref> we report the results across three shuffle orders.</p><p>In this work, we find that using LLM's existing knowledge (e.g. the model prediction) regarding the output label space of both the test example and the ICL demonstration pool is as important as considering the semantic similarity of the input text alone. We find that our proposed method consistently outperform the baselines for all three tasks. Although, we only consider the top-2 most ambiguous labels in selecting the ICL demonstrations, it would be interesting to expand the ambiguous label set to more than two labels. This would especially be more important for datasets like GoEmotions where the label space is large and much more fine-grained. We leave this effort for future work. Furthermore, in this work, we focus on sentence classification tasks, thus paving the way for others to use our proven techniques to also explore label ambiguity for other token/span-level tasks such as Named Entity Recognition (NER), Part-Of-Speech (POS) tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Prompt Construction</head><p>We show our templates in Table <ref type="table" target="#tab_8">5</ref> (we use 4-shot as an example for few-shot). Task definitions are listed below, denoted by x def n :</p><p>? EDOS: Given a text input, the task is to classify the input as being a Threat, Prejudiced, Animosity, or Derogation category of sexism. Threat refers to language where an individual expresses intent andor encourages others to take action against women which inflicts or incites serious harm and violence against them. It includes threats of physical, sexual or privacy harm. Prejudiced refers to language which denies the existence of discrimination, and justifies sexist treatment. It includes denial and justification of gender inequality, excusing women's mistreatment, and the ideology of male victimhood. Animosity refers to language which expresses implicit or subtle sexism, stereotypes or descriptive statements. It includes benevolent sexism, i.e., framed as a compliment. Derogation refers to language which explicitly derogates, dehumanises, demeans or insults women. It includes negative descriptions and stereotypes about women, objectification of their bodies, strong negative emotive statements, and dehumanising comparisons. It covers negative statements directed at a specific woman and women in general. ? SST: Given sentences from movie reviews, the task is to classify the sentences as being a Great, Good, Okay, Bad, or Terrible category of sentiment. Great refers to language that expresses extremely positive sentiment. Good refers to language that expresses positive sentiment, but not to the extreme. Okay refers to language that is neutral, i.e., neither expresses clear positive nor negative sentiments. Bad refers to language that expresses negative sentiment, but not to the extreme. Terrible refers to language that expresses extremely negative sentiment. ? GoEmotions: Given sentences from Reddit comments, the task is to classify the sentences as being an Admiration, Approval, Annoyance, Gratitude, Disapproval, Amusement, Curiosity, Love, Optimism, Disappointment, Joy, Realization, Anger, Sadness, Confusion, Caring, Excitement, Surprise, Disgust, Desire, Fear, Remorse, Embarrassment, Nervousness, Pride, Relief, or Grief category of emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Confusion Matrix</head><p>The confusion matrices of zero-shot experiments on SST are presented in Figure <ref type="figure" target="#fig_1">2</ref>. For details of experiment setups, please see ZERO in section 3.3, where we construct the prompts with a task definition and the test example, and ask the model to score each output label with log-likelihood to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Accuracy, Precision, Recall</head><p>Please refer to   with ZERO), and Table <ref type="table" target="#tab_13">10</ref> shows results that are label-wise.</p><p>A.5 Sorting order with predicted probability distribution entropy</p><p>Since we have predicted probability distribution of our models (Flan-PaLM 2) for ICL demonstrations, we try to sort it by increasing entropy order, however, it doesn't consistently improve model performance, which is shown in where M and L refers to Flan-PaLM 2 sizes. Table <ref type="table" target="#tab_2">12</ref>: The difference of F1 macro scores (%) between the "increased entropy order" and the "averaged over 3 random seeds".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Confusion Matrix of zero-shot experiments on SST. Labels: VPos (Very Positive), Pos (Positive), Neu (Neutral), Neg (Negative), VNeg (Very Negative).</figDesc><graphic url="image-3.png" coords="13,70.87,501.83,218.26,163.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Extract Mis-classified Demonstrations The final component in our recipe is to consider the model prediction of the training data. While prior work Min et al. (2022); Yoo et al. (2022); Margatina et al. (</figDesc><table><row><cell cols="6">? for the test example x t , and use the model log-</cell></row><row><cell cols="6">likelihood to score each output label l ? L given</cell></row><row><cell cols="6">the prompt. Using this we identify top-2 labels</cell></row><row><cell cols="6">that have the highest scores, which we refer to</cell></row><row><cell cols="6">as the "ambiguous label set" of x t , denoted as</cell></row><row><cell>L ambig,t = {? t , (1)</cell><cell>?(2) t }, where</cell><cell>?(1) t</cell><cell>and</cell><cell>?(2) t</cell><cell>are</cell></row><row><cell cols="6">the first and second most likely labels, respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of examples in each dataset split. For each test example x t , we append the task definition to each test input and prompt the models, please see Appendix A.1 to see the exact prompt. To obtain the model prediction, we use the model log-likelihood to score each output label l ? L, given the prompt. Then, we select the label with the highest score. y</figDesc><table><row><cell></cell><cell>train</cell><cell>dev</cell><cell>test</cell></row><row><cell>EDOS</cell><cell>3,398</cell><cell>486</cell><cell>970</cell></row><row><cell>SST</cell><cell cols="3">8,544 1,101 2,210</cell></row><row><cell>GoEmotions</cell><cell cols="3">23,485 2,952 2,978</cell></row><row><cell cols="4">Frequent Label (FREQ). Select the most fre-</cell></row><row><cell cols="4">quent label as the model prediction for all test ex-</cell></row><row><cell>amples.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zero-shot ICL (ZERO).</cell><cell></cell><cell></cell></row></table><note><p><p><p><p>t = arg max L score(l, ?) where ? refers to the prompt used specific for this setting and score refers to the model's log-likelihood.</p>Static N-shot ICL (STATIC-N ). We manually select N demonstrations from D train , one for each of the N output labels (N = |L|). Note that these demonstrations are static for all test examples. Thus, we concatenate the task definition, N demonstrations and test example x t as the prompt for ICL and use the log-likelihood scores, as described above, to get the model prediction. Please refer to Appendix A.1 for more details on the prompt template.</p>Retriever-based ICL (RETR). Unlike above, where we used the same prompt for all test inputs, in this baseline, we retrieve demonstrations for each test input x t . We use an off-the-shelf retriever R (subsection 3.1) to retrieve k nearest neighbors {x 1,t , ? ? ? , x k,t } from D train , similar to</p>Das et al.   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>F1 macro (%) comparison between our baselines (top) and our proposed methods (bottom) with Flan-PaLM 2 (M/L). 4 or 8 refers to the number of ICL demonstrations. The best performance across all method is highlighted and the best performing baseline is underlined. The "Avg." column shows the average scores across all datasets. The standard deviations are computed over three random seeds, with the order of demonstrations shuffled.</figDesc><table /><note><p>we summarize our proposed model variants. For each setting, we first retrieve the top-k most similar examples from the training data D train for each test example x t . We denote these candidates by R(x t ) = {(x 0,t , y 0,t ), ? ? ? , (x k,t , y k,t )}. At the same time, for each x t , we also identify the ambiguous label-set i.e. the top-2 labels which the model is most confused about, L ambig,t = {l i , l j |l ? L} where L is the set of all output labels. +GOLD Select those examples from R(x t ) as demonstrations where the ground truth label of each demonstration belongs to the ambiguous label set of x t denoted by:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Disappointment: I want to make friends too :( but I feel like I have nothing good to offer 2. Joy: I, too, am a lot of fun at parties. We can stand together in the corner! 3. Gratitude: Thanks. I am. I make some new friends. 4. Disapproval: Not really. My group of friends are awesome in every way possible except they are homophobic Example demonstrations selected by the RETR and our proposed method AMBIG for the GoEmotions task, for n = 4. Each demonstration comprises of the input text and the ground truth label, as selected from the training data. Flan-PaLM 2 (L), where RETR mis-classified it as "Joy", and AMBIG-ICL predicted correctly under all three settings.</figDesc><table><row><cell cols="2">Test Example: Ok! I like making friends</cell><cell>L ambig,t : Love, Joy</cell><cell>Gold label: Love</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Predicted:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Joy</cell></row><row><cell>AMBIG-ICL</cell><cell></cell><cell></cell></row><row><cell>+GOLD</cell><cell cols="2">1. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!</cell><cell>Predicted:</cell></row><row><cell></cell><cell>2. Love: I ... I like you</cell><cell></cell><cell>Love</cell></row><row><cell></cell><cell cols="2">3. Love: Married to the love of my life. LOL</cell></row><row><cell></cell><cell>4. Love: I do. but some people love it</cell><cell></cell></row><row><cell>+GOLD+MIS</cell><cell cols="2">1. Joy: I, too, am a lot of fun at parties. We can stand together in the corner!</cell><cell>Predicted:</cell></row><row><cell></cell><cell cols="2">2. Love: Too cute for me. Why cant i have a boyfriend *[NAME]*</cell><cell>Love</cell></row><row><cell></cell><cell cols="2">3. Joy: FaceTime with wifey!! Happy anniversary!</cell></row><row><cell></cell><cell cols="2">4. Love: Stick around! Would love your input POV!</cell></row><row><cell>+GOLD+MIS+PRED</cell><cell cols="2">1. Joy: FaceTime with wifey!! Happy anniversary!</cell><cell>Predicted:</cell></row><row><cell></cell><cell cols="3">2. Joy: She want to take it slow, I can see that... I deal with those girls all the time,</cell><cell>Love</cell></row><row><cell></cell><cell>they my favorite</cell><cell></cell></row><row><cell></cell><cell>3. Love: Ha! I like that one.</cell><cell></cell></row><row><cell></cell><cell>4. Love: Ooh I like that one :)</cell><cell></cell></row></table><note><p><p>RETR</p>1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>We see that for the test input "Ok! I like making friends", the RETR method retrieved similar examples from D train (all examples refer to friends). Now from the ZERO model, we calculated the model prediction scores and found that Love and Joy are the two labels the model is most confused about. However, because we do not consider any test example ambiguity in RETR, none of the retrieved examples represent the labels Love or Joy, which are the two labels the model is most confused about for this test example. Whereas, in the</figDesc><table><row><cell>Considering output label space compensates for</cell></row><row><cell>the sacrifice in semantic similarity. As we intro-</cell></row><row><cell>duce more constraints (i.e. +MIS and +PRED), we</cell></row><row><cell>find that we need to sacrifice the semantic similarity</cell></row><row><cell>to the test input. For example, consider the 4-shot</cell></row><row><cell>AMBIG-ICL experiment on EDOS (Task B), to sat-</cell></row><row><cell>isfy the constraints for the +GOLD setting we need</cell></row><row><cell>to select up to top-16 retrieved examples in order to</cell></row><row><cell>obtain the 4 ICL demonstrations; for +GOLD+MIS</cell></row><row><cell>we need top-55 retrieved examples and around top-</cell></row><row><cell>250 retrieved examples for +GOLD+MIS+PRED. 5</cell></row><row><cell>Clearly, by selecting lower ranked examples from</cell></row><row><cell>the retrieved set R(x t ) we are sacrificing the se-</cell></row><row><cell>mantic similarity to the test input. While previ-</cell></row><row><cell>ous studies, such as</cell></row></table><note><p>AMBIG setting, because of our constraints, all the examples chosen for ICL belong to the ambiguous label set. This allows all our proposed methods to better understand this fine-grained nuance across la-bel space and make the correct model prediction of Love. Below, we conduct some analysis to further explain the way our proposed methods work.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Average percentage (%) of examples in the top 4, 8 retrieved demonstrations that share the same gold labels with test example. Liu et al., 2022 propose a KNN-based method to select ICL demonstrations based on semantic similarities; Margatina et al., 2023 explores to select ICL demonstrations with active learning algorithms based on uncertainty, diversity, and similarity, and show that selecting based on similarities consistently outperforms other methods; and Agrawal</figDesc><table><row><cell></cell><cell>EDOS</cell><cell>SST</cell><cell cols="2">GoEmotions</cell></row><row><cell></cell><cell>M L</cell><cell>M L</cell><cell>M</cell><cell>L</cell></row><row><cell>4-shot</cell><cell>42.6</cell><cell>29.6</cell><cell></cell><cell>21.6</cell></row><row><cell>8-shot</cell><cell>42.5</cell><cell>28.6</cell><cell></cell><cell>20.5</cell></row><row><cell>AMBIG-4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+GOLD</cell><cell cols="4">49.5 50.3 46.5 47.1 41.3 41.9</cell></row><row><cell>+GOLD+MIS</cell><cell cols="4">46.4 44.3 46.1 44.3 38.7 38.8</cell></row><row><cell>+GOLD+MIS+PRED</cell><cell cols="4">48.3 42.3 46.1 44.6 37.8 40.7</cell></row><row><cell>AMBIG-8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+GOLD</cell><cell cols="4">50.3 50.3 46.0 46.8 41.2 41.7</cell></row><row><cell>+GOLD+MIS</cell><cell cols="4">46.9 43.8 46.4 44.7 38.7 38.6</cell></row><row><cell>+GOLD+MIS+PRED</cell><cell cols="4">48.8 42.9 46.5 44.9 37.5 40.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>, 7 and 8.</cell></row></table><note><p><p>A.4 Label-wise Percentage Analysis of Gold</p>Label Inclusion in L ambig,t</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table9shows the percentage of times that the test example's gold label is in L ambig,t (as obtained</figDesc><table><row><cell>x def n</cell></row><row><cell>Thus given the following input:</cell></row><row><cell>input: xt</cell></row><row><cell>answer:</cell></row><row><cell>x def n</cell></row><row><cell>Some examples are:</cell></row><row><cell>input: x1,t</cell></row><row><cell>answer: y1,t</cell></row><row><cell>input: x2,t</cell></row><row><cell>answer: y2,t</cell></row><row><cell>input: x3,t</cell></row><row><cell>answer: y3,t</cell></row><row><cell>input: x4,t</cell></row><row><cell>answer: y4,t</cell></row><row><cell>Thus given the following input:</cell></row><row><cell>input: xt</cell></row><row><cell>answer:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Prompt Templates for zero-shot and few-shot ICL. x t refers to the test example, and x i,t , y i,t refers to the text inputs and gold labels of ICL demonstrations selected for x t , respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Table 11 and 12. Precision (%) comparison between our proposed methods and baselines with Flan-PaLM 2 (M, L).</figDesc><table><row><cell></cell><cell>Prec. (%)</cell><cell>M</cell><cell>EDOS L</cell><cell>M</cell><cell>SST L</cell><cell>M</cell><cell>GoEmotions L</cell></row><row><cell></cell><cell>FREQ</cell><cell>11.7</cell><cell>11.7</cell><cell>4.6</cell><cell>4.6</cell><cell>0.4</cell><cell>0.4</cell></row><row><cell></cell><cell>ZERO</cell><cell>65</cell><cell>60.7</cell><cell>54</cell><cell>56.2</cell><cell cols="2">42.6</cell><cell>46.3</cell></row><row><cell>Baselines</cell><cell>STATIC-N</cell><cell cols="6">65.2?0.6 58.1?0.4 54.5?0.6 58.2?0.3 42.6?1.2 46.2?0.3</cell></row><row><cell></cell><cell>RETR-4</cell><cell cols="6">67.1?1.1 63.6?0.5 53.4?0.3 57.4?0.4 43.7?0.4 47.6?0.4</cell></row><row><cell></cell><cell>RETR-8</cell><cell cols="6">65.0?0.2 63.9?0.3 54.4?0.1 57.6?0.5 43.7?0.4 48.3?0.1</cell></row><row><cell></cell><cell>AMBIG-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+GOLD</cell><cell cols="6">65.9?0.8 63.6?0.4 54.1?0.3 57.7?0.1 45.7?0.3 50.5?0.2</cell></row><row><cell></cell><cell>+GOLD+MIS</cell><cell cols="6">66.6?1.1 63.6?1.0 54.1?0.2 58.8?0.1 44.8?0.4 49.2?0.1</cell></row><row><cell>Ours</cell><cell>+GOLD+MIS+PRED AMBIG-8</cell><cell cols="6">67.4?0.4 65.0?0.5 54.8?0.5 59.4?0.0 46.9?1.3 47.9?0.2</cell></row><row><cell></cell><cell>+GOLD</cell><cell cols="6">66.4?1.1 64.8?0.1 54.7?0.2 58.5?0.7 48.0?1.8 49.9?0.1</cell></row><row><cell></cell><cell>+GOLD+MIS</cell><cell cols="6">68.4?0.8 64.4?0.6 54.5?0.1 59.6?0.1 48.7?0.5 48.8?0.5</cell></row><row><cell></cell><cell>+GOLD+MIS+PRED</cell><cell cols="6">66.6?1.2 66.4?0.3 54.9?0.2 59.1?0.4 43.7?0.5 47.4?0.3</cell></row><row><cell></cell><cell>Rec. (%)</cell><cell>M</cell><cell>EDOS L</cell><cell>M</cell><cell>SST L</cell><cell>M</cell><cell>GoEmotions L</cell></row><row><cell></cell><cell>FREQ</cell><cell>25</cell><cell>25</cell><cell>20</cell><cell>20</cell><cell>3.7</cell><cell>3.7</cell></row><row><cell></cell><cell>ZERO</cell><cell>46</cell><cell>62.8</cell><cell>53.8</cell><cell>55.2</cell><cell cols="2">42.4</cell><cell>47.2</cell></row><row><cell>Baselines</cell><cell>STATIC-N</cell><cell cols="6">46.2?0.3 63.0?0.3 54.0?0.4 56.5?0.2 34.8?0.5 49.5?0.4</cell></row><row><cell></cell><cell>RETR-4</cell><cell cols="6">44.8?0.3 63.4?0.2 53.4?0.3 55.7?0.3 38.5?0.2 49.7?0.3</cell></row><row><cell></cell><cell>RETR-8</cell><cell cols="6">44.0?0.1 62.1?0.2 54.2?0.1 55.3?0.4 37.8?0.3 50.1?0.3</cell></row><row><cell></cell><cell>AMBIG-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+GOLD</cell><cell cols="6">45.1?0.6 64.1?0.2 54.6?0.4 56.4?0.1 41.4?0.3 51.3?0.2</cell></row><row><cell></cell><cell>+GOLD+MIS</cell><cell cols="6">48.0?0.4 62.1?0.9 54.9?0.1 57.3?0.1 40.9?0.1 51.0?0.4</cell></row><row><cell>Ours</cell><cell>+GOLD+MIS+PRED AMBIG-8</cell><cell cols="6">49.5?0.4 63.1?0.3 55.6?0.4 57.7?0.0 42.7?0.2 51.7?0.4</cell></row><row><cell></cell><cell>+GOLD</cell><cell cols="6">43.6?0.1 64.0?0.2 55.0?0.1 56.5?0.6 41.9?0.9 50.8?0.5</cell></row><row><cell></cell><cell>+GOLD+MIS</cell><cell cols="6">47.3?0.4 61.8?0.3 54.9?0.1 57.3?0.1 44.4?0.2 51.4?0.3</cell></row><row><cell></cell><cell>+GOLD+MIS+PRED</cell><cell cols="6">48.0?0.4 61.7?0.2 55.6?0.2 56.7?0.2 43.2?0.3 51.3?0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Recall (%) comparison between our proposed methods and baselines with Flan-PaLM 2 (M, L).</figDesc><table><row><cell></cell><cell>Acc. (%)</cell><cell>M</cell><cell>EDOS L</cell><cell>M</cell><cell>SST L</cell><cell>M</cell><cell>GoEmotions L</cell></row><row><cell></cell><cell>FREQ</cell><cell>46.8</cell><cell>46.8</cell><cell>23.1</cell><cell>23.1</cell><cell cols="2">11.7</cell><cell>11.7</cell></row><row><cell></cell><cell>ZERO</cell><cell>55.4</cell><cell>59.2</cell><cell>49.9</cell><cell>57.1</cell><cell cols="2">47.1</cell><cell>46.2</cell></row><row><cell>Baselines</cell><cell>STATIC-N</cell><cell cols="6">54.3?0.3 57.6?0.1 50.5?0.4 59.0?0.2 39.8?0.2 46.7?0.2</cell></row><row><cell></cell><cell>RETR-4</cell><cell cols="6">53.6?0.3 61.0?0.5 50.0?0.3 58.5?0.3 45.9?0.0 50.1?0.2</cell></row><row><cell></cell><cell>RETR-8</cell><cell cols="6">53.8?0.3 61.1?0.2 51.8?0.1 58.6?0.4 45.3?0.0 51.0?0.2</cell></row><row><cell></cell><cell>AMBIG-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+GOLD</cell><cell cols="6">54.3?0.4 61.3?0.4 51.5?0.4 58.9?0.1 46.6?0.2 50.3?0.1</cell></row><row><cell></cell><cell>+GOLD+MIS</cell><cell cols="6">56.1?0.2 60.9?0.6 52.1?0.1 59.7?0.1 45.6?0.1 49.5?0.1</cell></row><row><cell>Ours</cell><cell>+GOLD+MIS+PRED AMBIG-8</cell><cell cols="6">56.5?0.1 61.4?0.4 53.0?0.4 60.1?0.1 45.6?0.1 50.0?0.2</cell></row><row><cell></cell><cell>+GOLD</cell><cell cols="6">53.6?0.2 61.8?0.0 52.9?0.1 59.5?0.6 46.8?0.1 50.4?0.2</cell></row><row><cell></cell><cell>+GOLD+MIS</cell><cell cols="6">55.4?0.6 61.1?0.3 53.2?0.1 60.2?0.1 45.8?0.2 50.0?0.3</cell></row><row><cell></cell><cell>+GOLD+MIS+PRED</cell><cell cols="6">55.1?0.6 61.5?0.3 54.2?0.2 59.6?0.3 44.9?0.2 50.2?0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (%) comparison between our proposed methods and baselines with Flan-PaLM 2 (M, L).</figDesc><table><row><cell></cell><cell>EDOS</cell><cell>SST</cell><cell>GoEmotions</cell></row><row><cell cols="2">Flan-PaLM 2 (M) 91.2</cell><cell>85.8</cell><cell>61.2</cell></row><row><cell>Flan-PaLM 2 (L)</cell><cell>88.2</cell><cell>87.6</cell><cell>61.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Percentage of times the test example's gold label is in L ambig,t (as obtained from ZERO model). Animosity 99.1, Derogation 97.4, Prejudiced 52.1, Threat 71.9 L Animosity 90.1, Derogation 90.1, Prejudiced 68.1, Threat 93.3 SST M Bad 78.4, Good 98.0, Great 88.0, Okay 73.8, Terrible 93.9 L Bad 89.3, Good 99.2, Great 99.2, Okay 59.1, Terrible 85.3 GoEmotions M Admiration 72.7, Amusement 90.3, Anger 58.8, Annoyance 44.3, Approval 24.6, Caring 52.9, Confusion 73.2, Curiosity 64.2, Desire 35.7, Disappointment 58.0, Disapproval 52.3, Disgust 55.3, Embarrassment 30.4, Excitement 56.4, Fear 75.4, Gratitude 75.7, Grief 100.0, Joy 80.4, Love 86.8, Nervousness 83.3, Optimism 51.4, Pride 42.9, Realization 27.0, Relief 28.6, Remorse 38.6, Sadness 71.6, Surprise 62.1 L Admiration 40.2, Amusement 84.9, Anger 52.7, Annoyance 40.2, Approval 36.0, Caring 36.5, Confusion 74.2, Curiosity 64.8, Desire 58.9, Disappointment 59.1, Disapproval 81.0, Disgust 38.2, Embarrassment 30.4, Excitement 61.8, Fear 73.8, Gratitude 88.4, Grief 100.0, Joy 84.8, Love 86.2, Nervousness 83.3, Optimism 65.4, Pride 71.4, Realization 41.6, Relief 42.9, Remorse 70.5, Sadness 67.6, Surprise 64.4</figDesc><table /><note><p>EDOS M</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Label-wise percentage of times the test example's gold label is in L ambig,t (as obtained from ZERO),</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Please refer toAnil et al. (2023)  for more details on the models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We exclude 24,848 examples (19,925  from training set, 2,474 and 2,449 from dev and test set, respectively) that have multiple labels annotated for a single input, for a simpler experimental setting. We refer the reader to<ref type="bibr" target="#b11">Demszky et al. (2020)</ref> for more information on the single-label setting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We chose k =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>4, 8 for two reasons a) to limit the sequence length to 1024 tokens for faster inference and b) in some settings we found k = 4 often outperforming k = 8 (Table2) which led us to believe that adding more examples will not benefit much.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>We report the accuracy, precision and recall in A.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>We set a strict constraint on our selection in that if there aren't sufficient examples for any of our setting (e.g. +GOLD+MIS+PRED within the top-250 retrieved example, we fall-back on the previous setting (e.g. GOLD+MIS).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Case-based reasoning: Foundational issues, methodological variations, and system approaches</title>
		<author>
			<persName><forename type="first">Agnar</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.3233/AIC-1994-7104</idno>
	</analytic>
	<monogr>
		<title level="j">AI Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="59" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incontext examples selection for machine translation</title>
		<author>
			<persName><forename type="first">Sweta</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.564</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8857" to="8873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What learning algorithm is in-context learning? investigations with linear models</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Aky?rek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hern?ndez ?brego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Crepy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachi</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Fienber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.10403</idno>
		<imprint/>
	</monogr>
	<note>and et al. 2023. Palm 2 technical report. CoRR, abs/2305.10403</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>et al. 2020a</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformers generalize differently from information stored in context vs in weights</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkyung</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.05675</idno>
		<idno>CoRR, abs/2210.05675</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">UPRISE: universal prompt retrieval for improving zero-shot evaluation</title>
		<author>
			<persName><forename type="first">Daixuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denvy</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08518</idno>
		<idno>CoRR, abs/2303.08518</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.02311</idno>
		<editor>M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,</editor>
		<imprint>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.11416</idno>
		<idno>CoRR, abs/2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Casebased reasoning for natural language queries over knowledge bases</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Yoon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazaros</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.755</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9594" to="9611" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GoEmotions: A dataset of fine-grained emotions</title>
		<author>
			<persName><forename type="first">Dorottya</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.372</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4040" to="4054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incontext learning for few-shot dialogue state tracking</title>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Hsuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-emnlp.193</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2627" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards unsupervised dense information retrieval with contrastive learning</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno>CoRR, abs/2112.09118</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2023 task 10: Explainable detection of online sexism</title>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertie</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>R?ttger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.semeval-1.305</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)</title>
		<meeting>the 17th International Workshop on Semantic Evaluation (SemEval-2023)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2193" to="2210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reordering examples helps during priming-based few-shot learning</title>
		<author>
			<persName><forename type="first">Sawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.395</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4507" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unified demonstration retriever for incontext learning</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.256</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4644" to="4668" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.deelio-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out<address><addrLine>Dublin, Ireland and Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. DeeLIO 2022</date>
			<biblScope unit="page" from="100" to="114" />
		</imprint>
	</monogr>
	<note>What makes good in-context examples for GPT-3?. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">2023. 23-29 July 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="22631" to="22648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8086" to="8098" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dr.icl: Demonstration-retrieved in-context learning</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaiva</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Imbrasaite</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.14128</idno>
		<idno>CoRR, abs/2305.14128</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Z-ICL: Zero-shot in-context learning with pseudo-demonstrations</title>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2304" to="2317" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Active learning principles for in-context learning with large language models</title>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Margatina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.759</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11048" to="11064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synchromesh: Reliable code generation from pre-trained language models</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Poesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to retrieve prompts for in-context learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.191</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2655" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">XRICL: Cross-lingual retrieval-augmented incontext learning for cross-lingual text-to-SQL semantic parsing</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-emnlp.384</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5248" to="5259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An effective refinement strategy for KNN text classifier</title>
		<author>
			<persName><forename type="first">Songbo</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2005.07.019</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="290" to="298" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandra</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Hoffman-John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alena</forename><surname>Rajakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Butryna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktoriya</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Kuzmina</surname></persName>
		</author>
		<author>
			<persName><surname>Fenton</surname></persName>
		</author>
		<editor>Claire Cui, Marian Croak</editor>
		<imprint>
			<pubPlace>Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Ag?era y Arcas</pubPlace>
		</imprint>
	</monogr>
	<note>Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR, abs/2201.08239</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mis-classified vector guided softmax loss for face recognition</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6906</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12241" to="12248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Investigating the learning behaviour of in-context learning: A comparison with supervised learning</title>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.15411</idno>
		<idno>CoRR, abs/2307.15411</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Larger language models do in-context learning differently</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.03846</idno>
		<idno>CoRR, abs/2303.03846</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An explanation of in-context learning as implicit bayesian inference</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ground-truth labels matter: A deeper look into input-label demonstrations</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyeob</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Hyuhng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwiyeol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2422" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Active example selection for in-context learning</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9134" to="9148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">2021a. Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">2021b. Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
