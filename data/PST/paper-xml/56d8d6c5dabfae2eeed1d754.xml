<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Suggesting Accurate Method and Class Names</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
							<email>m.allamanis@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics ‡ Dept. of Computer Science</orgName>
								<orgName type="institution">Microsoft Research University of Edinburgh University College London Microsoft Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>London, Redmond</settlement>
									<region>WA</region>
									<country>UK, UK, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
							<email>e.barr@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics ‡ Dept. of Computer Science</orgName>
								<orgName type="institution">Microsoft Research University of Edinburgh University College London Microsoft Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>London, Redmond</settlement>
									<region>WA</region>
									<country>UK, UK, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Bird</surname></persName>
							<email>cbird@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics ‡ Dept. of Computer Science</orgName>
								<orgName type="institution">Microsoft Research University of Edinburgh University College London Microsoft Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>London, Redmond</settlement>
									<region>WA</region>
									<country>UK, UK, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
							<email>csutton@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics ‡ Dept. of Computer Science</orgName>
								<orgName type="institution">Microsoft Research University of Edinburgh University College London Microsoft Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>London, Redmond</settlement>
									<region>WA</region>
									<country>UK, UK, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Suggesting Accurate Method and Class Names</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C234FF3722480D65146C4A649A53165</idno>
					<idno type="DOI">10.1145/2786805.2786849</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D</term>
					<term>2</term>
					<term>3 [Software Engineering]: Coding Tools and Techniques Coding conventions, naturalness of software &quot;You shall know a word by the company it keeps</term>
					<term>&quot; -J</term>
					<term>R</term>
					<term>Firth</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names. However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens. Furthermore, we introduce a variant of our model that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Language starts with names. While programming, developers must name variables, parameters, functions, classes, and files. They strive to choose names that are meaningful and conventional, i.e. consistent with other names used in related contexts in their code base. Indeed, leading industrial experts, including Beck <ref type="bibr">[9]</ref>, Mc-Connell <ref type="bibr" target="#b34">[34]</ref>, and Martin <ref type="bibr" target="#b33">[33]</ref>, have stressed the importance of identifier naming in software. Finding good names for programming language constructs is difficult; poor names make code harder to understand and maintain <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b7">7]</ref>. Empirical evidence suggests that poor names also lead to software defects <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b1">1]</ref>. Code maintenance exacerbates the difficulty of finding good names, because the appropriateness of a name changes over time: an excellent choice, at the time a construct is introduced, can degrade into a poor name, as when a variable is used in new context or a function's semantics changes.</p><p>Names of methods and classes are particularly important, and can be difficult to choose. Høst et al. eloquently captured their importance: "Methods are the smallest named units of aggregated behavior in most conventional programming languages and hence the cornerstone of abstraction" <ref type="bibr" target="#b26">[26]</ref>. Semantically distinct method names are the basic tools for reasoning about program behaviour. Programmers directly think in terms of these names and their compositions, since a programmer chose them for the units into which the programmer decomposed a problem. Moreover, method names can be hard to change, especially when they are used in an API. When published in a popular library, method naming decisions are especially rigid and poor names can doom a project to irrelevance.</p><p>In this paper, we suggest that modern statistical tools allow us to automatically suggest descriptive, idiomatic method and class names to programmers. We tackle the method naming problem: the problem of inferring a method's name from its body (or a class from its methods). As developers spend approximately half of their development time trying to understand and comprehend code during maintenance alone <ref type="bibr" target="#b17">[17]</ref>, any progress toward solving the method naming problem will improve the comprehensibility of code <ref type="bibr" target="#b49">[49]</ref> leading to an increase programmer productivity <ref type="bibr" target="#b24">[24]</ref>.</p><p>In previous work, we introduced the NATURALIZE framework <ref type="bibr" target="#b2">[2]</ref>, which learns the coding conventions used in a code base and tackles one naming problem programmers face -that of naming variables -by exploiting the "naturalness" or predictability of code <ref type="bibr" target="#b25">[25]</ref>. However, the method naming problem is much more difficult than the variable naming problem, because the appropriateness of method and class names depends not solely on their uses but also on their internal structure -their body or their set of methods. An adequate name must describe not just what the method is, but what it does. Variable names, by contrast, can often be predicted solely from a few tokens of local context; for example, it is easy to predict the variable name that follows the tokens for ( int. Because method and class names must be functionally descriptive, they often have rich internal structure: method names are often verb phrases and class names are often noun phrases. But this means that method and class names are often neologisms, that is, names not seen in the training corpus. Existing probabilistic models of source code, including the n-gram models used in NATURALIZE, cannot suggest neologisms. These aspects of the method naming problem severely exacerbate the data sparsity problem faced by all probabilistic language models, because addressing them by building models that consider more context necessarily means that any individual context will be observed less often. Therefore, the method naming problem requires models that can better exploit the structure of code, taking into account long range dependencies and modeling the context surrounding their definitions more precisely than at the token-level, while minimizing the effects of data sparsity.</p><p>This paper tackles the method naming problem with a novel, neural logbilinear context model for code, inspired by neural probabilistic language models for natural language, which have seen many recent successes <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b31">31]</ref>. A particularly impressive success of these models has been that they assign words to continuous vectors that support analogical reasoning. For example, vector('king') -vector('man') + vector('woman') results in a vector close to vector('queen') <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref>. Although many of the basic ideas have a long history <ref type="bibr" target="#b10">[10]</ref>, this class of model is receiving increasing recent interest because of increased computational power from GPUs and because of more efficient learning algorithms such as noise contrastive estimation <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Intuitively, our model assigns to every identifier name used in a project a continuous vector in a high dimensional space, in such a way that identifiers with similar vectors, or "embeddings", tend to appear in similar contexts. Then, to name a method (or a class), we select the name that is most similar in this embedding space to those in the function body. In this way, our model realizes Firth's famous dictum, "You shall know a word by the company it keeps". This slogan encapsulates the distributional hypothesis, that semantically similar words tend to co-occur with the same other words. Two words are distributionally similar if they have similar distributions over surrounding words. For example, even if the words "hot" and "cold" never appear in the same sentence, they will be distributionally similar if they both often co-occur with words like "weather" and "tea". The distributional hypothesis is a cornerstone of much work in computational linguistics, but we are unaware of previous work that explores whether this hypothesis holds in source code. Earlier work on the naturalness of code <ref type="bibr" target="#b25">[25]</ref> found that code tends to repeat constructs and exploited this repetition for prediction, but did not consider the semantics of tokens. In contrast, the distributional hypothesis states that you shall recognize semantically similar tokens because they tend also to be distributionally similar.</p><p>Indeed, we qualitatively show in Section 4 that our context model produces embeddings that demonstrate implicit semantic knowledge about the similarity of identifiers. For instance, it successfully distinguishes getters and setters, assigns function names with similar functionality (like grow and resize) to similar locations, and discovers matching components of names, which we call subtokens, like min and max, and height and width.</p><p>Furthermore, to allow us to suggest neologisms, we introduce a new subtoken context model that exploits the internal structure of identifier names. In this model, we predict names by breaking them into parts, which we call subtokens, such as get, create, and Height, and then predicting names one subtoken at a time. The subtoken model automatically infers conventions about the internal structure of variable names, such as "an interface starts with an I", or "an abstract class starts with Abstract". Our subtoken model also learns conventions like prefixing names of boolean methods with is or has. This model also allows us to propose neologisms, by proposing sequences of subtokens that have not been seen before. Consider Figure <ref type="figure">1</ref>; our subtoken model builds and explores an embedding space that allows it to suggest createShaders, which is usefully close to the name a programmer actually chose.</p><p>Our contributions follow:</p><p>• We introduce a log-bilinear neural network to model code contexts that, unlike standard language models in NLP, integrates information from preceding, succeeding, and non-local tokens. Use Cases Our suggestion model can be embedded within a variety of tools to support code development and code review. During development, suppose that the developer is adding a method or a class to an existing project. After writing the body, the developer may be unsure if the name she chose is descriptive and conventional within the project. Our model suggests alternative names from patterns it learned from other methods in the project. During code review, our model can highlight those names to which our model assigns a low score. In either case, the system has two phases: a training phase, which takes as input a training set of source files (e.g. the current revision of the project) and returns a neural network model that can suggest names; and a testing or deployment phase, in which the input is a trained neural network and the source code of a method or class, and the output is a ranked list of suggested names.</p><p>Any suggestion system has the potential to suffer from what we have called the "Clippy effect" <ref type="bibr" target="#b2">[2]</ref>, in which too many low quality suggestions alienate the user. To prevent this, our suggestion model also returns a numeric score that reflects its degree of confidence in its suggestion; practical tools would only make a suggestion to the user if the confidence were sufficiently high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">NEURAL CONTEXT MODELS OF CODE</head><p>In this section, we introduce four language models of code, starting with the n-gram model to build intuition. Then we introduce neural probabilistic language modelling and follow it with two novel models that, specifically designed for method naming, refine the underlying neural model: our logbilinear context model, which adds context and features, and subtoken context model, which adds subtokens and can be used to generate neologisms.</p><p>Language models (LM) are probability distributions over strings of a language. These models assume that we are trying to predict a token t given a sequence of other tokens c = (c 0 , c 1 , . . . c N ) that we call the context. LMs are very general; for example, if the goal is to sequentially predict every token in a file, as a n-gram model does, then we can take t = y m and c = (y m-n+1 y m-n+2 . . . y m-1 ). Alternately, for the method naming problem, we can take t to be the identifier token in the declaration that names the function, and c to be a sequence that contains all identifiers in the function body. Obviously, we cannot store a probability value for every possible context, so we must make simplifying assumptions to make the modeling tractable. Different LMs make different simplifying assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>To build intuition, we begin by reviewing the n-gram LM, which is a standard technique in NLP and speech processing, and which has become increasingly popular in software engineering <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b2">2]</ref>. The n-gram model assumes that all of the information required to predict the next token is contained within the previous n -1 tokens i.e. P(y 1 . . . y M ) = ∏ M m=1 P(y m |y m-1 . . . y m-n+1 ). To specify this model we need (in principle) a table of V n numbers, where V is the number of possible lexemes, that specifies the conditional probabilities for each possible n-gram. These are the parameters of the model that we learn from data.</p><p>There is a large literature on methods for training these models <ref type="bibr" target="#b16">[16]</ref>, which basically revolve around counting the proportion of times that token y m follows y m-1 . . . y m-n+1 . However, even when n = 4 or n = 5, we cannot expect to estimate the counts of all ngrams reliably, as the number of possible n-grams is exponential in n. Therefore, smoothing methods are employed, which generally modify the count of a rare n-gram y 1 . . . y n to make it more similar to the count of a shorter suffix y 2 . . . y n , whose frequency we can estimate more reliably. This procedure involves the implicit assumption that two contexts are most similar if they share a long suffix. But this assumption does not always hold. Many similar contexts, such as x + y versus x + z, might be treated very differently by a n-gram model, because the final token is different. Logbilinear models Neural LMs <ref type="bibr" target="#b10">[10]</ref> address the challenge that the simple n-gram model has by making similar predictions for similar contexts. They predict the next token y m using a neural network that takes the previous tokens as input. This allows the network to flexibly learn which tokens, like int, provide much information about the immediately following token, and which tokens, like the semicolon ';', provide very little. Unlike an n-gram model, a neural LM makes it easy to add general long-distance features of the context into the prediction -we simply add them as additional inputs to the neural net. In our work, we focus on a simple type of neural LM that has been effective in practice, namely, the log-bilinear LM <ref type="bibr" target="#b37">[37]</ref> (LBL). We start with a general treatment of loglinear models considering models of the form</p><formula xml:id="formula_0">P(t|c) = exp(s θ (t, c)) ∑ t exp(s θ (t , c))</formula><p>.</p><p>Intuitively, s θ is a function that indicates how much the model likes to see both t and c together, the exp function maps this to be always positive, and the denominator ensures that the result is a probability distribution. This choice is very general. For example, if s θ is a linear function of the features in c, then the discriminative model is simply a logistic regression.</p><p>Logbilinear models learn a map from every possible target t to a vector q t ∈ R D , and from each context c to a vector rc ∈ R D . We interpret these as locations of each context and each target lexeme in a D dimensional space; these locations are called embeddings. The model predicts that the token t is more likely to appear in context c if the embedding q t of the token is similar to that rc of the context.</p><p>To encode this in the model, we choose</p><formula xml:id="formula_2">s θ (t, c) = r c q t + b t ,<label>(2)</label></formula><p>where b t is a scalar bias which represents how commonly t occurs regardless of the context. To understand this equation intuitively, note that, if the vectors rc and q t have norm 1, then their dot product is simply the cosine of the angle between them. So s θ , and hence p(t|c), is larger if either vector has a large norm, if b t is large, or if rc and q t have a small angle between them, that is, if they are more similar according to the commonly used cosine similarity metric.</p><p>To complete this description, we define the maps t → q t and c → rc . For the targets t, the most common choice is to simply include the vector q t for every t as a parameter of the model. That is, the training procedure has the freedom to learn an arbitrary map between t and q t . For the contexts c, this choice is not possible, as there are too many possible contexts. Instead, a common choice <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b39">39]</ref> is to represent the embedding rc of a context as the sum of embeddings of the tokens within it, that is,</p><formula xml:id="formula_3">rc = |C| ∑ t=1 C t r c t ,<label>(3)</label></formula><p>where r c t ∈ R D is a vector for each lexeme that is included in the model parameters. The variable t indexes every token in the context c, so if the same lexeme occurs multiple times in c, then it appears multiple times in the sum. The matrix C t is a diagonal matrix that serves as a scaling factor depending on the position of a lexeme within the context. This allows, for example, a lexeme's influence on c's position to depend on how close it is to the target. The D non-zero values in C t for each t are also included in the model parameters. Each lexeme v has two embeddings: an embedding q v for when it is used as a target and an embedding r v for when it appears in the context. To summarize, logbilinear models make the assumption that every token and every context can be mapped in a D-dimensional space. There are two kinds of embedding vectors: those directly learned (i.e. the parameters of the model) and those computed from the parameters of the model. To indicate this distinction, we place a hat on rc to indicate that it is computed from the model parameters, whereas we write q t without a hat to indicate that it is a parameter vector that is learned directly by the training procedure. These models can also be viewed as a three-layer neural network, in which the input layer encodes all of the lexemes in c using a 1-of-V encoding, the hidden layer outputs the vectors r c t for each token in the context, and the output layer computes the score functions s θ (t, c) and passes them to a softmax nonlinearity. For details on the neural network representation, see Bengio et al. <ref type="bibr" target="#b10">[10]</ref>.</p><p>To learn these parameters, it has recently been shown <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b38">38</ref>] that an alternative to the maximum likelihood method called noise contrastive estimation (NCE) <ref type="bibr" target="#b21">[21]</ref> is effective. NCE measures how well the model p(t|c) can distinguish the real data in the training set from "fantasy data" that is generated from a simple noise distribution. At a high level, this can be viewed as a black box alternative to maximum likelihood that measures how well the model fits the training data. We optimize the model parameters using stochastic gradient descent. We employ NCE for all models in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Logbilinear Context Models of Code</head><p>Now we present a new neural network, a novel LBL LM for code, which we call a logbilinear context model. The key idea Variable: isDone</p><formula xml:id="formula_4">q isDone rcontext R D s θ (.) = r context q isDone + b isDone Features: boolean , in:MethodBody , final r boolean + r in:MethodBody + r final rcontext = ∑ f ∈F tc r f + 1 |I t | ∑ i∈I t ∑ ∀k:K≥|k|&gt;0 C k r t i+k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contexts:</head><p>final boolean isDone = false ;  <ref type="formula" target="#formula_6">4</ref>; the final paragraph of Section 2.2 explains the sum over the I t locations. Each source code token and feature maps to a learned Ddimensional vector in continuous space. The token-vectors are multiplied with the position-dependent context matrix C i and summed, then added to the sum of all the feature-vectors. The resulting vector is the D-dimensional representation of the current source code identifier. Finally, the inner product of the context and the identifier vectors is added to a scalar bias b, producing a score for each identifier. This neural network is implemented by mapping its equations into code.</p><formula xml:id="formula_5">C -2 r final + C -1 r boolean + C 1 r = + C 2 r false while ( ! isDone ) { C -2 r ( + C -1 r ! + C 1 r ) + C 2 r }</formula><p>is that logbilinear models make it especially easy to exploit longdistance information; e.g. when predicting the name of a method, it is useful to take into account all of the identifiers that appear in the method body. We model long-distance context via a set of feature functions, such as "Whether any variable in the current method is named addCount", "Whether the return type of the current method is int," and so on. The logbilinear context model combines these features with the local context. As before, suppose that we are trying to predict a code token t given a sequence of context tokens c = (c 0 , c 1 , . . . , c N ). We assume that c contains all of the other tokens in the file that are relevant for predicting t; e.g. tokens from the body of the method that t names. The tokens in c that are nearest to the target t are treated specially. Suppose that t occurs in position i of the file, that is, if the file is the token sequence t 1 ,t 2 , . . ., then t = t i . Then the local context is the set of tokens that occur within K positions of t, that is, the set {t i+k } for -K ≤ k ≤ K, k = 0. The local context includes tokens that occur both before and after t.</p><p>The overall form of the context model will follow the generic form in (1) and ( <ref type="formula" target="#formula_2">2</ref>), except that the context representation rc is defined differently. In the context model, we define rc using two different types of context: local and global. First, the local context is handled in a very similar way to the logbilinear LM. Each possible lexeme v is assigned to a vector r v ∈ R D , and, for each token t k that occurs within K tokens of t in the file, we add its representation r t k into the context representation.</p><p>The global context is handled using a set of features. Each feature is a binary function based on the context tokens c, such as the examples described at the beginning of this section. Formally, each feature f maps a c value to either 0 or 1. Maddison and Tarlow <ref type="bibr" target="#b31">[31]</ref> use a similar idea to represent features of a syntactic context, that is, a node in an AST. Here, we extend this idea to incorporate arbitrary features of long-distance context tokens c. The first column of Table <ref type="table">4</ref> presents the full list of features that we use in this work.</p><p>To learn an embedding, we assign each feature function to a single vector in the continuous space, in the same way as we did for tokens. Mathematically, let F be the set of all features in the model, and let F c , for a context c, be the set of all features f with f (c) = 1. Then for each feature f ∈ F, we learn an embedding r f ∈ R D , which is included as a parameter to the model in exactly the same way that r t was for the language modeling case. Now, we can formally define a context model of code as a probability distribution P(t|c) that follows the form (1) and <ref type="bibr" target="#b2">(2)</ref>, where rc = rcontext , where rcontext is</p><formula xml:id="formula_6">rcontext = ∑ f ∈F tc r f + ∑ ∀k:K≥|k|&gt;0 C k r t i+k ,<label>(4)</label></formula><p>where, as before, C k is a position-dependent D × D diagonal context matrix that is also learned during training <ref type="foot" target="#foot_0">1</ref> . Intuitively, this equation sums the embeddings of each token t k that occurs near t in the file, and sums the embeddings of each feature function f that returns true (i.e., 1) for the context c. Once we have this vector rcontext , just as before, we can select a token t such that the probability P(t|c) is high, which happens exactly when r context q t is high -in other words, when the embedding q t of the proposed target t is close to the embedding rcontext of the context. Figure <ref type="figure" target="#fig_0">2</ref> gives a visual explanation of the probabilistic model. This figure depicts how the model assigns probability to the token isDone if the preceding two tokens are final boolean and the succeeding two are = false. Reading from right to left, the figure describes how the continuous embedding of the context is computed. Following the dashed (pink) arrows, the tokens in the local context are each assigned to D-dimensional vectors r final , r boolean , and so on, which are added together (after multiplication by the C -k matrices that model the effect of distance), to obtain the effect of the local context on the embedding rcontext . The solid (blue) arrows represent the global context, pointing from the names of the feature functions that return true to the continuous embeddings of those features. Adding the feature embeddings to the local context embeddings yields the final context embedding rcontext . The similarity between this vector and embedding of the target vector q isDone is computed using a dot product, which yields the value of s θ (isDone, c) which is necessary for computing the probability P(isDone|c) via <ref type="bibr" target="#b1">(1)</ref>.</p><p>Multiple Target Tokens Up to now, we have presented the model in the case where we are renaming a target token t that occurs at only one location, such as the name of a method. Other cases, such as when suggesting variable names, require taking all of the occurrences of a name into account <ref type="bibr" target="#b2">[2]</ref>. When a token t appears at a set of locations I t , we compute the context vectors rcontext separately for each token t i , for i ∈ I t , then average them. When we do this, we carefully rename all occurrences of t to a special token called SELF to remove t from its own context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Subtoken Context Models of Code</head><p>A limitation of all of the previous models is that they are unable to predict neologisms, that is, unseen identifier names that have not been used in the training set. The reason for this is that we allow the map from a lexeme v to its embedding q v to be arbitrary (i.e. without learning a functional form for the relationship), so we have no basis to assign continuous vectors to identifier names that have not been observed. In this section, we sidestep this problem by exploiting the internal structure of identifier names, resulting in a new model which we call a subtoken context model.</p><p>The subtoken context model exploits the fact that identifier names are often formed by concatenating words in a phrase, such as getLocation or setContentLengthHeader. We call each of the smaller words in an identifier a subtoken. We split identifier names into subtokens based on camel case and underscores, resulting in a set of subtokens that we use to compose new identifiers. To do this, we exploit the summation trick we used in rcontext . Recall that we constructed this vector as a sum of embedding vectors for particular features in the context. Here, we define the embedding of a target vector to be the sum of the embeddings of its subtokens.</p><p>Let t be the token that we are trying to predict from a context c. As in the context model, c can contain tokens before and after t, and tokens from the global context. In the subtoken model, we additionally suppose that t is split up into a sequence of M subtokens, that is, t = s 1 s 2 . . . s M , where s M is always a special END subtoken that signifies the end of the subtoken sequence. That is, the context model now needs to predict a sequence of subtokens in order to predict a full identifier. We begin by breaking up the prediction one subtoken at a time, using the chain rule of probability:</p><formula xml:id="formula_7">P(s 1 s 2 . . . s M |c) = ∏ M m=1 P(s m |s 1 . . . s m-1 , c)</formula><p>. Then, we model the probability P(s m |s 1 . . . s m-1 , c) of the next subtoken s m given all of the previous ones and the context. Since preliminary experiments with an n-gram version of a subtoken model showed that n-grams did not yield good results, we employ a logbilinear model</p><formula xml:id="formula_8">P(s m |s 1 . . . s m-1 , c) = exp{s θ (s m , s 1 . . . s m-1 , c)} ∑ s exp{s θ (s m , s 1 . . . s m-1 , c)} . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>As before, s θ (s m , s 1 . . . s m-1 , c) can be interpreted as a score, which can be positive or negative and indicates how much the model "likes" to see the subtoken s m , given the previous subtokens and the context. The exponential functions and the denominator are a mathematical device to convert the score into a probability distribution. We choose a bilinear form for s θ , with the difference being that in addition to tokens having embedding vectors, subtokens have embeddings as well. Mathematically, we define the score as</p><formula xml:id="formula_10">s θ (s m , s 1 . . . s m-1 , c) = r SUBC q s m + b s m ,<label>(6)</label></formula><p>where q s m ∈ R D is an embedding for the subtoken s m , and rSUBC is a continuous vector that represents the previous subtokens and the context. To define a continuous representation rSUBC of the context, we break this down further into a sum of other embedding features as rSUBC = rcontext + rSUBC-TOK .</p><p>In other words, the continuous representation of the context breaks down into a sum of two vectors: the first term rcontext represents the effect of the surrounding tokens c -both local and global -and is defined exactly as in the context model via <ref type="bibr" target="#b4">(4)</ref>.</p><p>The new aspect is how we model the effect of the previous subtokens s 1 . . . s m-1 in the second term rSUBC-TOK . We handle this by assigning each subtoken s a second embedding vector r s ∈ R D that represents its influence when used as a previous subtoken; we call this a history embedding. We weight these vectors by a diagonal matrix C SUBC -k , to allow the model to learn that subtokens have decaying influence the farther that they are from the token that is being predicted. Putting this all together, we define We estimate all of these parameters from the training corpus.</p><formula xml:id="formula_12">rSUBC-TOK = M ∑ i=1 C SUBC -i r s m-i .<label>(8)</label></formula><p>Although this may seem a large number of parameters, this is typical for language models, e.g., consider the V 5 parameters, if V is the number of lexemes, required by a 5-gram language model. How can we handle so many parameters? The reason is simple: in the era of vast, publicly available source code repositories like GitHub and Bitbucket, code scarcity is a thing of the past. Generating Neologisms A final question is "Given the context c, how do we find the lexeme t that maximizes P(t|c)?". Previous models could answer this question simply by looping over all possible lexemes in the model, but this is impossible for a subtoken model, because there are infinitely many possible neologisms. So we employ beam search (see Russell and Norvig <ref type="bibr" target="#b44">[44]</ref> for details) to find the B tokens (i.e., subtoken sequences) with the highest probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Source Code Features for Context Models</head><p>In this section, we describe the features we use to capture global context. Identifying software measures and features that effectively capture semantic properties like comprehensibility or bug-proneness is a seminal software engineering problem that we do not tackle in this paper. Here, we have selected measures and features heavily used in the literature and industry. For instance, control flow is indisputably important; we selected Cyclomatic complexity, despite its correlation with code size, to measure it. The first column of Table <ref type="table">4</ref> defines the features we used in this work. In the table, "Variable Type" tracks whether the type is generic, its type after erasure, and, if the type is an array, its size. "Contained Methods" and "Sibling Methods" exclude method overloads and recursion.</p><p>The features of a target token are its target features; we assign a r f vector to each of them; this vector is added in the left summation of Equation 4 if a feature's indicator function f returns 1 for a particular token. Although features are binary, we describe somelike the modifiers of a declaration, the node type of a AST, etc. -as categorical. All categorical features are converted into binary using a 1-of-K encoding. For methods, we include Cyclomatic complexity, clipping it to 10 and treating it as categorical. When features do not make sense for a particular token, like the Cyclomatic complexity of a variable, the feature's function simply returns zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head><p>The core challenge of solving the method naming problem from code is data sparsity. Our guiding intuition is that source code contains rich structure that can alleviate the sparsity problem. We therefore pose the following question: How can we better maximally exploit the structure inherent to source code? This question in turn leads us to the research questions:</p><p>RQ1. Can we identify and extract long and short-range context features of identifiers for naming? RQ2. Do identifiers contain exploitable substructure? Answering both of these questions in the affirmative, we turn our attention to exploiting the resulting naming information; here, we ask if this new information is sufficiently rich to allow us to accurately suggest names. More concretely:</p><p>RQ3. Can we accurately suggest method declaration names, looking only at the context of the declared method?</p><p>RQ4. Can we do the same for class (i.e. type) names? In reference to the first two research questions we describe the features that we use and how we capture substructure in the next section. We then definitively answer these research questions by comparing our approach with previous techniques for suggesting variable names on a broad software corpus. There is little to no research that tackles the second two research questions to compare against. Nonetheless, we use an n-gram model as a point of comparison for naming methods and classes to demonstrate the performance of our approach as that model has performed the best for variable naming in the past and we hypothesize is reasonable for these new naming tasks. The rest of this section describes our experimental setup and methodology. Data We picked the top active Java GitHub projects on January 22nd 2015. We obtained the most popular projects by taking the sum of the z-scores of the number of watchers and forks of each project, using the GitHub Archive. Starting from the top project, we selected the top 20 projects excluding projects that were in a domain that was previously selected. We also included only projects with more than 50 collaborators and more than 500 commits. The projects along with short descriptions are shown in Table <ref type="table" target="#tab_2">1</ref>. We used this procedure to select a mature, active, and diverse corpus with large development teams. Finally, we split the files uniformly into a training (70%) and a test (30%) set. Methodology We train all models on the train sets formed over the files of each project. To evaluate the models, for each of the test files and for each variable (all identifiers that resolve to the same symbol), method declaration or class declaration, we compute the features and context of the location of the identifier and ask the model to predict the actual target token the developer used (which is unknown to the model), as in Allamanis et al. <ref type="bibr" target="#b2">[2]</ref>. In the use cases we consider (see Section 1), models that are deployed with a "confidence filter", that is, the model will only present a suggestion to the user when the probability of the top ranked name is above some threshold. This is to avoid annoying the user with low-quality suggestions. To reflect this in our evaluation, we measure the degree to which the quality of the results changes as a function of the threshold. Rather than reporting the threshold, which is not directly interpretable, we instead report the suggestion frequency, which is the percentage of names in the test set for which the model decides to make a prediction for a given threshold.</p><p>To measure the quality of a suggestion, we compute the F1 score and the accuracy for the retrieval over the subtokens of each correct token. Thus, all methods are given partial credit if the predicted name is not an exact match but shares subtokens with the correct name. F1 is the harmonic mean of precision and recall and is a standard measure <ref type="bibr" target="#b32">[32]</ref> because it conservative: as a harmonic mean, its value is influenced most by the lowest of precision and recall. We also compute the accuracy of each prediction: a prediction is correct when the model predicts exactly (exact match) the actual token. When computing the F1 score for suggestion rank k &gt; 1, we pick the precision, recall, and F1 of the rank l ≤ k that results in the highest F1 score.</p><p>Because this evaluation focuses on popular projects, the results may not reflect performance on a low quality project in which many names are poor. For such projects, we recommend training on a different project that has high quality names, but leave evaluating this approach to future work. Alternatively, one could argue that, because we measure whether the model can reproduce existing names, the evaluation is too harsh: if a predicted name does not match the existing name, it could be equally good, or even an improvement. Nonetheless, matching existing names in high quality projects, as we do, still provides evidence of overall suggestion quality, and, compared to a user study, an automatic evaluation has the great advantage that it allows efficient comparison of a larger number of different methods.</p><p>Finally, during training, we substitute rare identifiers, subtokens and features (i.e. those seen less than two times in the training data) with a special UNK token. During testing, when any of the models suggests the UNK token, we do not make any suggestions; that is, the UNK token indicates that the model expects a neologism that it cannot predict. For the subtoken model, during testing, we may produce suggestions that contain UNK subtokens. In the unlikely case that a context token t i+k = t i (i.e. is the same token), we replace t i+k with a special SELF token. This makes sure that the context of the model includes no information about the target token.</p><p>Training Parameters We used learning rate 0.07, D = 50, minibatch size 100, dropout 20% <ref type="bibr" target="#b48">[48]</ref>, generated 10 distractors for each sample for each epoch and trained for a maximum of 25 epochs picking the parameters that achieved the maximum likelihood in a held out validation set (the 10% of the training data). The context size was set to K = 6 and subtoken context size was set to M = 3. Before the training started, parameters were initialized around 0 with uniform additive noise (scaled by 10 -4 ). The bias parameters b were initialized such that P(t|c) matches the empirical (unigram) distribution of the tokens (or subtokens for the subtoken model). All the hyperparameters except for D were tuned using Bayesian optimization on bigbluebutton for method declarations. The parameter D is special in that as we increase it, the performance of each model increases monotonically (assuming a good validation set), with diminishing returns. Also, an increase in D increases the computational complexity of training and testing each model. We picked D = 50 that resulted in a good trade-off of the computational complexity vs. performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IDENTIFIER REPRESENTATION</head><p>First we evaluate our model qualitatively, by visualizing its output. All of the models that we have described assign tokens, features, and subtokens to embeddings, which are locations in a D-dimensional continuous space. These locations have been selected by the training procedure to explain statistical properties of tokens, but it does not necessarily follow that the embeddings capture anything about the semantics of names. To explore this question, we examine qualitatively whether names that appear semantically similar to us are assigned to similar embeddings, by visualizing the continuous embeddings assigned to names from a few projects. This raises the immediate difficulty of how to visualize vectors in a D = 50 dimensional space. Fortunately, there is a rich literature in statistics and machine learning about dimensionality reduction methods that map high dimensional vectors to two-dimensional vectors while preserving important properties of the original space.There are various ideas behind such techniques, such as preserving distances or angles Figure <ref type="figure">3</ref>: A 2D non-linear projection, using t-SNE <ref type="bibr" target="#b51">[51]</ref>, of embeddings of method names appearing in method declarations in the elasticsearch project. Similar methods have been grouped together, even though the model has no notion of the textual similarity of the method names, for example, the assert-like methods on the left or the new array methods on the right. between nearby points, or minimizing the distance between each point and its image in the 2D space. Classical techniques for dimensionality reduction include principal components analysis (PCA) and multidimensional scaling. We will also employ a more modern method called t-SNE <ref type="bibr" target="#b51">[51]</ref>.</p><p>Figure <ref type="figure">3</ref> displays the vectors assigned to a few method names from a typical project (elasticsearch). Each point represents the q vector of the indicated token. To interpret this, recall that the model uses the q t vectors to predict whether token t will occur in particular context. Therefore, tokens t and t with similar vectors q t and q t are tokens that the model expects to occur in similar contexts. These embeddings were generated from the logbilinear context model -that is, without using subtokens -so the model has no information about which tokens are textually similar. Rather, the only information that the model can exploit is the contexts in which the tokens are used. Despite this, we notice that many of the names which are grouped together seem to have similar functions. For example, there is a group of assertXXXX methods on the left hand side. Especially striking is the clump of construction methods on the right-hand side newDoubleArray, newIntArray, newLongArray, and so on. It is also telling that near this clump, the names grow and resize are also close together. Analysis reveals that these names do indeed seem to name methods of different classes that seem to have similar functionality. Our previous work <ref type="bibr" target="#b2">[2]</ref> indicates that developers often prefer such entities to have consistent names.</p><p>Additionally we examine the nearest neighbors of tokens in the D-dimensional space. This type of analysis avoids the risk, inherent in any dimensionality reduction method, that important information is lost in the projection from D dimensions to 2D. Table <ref type="table" target="#tab_3">2</ref> shows some identifiers on a different project, clojure, for each identifier giving a list of other identifiers that are nearest in the continuous space. The nearest neighbors of a token t are those tokens v such that the inner product of the embeddings, that is, q t q v , is maximized. We choose this measure because it most closely matches the notion of similarity in the model. Again, we are using the logbilinear context model without subtoken information. We again see that the nearest neighbors in the continuous space seem to have similar semantic function such as the triple fieldName, methodName, and className or the names returnType, typ, and type.</p><p>Table <ref type="table" target="#tab_4">3</ref> takes this analysis a bit further for the subtoken model. This table shows the "nearest nearest neighbors": those pairs of tokens or subtokens that are closest in the embedding space out of all possible pairs of tokens. On the left column, we see pairs of close neighbors from the feature-based bilinear context model without subtokens. These contain many similar pairs, such as width and height. It is striking how many of these pairs contain similar subtokens even though this model does not contain subtokens. Moving to the subtoken model, the right column of Table <ref type="table" target="#tab_4">3</ref> shows pairs of subtokens that are closest in the embedding space. The model learns that pairs like numerals, Min and Max, and Height and Width should be placed near to each other in the continuous space. This is further evidence that the model is learning semantic similarities given only statistical relationships between tokens.</p><p>We can also attempt to be a bit more specific in our analysis. In this we are inspired by Mikolov et al. <ref type="bibr" target="#b35">[35]</ref>, who noticed that adding together two of their embeddings of natural language words often yielded a compositional semantics -e.g. embedding("Paris") -embedding("France") + embedding("Vietnam") yielded a vector whose nearest neighbor was the embedding of "Hanoi". To attempt something similar for source code, we consider semantic relationships that pairs of identifiers have with each other.</p><p>For Figures <ref type="figure" target="#fig_2">4</ref> and<ref type="figure" target="#fig_3">5</ref>, we project the D-dimensional embeddings to 2D using PCA rather than t-SNE. Although a technical point, this is important. Unlike t-SNE, PCA is a linear method, that is, the mapping between the D-dimensional points and the 2D points is linear. Therefore, if groups of points are separated by a plane in the 2D space, then we know that they are separated by a plane in the higher-dimensional space as well. Figure <ref type="figure" target="#fig_2">4</ref> shows the embeddings of all pairs of setter and getter methods for the project netty. The subtoken model did not generate these models, so the model cannot cluster these tokens based on textual similarity. Nevertheless, we find that getter and setter tokens are reasonably well separated in the continuous space, because they are used in similar contexts. In Figure <ref type="figure" target="#fig_3">5</ref>, we match pairs of variable names in the libgdx project in which one name (the "plural name") equals another name (the "singular name") plus the character s. The Java convention that Collection objects are often named by plural variable names motivates this choice. Although this mapping is more noisy than the last, we still see that plural names tend to appear on the left side of the figure, and singular names on the right.</p><p>From this exploration we conclude that the continuous locations of each name seem to be capturing semantic regularities. Readers who wish to explore further can view the embeddings at http: //groups.inf.ed.ac.uk/cup/naturalize/.</p><p>Even though the continuous embeddings are learned from context alone, these visualizations suggest that these embeddings also contain, to some extent, semantic information about which identifiers are similar. This suggests that local and global context do provide information that can be represented and exploited, that is, semantically similar names are used in similar contexts. This is evidence pointing towards an affirmative answer to RQ1. It is especially striking that we have consistently found that nearby tokens in the continuous space tend to share subtokens, even when the model does not include subtoken information. The right column of Table <ref type="table" target="#tab_4">3</ref> reinforces this point since it shows that, when we do use the subtoken model, nearby pairs of subtokens in the continuous space seem to be meaningfully related. This provides some evidence for an affirmative answer to RQ2.</p><p>Finally, it can be objected that this type of analysis is necessarily subjective. When backed and validated by quantitative analysis,  however, this analysis provides visual insight, gained from looking at the embedding vectors. Thus, we complement this qualitative analysis with a more quantitative one, in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>In this section, we quantitatively evaluate the performance of the neural model on the data set (Table <ref type="table" target="#tab_2">1</ref>) answering all the RQs. Variable Naming Renaming variables and method invocations has been previously shown <ref type="bibr" target="#b2">[2]</ref> to achieve good performance using ngram LMs. Figure <ref type="figure">6</ref> shows the performance of the baseline n-gram model along with the performance of the other neural models for variable names. For low frequency of suggestions (high confidence decisions), the neural models overperform the n-gram-based suggestions. This is expected since such models perform better than plain n-gram models in NLP <ref type="bibr" target="#b39">[39]</ref>. Additionally, the features give a substantial performance increase over the models that lack features.</p><p>The subtoken model performs worse compared to the token-level model for suggestion frequencies higher than 6%. This is to be expected, since the subtoken model has to make a sequence of increasingly uncertain decisions, predicting each subtoken sequentially, increasing the possibility of making a mistake at some point. For suggestion frequencies lower than 6% the performance of the subtoken model is slightly better compared to the token-level model, thanks to its ability to generate novel identifiers. Thus, we positively answer RQ1 and RQ2.</p><p>We computed Table <ref type="table">4</ref> over only three classes because of the cost of retraining the model one feature at a time. Looking at Table <ref type="table">4</ref> for variable names one may see how each feature affects the performance of the models over the baseline neural model with no features at rank k = 5 2 . First, we observe that the features 2 We chose five because subitizing, the ability to count at a glance,  endstart spriteBatchbatch loclocation help mostly at high suggestion frequencies. This is due to the fact that for high-confidence (low suggestion frequency) decision the models are already good at predicting those names. Additionally, combining all the features yields a performance increase, suggesting that for variable names, only the combination of the features gives sufficiently better information about variable naming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Declaration Naming Accuracy</head><p>We now attempt to use the neural model for suggesting method names, using only features available during the declaration of a method. Surprisingly, the neural model is exceptionally good at predicting method declaration names. Figure <ref type="figure">7a</ref> shows the performance of the models on all method declarations excluding any method declarations that are method overrides. We exclude overrides so as to avoid giving the models credit for predicting easy names like toString. When we include overrides, the performance of all models improves. To exclude method overrides, we remove methods that contain the @Override annotation as well as those methods that we can statically determine as method overrides.</p><p>The graphs in Figures 7a show that the neural models are substantially better at suggesting method names, compared to the n-gram language model. Adding features increases the performance of the models, indicating that the model is able to use non-local context to make better predictions. Naturally, the performance degrades handles 5 objects and because short term memory is usually 7 ± 2 is the size of human short term memory.</p><p>Table <ref type="table">4</ref>: Absolute increase in performance for each type of feature compared to the normal and subtoken models with no features at 5% suggestion frequency and at 20% suggestion frequency for rank k = 5. Averages from clojure, elasticsearch and libgdx, chosen uniformly at random from our corpus. If a model does not produce suggestions at a given frequency, it is not counted in the average. The "vocabulary" of an identifier (e.g. method name) are all the subtokens of that identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Absolute F1 Increase (%)</head><p>Absolute Accuracy Increase (%) Simple Subtoken Simple Subtoken @5% @20% @5% @20% @5% @20% @5% @20% Variables AST Ancestors -0.  <ref type="figure">6</ref>: Evaluation of single point suggestions for variables at rank k = 1 averaged across all projects. The "features" and "nofeatures" models lack sufficient confidence to make suggestions at the higher suggestion frequencies. slightly as the prediction confidence decreases. Interestingly, the token-level models are unable to make any suggestions beyond a suggestion frequency of 15%. For all other tokens, the token-level methods return the special UNK token, indicating that the models expect a neologism which they cannot predict. In contrast, the subtoken models sustain a good F1 score, even for large suggestion frequencies. This is due to the fact that the subtoken models learn naming conventions at the subtoken level, capturing linguistic patterns <ref type="bibr" target="#b5">[5]</ref> such as that specific functions may contain various subtokens e.g. get, set, has, is.</p><p>Table <ref type="table">4</ref> shows a full list of the effect that each feature has on the performance of the neural models at rank k = 5. As expected, the return type, the subtokens of the class where the method is declared in and the subtokens of the variables and method invocations inside that method provide the most substantial performance increases.</p><p>Based on these results, we conclude that we are able to suggest accurate method names and that our suggestions are better than previous approaches. We therefore answer RQ3 in the affirmative.</p><p>Class Definitions Accuracy In the previous section, the performance of the neural model on suggesting names for method declarations was shown. In this section, we evaluate the neural model when making suggestions for class definitions. Figure <ref type="figure">7b</ref> shows the performance of the n-gram language model and the neural models for class name definitions. In contrast to the previous models, the token-level models cannot make any suggestions, always suggesting the UNK token. However, the subtoken model is able to make suggestions even at high suggestion frequencies maintaining an F1 score of more than 40% outperforming the n-gram model.</p><p>Thanks to the ability of the subtoken model to suggest neologisms the subtoken-level model is able to suggest class definition names that it has never seen before, with a good F1 score. Table <ref type="table">4</ref> shows that the subtokens of the superclass and interfaces that a type is implementing are informative about the name of the class. Additionally, when combining all the available features, we get a significant increase in F1 score. Thus, we answer yes to RQ4 as well; we are able to suggest accurate type (class) names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Naming In Software Engineering Naming in code has achieved a fair amount of research attention. There has been prior research into identifying poorly named artifacts. Høst and Østvold <ref type="bibr" target="#b26">[26]</ref> developed a technique for automatically inferring naming rules for methods based on the return type, control flow, and parameters of methods. Using these rules they found and reported "naming bugs" by identifying methods whose names contained rule violations. Arnaoudova et al. presented a catalog of "linguistic anti-patterns" in code that lead to developers misunderstanding code and built a detector of such anti-patterns <ref type="bibr" target="#b5">[5]</ref>. Binkley used part of speech tagging to find field names that violate accepted patterns, e.g. the field create_mp4 begins with a verb and implies an action which is a common pattern for a method rather than a field <ref type="bibr" target="#b11">[11]</ref>. Our work is complementary, as we make suggestions for names when naming bugs are found, anti-patterns occur, or naming rules are violated.</p><p>De Lucio et al. attempted to automatically name source code artifacts using LSI and LDA and found that this approach doesn't work as well as simpler methods such as using words from class and   <ref type="figure">7</ref>: Evaluation of single point suggestions for declarations at k = 1. Overriden method declarations are easier to predict, so we exclude them. The "features" model achieves the best F1 scores for method declarations, but lacks confidence at higher suggestion frequencies (where the line stops). In contrast, the "subtoken" model achieves a good F1 score for all suggestion frequencies for method names and is the only model to accurately suggest class names.</p><p>method names <ref type="bibr" target="#b18">[18]</ref>. Many studies of naming have also been conducted giving us insight into its importance. Butler et al. found that "flawed" identifier names (those that violate naming conventions or do not follow coding practice guidelines) are related to certain types of defects <ref type="bibr" target="#b14">[14]</ref>. Later they also examined the most frequent grammatical structures of method names using part of speech tagging <ref type="bibr" target="#b15">[15]</ref>. Lawrie et al. <ref type="bibr" target="#b29">[29]</ref> and Takang et al. <ref type="bibr" target="#b50">[50]</ref> both conducted empirical studies and concluded that the quality of identifier names in code have a profound effect on program comprehension. Liblit et al. explored how names in code "combine together to form larger phrases that convey additional meaning about the code." <ref type="bibr" target="#b30">[30]</ref>. Arnaoudova et al. <ref type="bibr" target="#b6">[6]</ref> studied identifier renamings, showing that naming is an important part of software construction. Additionally, in a survey of 94 developers, they found that about 68% of developers think that recommending identifiers would be useful. These studies highlight the importance of our work, by being able to suggest quality names or parts of names.</p><p>As method and class names are expected to indicate their semantics, they can be viewed as a special case of code summarization. Haiduc et al. showed that NL text summarization does not work well for code <ref type="bibr" target="#b23">[23]</ref> and such techniques must be adapted to be effective. They later developed summaries that are used to improve comprehension <ref type="bibr" target="#b22">[22]</ref>. Sridhara et al. used idioms and structure in the code of methods to generate high level abstract summaries. While they don't suggest method names, they discuss how their approach may be extended to provide them <ref type="bibr" target="#b47">[47]</ref>. Sridhara also showed how to generate code summaries appropriate for comments within the code (e.g. as method headers) <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b45">45]</ref>. For more work in this area, Eddy et al. provide a survey of code summarization methods <ref type="bibr" target="#b20">[20]</ref>. We note that most studies and approaches in this area focus on names of variables, fields, and methods. Although some examine all identifiers in the code, we are unaware of any work that focuses on type (class) names as we do.</p><p>Language Models In Software Engineering Probabilistic models of source code have been applied in software engineering. Hindle et al. and Ngyuen et al. <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b41">41]</ref> used n-gram models to improve code autocompletion. Allamanis and Sutton <ref type="bibr" target="#b3">[3]</ref> present an application of code n-gram models at scale. Maddison and Tarlow <ref type="bibr" target="#b31">[31]</ref> built a more sophisticated generative model of source code using log-bilinear models that reflects the syntactic structure of the code. Although the machine learning principles we use are similar, their model differs significantly from ours, because their purpose is to build models that generate source code rather than improve existing code. In other words, our model is discriminative rather than generative. Mou et al. <ref type="bibr" target="#b40">[40]</ref> use a convolutional neural network to classify code from programming competition problems. Karaivanov et al. <ref type="bibr" target="#b27">[27]</ref> combine LMs with static program analysis to suggest method calls and fill-in gaps. Other applications of probabilistic source code models are extracting code idioms <ref type="bibr" target="#b4">[4]</ref> and code migration <ref type="bibr" target="#b27">[27]</ref>. Closely related to this work is our previous work where we infer formatting and naming conventions <ref type="bibr" target="#b2">[2]</ref> using n-gram LMs to suggest natural renamings. Raychev et al. <ref type="bibr" target="#b43">[43]</ref> present a discriminative probabilistic model to predict types and names of variables in JavaScript. In contrast, our current work introduces a log-bilinear model that greatly improves on the n-gram LM, especially on method and class naming, proposing neologisms by taking into account subtokens and non-local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other Applications of Neural Logbilinear Models</head><p>Neural logbilinear models have been used in NLP for LMs <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b39">39]</ref> and describing images with NL <ref type="bibr" target="#b28">[28]</ref>. Log-bilinear models have been shown in NLP to produce semantically consistent and interesting vector space representations (embeddings). Notable systems include word2vec <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref> and GloVe <ref type="bibr" target="#b42">[42]</ref>. In contrast to these approaches, we use a rich notion of non-local context by incorporating features specific to source code while we produce similar vector space models for method names, variables and types. Additionally, we present a novel sub-token model. Related to our subtoken model is the work of Botha and Blunsom <ref type="bibr" target="#b12">[12]</ref> that integrate compositional morphological representations of words into a log-bilinear LM but the morphological features are only used in the context of an LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We introduced the method naming problem, that of automatically determining a functionally descriptive name of a method or class. Previous work on automatically assigning names <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b43">43]</ref> focuses on local variables, and relies on relatively local context. Naming methods is more difficult because it requires integrating non-local information from the body of the method or class. We presented a first solution using a log-bilinear neural language model, which includes feature functions that capture long-distance context, and a subtoken model that can predict neologisms, names that did not appear in the training set. The model embeds each token into a high dimensional continuous space.</p><p>Continuous embeddings of identifiers have many other potential applications in software engineering, such as rejecting commits whose names violate project conventions; exploration of linguistic anti-patterns, such as a getter starting with set <ref type="bibr" target="#b5">[5]</ref> and feature localization. Finally, a problem similar to method naming arises in NLP, namely the problem of generating a headline from the text of an article <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b19">19]</ref>. It is possible that models similar to ours could shed light on that problem as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visual explanation of the representation and computation of context in the D-dimensional space as defined in Equation4; the final paragraph of Section 2.2 explains the sum over the I t locations. Each source code token and feature maps to a learned Ddimensional vector in continuous space. The token-vectors are multiplied with the position-dependent context matrix C i and summed, then added to the sum of all the feature-vectors. The resulting vector is the D-dimensional representation of the current source code identifier. Finally, the inner product of the context and the identifier vectors is added to a scalar bias b, producing a score for each identifier. This neural network is implemented by mapping its equations into code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This completes the definition of the subtoken context model. To sum up, the parameters of the subtoken context model are (a) the target embeddings q s for each subtoken s that occurs in the data, (b) the history embeddings r s for each subtoken s, (c) the diagonal weight matrices C SUBC -m for m = 1, 2, . . . , M that represent the effect of distance on the subtoken history (we use M = 3, yielding a 4-gramlike model on subtokens) and the parameters that we carried over from the logbilinear context model: (d) the local context embeddings r t for each token t that appears in the context, (e) the local context weight matrices C -k and C k for -K ≤ k ≤ K, k = 0, and (f) the feature embeddings r f for each feature f (c) of the global context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: A 2D linear projection, using PCA, of the embeddings of setters and getters for netty method declarations. Matched getter/setting pairs are connected with a dotted line. The embeddings seem to separate setters from the getters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: A 2D linear projection, using PCA, of the embeddings of singular and plural names in libgdx. Pairs are connected with a dotted line. The embeddings mostly separate singular and plural names. We expect most of the plural variables to refer to Collections of objects whose names appear in singular.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Class (Type) Declarations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,66.35,53.80,477.00,109.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Evaluation projects (Java). Ordered by popularity.</figDesc><table><row><cell>Name</cell><cell>Git SHA</cell><cell>Description</cell></row><row><cell>elasticsearch</cell><cell>d3e10f9</cell><cell>REST Search Engine</cell></row><row><cell>Android-Universal-Image-Loader</cell><cell>19ce389</cell><cell>Android Library</cell></row><row><cell>spring-framework</cell><cell>2bf6b41</cell><cell>Application Framework</cell></row><row><cell>libgdx</cell><cell>789aa0b</cell><cell>Game Dev Framework</cell></row><row><cell>storm</cell><cell>bc54e8e</cell><cell>Distributed Computation</cell></row><row><cell>zxing</cell><cell>71d8395</cell><cell>Barcode image processing</cell></row><row><cell>netty</cell><cell>3b1f15e</cell><cell>Network App Framework</cell></row><row><cell>platform_frameworks_base</cell><cell>f19176f</cell><cell>Android Base Framework</cell></row><row><cell>bigbluebutton</cell><cell>02bc78c</cell><cell>Web Conferencing</cell></row><row><cell>junit</cell><cell>c730003</cell><cell>Testing Framework</cell></row><row><cell>rxjava</cell><cell>cf5ae70</cell><cell>Reactive JVM extensions</cell></row><row><cell>retrofit</cell><cell>afd00fd</cell><cell>REST client</cell></row><row><cell>clojure</cell><cell>f437b85</cell><cell>Programming Language</cell></row><row><cell>dropwizard</cell><cell>741a161</cell><cell>RESTful web server</cell></row><row><cell>okhttp</cell><cell>0a19746</cell><cell>HTTP+SPDY client</cell></row><row><cell>presto</cell><cell>6005478</cell><cell>Distributed SQL engine</cell></row><row><cell>metrics</cell><cell>4984fb6</cell><cell>Metrics Framework</cell></row><row><cell>spring-boot</cell><cell>b542aa3</cell><cell>App Framework Wrapper</cell></row><row><cell>bukkit</cell><cell>f210234</cell><cell>Mincraft Mod API</cell></row><row><cell>nokgiri</cell><cell>a93cde6</cell><cell>HTML/XML/CSS parser</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Examples of nearest neighbors in the continuous space for variable names in clojure. Ordered by higher inner product q t 1 q t 2 where t 1 is in the first column and t 2 in the second.</figDesc><table><row><cell>Identifier</cell><cell>Nearest Neighbors (ordered by distance)</cell></row><row><cell>fieldName</cell><cell>className, methodName, target, method,</cell></row><row><cell></cell><cell>methods</cell></row><row><cell cols="2">returnType sb, typ, type, methodName, t</cell></row><row><cell>keyvals</cell><cell>items, seq, form, rest, valOrNode</cell></row><row><cell>params</cell><cell>paramType, ctor, methodName, args, arg</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Closely related (sub-)tokens for libgdx variables. The top 10 pairs that have the highest q t 1 q t 2 are shown. For the subtoken model some numeral pairs (e.g. 9-8) are omitted.</figDesc><table><row><cell>Feature Model</cell><cell>Subtoken</cell></row><row><cell>camera -cam</cell><cell>6 -5</cell></row><row><cell>padBottom -padLeft</cell><cell>Height -Width</cell></row><row><cell>dataOut -dataIn</cell><cell>swig -class</cell></row><row><cell>localAnchorA -localAnchorB</cell><cell>Min -Max</cell></row><row><cell>bodyA -bodyB</cell><cell>shape -collision</cell></row><row><cell>framebuffers -buffers</cell><cell>Left -Right</cell></row><row><cell>worldWidth -worldHeight</cell><cell>camera -cam</cell></row><row><cell>padRight -padLeft</cell><cell>TOUCH -KEY</cell></row><row><cell>jarg7 -jarg6_</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that k can be positive or negative, so that in general C -2 = C</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>This work was supported by Microsoft Research through its PhD Scholarship Programme. Charles Sutton was supported by the Engineering and Physical Sciences Research Council [grant number EP/K024043/1].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Can lexicon bad smells improve fault prediction?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arnaoudova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tonella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gueheneuc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>WCRE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning natural coding conventions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mining source code repositories at massive scale using language modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining Idioms from Source Code</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new family of software anti-patterns: Linguistic anti-patterns</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arnaoudova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Gueheneuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSMR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">REPENT: analyzing the nature of identifier renamings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arnaoudova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Eshkevari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guéhéneuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linguistic antipatterns: What they are and how developers perceive them</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arnaoudova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMSE</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Headline generation based on statistical translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Implementation patterns</title>
		<author>
			<persName><forename type="first">K</forename><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving identifier informativeness using part of speech information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Relating identifier naming flaws and code quality: An empirical study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wermelinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>WCRE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the influence of identifier names on code quality: An empirical study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wermelinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th European Conference on Software Maintenance and Reengineering (CSMR&apos;2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining Java class naming conventions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wermelinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Program understanding: Challenge for the 1990s. IBM Systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Corbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="306" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using IR methods for labeling source code artifacts: Is it worthwhile?</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Lucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panichella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panichella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hedge trimmer: A parse-and-trim approach to headline generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL-03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating source code summarization techniques: Replication and expansion</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Carver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supporting program comprehension with source code summarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haiduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On the use of automated text summarization techniques for summarizing source code</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haiduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<editor>WCRE</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The effectiveness of control structure diagrams in source code comprehension activities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maghsoodloo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSE</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Debugging method names</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Høst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Østvold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOOP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Phrase-based statistical translation of programming languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karaivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Vechev</surname></persName>
		</author>
		<editor>Onward!</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? a study of identifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cognitive perspectives on the role of naming in computer programs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liblit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sweetser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Psychology of Programming Workshop</title>
		<meeting>the 18th Annual Psychology of Programming Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured generative models of natural source code</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Clean code: a handbook of agile software craftsmanship</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Code Complete</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcconnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Microsoft Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">TBCNN: a tree-based convolutional neural network for programming language processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5718</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A statistical semantic language model for source code</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting program properties from &quot;big code</title>
		<author>
			<persName><forename type="first">V</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Modern Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Automatic generation of descriptive summary comments for methods in object-oriented programs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sridhara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of Delaware</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards automatically generating summary comments for java methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Muppaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatically detecting and describing high level actions within methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The effects of comments and identifier names on program comprehensibility: an experiential study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Takang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Macredie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Program Languages</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="167" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The effects of comments and identifier names on program comprehensibility: an experimental investigation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Takang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Grubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Macredie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Prog. Lang</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="167" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
