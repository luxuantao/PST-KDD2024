<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Informative Video Segments for Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Che</forename><surname>Sun</surname></persName>
							<email>sunche@bit.edu.cn</email>
							<idno type="ORCID">0000-0002-7555-9146</idno>
						</author>
						<author>
							<persName><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
							<email>wuxinxiao@bit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Song</surname></persName>
							<email>songhao@bit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yunde</forename><surname>Jia</surname></persName>
							<email>jiayunde@bit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jiebo.luo@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Beijing Labora-tory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit2">Bei-jing Institute of Technology</orgName>
								<address>
									<postCode>10081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">China Electric Power University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">jing Laboratory of Intelligent Information Technol-ogy</orgName>
								<orgName type="institution">Beijing Insti-tute of Technology</orgName>
								<address>
									<postCode>2018</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Informative Video Segments for Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2021.3050067</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Temporal action localization</term>
					<term>informative video segments</term>
					<term>supervised temporal attention network</term>
					<term>attention mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognize actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segmentlevel attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks, i.e., THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with existing stateof-the-art methods. Moreover, compared with the baseline method that treats video segments equally, STAN achieves significant improvements with an increase of the mean average precision from 30.4% to 39.8% on the THUMOS2014 dataset, and from 31.4% to 35.9% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T EMPORAL action localization in untrimmed videos aims to analyze whether a specific action occurs in videos and determine the temporal boundaries (the start and the times) of the action simultaneously. Although there have been numerous studies conducted on temporal action localization in untrimmed videos <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, achieving accurate localization remains challenging owing to the cluttered background, large variances of appearance and motion, and low resolution. Moreover, the same action may occur several times in a video and the durations of the action instances with the same class may vary from a few seconds to a few minutes, which further makes it extremely difficult to localize actions in untrimmed videos.</p><p>To tackle these problems, many methods based on deep neural networks have been proposed and have achieved remarkable progress in temporal action localization, owing to the successes of deep learning on various visual tasks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, especially on video analysis <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Some prominent methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref> resort to sliding windows to produce temporal boundaries of actions and many other methods <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[16]</ref> generate proposals as candidate action instances for localization. These deep methods treat each video segment equally within the sliding windows or proposals and directly aggregate the video segments for temporal action localization. In practice, different segments embody diverse information in a video sequence. Some segments contain the intrinsic motion and appearance of an action, which will play a vital role in action localization. Taking a triple jump action as an example, a jumping action segment is obviously more important than other segments in localizing the triple jump in a video because the jumping motion reflects the essential characteristics of a triple jump. It is therefore necessary to exploit the informative video segments to represent the intrinsic motion and appearance information. In this paper, we propose a novel method that exploits informative video segments by learning video segment weights for temporal action localization in untrimmed videos. The learned weights represent the importance of the corresponding video segments in recognizing actions and predicting temporal boundaries, as shown in Fig. <ref type="figure">1</ref>. We build a supervised temporal attention network (STAN) that includes three modules, i.e., a segment-level attention module, a feature-level attention module, and a localization module. The segment-level attention module is designed to dynamically learn the weights of video segments by using a supervised attention mechanism. With the learned weights, the segments are fed into a long short-term memory (LSTM) model to capture the temporal relationships between them. The feature-level attention module is introduced to softly aggregate the static appearance and dynamic motion features of each segment by computing the weights of these two features. Through a cascade of the segment-level attention module and feature-level Fig. <ref type="figure">1</ref>. Illustration of using the proposed STAN to temporally localize an action in a video. An input video of any temporal length is split into a series of segments with equal temporal lengths. STAN learns the weights of the segments, recognizes the action categories, and estimates the boundaries of the actions. Fig. <ref type="figure">2</ref>. Architecture of STAN, which includes three modules: a segment-level attention module, a feature-level attention module, and a localization module. The segment-level attention module learns the weights of video segments with dual attention blocks. The feature-level attention module combines the appearance and dynamic features of each segment by computing the weights of these two features. Through a cascade of the segment-level attention module and the feature-level attention module, STAN learns informative video segments. Moreover, the localization module is designed to classify action classes and determine the temporal action boundaries for the input videos, including a proposal generator and a classifier. attention module, STAN exploits the informative video segments and generates video representation with superior performance. Moreover, the localization module is designed to classify the action classes and determine the temporal action boundaries for the input videos, consisting of a proposal generator and a classifier. The proposal generator is used to identify the input video as either a background proposal or an action proposal, and the classifier is used to classify the action classes of the identified action proposal. Finally, a non-maximum suppression (NMS) strategy is employed to remove the videos with small classification scores and produce the temporal boundaries of the action instances. Fig. <ref type="figure">2</ref> shows the architecture of STAN.</p><p>The contributions of this paper are summarized as follows.</p><p>r We propose a novel method for temporal action localization by exploiting informative segments in untrimmed videos. These informative segments reflect the intrinsic motion and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Action Localization</head><p>Early methods of temporal action localization use sliding windows to sample candidate video segments with multiple temporal scales, and adopt classifiers to classify the segments. Karaman et al. <ref type="bibr" target="#b17">[17]</ref> proposed a saliency-based pooling method to improve the fisher vector encoding <ref type="bibr" target="#b18">[18]</ref> of the improved dense trajectory (iDT) <ref type="bibr" target="#b19">[19]</ref>, and then fused the frame-level CNN features for action classification. Wang et al. <ref type="bibr" target="#b20">[20]</ref> fused the features of iDT and CNN to design an action recognition and detection system. They also used a post-processing method to boost the localization performance. Xu et al. <ref type="bibr" target="#b21">[21]</ref> extracted CNN features and improved dense trajectories by using the vector of a locally aggregated descriptor encoding method <ref type="bibr" target="#b22">[22]</ref> to recognize and localize the action in a video. Shou et al. <ref type="bibr" target="#b1">[2]</ref> built a three-stage framework for temporal action localization with an overlap loss function. In <ref type="bibr" target="#b23">[23]</ref>, a multi-task learning framework is proposed, which consists of three highly related steps, i.e., generating action proposals, recognizing actions and refining action localization. Zhao et al. <ref type="bibr" target="#b24">[24]</ref> used a structured temporal pyramid to model the temporal structure of each action instance, where the context information of an action instance is explored to generate features for temporal action localization. These methods equally treat each video segment within the sliding windows. By contrast, our method dynamically learns the weights of video segments to discover the informative segments that contain the intrinsic motion and appearance information of actions for temporal action localization.</p><p>Many recent studies have attempted to extract action proposals from videos and classify the proposals into action classes. Different aggregation methods have frequently been used to combine representations of segments or frames in a video for action localization by learning action prototypes and actions jointly. Buch et al. <ref type="bibr" target="#b25">[25]</ref> employed a temporal segment network (TSN) <ref type="bibr" target="#b9">[10]</ref> and a recurrent sequence encoder to aggregate video segments for generating action proposals. Gao et al. <ref type="bibr" target="#b12">[13]</ref> used a cascaded boundary regression model to produce class-agnostic proposals and detect specific actions by using a pooling aggregation method. Xu et al. <ref type="bibr" target="#b13">[14]</ref> applied a region-based method to temporal action localization and generated candidate temporal regions containing actions by performing temporal convolutions. Based on the work of <ref type="bibr" target="#b13">[14]</ref>, Chao et al. <ref type="bibr" target="#b26">[26]</ref> improved receptive field alignment to exploit the temporal context of actions for generating proposals and classifying actions. Gao et al. <ref type="bibr" target="#b27">[27]</ref> presented a temporal unit regression network to classify actions and regress the boundaries. Differing from these methods that treat each video segment or frame equally within a video, our method dynamically learns the weight of each segment to effectively eliminate the influence of the background and fully exploit action informativeness in a video. Closely related to our work, Buch et al. <ref type="bibr" target="#b28">[28]</ref> used semantically constrained recurrent memory modules to selectively the aggregate relevant context for action localization. The one-way chained structure of the recurrent memory module weighs the contributions of most segments in the local context. In contrast, we design dual attention blocks of the segment-level attention module to learn the segment weights over the entire action video at the same time, which is beneficial for exploiting each informative segment with consideration of the global context for action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>Inspired by the successes of attention mechanisms in natural language processing <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, many researchers have applied attention mechanisms to computer vision. Mnih et al. <ref type="bibr" target="#b31">[31]</ref> first used the attention mechanism with recurrent neural networks to locate the highlighted regions for image classification. Ba et al. <ref type="bibr" target="#b32">[32]</ref> proposed deep recurrent neural networks trained with reinforcement learning and attention mechanism to find the most relevant regions of an image for object recognition.</p><p>Attention mechanisms have also been introduced to video analysis <ref type="bibr" target="#b33">[33]</ref>- <ref type="bibr" target="#b36">[36]</ref>. Wang et al. <ref type="bibr" target="#b33">[33]</ref> presented hierarchical attention networks to combine the spatial information and temporal information for action understanding. Shi et al. <ref type="bibr" target="#b37">[37]</ref> used the attention-based LSTM to capture the long-term dependence and find the salient portions. Nguyen et al. <ref type="bibr" target="#b34">[34]</ref> used the attention mechanism to find the background or action segments for weakly supervised temporal action localization. Li et al. <ref type="bibr" target="#b36">[36]</ref> used the spatial and temporal attention mechanism and fused the video features of multiple modalities for action recognition. These methods use attention mechanisms to capture more important parts, and then generate a more discriminative representation for a video analysis task. However, these methods calculate the attention weights without regard to the temporal structure of the entire action video, which may focus more on the importance of a single part and ignore the context information of the entire action. In contrast to existing attention-based methods, we use a segment-level attention module to learn the weighted segments when considering the temporal context over the entire action video, which is beneficial for capturing correlations among segments to represent intrinsic motion and appearance of an action instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>A video is usually split into a series of segments with equal temporal length to deal with actions with any temporal length.</p><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply. Fig. <ref type="figure">3</ref>. Architecture of the dual attention blocks. f i represents the feature vector of the i-th segment. α 0 i and α 1 i denote the attention weights of the i-th segment in the first and second attention blocks, respectively. h 0 i and h 1 i are the outputs of the corresponding LSTM blocks. The first attention block generates the video representation h 0 k with context information that is used to select context-aware segments in the second attention block.</p><p>A common strategy is to use average pooling or max pooling on these segments to generate a feature representation of the entire video from these segments for temporal action localization. Feature encoding methods, such as the fisher vector and the vectors of locally aggregated descriptors, are also extensively used in previous work to generate video representations. Among these methods, the video segments are usually treated equally without considering their informativeness, and the temporal relationships between segments have not been effectively investigated. Therefore, we propose to exploit informative video segments to represent the intrinsic motion and appearance information for temporal action localization.</p><p>Encouraged by the successes of the attention mechanism on various applications <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b38">[38]</ref>- <ref type="bibr" target="#b40">[40]</ref>, we build a supervised temporal attention network (STAN) to exploit the informative video segments by learning video segment weights. As shown in Fig. <ref type="figure">2</ref>, STAN includes three modules: a segment-level attention module, a feature-level attention module, and a localization module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Segment-Level Attention Module</head><p>For the segment-level attention module, we design dual attention blocks to refine and encode features of local segments under consideration of global context information, where the first attention block learns the the measurement of the universal video segments and the second attention block learns the measurement of the context-aware video segments, as shown in Fig. <ref type="figure">3</ref>. In each attention block, we use the long short-term memory (LSTM) model to aggregate all weighted segments for capturing the temporal relationships. Furthermore, we add a supervised constraint to the second attention block to eliminate the influence of background segments. The supervised constraint ensures that the learned weighted segments cover the complete action durations.</p><p>1) First Attention Block: Given an input video v and its action class label y, v is split into K non-overlap segments, denoted by {s 1 , s 2 , . . . , s K }. Let {f 1 , f 2 , . . . , f K } be the feature vectors of the segments and {α 0 1 , α 0 2 , . . . , α 0 K } be the weights of the segments in the first attention block. We build an attention layer that filters the feature vectors {f 1 , f 2 , . . . , f K } by taking the inner product to obtain the corresponding encodings {e 0  1 , e 0 2 , . . . , e 0 K } by</p><formula xml:id="formula_0">e 0 t = u 0 • f t ,<label>(1)</label></formula><p>where u 0 is the parameter of the first attention layer with the same size of the feature vector, and f t refers to the feature vector of the t-th segment. Then the encodings {e 0 1 , e 0 2 , . . . , e 0 K } are passed to a softmax operator to calculate the positive weights {α 0 t } with the constraint of K t=1 α 0 t = 1 by</p><formula xml:id="formula_1">α 0 t = exp(e 0 t ) K j=1 exp(e 0 j ) . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Different from the existing attention models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[24]</ref>, [41] that use average pooling or a concatenation operation, we aggregate the weighted segments using an LSTM model to generate the video representations for capturing temporal information. The weighted segments are calculated using x t = α 0 t * f t , which are treated as the input of the LSTM model. We calculate the last hidden state h 0 K as the feature representation of the input video by</p><formula xml:id="formula_3">h 0 K = LST M (α 0 t * f t , V 0 ),<label>(3)</label></formula><p>where V 0 refers to the set of parameters of the LSTM.</p><p>2) Second Attention Block: In the first attention block, the process of calculating attention weights α 0 t does not take the context information into consideration. Intuitively, weighting a video segment can benefit from other segments, where the segments are often correlated but temporally separated. This correlation reflects the informativeness of segments, which may play an important role in action localization. Thus, we introduce the second attention block to select context-aware segments that are more discriminative. The weight of a segment in the second attention block is learned by the current segment representation f K and the entire video representation h 0 K , which takes the context information into consideration.</p><p>Let u 0 be the parameter of the first attention layer, and h 0 K be the learned feature representation, where h 0 K is computed by u 0 using Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_3">3</ref>). The parameter of the second attention layer u 1 is calculated using a transfer layer with the input h 0 K :</p><formula xml:id="formula_4">u 1 = tanh(W 1 h 0 K + b 1 ),<label>(4)</label></formula><p>where W 1 and b 1 are the weight matrix and the bias vector, respectively, and tanh(x) = e x −e −x e x +e −x imposes the hyperbolic tangent nonlinearity. We replace u 0 by u 1 , and then reuse Eqs. (1)(2)(3) with another set of parameters to generate the output of the second attention block</p><formula xml:id="formula_5">h 1 K . The parameters {u 0 , V 0 , W 1 , b 1 , V 1 }</formula><p>are all trainable at the segment-level attention module, where V 0 and V 1 indicate the parameters of the LSTMs in the first and second attention blocks, respectively.</p><p>3) Supervised Constraint: The segment-level attention module with the dual attention blocks captures the informative segments of an input video for action localization, but the background segments in the sliding window are non-negligible noises. The background segments usually have unique features that may get a higher attention weight under the conventional unconstrained method, however, they essentially contain less action information. To eliminate these noises, we impose a supervised constraint on the segment-level attention module to filter out the background segments and retain the meaningful action segments. According to the ground truth action boundaries, we assign an "actionness" label to each segment as the supervised information to guide the learning of the segment weights. The "actionness" label represents whether the segment contains an action frame or not. In practice, we relax the supervised constraint in the learning progress to fully exploit the ability of the attention mechanism. We use a multi-class loss function as the supervised constraint to train the attention module, as discussed in Section III-D.</p><p>Through supervised learning, the segment-level attention module not only distinguishes the action segments from the background segments, but also captures the informative segments covering the complete action in the input video for action localization. The impact of useless segments will be reduced to produce more effective representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature-Level Attention Module</head><p>In temporal action localization, appearance features from each frame and motion features from each video are both helpful to improve the localization accuracy. Thus, we extract appearance feature f s t and motion feature f m t of the t-th video segment to describe a video from spatial and temporal viewpoints, respectively, and build a feature-level attention module to weigh multiple features for fusion.</p><p>Using the LSTM model in the segment-level attention module, the learned appearance and motion feature representations of an entire video are represented as h s K and h m K , respectively. We introduce an attention layer to dynamically fuse the appearance and motion features of the video. Specifically, the attention layer with the trainable parameter q encodes the feature h s K and h m K , and outputs g s and g m by</p><formula xml:id="formula_6">g s = q • h s K , g m = q • h m K .</formula><p>(</p><p>The weights γ s and γ m of h s K and h m K are adaptively computed with γ s + γ m = 1 by</p><formula xml:id="formula_8">γ s = exp(g s ) exp(g s ) + exp(g m ) , γ m = exp(g m ) exp(g s ) + exp(g m ) . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>The combined feature representation h K of the video v is given by</p><formula xml:id="formula_10">h K = γ s * h s K + γ m * h m K . (<label>7</label></formula><formula xml:id="formula_11">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Localization Module</head><p>The localization module aims to infer action boundaries and complete action classification. This module includes a proposal generator and a classifier. The proposal generator generates video proposals that contain action instances. The classifier classifies the generated video proposal into a specific class. There are a total of N + 1 classes, including the background class and N action classes.</p><p>1) Proposal Generator: The proposal generator generates potential proposal video clips with respect to the video representation produced by sliding windows and outputs a binary label to represent whether the generated proposal contains an action instance. Moreover, we adopt the boundary regression method <ref type="bibr" target="#b27">[27]</ref> to accurately locate the boundary of the action.</p><p>We use sliding windows to construct videos of different lengths. The representation h K of the video v is learned via the segment-level attention module and the feature-level attention module by h K = Attention(v). h K is then fed into the proposal generator to output a binary score p and a relative offsets {s i , e i }. If p is larger than a threshold, the video v in the sliding window is treated as an action proposal and its boundary is adjusted by the {s i , e i }, otherwise, it is treated as the background. By applying the sliding windows with different lengths, we get multiple videos and action proposals. A soft non-maximum suppression (Soft-NMS) <ref type="bibr" target="#b42">[42]</ref> is used to eliminate highly overlapping for final action proposal sets.</p><p>The training samples are selected using the following strategy. For the untrimmed videos, we only select segments from the ground truth as positive samples. The negative samples consist of background segments that are randomly sampled from the background videos. The temporal Intersection-over-Union (tIoU) between the training video and its ground truth is the main criterion: (1) If the tIoU of the video is larger than 0.7, a positive label is assigned according to its action class; (2) If the tIoU of the video is smaller than 0.3, we treat the video as the background. We train the proposal generator with a positive/negative ratio of 1:1.</p><p>2) Classifier: After eliminating background videos using the proposal generator, we train the classifier for N + 1 classes. Similar to the proposal generator, the classifier consists of two separate fully connected layers to output action scores and a relative offsets. Both the proposal generator and classifier are built on the segment-level and feature-level attention modules with the same structure but non-shared parameters. For training the classifier, we follow a similar training dataset construction strategy to the proposal generator. As the differences, (1) we explicitly set the action class label y ∈ {1, 2, . . . , N} when assigning a label for the positive training sample, and (2) we train the classifier with a positive/negative ratio of 1:3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objective Function</head><p>The objective function of our network includes three parts: the classification loss, the regression loss, and the supervised attention loss. We use the softmax cross-entropy loss function for classification and the smooth L1 loss function <ref type="bibr" target="#b43">[43]</ref> for regression. The supervised attention loss is used to train the segment-level attention module such that the attention module is able to effectively select the "actionness" information from video segments containing actions. We treat the supervised attention learning as a multi-class classification, and use the sigmoid cross-entropy loss to constrain the attention module.</p><p>The classification loss is given by</p><formula xml:id="formula_12">L cls = 1 N t i −y i ln(p i ),<label>(8)</label></formula><p>where y i is a one-hot encoding label of the action class and N t denotes the batch size. p i is the prediction score that is calculated by the proposal generator or classifier after the softmax layer.</p><p>The regression loss is formulated as</p><formula xml:id="formula_13">L reg = 1 N pos i l * i ( s i − s * i smooth 1 + e i − e * i smooth 1</formula><p>),</p><p>where N pos stands for the number of positive samples in a batch. s i and e i are the predicted start and end offsets. s * i and e * i are the ground truth start and end offsets, respectively. • smooth 1 represents the smooth L1 loss function. l * i is the actionness label, that is, l * i = 1 for positive samples, and l * i = 0 for negative samples.</p><p>The supervised attention loss is expressed as</p><formula xml:id="formula_15">L sat = 1 N pos i 1 N seg j l * i y s ij ln 1 1 + exp (− log e 1 ij ) + (1 − y s ij ) ln exp (− log e 1 ij ) 1 + exp (− log e 1 ij ) , (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where N seg stands for the number of segments in each video. y s ij is the label of the j-th segment in the i-th training sample. If the j-th segment contains any action frame, y s ij is set to 1; otherwise, y s ij is set to 0. e 1 ij represents the attention encoding of the j-th segment in the i-th training sample in the second attention block. The supervised attention loss is utilized to force the attention encoding to contain more "actionness" information.</p><p>The overall objective function is defined as</p><formula xml:id="formula_17">L = L cls + λ 1 L reg + λ 2 L sat , (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where λ 1 and λ 2 are the trade-off parameters. λ 1 is set to 1. For λ 2 , we set the initial value of λ 2 to 0.95 and then decrease its value with the iterations to relax the constraint. We find that the best models are obtained when λ 2 is multiplied by 0.95 after 1 K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To evaluate the effectiveness of our method, we conduct experiments on two challenging datasets: THUMOS2014 <ref type="bibr" target="#b44">[44]</ref> and ActivityNet1.3 <ref type="bibr" target="#b45">[45]</ref>.</p><p>The THUMOS2014 dataset contains videos from 20 classes. Because the training subset is constructed by the UCF101 dataset <ref type="bibr" target="#b46">[46]</ref> which consists of many trimmed videos, we use 200 and 213 annotated untrimmed videos from the validation and test subsets for training and testing, respectively. The validation subset consists of 3007 action instances and the test subset consists of 3358 action instances. Each video in the validation and test subsets contains more than 15 action instances on average.</p><p>The ActivityNet1.3 dataset includes approximately 19994 videos with 200 classes. It is divided into three subsets: a training subset of 10024 videos, a validation subset of 4926 videos and a test subset of 5044 videos. Each video contains 1.5 action instances on average. Compared with the THUMOS2014 dataset, the ActivityNet1.3 dataset is more complex because the action instances in videos usually last for more than 15 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metric</head><p>We adopt a conventional evaluation strategy in the THUMOS Challenge and calculate the temporal Intersection over Union (tIoU) with the ground truth. Localization is marked as correct only when it has a correct action class prediction and has a tIoU higher than a threshold. We report the mean Average Precision (mAP) at different tIoU thresholds as the evaluation metric. On the ActivityNet1.3 dataset, the tIoU thresholds are set to {0.5, 0.75, 0.95}. On the THUMOS2014 dataset, the tIoU thresholds are set to {0.1, 0.2, 0.3, 0.4, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Setup 1) Implementation Details:</head><p>We split the untrimmed video into short segments with equal temporal length. The length of video segments is set to 15 frames for the THUMOS2014 dataset and 75 frames for the ActivityNet1.3 dataset. To reduce the computation cost and improve the training efficiency, we set the maximum length of the sliding window to 32 segments on the THUMOS2014 dataset and 64 segments on the ActivityNet1.3 dataset. For the THUMOS2014 dataset, the sliding window of 480 frames (32 × 15 = 480) is able to completely cover 98.9% of the action instances. For the ActivityNet1.3 dataset, a sliding window of 4800 frames (64 × 75 = 4800) can completely cover 93.5% of the action instances. For the THUMOS2014 dataset, we only use the validation dataset to train our proposed STAN. For the ActivityNet1.3 dataset, we use the training set to train STAN and the validation dataset for testing.</p><p>We extract the appearance and motion features of the short segments using VGG-16 <ref type="bibr" target="#b47">[47]</ref> and temporal segment networks (TSN) <ref type="bibr" target="#b9">[10]</ref>, respectively. The TSN is constructed by two convolutional neural networks: spatial stream ConvNets and temporal stream ConvNets, both of which adopt a BN-Inception architecture <ref type="bibr" target="#b48">[48]</ref>. The two-stream networks are trained within multiple snippets in a video and then fused by segmental consensus modules for action recognition, and thus the extracted TSN features are more likely to represent the dynamic motion information. In our study, the TSN is trained by the ActivityNet1.3 dataset under the experiment setup in <ref type="bibr" target="#b9">[10]</ref>. Moreover, we need extra spatial features from each single frame to enhance the performance of our model. We adopt the VGG-16 network that takes a single 224 × 224 RGB image as input, and train VGG-16 using the ILSVRC-2012 dataset <ref type="bibr" target="#b5">[6]</ref>. The feature extraction part of the VGG-16 and TSN is implemented by using the Caffe toolkit <ref type="bibr" target="#b49">[49]</ref>.</p><p>The outputs of the fc-4096 layer of the VGG-16 network are treated as the appearance features of segments. For the motion features, we follow the operation in <ref type="bibr" target="#b50">[50]</ref> and extract the 400dimensional feature vectors from the TSN for every five frames. All segment features are normalized using L2-normalization. The segment scales are set to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b32">32]</ref> on the THUMOS2014 dataset and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr">64</ref>] on the ActivityNet1.3 dataset. The overlap segment of sliding windows with different scales is set to [0, <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b24">24]</ref> and [0.6, 1, 2, 3, 4, 5, 7, 8, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b56">56]</ref> on the THUMOS2014 and ActivityNet1.3 datasets, respectively. We sample a single frame in the middle of a segment to extract the VGG-16 feature for the segment. We cascade the TSN features for every five frames of the segment as the motion representation.</p><p>The VGG-16 and TSN features are fed into dual attention blocks of the same structure. Before the segment-level attention module, we reduce the number of feature vector dimensions to 1024 using a fully connected layer. In the first attention block of dual attention blocks, the attention weight α 0 i is calculated from a 1024 × 1 fully connection layer followed by a soft-max layer. Then α 0 i is dot-multiplied by the i-th segment. The number of dimensions of the hidden state in the LSTM model of the first attention block is set to 1024. In the second attention block, the number of dimensions of the hidden state in the LSTM model is also set to 1024. The kernel size of the feature fusion layer in the feature-level attention module is set to 1024 × 1. The fused features are then utilized for temporal action localization.</p><p>2) Post-Processing: During the test procedure, we first generate videos with different temporal lengths using sliding windows. Then we use the proposal network in STAN to remove the background videos and adjust the boundary of positive samples according to the results of boundary regression. These positive proposals may highly overlap with each other, so we adopt soft non-maximum suppression (Soft-NMS) <ref type="bibr" target="#b42">[42]</ref> to eliminate high overlapping. The threshold of Soft-NMS is set to 0.8 for the ActivityNet1.3 dataset and 0.65 for the THUMOS2014 dataset. We keep the top-300 proposals after Soft-NMS for action classification. Subsequently, the classifier accepts these processed proposals to produce the prediction scores and refine the temporal boundaries of the action instances. Finally, we conduct a greedy non-maximum suppression (Greedy-NMS) to remove redundant localization results and set the overlap threshold of NMS to α − 0.1 in this paper, where α is the mAP threshold in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on THUMOS2014 Dataset 1) mAP Results:</head><p>We report the comparison results between our method and the state-of-the-art methods in Table <ref type="table" target="#tab_0">I</ref>. From Table <ref type="table" target="#tab_0">I</ref>, we can observe the following: (1) STAN outperforms most existing methods especially when α is greater than 0.3, which demonstrates that our method localizes the action boundaries with higher accuracy in more difficult situations. ( <ref type="formula" target="#formula_1">2</ref>) When using an extra proposal post-processing method PGCN that has been employed by <ref type="bibr" target="#b4">[5]</ref>, our method (STAN+PGCN) can achieve the state-of-the-art result with an mAP of 51.7% (α = 0.5). (3) Compared with existing methods using handcrafted features, our network can produce more discriminative video representations with the attention mechanism. (4) Compared with methods using deep one-stream features, our method still performs better than them in most cases. Concretely, STAN outperforms RNN-based methods <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref>, because it effectively couples the attention mechanism and the LSTM model in dual attention blocks and exploits the informative video segments for temporal modeling of the entire video to further enhance the action localization.</p><p>(5) STAN also performs better than the state-of-the-art methods using deep two-stream features <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b56">[56]</ref>. These methods usually use average pooling or concatenation operations to generate final video representations. This proves that the two-stream features of our method are more descriptive and discriminative  Table <ref type="table" target="#tab_0">II</ref> shows the comparison results of the per-class AP between our method and existing approaches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b53">[53]</ref> on the THUMOS2014 dataset. It is interesting to notice that our method achieves improvements on some challenging classes such as "CliffDiving," "LongJump" and "PoleVault," and performs more stable on different action classes. The results of our method are not ideal for locating the action of "FrisbeeCatch," probably because there are fewer differences between the action and background of "FrisbeeCatch". In other words, the "FrisbeeCatch" action has no clear decomposition structure, and our segment-based method can not accurately predict the action boundary.</p><p>2) Qualitative Results: Fig. <ref type="figure" target="#fig_3">4</ref> shows some examples of the prediction results on the THUMOS2014 dataset, i.e., "Bas-ketballDunk," "CricketShot," and "HighJump". Several video frames are sampled from video segments to represent the entire action instance. The temporal boundary of each localized action instance is measured in seconds. Each prediction duration with the highest classification score is associated with the nearest ground truth annotation. We observe that the temporal boundary of actions estimated by our method has a high tIoU with the corresponding ground truth. For the examples of "Basketball-Dunk" and "CricketShot," our method accurately localizes the action instance. For the "HighJump" example, the start of the predicted action is slightly earlier than the ground truth because it is difficult to determine the boundary between the preparation and the start of the "HighJump". In Fig. <ref type="figure" target="#fig_4">5</ref>, we also show several segment snapshots along with their attention weights of the second attention layer. As shown in Fig. <ref type="figure" target="#fig_4">5(b)</ref>, the video segments in the middle columns represent the important sub-actions of cliff diving, the weights of which are obviously larger than those of other segments. This means that the segments in the middle columns are more informative than the other segments, which is also in line with human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on ActivityNet Dataset 1) mAP Results:</head><p>We also compare STAN with the existing methods on the more complex ActivityNet1.3 dataset with various action lengths. From Tables I and III, we can see that our method does not perform as well on the ActivityNet1.3 dataset as it does on the THUMOS2014 dataset compared with several existing methods <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>, probably due to that the segment length on the ActivityNet1.3 dataset is much longer than that on the THUMOS2014 dataset (75 frames versus 15 frames), and thus </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III TEMPORAL ACTION LOCALIZATION RESULTS (MAP) (%) ON THE ACTIVITYNET1.3 DATASET. THE TWO HIGHEST SCORES ARE HIGHLIGHTED</head><p>our segment-level sliding window-based method may regress unclear frame-level action boundaries on the ActivityNet1.3 dataset. Specifically, both our method and the work of <ref type="bibr" target="#b59">[59]</ref> adopt segment-level sliding windows, so our method achieves comparable results compared with the method of <ref type="bibr" target="#b59">[59]</ref> on the Ac-tivityNet1.3 dataset at thresholds of 0.5 and 0.75. Our method performs slightly worse than the approach of <ref type="bibr" target="#b59">[59]</ref> at a threshold of 0.95, probably due to that the method of <ref type="bibr" target="#b59">[59]</ref> uses an extra longer context window to ensure that the boundaries of long action instances are captured. Our method performs slightly worse than <ref type="bibr" target="#b58">[58]</ref>, because the method of <ref type="bibr" target="#b58">[58]</ref> conducts frame-level predictions rather than segment-level predictions to generate proposals, which is more suitable for action localization on the Ac-tivityNet1.3 dataset. Nevertheless, our method yields a higher mAP at a threshold of 0.95 than the method of <ref type="bibr" target="#b58">[58]</ref>, which indicates that our method locates the action boundaries more accurately especially on the more difficult scenarios. Furthermore, although the average mAP of our method is 4% worse than that of <ref type="bibr" target="#b58">[58]</ref> on the ActivityNet1.3 dataset, our method achieves a significant improvement over the method of <ref type="bibr" target="#b58">[58]</ref> on the THU-MOS2014 dataset, and the mAP at the threshold of 0.5 has increased from 23.3% to 39.8%.</p><p>2) Qualitative Results: In Fig. <ref type="figure" target="#fig_5">6</ref>, we provide several localization results on the ActivityNet1.3 dataset, and in Fig. <ref type="figure" target="#fig_6">7</ref>, we provide some segment snapshots with the attention weights of the second attention layer. These weights reflect the importance of different segments for action classification and localization. For example, in Fig. <ref type="figure" target="#fig_6">7</ref>(c), we observe that a person is playing guitar and there is no obvious difference among these segments, so their weights are almost the same.    <ref type="bibr" target="#b10">(11)</ref>. It means that only segments containing "actionness" are focused on and background segments are ignored, where λ 2 in Eq. ( <ref type="formula" target="#formula_17">11</ref>) is fixed to 0.95 and is no longer decreased during training. "w/o LSTM" denotes replacing the LSTM models by weighted average pooling after attention modules, where Eq. ( <ref type="formula" target="#formula_3">3</ref>) is changed into h  It is interesting to observe that: (1) The motion information and the appearance information of videos are complementary. Both the TSN and VGG features can contribute to producing informative features with video segments. Moreover, the TSN feature is more effective than the VGG feature for action localization of videos. (2) The attention mechanism is useful in generating informative features of videos for temporal action localization, with the mAP gains of 9.4% and 4.5% on the THU-MOS2014 and ActivityNet1.3 datasets, respectively. (3) When TSN and VGG features are treated equally ("w/o feature-level attention"), the experiment results are even worse than that only using TSN features, possibly because the VGG feature misleads the action localization in certain cases. Therefore, it is useful to use feature-level attention to dynamically weigh different features. (4) The experiment results of "w/o second attention" also show the effectiveness of learning the measurement of globally context-aware video segments. <ref type="bibr" target="#b4">(5)</ref> The supervised attention learning can benefit from discarding the negative segments and the relaxation can improve the performance of the attention mechanism. <ref type="bibr" target="#b5">(6)</ref> The performance of "w/o LSTM" degrades, showing the effectiveness of the LSTM models in aggregating temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Evaluations 1) Ablation Study:</head><formula xml:id="formula_19">0 K = 1 K t (α 0 t * f t ).</formula><p>2) Evaluation of Different Segment Lengths: The lengths of video segments will influence the overall performance, because we use segment-level feature vectors for frame-level boundary regression. We conduct experiments to compare the results of different segment lengths on the THUMOS2014 dataset, as shown in Table <ref type="table">V</ref>. When the segment length is set to 15 frames, our method achieves the highest mAP at a threshold of 0.5. A possible reason for this is the longer segments failing to locate accurate frame-level action boundaries whereas shorter segments can not cover most action instances on the THUMOS2014 dataset.</p><p>3) Evaluation of Different Thresholds of NMS: We analyze the impact of the non-maximum suppression (NMS) on the boundary finding of our proposal generator. Table <ref type="table" target="#tab_0">VI</ref> shows the performance of the proposal generator in terms of different thresholds of Soft-NMS and Greedy-NMS on the Activi-tyNet1.3 dataset, where the threshold of 1 means that the NMS is not used. We use the conventional average recall with 100 proposals (AR@100) <ref type="bibr" target="#b50">[50]</ref> to evaluate the performance of the proposal generator. We observe that Soft-NMS performs better than Greedy-NMS, and the best result is achieved when the threshold is set to 0.8.  <ref type="table" target="#tab_5">VII</ref> , we make a comparison of the inference speed. We choose two segmentlevel sliding window-based methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b62">[62]</ref> and two framelevel proposal-based methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b58">[58]</ref> for comparison, and the inference speed is directly copied from their original papers, except for the speed of S-CNN <ref type="bibr" target="#b1">[2]</ref> is reported from <ref type="bibr" target="#b62">[62]</ref>. Our method achieves a rate of 203 FPS using a single NVIDIA GTX1080Ti GPU with a pre-trained TSN and VGG features, and is faster than the segment-level sliding window-based methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b62">[62]</ref> that use an end-to-end method to process the original high-dimensional video data. Our method is slower than the frame-level proposal-based methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b58">[58]</ref> that perform fully convolutional operations on the frame level, probably due to the recurrent architectures of the LSTMs for segment-level prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Weight Analysis of Different Features:</head><p>As shown in Table VIII, we compare the mean and standard deviation (std. dev.) of the weights of different features γ s and γ m in Eq. ( <ref type="formula" target="#formula_8">6</ref>) On both the THUMOS2014 and ActivityNet1.3 datasets, the mean value of γ s is much smaller than that of γ s , and their std. dev. values are small. This indicates that in most cases, the importance of motion features (TSN) is much greater than the static features (VGG) for action localization, which is consistent with the results of our ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) DETAD Analysis:</head><p>To further evaluate our method, we conduct the DETAD analysis <ref type="bibr" target="#b63">[63]</ref> on the THUMOS2014 dataset, including false positive analysis, average-mAP N sensitivity, and false negative analysis.</p><p>False Positive Analysis: Fig. <ref type="figure">8</ref>(a) shows the false positive profiles and the impact of error types on the average-mAP N of our method. We observe that the true positive rate is high in the top-1 G and top-2 G predictions, meaning that our method scores are higher on the true predictions and lower on the wrong predictions. This verifies that our method achieves good action localization results with fewer predictions. The background error of our method is high, mainly because we retain more sliding windows for higher recall rates when performing action proposals. The impact of the error types on the average-mAP N shows Fig. <ref type="figure">8</ref>. Illustration of the three types of analyses of the diagnostic tool on the THUMOS2014 dataset. (a) The false positive profiles of our methods and the impact of error types on the average-mAP N (0.5 tIOU). (b) The average-mAP of our method for different characteristics and the sensitivity profile. The dashed line is the overall performance. (c) The average false negative rate of our method for characteristics of the coverages, lengths, and numbers of instances. that eliminating more backgrounds and regressing action boundaries more accurately are two important ways to improve the performance of our method.</p><p>Average-mAP N Sensitivity: <ref type="bibr">Fig 8(b)</ref> shows the sensitivity of our method mAP N (0.5 tIoU) to the action characteristics of the coverages, lengths, and numbers of instances. The dashed line represents the overall performance. We find that our method achieves a higher mAP on the small (S) and medium (M) durations of videos as well as for the lengths of action instances. The sensitivity profile also shows that the performance of our method is related to the length of the video and the length of the action instances, probably because our method adopts segment-level LSTMs to aggregate sliding windows, which may be highly influenced by the temporal information. It is interesting to notice that our method does not show a strong sensitivity to the characteristic of the total number of instances in videos, which verifies that our method can effectively find multiple instances for each video.</p><p>False Negative Analysis: Fig. <ref type="figure">8(c</ref>) illustrates the false negative rate for three pairs of characteristics. The results are inverse to those of the average-mAP N sensitivity shown in Fig. <ref type="figure">8</ref>(b), and our method prefers to find multiple instances per video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. The learned weights can effectively capture the informativeness of video segments to represent the intrinsic motion and appearance of an action. The method is implemented through a supervised attention temporal network (STAN) consisting of a cascade attention module for temporal action localization. With the supervision of "actionness" information, the segment-level attention module can dynamically learn the weights of video segments to represent their contributions to action localization. The feature-level attention module can learn the weights of multiple segment features for combinations to further boost the localization performance. Extensive experiments on commonly used public datasets show the superior performance of STAN for temporally localizing actions in untrimmed videos. We believe that STAN is a general solution for capturing the intrinsic motion and appearance information in videos, and in the future, we plan to apply it to other video analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply. appearance characteristics of actions, thus contributes significantly to the action localization. r We build a supervised temporal attention network (STAN) to dynamically learn the weights of video segments through a supervised attention mechanism for representing the importance of different segments. r We design dual attention blocks to refine and encode the features of local segments with consideration of the global context information, where the first attention block measures the local video segments and the second attention block measures the globally context-aware video segments. r Experiment results on two challenging datasets of THU- MOS2014 and ActivityNet1.3 demonstrating the effectiveness of learning informative video segments for temporal action localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Prediction results of three action instances on the THUMOS2014 test dataset. The ground truth and prediction results are shown below the image sequences. The three action classes are "BasketballDunk," "CrieckShot", and "HighJump".</figDesc><graphic url="image-3.png" coords="8,68.75,74.21,457.46,175.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Typical examples showing the weights of segments in the segment-level attention module on the THUMOS2014 dataset. The action classes are (a) "Clean And Jerk," (b) "Cliff Diving," and (c) " ThrowDiscus". We only display 5 segments of each video clip and each segment is represented by only one frame. The values on the top-left of each frame represent the weights of each segment with the motion features. The values on the top-right of each frame represent the weights of each segment with the appearance features.</figDesc><graphic url="image-4.png" coords="9,44.63,66.65,509.06,238.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Prediction results of three action instances on the ActivityNet 1.3 validation dataset. The truth and the prediction results are shown below the image sequences. The three action classes are "Shaving," "Doing Karate", and "Playing Saxophone".</figDesc><graphic url="image-5.png" coords="10,66.23,73.49,462.86,176.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. examples showing the weights of segments in the segment-level attention module on the ActivityNet1.3 dataset. The action classes are (a) "Plataform Diving," (b) "Javelin Throw," and (c) " Playing Guitarra". We only display 5 segments of each video clip and each segment is represented by only one frame. The values at the top-left of each frame represent the weights of each segment with the motion features. The values at the top-right of each frame represent the weights of each segment with appearance features.</figDesc><graphic url="image-6.png" coords="10,45.35,330.17,498.26,231.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON THE THUMOS2014 DATASET WITH VARIED TIOU THRESHOLD α. WE USE THE MEAN AVERAGE PRESISION (MAP) (%) AS THE LOCALIZATION RESULTS. THE TWO HIGHEST SCORES ARE HIGHLIGHTED</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV TEMPORAL</head><label>IV</label><figDesc>ACTION LOCALIZATION RESULTS (MAP) (%) OF DIFFERENT COMPONENTS OF STAN (α = 0.5)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table IV shows the efficacy of different individual components on the action localization. "VGG" indicates that only VGG features are fed into the segment-level attention module to generate the final video representation for action localization, where the feature-level attention module Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF THE ACTION DETECTION SPEED TABLE VIII MEAN AND STD. DEV. OF γ s AND γ m ON THE THUMOS2014 AND ACTIVITYNET1.3 DATASETS</figDesc><table /><note>4) Speed Comparison: As shown in Table</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Natural Science Foundation of China (NSFC) under Grants 61673062 and 62072041. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Elisa Ricci.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ADSC submission at thumos challenge 2015</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit. THUMOS Workshop</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit. THUMOS Workshop</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assoc. Advance. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="11612" to="11619" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos using action pattern trees</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="717" to="730" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Infor. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A continuous learning framework for activity recognition using deep hybrid feature models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1909" to="1922" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tran</forename><forename type="middle">L</forename><surname>Bourdev R. Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro. Conf. Comput. Vis</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Content-attention representation by factorized action-scene network for action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1537" to="1547" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient action detection in untrimmed videos via multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comput. Vis</title>
				<meeting>IEEE Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vis. Conf</title>
				<meeting>British Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">R-C3D: Region convolutional 3 d network for temporal activity detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Authorized licensed use limited to: Tsinghua University</title>
		<imprint/>
	</monogr>
	<note>Downloaded on December 31,2022 at 14:56:46 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional network for multiscale temporal action proposals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3428" to="3438" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. THUMOS Workshop</title>
				<meeting>Eur. Conf. Comput. Vis. THUMOS Workshop</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sànchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. THUMOS14 Action Recognit</title>
				<meeting>THUMOS14 Action Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UTS-CMU at thumos 2015</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit. Workshop Competition Action Recog</title>
				<meeting>Conf. Comput. Vis. Pattern Recognit. Workshop Competition Action Recog</meeting>
		<imprint>
			<publisher>Large No. Classes</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient action detection in untrimmed videos via multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comput. Vis</title>
				<meeting>IEEE Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SST: Singlestream temporal action proposals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recognit</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the faster R-CNN architecture for temporal action localization</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vis. Conf</title>
				<meeting>British Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention-based convolutional neural network for machine comprehension</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04341</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput</title>
				<meeting>Eur. Conf. Comput<address><addrLine>Vis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Infor. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2204" to="2212" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical attention network for action recognition in videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="798" to="718" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based lstm and semantic consistency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unified spatio-temporal attention networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">T</forename><surname>Yao L. Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Joint network based attention for action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05215</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Rep</title>
				<meeting>Int. Conf. Learn. Rep</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05474</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3215" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Soft-NMS-improving object detection with one line of code</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">THUMOS Challenge: Action recognition with a large number of classes</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Infor. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Inter. Conf. Multimedia</title>
				<meeting>22nd ACM Inter. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">BSN: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lear submission at thumos 2014</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
				<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Precise temporal action localization by evolving temporal proposals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia Retrieval</title>
				<meeting>ACM Int. Conf. Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="388" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Action search: Spotting actions in videos and its application to temporal action localization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis</title>
				<meeting>Euro. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BLP-boundary likelihood pinpointing networks for accurate temporal action localization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acou. Speech Signal</title>
				<meeting>IEEE Int. Conf. Acou. Speech Signal</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1647" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf</title>
				<meeting>ACM Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CDC: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis., IEEE Int</title>
				<meeting>Comput. Vis., IEEE Int</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5727" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: Submission to activitynet challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph attention based proposal 3d convnets for action detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
				<meeting>Assoc</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4626" to="4633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DAPs: Deep action proposals for action understanding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
