<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Quality in Linear Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<email>zihangd@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<email>hanxiaol@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer Quality in Linear Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH 2 , matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9? on Wiki-40B and 12.1? on PG-19 for auto-regressive language modeling, and 4.8? on C4 for masked language modeling. * Applied to all models except the vanilla Transformer.</p><p>Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table <ref type="table">6</ref>. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref> and vision <ref type="bibr" target="#b7">(Dosovitskiy et al., 2020)</ref>. Although they have been growing in model size, most Transformers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications.</p><p>Many techniques have been proposed to speedup Transformers over extended context via more efficient attention mechanisms <ref type="bibr" target="#b3">(Child et al., 2019;</ref><ref type="bibr" target="#b5">Dai et al., 2019;</ref><ref type="bibr" target="#b22">Rae et al., 2019;</ref><ref type="bibr" target="#b4">Choromanski et al., 2020;</ref><ref type="bibr" target="#b32">Wang et al., 2020;</ref><ref type="bibr" target="#b16">Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr" target="#b33">Zaheer et al., 2020;</ref><ref type="bibr" target="#b17">Kitaev et al., 2020;</ref><ref type="bibr" target="#b26">Roy et al., 2021;</ref><ref type="bibr" target="#b15">Jaegle et al., 2021)</ref>. Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in state-of-the-art systems. Here we examine this issue from * Equal contribution.   a practical perspective, and find existing efficient attention methods suffer from at least one of the following drawbacks:</p><p>? Inferior Quality. Our studies reveal that vanilla Transformers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure <ref type="figure" target="#fig_1">1</ref>). Existing efficient attention methods often incur significant quality drop compared to augmented Transformers, and this drop outweighs their efficiency benefits.</p><p>? Overhead in Practice. Since efficient attention methods often complicate Transformer layers and require extensive data/memory formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPU or TPU.</p><p>? Inefficient Auto-regressive Training. Most attention linearization techniques enjoy fast decoding during inference, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large We address the above issues by developing a new model family that, for the first time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys linear scalability over the context size on modern accelerators. Unlike existing efficient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which naturally enables higher-quality approximation. Specifically, our model, named FLASH, is developed in two steps:</p><p>First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit (GAU) in Figure <ref type="figure">2</ref>. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of attention. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss.</p><p>We then propose an efficient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to first group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure <ref type="figure">4</ref>. We further describe how an accelerator-efficient implementation can be naturally derived from this formulation, achieving linear scalability in practice with only a few lines of code change.</p><p>We conduct extensive experiments to demonstrate the efficacy of FLASH over a variety of tasks (masked and autoregressive language modeling), datasets (C4, Wiki- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gated Attention Unit</head><p>Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers:</p><p>Vanilla MLP. Let X ? R T ?d be the representations over T tokens. The output for Transformer's MLP can be formulated as</p><formula xml:id="formula_0">O = ?(XW u )W o where W u ? R d?e , W o ? R e?d .</formula><p>Here d denotes the model size, e denotes the expanded intermediate size, and ? is an element-wise activation function.</p><p>Gated Linear Unit (GLU). This is an improved MLP variant augmented with gating <ref type="bibr" target="#b27">(Shazeer, 2020)</ref>. GLU has been proven effective in many cases <ref type="bibr" target="#b20">(Narang et al., 2021)</ref> and is used in state-of-the-art Transformer language models <ref type="bibr" target="#b8">(Du et al., 2021;</ref><ref type="bibr" target="#b30">Thoppilan et al., 2022)</ref>.</p><formula xml:id="formula_1">U = ? u (XW u ), V = ? v (XW v ) ? R T ?e (1) O = (U V )W o ? R T ?d (2)</formula><p>where stands for element-wise multiplication. In GLU, each representation u i is gated by another representation v i associated with the same token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Attention Unit (GAU).</head><p>The key idea is to formulate attention and GLU as a unified layer and to share their computation as much as possible (Figure <ref type="figure">2</ref>). This not only results in higher param/compute efficiency, but also naturally enables a powerful attentive gating mechanism. Specifically, GAU generalizes Eq. ( <ref type="formula">2</ref>) in GLU as follows:</p><formula xml:id="formula_2">O = (U V )W o where V = AV<label>(3)</label></formula><p>where A ? R T ?T contains token-token attention weights. Unlike GLU which always uses v i to gate u i (both associated with the same token), our GAU replaces v i with a potentially more relevant representation vi = j a ij v j "retrieved" from all available tokens using attention. The above will reduce to GLU when A is an identity matrix.</p><p>Consistent with the findings in <ref type="bibr" target="#b19">Liu et al. (2021)</ref>, the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss:</p><formula xml:id="formula_3">Z = ? z (XW z ) ? R T ?s (4) A = relu 2 Q(Z)K(Z) + b ? R T ?T (5)</formula><p>where Z is a shared representation (s d) 3 , Q and K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay-erNorms), and b is the relative position bias. We also find 3 Unless otherwise specified, we set s =128 in this work.</p><p>the softmax in MHSA can be simplified as a regular activation function in the case of GAU<ref type="foot" target="#foot_0">4</ref> . The GAU layer and its pseudocode are illustrated in Figure <ref type="figure">2</ref>. Unlike Transformer's MHSA which comes with 4d 2 parameters, GAU's attention introduces only a single small dense matrix W z with ds parameters on top of GLU (scalars and offsets in Q and K are negligible). By setting e = 2d for GAU, this compact design allows us to replace each Transformer block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed.</p><p>GAU vs. Transformers. Figure <ref type="figure">3</ref> shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention.</p><p>Layer Ablations. In Table <ref type="table" target="#tab_2">1</ref> &amp; 2 we show that both GAUs and Transformers are locally optimal on their own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fast Linear Attention with GAU</head><p>There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences:</p><p>? First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention without quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.</p><p>? In addition, the number of attention modules is naturally doubled with GAU -recall MLP+MHSA?2?GAU in terms of cost (Section 2). Since approximate attention usually requires more layers to capture full dependency <ref type="bibr" target="#b5">(Dai et al., 2019;</ref><ref type="bibr" target="#b3">Child et al., 2019)</ref>, this property also makes GAU more appealing in modeling long sequences.</p><p>With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformerlevel quality in linear time on long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Existing Linear-Complexity Variants</head><p>Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/sparse patterns, including local window <ref type="bibr" target="#b5">(Dai et al., 2019;</ref><ref type="bibr" target="#b22">Rae et al., 2019)</ref>, local+sparse <ref type="bibr" target="#b3">(Child et al., 2019;</ref><ref type="bibr" target="#b18">Li et al., 2019;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr" target="#b33">Zaheer et al., 2020)</ref>, axial <ref type="bibr" target="#b13">(Ho et al., 2019;</ref><ref type="bibr" target="#b14">Huang et al., 2019)</ref>, learnable patterns through hashing <ref type="bibr" target="#b17">(Kitaev et al., 2020)</ref> or clustering <ref type="bibr" target="#b26">(Roy et al., 2021)</ref>.</p><p>Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical benefits (speed and RAM efficiency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model.</p><p>Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decomposing the attention matrix and then re-arranging the order of matrix multiplications <ref type="bibr" target="#b4">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b32">Wang et al., 2020;</ref><ref type="bibr" target="#b16">Katharopoulos et al., 2020)</ref>. Schematically, the linear attention can be expressed as</p><formula xml:id="formula_4">Vlin = Q K V R d?d approx ---? Vquad = Softmax QK R T ?T V</formula><p>where Q, K, V ? R T ?d are the query, key and value representations, respectively. Re-arranging the computation reduces the complexity w.r.t T from quadratic to linear.</p><p>Another desirable property of linear attention is its constant 5 computation and memory for each auto-regressive decoding step at inference time. To see that, define M t = K :t V :t and notice that the computation of M t can be fully incremental: Our method significantly reduces the compute in quadratic attention (red links), while requiring substantially less RNNstyle steps (green squares) in conventional linear attention.</p><formula xml:id="formula_5">M t = M t-1 + K t V t (6)</formula><p>However, on the other hand, re-arranging the computation in linear attention leads to a severe inefficiency during autoregressive training. As shown in Fig. <ref type="figure">4</ref> (mid), due to the causal constraint for auto-regressive training, the query vector at each time step Q t corresponds to a different cache value M t = K :t V :t . This requires the model to compute and cache T different values {M t } T t=1 instead of only one value K V in the non-autoregressive case. In theory, the sequence {M t } T t=1 can be obtained in O(T d 2 ) by first computing {K t V t } T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependency of T steps, where an O(d 2 ) state needs to be processed each step. The sequential dependency not only limits the degree of parallelism, but more importantly requires T memory access in the loop, which usually costs much more time than computing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoretical complexity and actual running time. In practice, we find that directly computing the full quadratic attention matrix is even faster than the re-arranged (linearized) version on both TPUs (Figure <ref type="figure">6</ref>(a)) and GPUs (Appendix C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Method: Mixed Chunk Attention</head><p>Based on the strengths and weaknesses of existing linearcomplexity attentions, we propose mixed chunk attention, which merges the benefits from both partial attention and linear attention. The high-level idea is illustrated in Figure <ref type="figure">4</ref>. Below we reformulate GAU to incorporate this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preparation. The input sequence is first chunked into</head><formula xml:id="formula_6">G non-overlapping chunks of size C, i.e. [T ] ? [T /C ? C]. Then, U g ? R C?e , V g ? R C?e</formula><p>and Z g ? R C?s are produced for each chunk g following the GAU formulation in Eq. (1) and Eq. ( <ref type="formula">4</ref>). Next, four types of attention heads Q quad g , K quad g , Q lin g , K lin g are produced from Z g by applying per-dim scaling and offset (this is very cheap).</p><p>We will describe how GAU's attention can be efficiently approximated using a local attention plus a global attention. Note all the major tensors U g , V g and Z g are shared between the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Q lin g and K lin g (4?s parameters).</p><p>Local attention per chunk. First, a local quadratic attention is independently applied to each chunk of length C to produce part of the pre-gating state:</p><formula xml:id="formula_7">V quad g = relu 2 Q quad g K quad g + b V g . The complexity of this part is O(G ? C 2 ? d) = O(T Cd),</formula><p>which is linear in T given that C remains constant.</p><p>Global attention across chunks. In addition, a global linear attention mechanism is employed to capture longrange interaction across chunks Non-Causal:</p><formula xml:id="formula_8">V lin g = Q lin g G h=1 K lin h V h ,<label>(7)</label></formula><p>Causal:</p><formula xml:id="formula_9">V lin g = Q lin g g-1 h=1 K lin h V h .<label>(8)</label></formula><p>Note the summations in Eq. ( <ref type="formula" target="#formula_8">7</ref>) and Eq. ( <ref type="formula" target="#formula_9">8</ref>) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in tokenlevel linear attention by a factor of C (a typical C is 256 in our experiments), leading to a significant training speedup.</p><p>Finally, V quad g and V lin g are added together, followed by gating and a post-attention projection analogous to Eq. ( <ref type="formula" target="#formula_2">3</ref>):</p><formula xml:id="formula_10">O g = U g V quad g + V lin g W o .</formula><p>The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">DISCUSSIONS</head><p>Fast Auto-regressive Training. Importantly, as depicted in Fig. <ref type="figure">4</ref> (bottom), thanks to chunking, the sequential dependency in the auto-regressive case reduces from T steps in the standard linear attention to G = T /C steps in the chunked version in Eq. ( <ref type="formula" target="#formula_9">8</ref>). Therefore, we observe the autoregressive training becomes dramatically faster with the def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum( bgcs,bgce?bgse , k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum( bgcs,bgse?bgce , q, kv) else: kv = tf.einsum( bgcs,bgce?bse , k, v) return tf.einsum( bgcs,bse?bgce , q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum( bgns,bgms?bgnm , q, k) a = relu(qk + rel_pos_bias(q, k)) * * 2 a = causal_mask(a) if causal else a return tf.einsum( bgnm,bgme?bgne , a, v) def attn(x, v, causal, s=128): On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, instead of using the non-overlapping local attention, any partial attention variant could be used as a substitute while keeping the chunked linear attention fixed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> and BigBird <ref type="bibr" target="#b33">(Zaheer et al., 2020)</ref>. While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-benefit trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal partial attention variant is task-specific, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.</p><formula xml:id="formula_11"># x: [B x G x C x D]; v: [B x G x C x E] z = dense(x,</formula><p>Connections to Combiner. Similar to our method, Combiner <ref type="bibr" target="#b25">(Ren et al., 2021)</ref> also splits the sequence into nonoverlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the longrange information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We focus on two of our models that have different complexities with respect to the context length. The quadraticcomplexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH consists of both GAUs and the proposed mixed chunk attention.</p><p>To demonstrate their efficacy and general applicability, we evaluate them on both bidirectional and auto-regressive sequence modeling tasks over multiple large-scale datasets.</p><p>Baselines. First of all, the vanilla Transformer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> with GELU activation function <ref type="bibr" target="#b12">(Hendrycks &amp; Gimpel, 2016)</ref> is included as a standard baseline for calibration. Despite of being a popular baseline in the literature, we find that RoPE <ref type="bibr" target="#b29">(Su et al., 2021)</ref> and GLU <ref type="bibr" target="#b27">(Shazeer, 2020)</ref> can lead to significant performance boosts. We therefore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity.</p><p>To demonstrate the advantages of our models on long sequences, we further compare our models with two notable linear-complexity Transformer variants-Performer <ref type="bibr" target="#b4">(Choromanski et al., 2020)</ref> and Combiner <ref type="bibr" target="#b25">(Ren et al., 2021)</ref>, where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-benefit trade-off over many other approaches <ref type="bibr" target="#b25">(Ren et al., 2021)</ref>. To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner-Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE.</p><p>For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Profiler. See Appendix B for detailed settings and model specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bidirectional Language Modeling</head><p>In BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on the C4 dataset <ref type="bibr" target="#b23">(Raffel et al., 2020)</ref>. We consistently train each model with 2 18 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2? as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures <ref type="figure" target="#fig_4">5(b</ref>)-5(f), for all sequence lengths ranging from 512 to 8192, our models always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++'s final perplexity at step 125K, FLASH-Quad and FLASH can reduce the training cost by 1.1?-2.5? and 1.0?-4.8?, respectively. It is also worth mentioning that FLASH is the only linearcomplexity model that achieves competitive perplexity with its quadratic-complexity counterpart. See Appendix C.2 for detailed quality and speed comparisons for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Auto-regressive Language Modeling</head><p>For auto-regressive language modeling, we focus on the Wiki-40B <ref type="bibr" target="#b11">(Guo et al., 2020)</ref> and PG-19 <ref type="bibr" target="#b22">(Rae et al., 2019)</ref> datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model performance over long context lengths. We train and evaluate all models with 2 18 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19.</p><p>Figure <ref type="figure">6</ref>(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasing context lengths in Figures <ref type="figure">6(b</ref>)-6(f). Similar to the findings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Specifically, FLASH-Quad reduces the training time of Transformer++ by 1.2? to 2.5? and FLASH cuts the compute cost by 1.2? to 4.9? while reaching a similar perplexity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the context length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2.</p><p>For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table <ref type="table">9</ref>) is used for all models in comparison. The results are summarized in Table <ref type="table" target="#tab_5">3</ref>. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and training time over Transformer+ on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the final perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23? and 12.12? of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long-range nature of PG-19 (which consists of books). Similar to our previous  Significance of GAU. Here we study the importance of using GAU in FLASH. To achieve this, we apply the same idea of mixed chunk attention to Transformer++. We refer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC-TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> shows that FLASH outperforms MC-TFM++ by a large margin (more than 2? speedup when the sequence length is greater than 2048), confirming the importance of GAU in our design. We further look into the perplexity increase due to our approximation method in Table <ref type="table" target="#tab_6">4</ref>, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than going from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneficial to weaker/approximate attention mechanisms. Impact of chunk size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efficient Transformer variants. This is achieved by designing a performant layer (gated linear unit) and by combining it with an accelerator-efficient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art.</p><p>A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki-40B -All models are comparable in size at around 110M and trained for 125K steps with 2 18 tokens per batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. number of steps, making it infeasible to fully leverage the strength of modern accelerators during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Pseudocode for mixed chunk attention. chunk size is in {128, 256, 512}. With the inefficiency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and computation of O(Cd 2 ), where the additional constant C comes from the local quadratic attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Masked language modeling validation-set results on the C4 dataset -All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 2 18 tokens per batch. The quality is measured in negative log perplexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 Figure 6 :</head><label>56</label><figDesc>Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans-former+ are omitted for brevity as it lies in between Transformer and Transformer++. Across all the six models, latencies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating linear complexity with respect to context length. FLASH-Quad</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ablation study of the proposed FLASH architecture.experiments, FLASH achieves a lower perplexity than all of the full-attention Transformer variants, demonstrating the effectiveness of our efficient attention design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="15,57.87,86.75,481.16,124.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Impact of various modifications on GAU.</figDesc><table><row><cell>Neg. log pplx</cell><cell>3.1 3.0 2.9 2.8 2.7</cell><cell cols="2">0.0 335M Step/sec/TPU-core 0.2 42M 110M 41M 0.4 316M LM (Wiki-40B) 105M MHSA+MLP MHSA+GLU GAU</cell><cell>Neg. log pplx</cell><cell>1.3 1.2 1.5 1.4 1.6 1.7</cell><cell>335M 110M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU 105M 0.0 0.5 42M 41M 1.0 Steps/sec/TPU-core</cell><cell>Layer Type MHSA+MLP MHSA+GLU GAU</cell><cell># of Layers 8+8 12+12 24+24 8+8 12+12 24+24 15 22 46</cell><cell>d 512 768 1024 512 768 1024 512 768 1024</cell></row><row><cell></cell><cell cols="9">Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512).</cell></row><row><cell cols="3">Modifications</cell><cell cols="4">PPLX (LM/MLM) Params (M)</cell><cell></cell><cell></cell></row><row><cell cols="3">original GAU</cell><cell cols="2">16.78 / 4.23</cell><cell></cell><cell>105</cell><cell></cell><cell></cell></row><row><cell cols="3">relu 2 -? softmax</cell><cell cols="2">17.04 / 4.31</cell><cell></cell><cell>105</cell><cell></cell><cell></cell></row><row><cell cols="3">single-head -? multi-head</cell><cell cols="2">17.76 / 4.48</cell><cell></cell><cell>105</cell><cell></cell><cell></cell></row><row><cell cols="2">no gating</cell><cell></cell><cell cols="2">17.45 / 4.58</cell><cell></cell><cell>131</cell><cell></cell><cell></cell></row><row><cell cols="3">Modifications</cell><cell cols="4">PPLX (LM/MLM) Params (M)</cell><cell></cell><cell></cell></row><row><cell cols="3">original MHSA</cell><cell cols="2">16.87 / 4.35</cell><cell></cell><cell>110</cell><cell></cell><cell></cell></row><row><cell cols="3">softmax -? relu 2</cell><cell cols="2">17.15 / 4.77</cell><cell></cell><cell>110</cell><cell></cell><cell></cell></row><row><cell cols="3">multi-head -? single-head</cell><cell cols="2">17.89 / 4.73</cell><cell></cell><cell>110</cell><cell></cell><cell></cell></row><row><cell cols="2">add gating</cell><cell></cell><cell cols="2">17.25 / 4.43</cell><cell></cell><cell>106</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Impact of various modifications on MHSA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Auto-regressive language models on the PG-19 dataset -Latency (Lat.) is measured with 64 TPU-v4 cores. Measured based on time taken to match Transformer+'s final quality (at step 125K) on TPU.-Indicates that the specific model fails to achieve the same perplexity as Transformer+.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Context Length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Model</cell><cell></cell><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell></cell><cell cols="2">4096</cell><cell></cell><cell></cell><cell>8192</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="15">PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup*</cell></row><row><cell cols="4">Transformer+</cell><cell cols="2">44.45 282</cell><cell cols="2">1.00?</cell><cell></cell><cell cols="2">43.14 433</cell><cell>1.00?</cell><cell cols="2">42.80 698</cell><cell></cell><cell>1.00?</cell><cell cols="2">43.27 1292</cell><cell>1.00?</cell></row><row><cell cols="6">Transformer++ 44.47 292</cell><cell></cell><cell>-</cell><cell></cell><cell cols="2">43.18 441</cell><cell>-</cell><cell cols="2">43.13 712</cell><cell></cell><cell>-</cell><cell cols="2">43.26 1272</cell><cell>1.21?</cell></row><row><cell cols="3">Combiner</cell><cell></cell><cell cols="2">46.04 386</cell><cell></cell><cell>-</cell><cell></cell><cell cols="2">44.68 376</cell><cell>-</cell><cell cols="2">43.99 374</cell><cell></cell><cell>-</cell><cell>44.12</cell><cell>407</cell><cell>-</cell></row><row><cell cols="4">FLASH-Quad</cell><cell cols="2">43.40 231</cell><cell cols="2">2.18?</cell><cell></cell><cell cols="2">42.01 273</cell><cell>3.29?</cell><cell cols="2">41.46 371</cell><cell></cell><cell>3.59?</cell><cell>41.68</cell><cell>560</cell><cell>5.23?</cell></row><row><cell cols="3">FLASH</cell><cell></cell><cell cols="2">44.06 234</cell><cell cols="2">1.66?</cell><cell></cell><cell cols="2">42.17 237</cell><cell>3.85?</cell><cell cols="2">40.72 234</cell><cell></cell><cell>6.75?</cell><cell>41.07</cell><cell>250</cell><cell>12.12?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FLASH</cell><cell></cell><cell></cell><cell cols="2">FLASH (LocalOnly)</cell><cell cols="3">FLASH (GlobalOnly)</cell><cell></cell><cell cols="2">MC-TFM++</cell><cell></cell></row><row><cell></cell><cell>-2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Neg. log pplx</cell><cell>-3.2 -3.0 -4.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-4.6</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell cols="2">6 TPU-core-days 8 0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8 0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Perplexity increase when mixed chunk attention is applied to GAU (? FLASH) or to TFM++ (? MC-TFM++) -Lower the better. Results are over increasing context lengths.</figDesc><table><row><cell>MLM on C4</cell><cell>512</cell><cell cols="4">1024 2048 4096 8192</cell></row><row><cell>FLASH-Quad ? FLASH</cell><cell>0.0</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell><cell>0.07</cell></row><row><cell>TFM++ ? MC-TFM++</cell><cell>0.36</cell><cell>0.37</cell><cell>0.49</cell><cell>0.48</cell><cell>0.43</cell></row><row><cell>LM on Wiki-40B</cell><cell>512</cell><cell cols="4">1024 2048 4096 8192</cell></row><row><cell cols="3">FLASH-Quad ? FLASH -0.05 0.06</cell><cell>0.22</cell><cell>0.30</cell><cell>0.11</cell></row><row><cell>TFM++ ? MC-TFM++</cell><cell>0.54</cell><cell>0.75</cell><cell>0.86</cell><cell>0.90</cell><cell>0.87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>We use squared ReLU<ref type="bibr" target="#b28">(So et al., 2021)</ref> throughout this paper, which empirically works well on language tasks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors thank <rs type="person">Gabriel Bender</rs>, <rs type="person">John Blitzer</rs>, <rs type="person">Maarten Bosma</rs>, <rs type="person">Ed Chi</rs>, <rs type="person">Hanjun Dai</rs>, <rs type="person">Pieter-Jan Kindermans</rs> and <rs type="person">David So</rs> for their feedback.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code 7: Pseudocode for generating segment mask. WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02)   </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Connections to Combiner</head><p>To capture long-term information, Combiner <ref type="bibr" target="#b25">(Ren et al., 2021)</ref> additionally summarizes each chunk into summary key and value vectors K sum , V sum ? R T /C?d and concatenate them into the local quadratic attention, i.e.</p><p>Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix K lin h V h of size O(sd) which is s times larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners.</p><p>Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra [T /C ? T /C] attention matrix to combine the chunk summaries, e.g.</p><p>We indeed briefly experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C + T /C)d 2 ) which is length-dependent and no longer constant. Hence, we do not include this feature in our default configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table <ref type="table">5</ref>. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Model Specifications</head><p>Detailed specifications of all models used in our experiments are summarized in Tables <ref type="table">7,</ref><ref type="table">8</ref>, and 9. In the experiments, SiLU/Swish <ref type="bibr" target="#b9">(Elfwing et al., 2018;</ref><ref type="bibr" target="#b12">Hendrycks &amp; Gimpel, 2016;</ref><ref type="bibr" target="#b24">Ramachandran et al., 2017)</ref> is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU <ref type="bibr" target="#b12">(Hendrycks &amp; Gimpel, 2016)</ref> in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model.  <ref type="bibr" target="#b21">Nguyen &amp; Salazar (2019)</ref> and <ref type="bibr" target="#b0">Ba et al. (2016)</ref>, respectively. 4 ScaleSin re-scales sinusoidal position embedding <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> with a linearnable scalar for stability. 5 The learnable position embedding is proposed by <ref type="bibr" target="#b10">Gehring et al. (2017)</ref>. 6 The model is consist of 12 attention layers and 12 FFN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results</head><p>Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table <ref type="table">10</ref>) and the ablation study of chunk size for FLASH (in Figure <ref type="figure">8</ref>). 3 ScaleSin re-scales sinusoidal position embedding <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> with a linearnable scalar for stability. 4 The learnable position embedding is proposed by <ref type="bibr" target="#b10">Gehring et al. (2017)</ref>. 5 The model is consist of 12 attention layers and 12 FFN layers. 3 ScaleSin re-scales sinusoidal position embedding <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. We observe that the inefficiency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table <ref type="table">10</ref>, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Tabular MLM and LM Results</head><p>We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables <ref type="table">11</ref> and<ref type="table">12</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Ablation Study of Chunk Size</head><p>The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefficient auto-regressive training. Figure <ref type="figure">8</ref> shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pseudocode For FLASH-Quad and FLASH</head><p>We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8. def norm(x, begin_axis=-1, eps=1e-5, norm_type= layer_norm ):</p><p>"""Normalization layer.""" shape = x.shape.as_list() axes = list(range(len(shape)) </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Glam: Efficient scaling of language models with mixtureof-experts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06905</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural function approximation in reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
	<note>ICML&apos;17</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wiki-40b: Multilingual language model dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">General perception with iterative attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><surname>Perceiver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4651" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5243" to="5253" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pay attention to mlps</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Malkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11972</idno>
		<title level="m">Do transformer modifications transfer across implementations and applications? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<idno>CoRR, abs/1910.05895</idno>
		<ptr target="http://arxiv.org/abs/1910.05895" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1710.05941</idno>
		<ptr target="http://arxiv.org/abs/1710.05941" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Full attention transformer with sparse computation cost</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Combiner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MQQeeDiO5vv" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GLU variants improve transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>CoRR, abs/2002.05202</idno>
		<ptr target="https://arxiv.org/abs/2002.05202" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Primer: Searching for efficient transformers for language modeling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma?ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
