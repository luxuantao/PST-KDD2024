<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Register File Prefetching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sudhanshu</forename><surname>Shukla</surname></persName>
							<email>sudhanshu.shukla@intel.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Architecture Research Lab</orgName>
								<orgName type="institution">Intel Labs Jayesh Gaur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Register File Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3470496.3527398</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computer systems organization ‚Üí Superscalar architectures</term>
					<term>Pipeline computing</term>
					<term>Reduced instruction set computing Microarchitecture, Value Prediction, Address Prediction, Load Value Prefetching, Pipeline Prefetching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The memory wall continues to limit the performance of modern out-of-order (OOO) processors, despite the expensive provisioning of large multi-level caches and advancements in memory prefetching. In this paper, we put forth an important observation that the memory wall is not monolithic, but is constituted of many latency walls arising due to the latency of each tier of cache/memory. Our results show that even though level-1 (L1) data cache latency is nearly 40X lower than main memory latency, mitigating this latency offers a very similar performance opportunity as the more widely studied, main memory latency.</p><p>This motivates our proposal Register File Prefetch (RFP) that intelligently utilizes the existing OOO scheduling pipeline and available L1 data cache/Register File bandwidth to successfully prefetch 43.4% of load requests from the L1 cache to the Register File. Simulation results on 65 diverse workloads show that this translates to 3.1% performance gain over a baseline with parameters similar to Intel Tiger Lake processor, which further increases to 5.7% for a futuristic up-scaled core. We also contrast and differentiate register file prefetching from techniques like load value and address prediction that enhance performance by speculatively breaking data dependencies. Our analysis shows that RFP is synergistic with value prediction, with both the features together delivering 4.1% average performance improvement, which is significantly higher than the 2.2% performance gain obtained from just doing value prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Modern Out-of-Order (OOO) processors are limited by the latency of load instructions, figuratively called the memory wall. Multi-level cache hierarchies have hence become ubiquitous, with every generation of new processors investing in even larger and expensive caches than before <ref type="bibr" target="#b52">[53]</ref>. Figure <ref type="figure">1</ref> shows a typical multi-level cache hierarchy of a modern OOO processor, similar in parameters to Intel Tigerlake <ref type="bibr" target="#b21">[22]</ref>. There are three levels of cache hierarchy with increasing latency, followed by a slow DRAM memory. Load instructions bring data from the caches/memory into the Register File, from where they are supplied to the dependent instructions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b81">80]</ref>. As shown, the latency of slow main memory (200 cycles) is much higher than that of the lower-level caches (40X higher as compared to the level-1 cache which has a latency of 5 cycles). Therefore, significant efforts continue to be put into advance memory prefetching and cache management to overcome it <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>Figure <ref type="figure">1</ref> also shows the performance headroom from oracle prefetching across the different levels of cache and memory hierarchy on our experimental setup. Unsurprisingly, an oracle prefetching between the memory and the last level cache (LLC) shows a huge performance potential of 13.3%, which is why memory prefetching continues to be a very active area of research. Interestingly, Figure <ref type="figure">1</ref> also shows that mitigating level-1 (L1) data cache latency has a 9% performance headroom despite having 40X lower latency (5 cycles in Tiger Lake <ref type="bibr" target="#b21">[22]</ref>) than the main memory. However, this inconsistency becomes clear when we observe that an overwhelming majority of load requests (92.8%) hit in the L1 data cache, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. Given the sheer number of L1 hits, L1 data cache latency has a highly magnified impact on performance. To further understand this, Figure <ref type="figure" target="#fig_1">3</ref> shows an example program with its critical path of execution <ref type="bibr" target="#b25">[26]</ref> highlighted. The critical path, in this example, is created by an LLC miss but also comprises all the load instructions needed to calculate the address of the LLC miss load. To shorten the critical path, not only do we need to mitigate the LLC miss but also need to accelerate the load instructions which feed the dependence chains of these critical events. Evidently, the figurative memory wall is not just a monolithic wall between the LLC and DRAM but comprises multiple latency walls between the Register File, the on-die caches and the DRAM.</p><p>Concepts, techniques and implementations presented in this paper are subject matter of pending patent applications, which have been filed by Intel Corporation.</p><p>Figure <ref type="figure">1</ref>: Performance headroom from oracle prefetching across the different levels of cache and memory hierarchy for a processor similar to Intel Tiger Lake <ref type="bibr" target="#b21">[22]</ref>. An oracle prefetching from level N to level N-1 will ensure all hits at level N will be served at the latency of level N -1. Prefetching between L1 to RF and between Memory to LLC provide the highest headroom.</p><p>Overcoming the memory wall hence not only needs advancements in prefetching between the DRAM and LLC, but also between different levels of the cache hierarchy and Register File. This observation motivates us to focus our attention on prefetching between the L1 data cache and the Register File, and we make the following contributions:</p><p>(1) We show that despite being significantly lesser than main memory latency, L1 data cache latency has a highly magnified impact on the performance of modern OOO processors, a new insight not exploited by previous prefetching works that focus primarily on mitigating main memory latency. <ref type="bibr" target="#b1">(2)</ref> We hence propose RFP (Register File Prefetch) that prefetches load data from the L1 data cache to the Register File and hides the performance-critical latency of the L1 data cache. We demonstrate how RFP intelligently uses the existing OOO scheduling logic and does not entail additional bandwidth/power overheads or expensive pipeline modifications. (3) We show RFP's superiority to prior techniques like load address prediction which speculatively fetch data from the caches while the load is being decoded <ref type="bibr" target="#b66">[67]</ref>. We show why these prior techniques have diminishing opportunity due to constrained L1 bandwidth, in-flight stores and recent advancements in front-end latency reduction techniques like uop-cache <ref type="bibr" target="#b39">[40]</ref>. Consequently, we demonstrate how RFP's unique design makes it robust to each of these limitations and achieves higher performance at lower power. (4) We evaluate our proposal alongside load value prediction which speculatively breaks data dependencies and show that RFP and value prediction are synergistic in operation. Value prediction breaks true dependencies but has limited coverage due to costly pipeline flushes. On the other hand, RFP does not require pipeline flushes which allows it to target a much larger set of loads, otherwise not targeted by value prediction. We further show how RFP does not entail expensive modifications to a modern OOO pipeline, unlike value   prediction which requires costly ports or immediate moves to Register File <ref type="bibr" target="#b55">[56]</ref>.</p><p>Detailed performance evaluation of RFP on a baseline processor model with parameters similar to Intel TigerLake <ref type="bibr" target="#b21">[22]</ref>, shows that we achieve, on average, 43.4% prefetch coverage of all load accesses, and provide a substantial 3.1% gain in performance on a diverse set of 65 workloads. We also show that if L1 bandwidth issues can be addressed efficiently, this performance gain can increase to 4%, with coverage increasing to 57.4%. This result compares favourably to the oracle prefetch of Figure <ref type="figure">1</ref> which shows 9% performance potential for an oracular 100% coverage. Furthermore, the performance gains of RFP increases to 5.7% when evaluated on a futuristic core configuration with more execution resources. Finally, we show that RFP is synergistic with value prediction (VP) and both features together deliver 4.1% speedup which is higher than standalone VP (2.2%) and standalone RFP (3.1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we first discuss why prior works on load value and address prediction, memory prefetching and caching, cannot sufficiently mitigate L1 data cache latency. We then build an understanding into a typical OOO scheduling pipeline to understand how our proposal can be implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Value Prediction</head><p>Value Prediction (VP) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65]</ref> speculatively breaks dependencies to unlock higher instruction-level parallelism (ILP). It, however, offers limited coverage because of the high accuracy needed to avoid costly flushes on mis-speculations. RFP, in contrast, does not require flushes and thus allows low-confidence predictions. This enables RFP to cover a substantial fraction of loads not targeted by VP. However, unlike VP, RFP is constrained by L1 bandwidth and pipeline latencies. In Section 5.3, we demonstrates how RFP and VP complement each other and when combined they offer much higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Load Address Prediction</head><p>An idea related to VP is Address Prediction (AP) of loads, as address patterns are easier to predict than data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref>. DLVP <ref type="bibr" target="#b66">[67]</ref> proposed an address predictor based on load path history, which was extended to include stride address patterns by the Composite predictor <ref type="bibr" target="#b67">[68]</ref>. These schemes predict load addresses at instruction fetch and launch a request to the L1 data cache. The data is read from the cache by the time the instruction allocates in the OOO and is used as a predicted value for the instruction. If the predicted address is wrong, a pipeline flush is needed. As a result, these proposals, like VP, require very high accuracy, which in turn reduces their coverage. Moreover, modern OOO processors employ mechanisms like uopcache <ref type="bibr" target="#b39">[40]</ref> and loop stream detector <ref type="bibr" target="#b79">[79]</ref> to hide the front-end fetch latency. Since DLVP/Composite predictor relies on the early launch of loads during fetch, the opportunity to read data from the L1 cache on time is greatly diminished. Additionally, the DLVP proposal assumed a latency of 1 cycle for the L1 cache. However most modern OOO processors like Intel Tigerlake <ref type="bibr" target="#b21">[22]</ref> and AMD Ryzen <ref type="bibr" target="#b77">[78]</ref> have a higher latency (5 cycles), and a very limited number of L1 read ports. This again limits the opportunity to fetch early from the L1 cache. Address predictors also need extra bandwidth to validate the correctness of data, which further reduces the spare bandwidth available for an early read of L1 cache data. Finally, as the OOO machine becomes deeper, launching early load requests from fetch increases the likelihood of potential address conflicts with a large number of in-flight stores that are yet to execute. As a result, the address predictors have to be severely throttled using memory aliasing predictors <ref type="bibr" target="#b66">[67]</ref> to limit wrong execution and flushes.</p><p>A recent work, Efficient Pipeline Prefetch (EPP) by Alves et al. <ref type="bibr" target="#b1">[2]</ref>, attempts to reduce L1 accesses and power of DLVP <ref type="bibr" target="#b66">[67]</ref> predictor. It predicts load addresses in the front-end and enables load-load forwarding by sharing the register file entry between loads having the same predicted address. Just like DLVP and the Composite <ref type="bibr" target="#b67">[68]</ref> predictors, EPP also needs to flush the pipeline on a wrong address prediction and cannot predict loads that are likely to conflict with in-flight stores. Both these factors reduce the coverage of EPP, and it delivers performance similar to DLVP. Furthermore, to reduce L1 accesses and power, EPP employs complex multi-ported RF tags and a large multi-ported Store Sequence Bloom Filter (SSBF) <ref type="bibr" target="#b60">[61]</ref> to detect memory ordering violations. While SSBF simplifies validation of address predictions, it creates false-positives causing a fraction of loads to be re-executed at retirement. Overall, EPP introduces large hardware complexity while offering little performance benefit over previous load address predictors. In section 5 we quantitatively discuss how EPP, VP and AP compare with our proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Memory Prefetching</head><p>Memory Prefetching <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref> is a very well studied technique to hide the DRAM memory latency by learning address patterns of cache misses. Works on helper threads <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b83">82]</ref> employ runahead execution to prefetch data and effectively hide the memory latency. However, as we showed in Section 1, the performance potential from L1-RF prefetching is of considerable importance, which is traditionally overlooked by previous works.</p><p>Our problem of L1 cache to Register File prefetching is quite distinct from prior memory/cache prefetching approaches. For example, unlike caches, Register Files (RF) are not tagged and therefore will need an identifier in the RF to associate the prefetched data with the incoming load request. Additionally, due to latency optimizations like uop-cache and loop-stream-detector in the frontend, slack available to launch and retrieve a prefetch before a consumer needs it becomes very small. This warrants strict timeliness guarantees for RFP. We show in Section 3 how to address these requirements and efficiently prefetch from L1 to Register File.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Caching</head><p>Multi-level caching is ubiquitous in modern processors to hide memory latency, and continuous advances are being made to improve cache management <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53]</ref>. As reasoned in Section 1, the latency of the on-die caches has a significant impact on performance, with the L1 latency having the highest effect. Some past proposals <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> have tried to do early address calculation to reduce L1 latency for simple five-stage pipeline processors, but they do not apply to modern-day deep CPU pipelines. An option to reduce L1 latency could be to deploy an even smaller L0 cache. However, it is to be noted that L1 latency is composed of not just the L1 cache lookup latency, but also of many other components like address calculation, address translation as well as rotator circuitry to extract the required bytes from the 64B cacheline <ref type="bibr" target="#b84">[83]</ref>. This limits the fraction of latency that an L0 cache can save. To overcome this, L0 caches need to bypass address translation, which introduces high complexity in managing memory consistency and coherency. Also, they need to be of exact load size to avoid rotators, and cannot address vector and non-vector loads together. Additionally, loads need to check older stores for correctness and this check is typically done in parallel to L1 lookup ( <ref type="bibr" target="#b76">[77]</ref>). Predictors and flush mechanisms would be needed to avoid store forwarding checks. A smaller latency L0 cache with good coverage is hence very complex to build.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">OOO Pipeline</head><p>Figure <ref type="figure" target="#fig_2">4</ref> shows a typical OOO pipeline <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b81">80]</ref>. After fetch, instructions are buffered in the Instruction Queue <ref type="bibr" target="#b31">[32]</ref>. This is followed by register renaming where dependencies are detected between instructions. When a source instruction completes, it wakes up dependent instructions that are then scheduled. The typical pipeline for load scheduling involves wake up followed by a scheduler to pick ready loads and a few cycles to read the register files for dependent data. This is followed by an address calculation using an address generation unit. The load can now be sent to the L1 pipeline where it will perform address translation, L1 fetch and rotation to return the required bytes of data <ref type="bibr" target="#b81">[80]</ref>. Yoaz et al. <ref type="bibr" target="#b81">[80]</ref> describe how a typical load wakes up its dependents. To hide the execution pipeline latency (wake up, schedule, register file read etc.) dependents need to be speculatively scheduled before it is known whether the source load will hit or miss in the cache. For this, Yoaz et al. <ref type="bibr" target="#b81">[80]</ref> propose a hit-miss predictor which predicts if a load hits the L1 cache and speculatively wakes up its dependents. In case of a miss, all dependents (and their subsequent dependents) need to be re-executed. Note that this is not a full pipeline flush, since all these dependents are in flight and have not yet been executed. However, this takes some additional scheduler bandwidth for re-dispatches. We assume a similar hit-miss predictor in our baseline. We next use these learnings to describe our work Register File Prefetch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REGISTER FILE PREFETCH (RFP)</head><p>The goal of RFP is to hide the L1 latency by prefetching load data into the Register File. An RFP prefetch is performed using a predicted address, the corresponding load on execution checks if the predicted address matches the load address, and if so, the prefetched data is supplied to the dependents, and the load bypasses the caches completely. Otherwise, the load proceeds with cache access like a conventional OOO pipeline and supplies data to the dependents itself. We now describe how to build RFP starting with the three important design choices inherent to any prefetcher design -Timeliness, Bandwidth and Accuracy.</p><p>Timeliness: To increase timeliness, prefetch can be launched as soon as the load has been fetched in the front-end, similar to prior works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b66">67]</ref>. However, as discussed in Section 2, dispatching from the front-end in the presence of fetch latency optimizations like uop-caches does not yield much run-ahead. Moreover, the goal of RFP is not to add any extra L1 bandwidth pressure of re-checking data. Therefore, launching at fetch would require tracking all inflight stores to guarantee correctness, which would be extremely complex, as the OOO window continues to grow in processors. Finally, Register Files are not tagged (in contrast to caches) which means we need a location in the physical register file to store the prefetched data. We hence perform RFP prefetch after Rename stage where we can reuse the register file entry assigned to the load. This also allows us to reuse existing memory disambiguation logic to ensure correctness and data forwarding. Experimental data on our trace list shows that 63% of loads do not have their operands ready when they are allocated which implies we have enough opportunity to complete an RFP prefetch if it is launched after Renaming. For the remaining 37% loads, even though their operands are ready at allocation, the loads can only execute after they are picked by the scheduling policy in the OOO and have navigated the OOO pipeline (Figure <ref type="figure" target="#fig_4">6</ref>) to reach the execution units, as discussed in Section 2.5. This latency (atleast 3 cycles) still gives us a modest run-ahead window to dispatch and retrieve the prefetch request while the load is getting scheduled and hide a fraction of the load's latency.</p><p>Bandwidth: Traditional AP schemes <ref type="bibr" target="#b28">[29]</ref> [67] require two cache accesses-first for retrieving predicted data and second for validation, which applies significant pressure on scarce L1 bandwidth. To remedy this, we design RFP to guarantee that if the prefetched address was correct, the data in the Register File is up-to-date with the caches and inflight stores and needs no validation which means RFP is very frugal with L1 bandwidth. Since an RFP prefetch would be launched after renaming, it will perform memory disambiguation, store checks etc. and ensure the correctness of data, as long as the prefetch address was correct.</p><p>Accuracy: In the case of RFP, an incorrect prefetch requires that the original load request retrieve the correct data. Therefore, an RFP misprediction consumes additional L1 bandwidth and hence needs to be minimized. We should note that RFP's incorrect prefetches are less costly as compared to traditional AP which need a full pipeline flush when the prediction is incorrect. Hence, RFP can prefetch more aggressively, but to reduce interference with demand loads (non-prefetched), RFP prefetch should be given lower priority for L1 cache access.</p><p>We next detail steps to implement RFP in the OOO pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RFP Prefetcher</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows an RFP implementation in a typical OOO processor pipeline. We use a static-load PC indexed Prefetch Table (PT) to track and predict address of loads. The PT works similar to conventional stride prefetchers <ref type="bibr" target="#b53">[54]</ref> [27] <ref type="bibr" target="#b41">[42]</ref> [5] and caters to strided address which cover a large portion of modern-day applications. Logically, each entry holds a 16-bit tag, 2-bit utility, 1-bit confidence, 8-bit stride, 7-bit inflight counter and 64-bit virtual address. In Section 3.5, we will discuss area-efficient ways to build this table.</p><p>The PT is trained on load retirement which simplifies the logic for determining strides across addresses. On every load retirement, we look up the PT using the load PC. If the stride repeats, we increment the confidence with a probability of 1/16. The utility is also incremented. Once the confidence saturates, the load PC is eligible for RFP. If the stride changes, the confidence and utility are reset. If the stride keeps fluctuating, its utility remains low and the entry eventually gets evicted. The inflight counter tracks the number of outstanding instances of loads corresponding to a PT entry, similar to prior proposals <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b64">65]</ref>. It is incremented on load allocation and decremented on load commit. On a branch misprediction, this counter is decremented for each squashed load. The prefetch address is obtained from the base address, stride and the inflight counter present in the PT entry <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b67">68]</ref>. Some prior works suggest a good correlation between load addresses and program context <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b66">67]</ref>. Hence, we also experimented with a context-based prefetcher implementation similar to the Path-based Address Predictor <ref type="bibr" target="#b66">[67]</ref>. We show in Section 5 that this context-driven prefetcher only gives a small speedup over the strided prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Launching Register File Prefetch</head><p>We look up PT using the load PC to mark RFP-eligible loads. When a load is marked RFP-eligible, a prefetch packet for this load is sent to the LSQ (load-store-queue) and L1 cache for further processing. The prefetch packet contains the virtual address to be prefetched and the load's physical destination register id (prfid) into which the prefetched data will be written. Note that an instruction's prfid is only known after register renaming. Therefore, a prefetch request is triggered immediately after register renaming. Strictly, the prfid can be supplied later in the RFP pipeline, as it is needed only on a writeback. The PT lookup can happen earlier in the pipeline, before renaming, and we do not expect it to be on the timing critical path. The prefetch packet first arbitrates with other loads/RFP requests for access to the L1 ports. We give RFP prefetches the lowest priority when bidding against regular loads. This prevents latency escalation for regular loads. The RFP queue is organized as a FIFO, and older RFP requests get priority over younger ones. The prefetch will bring data from the L1 and write it into the register file at the prfid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Handling in-flight stores and Coherency. When an RFP is issued at load allocation, there may be older stores in reorderbuffer (ROB) that are yet to execute or are inflight. These stores could potentially modify the data at the load's memory address. To guarantee correctness, RFP accounts for all such stores which can potentially modify the load's data. When an RFP is launched to the L1 and LSQ, it scans all older stores (in a youngest-first order) and matches its address against the store's address. On a match, it waits for the store to complete and uses the store data instead of the cached data for the prefetch. If a store's address is unavailable (implying the store is yet to execute), we rely on the Memory Disambiguation (MD) <ref type="bibr" target="#b13">[14]</ref> mechanism to decide whether the RFP needs to wait on the store or skip it. Based on the MD prediction, the RFP request obtains the most up to date data from either an older store or the L1 cache. If the MD prediction turns out to be wrong, we flush the pipeline and restart execution from the load. However, if the MD prediction for the prefetch is incorrect but the load has not yet been dispatched, we do not need to flush the pipeline. In this case, the load does not use the prefetched data but looks up the LSQ and the caches again.</p><p>We should note that the entire sequence described above for handling inflight stores in RFP is the same as a conventional load pipeline <ref type="bibr" target="#b13">[14]</ref>. This guarantees that if the RFP's predicted address is correct, the prefetched data is also correct. This guarantee also removes the need for a second L1 access to verify the prefetched data. It further implies that when an RFP is correct, the number of L1 lookups, and hence L1 bandwidth requirements, remains the same. Only for wrong prefetching, extra L1 bandwidth is needed.</p><p>Coherency related flushes will also be addressed similar to inflight stores. Effectively, till the load is dispatched, RFP prefetch is treated as a proxy for the load (as if the load itself was launched earlier), and hence no changes are needed to enforce memory consistency. For example, the memory unit will automatically disallow reordering of the prefetch around memory barriers, enforcing same constraints that it would have put on the actual load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pipeline Simplifications.</head><p>If an RFP request misses the L1, we allow it to proceed further and bring data from the lower level caches similar to a regular demand load. In case of a DTLB miss, we drop the RFP for pipeline simplifications as TLB misses take long latency and RFP will not have enough run ahead left in such cases. In Section 5.5.5 we show sensitivity to these simplifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RFP Pipeline</head><p>In this section we revisit the working of a typical wakeup and scheduling logic in an OOO pipeline to help build the case for how RFP can be efficiently issued in the OOO pipeline. Figure <ref type="figure" target="#fig_4">6</ref> shows a typical OOO execution pipeline, as described by Stark et al. <ref type="bibr" target="#b75">[76]</ref> and Brown et al. <ref type="bibr" target="#b11">[12]</ref>. After fetch and decode, the instruction's logical registers are mapped to physical registers and register dependencies are established in the Rename stage. The Schedule stage picks instructions that are ready for execution after which the Register File is read in the Register Read stage. After reading all its sources, the instruction executes and ultimately retires from the OOO window. We assume a wire-OR style logic <ref type="bibr" target="#b75">[76]</ref> for instruction wakeup. Each Reservation Station (RS, also called Instruction Queue) entry contains 2 bit-vectors, called Dependence Vector, one for each operand of an x86 load instruction. The Dependence Vector has as many bits as the total number of RS entries. Each bit signifies if that operand has a dependency on the indicated RS entry.</p><p>When an instruction schedules, it resets the bit corresponding to its RS entry in the Dependence Vector of all younger instructions. When bits of all operands of an instruction become 0, it receives a wake-up. On wake-up, the instruction is speculatively scheduled which resets the bits for its dependents next cycle. If the dependent's Dependence Vector becomes zero, the dependent will schedule the next cycle. This ensures back-to-back speculative scheduling. However, a scheduled instruction may not be granted execution because of incorrect speculation or it did not win the arbitration for the execution unit. Such incorrectly scheduled instructions and their dependents need to be re-issued. Therefore, a Scoreboard is placed after the Register Read stage which drops incorrectly scheduled instructions from execution and reissues them. Hence, as specified by Stark et al. <ref type="bibr" target="#b75">[76]</ref> the scheduling pipeline is 3 cycles -1 cycle for instruction wakeup, 1 cycle for instruction selection and 1 cycle for PRF read/scoreboard, followed by execution.</p><p>Figures <ref type="figure" target="#fig_6">7 and 8</ref> show execution for two types of instructionsan ADD instruction dependent on another ADD instruction and an ADD instruction dependent on a LOAD instruction, respectively. As shown in Figure <ref type="figure" target="#fig_5">7</ref>, ADD instructions take 1 cycle for execution (ùê¥1, ùê¥2). To ensure back-to-back execution, when ùê¥1 is at instruction selection, ùê¥2 is speculatively woken. This aligns the pipeline of ùê¥2 to be 1-cycle after ùê¥1 which ultimately facilitates back-to-back execution. In Figure <ref type="figure" target="#fig_6">8</ref>, the LOAD instruction takes 5 cycles for execution. Therefore, ùê¥1 is woken 5-cycles after ùêø1 is scheduled which gives contiguous execution. At this point, the LOAD is underway in the L1 cache as shown in the figure. Since it is not known if the LOAD will hit or miss the L1 cache, the wakeup of the dependent is speculative. Yoaz et al. <ref type="bibr" target="#b81">[80]</ref> describe how a hit-miss predictor can be used to improve this speculation. Wrongly woken dependents will need to be cancelled and rescheduled. These scenarios largely summarize the sequence of operations in a modern OOO execution pipeline, for single-cycle as well as multi-cycle instructions. We now build on these learnings to build the implementation for RFP. To enable RFP, we first need a mechanism to communicate the RFP's progress to its corresponding load instruction. For this, we add a bit, called RFP-inflight, to the Load's RS entry which signals if the RFP has started execution for the load. In the default scenario, RFP-inflight is not set, so the dependents are woken up 5 cycles (L1 latency) after the load wake-up (assuming the hit-miss predictor indicates that the load hits in the L1). If the bit is set, the dependents are woken up such that they reach execution by the expected completion time of the RFP. Therefore, the timing of setting the RFP-inflight bit becomes important.</p><p>We cannot set RFP-inflight when the prefetch packet is generated because it usually takes some cycles to start prefetching due to port constraints. During this time, if the load becomes ready, the prefetch will get dropped and the load will continue. Hence, RFP-inflight cannot be set unless the prefetch has won arbitration to the LSQ and L1 which guarantees it will bring data. Otherwise, it is still the load's responsibility to wake up the dependents. In contrast, if we set the RFP-inflight bit too late, the load will be unaware of the in-flight prefetch because of which it may initiate its request leading to contention in the LSQ, caches. Therefore, we set the RFP-inflight bit once the prefetch starts looking up the cache as this is the earliest point when the RFP packet is guaranteed to not get dropped. This implies it is set in the first L1 lookup cycle after the Addr calculation stage. Based on the status of the RFP-inflight bit, the load's timing gets modified.</p><p>As shown in Figure <ref type="figure" target="#fig_7">9</ref>, when the load wakes up, it checks the RFP-inflight bit. Note that RFP-inflight is set 3 cycles before the RFP completes (assuming the RFP hits in L1) and the scheduling pipeline latency is also 3 cycles. This is not a coincidence. This equality ensures that, if the bit is set at load wake-up, the dependents reach execution just after the RFP completes (assuming RFP hits in L1) and no stalls are seen during execution. In the above case, the full load latency is saved by the RFP.</p><p>In some cases, RFP-inflight gets triggered sometime after load wake-up which means there is some delay in waking up the dependents. In this case, a fraction of the load latency is saved. Also, if the RFP misses the L1, it cancels the speculatively scheduled dependents and re-issues them once the data arrives from the lower level caches.</p><p>The above sequence of operations assumes that the Load's address and prefetch address are the same. However, if the prefetch address is incorrect, the prefetched data is incorrect and the load needs to re-lookup the cache to retrieve the correct data. Therefore, when we detect an address mismatch for the RFP, we cancel all its speculatively scheduled dependents. These dependents are re-issued later when the load dispatches. We should recall that cancellation and redispatch of speculatively scheduled dependents already happen in the OOO pipeline <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b81">80]</ref>, so this is not a new pipeline change.</p><p>As we demonstrated, by delegating the control of a load to its prefetch request, RFP enables eager instruction scheduling which shortens the load-to-use latency. Moreover, RFP assures that when the prefetched address is correct, the data is also correct and therefore, the number of L1 accesses remains the same.</p><p>Pipeline Variations. We have so far assumed that the physical register file is allocated when a load is allocated to the ROB. Hence the corresponding prefetch already knows the final destination to write the prefetched value. There are variations for Register File where a virtual pointer may be supplied to the original load request, whereas a final register file entry may be supplied at writeback <ref type="bibr" target="#b50">[51]</ref>. It is easy to adapt RFP to such a pipeline. RFP after dispatching behaves like the load and can be assigned the RF entry. In case of an incorrect prefetch, the same RF entry that was assigned to the prefetcher can be assigned back to the demand load request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RFP Timeliness</head><p>RFP is only issued after the original load has been allocated. If the original load request is ready to dispatch at allocation, RFP has a shorter window to launch in time. For such loads, RFP will have lower effectiveness. Also, RFP does not add any additional load ports. Instead, we opportunistically use free load ports. Load ports are known to be costly for timing and area <ref type="bibr" target="#b54">[55]</ref>, and hence RFP's timeliness is reduced because of limited L1 bandwidth. In Section 5.2   we will show the performance headroom if RFP timeliness can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Storage Calculations</head><p>RFP uses an 8-way set-associative PT, which comprises the bulk of the RFP hardware. To become more storage efficient, we leverage the fact that a lot of load addresses common Page Frame numbers (bits 63:12 of the address). Therefore, we record the most frequently occurring Page Frame numbers in a separate 64-entry, 4-way set-associative Page Address Table (PAT). Prior proposals have also exploited similar observations for designing area efficient BTB <ref type="bibr" target="#b74">[75]</ref>. The PT table only records a 6-bit pointer to the PAT entry and a 12-bit page offset using which it constructs the full virtual address for the RFP. This predicted address is later checked against the load address. If the predicted address is wrong, the delinquent PAT entry is replaced with the updated Page Address and the pointer in the PT entry is also adjusted accordingly. If a PAT entry is evicted, the pointer to it will become stale. RFP will relearn the correct address again after a misprediction. In Section 5, we will show that this area optimization has negligible performance impact but saves almost 50% storage.</p><p>As shown in Table <ref type="table" target="#tab_2">1</ref>, 1K entries results in a 6.5 KB PT, whereas 2K entries results in a 12 KB PT. Additionally, we assume a 64 entry RFP FIFO queue to hold RFP requests. We also add an extra bit to the RS entries to track RFP. In section 5.5.1, we show sensitivity to Prefetch Table size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION METHODOLOGY</head><p>We simulate a dynamically scheduled x86 core using an in-house, execution driven cycle-accurate simulator that models a 5-wide OOO core clocked at 4 GHz. The core parameters are similar to the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Field(bits) Storage</head><p>Prefetch Table <ref type="table">(</ref> Intel TigerLake processor <ref type="bibr" target="#b21">[22]</ref>. The main parameters are summarized in Table <ref type="table" target="#tab_3">2</ref>. We call this as our Baseline configuration in next sections. For our studies, the flush penalty of a value misprediction is 20 cycles <ref type="bibr" target="#b6">[7]</ref>. We selected 65 diverse, single-threaded applications for our experiments. The application suite comprises all SPEC17 workloads <ref type="bibr" target="#b18">[19]</ref>. Additionally, we use SPEC06 <ref type="bibr" target="#b15">[16]</ref> workloads and some well-known Cloud/Client benchmarks. Table <ref type="table">3</ref> summarises the complete list of benchmarks. All benchmarks are compiled for the x86 ISA (with AVX2 optimization) and carefully selected based on representation <ref type="bibr" target="#b68">[69]</ref>. The performance is measured in Instructions Per Cycle (IPC), and we use Geometric Mean to compute the mean speedup. We show category wise, as well as overall performance results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SIMULATION RESULTS</head><p>We first present the performance of RFP on our workloads in Section 5.1. All results assume 1K-entries for, both, stride-prefetcher and context-prefetcher and also that the full <ref type="bibr" target="#b63">64</ref>  <ref type="bibr" target="#b20">[21]</ref>, spark <ref type="bibr" target="#b82">[81]</ref>, bigbench <ref type="bibr" target="#b27">[28]</ref>, specjbb <ref type="bibr" target="#b17">[18]</ref>, specjenterprise <ref type="bibr" target="#b16">[17]</ref>, hadoop <ref type="bibr" target="#b2">[3]</ref>, tpcc <ref type="bibr" target="#b19">[20]</ref> Cloud sysmark <ref type="bibr" target="#b7">[8]</ref>, geekbench <ref type="bibr" target="#b42">[43]</ref> Client</p><p>Table <ref type="table">3</ref>: Applications used in this study is recorded in the PT. Later in Section 5.5, we show RFP's sensitivity to area optimizations like removing context-prefetching and employing the PAT table. Section 5.2 presents the timeliness and effectiveness of RFP. We compare our proposal to the state-of-theart VP in Section 5.3 and contrast with AP in Section 5.4. For fair performance comparison, we assume very large storage for all prior works. Finally, we perform a sensitivity analysis in Section 5.5 and conclude with a qualitative power analysis in Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance and Coverage of RFP</head><p>Figure <ref type="figure" target="#fig_8">10</ref> summarizes speedup delivered by RFP on different categories of workloads for the baseline processor. It also shows coverage achieved by RFP, which we define as the fraction of all loads for which prefetches were useful and the load correctly obtained data through a prefetch. RFP delivers a 3.1% speedup (geometric mean) over the baseline, while prefetching 43.4% of all loads (coverage). This 3.1% speedup is about 34% of the 9% performance headroom available from oracle prefetching between the L1 data cache and the register file presented in Figure <ref type="figure">1</ref>.  The leftmost side of the chart has workloads like spec06_tonto, spec06_gamess and spec06_milc, which have the lowest coverage for RFP and thus end up having the smallest IPC gains. Some workloads have negligible gains in IPC despite having high coverage, because they have performance bottlenecks elsewhere. For example, spec17_wrf, part of the FSPEC suite, shows negligible IPC gains from RFP. In general, we observed lower sensitivity for FSPEC17 because these applications are bottlenecked by AVX ports/FMA latency and not just by the L1/memory latency.</p><p>Figure <ref type="figure" target="#fig_10">11</ref> also illustrates that baseline performance is not hindered in presence of RFP, as RFP prefetches have lower priority than demand loads. Further, the right side of the chart has workloads that have the largest IPC gain from RFP. These workloads generally have high coverage for RFP. However, we also observe that some workloads are more sensitive to RFP compared to others. Applications like lammps, spec06_namd, spec17_xalancbmk and hadoop gains more than 4% in IPC despite having less than 40% coverage. These results highlight that some prefetches are more critical for performance and that not all prefetches have a high impact on  performance. This is an important observation given the scarce L1 bandwidth. Past proposals like Focused Value Prediction <ref type="bibr" target="#b6">[7]</ref> have exploited similar criticality phenomena to identify candidates that maximize performance gains while minimizing the value predictor footprint. CATCH <ref type="bibr" target="#b52">[53]</ref> also uses similar criticality learning to identify high impact candidates for prefetching from LLC to a reduced L2 cache. We leave the research on targeted prefetching for specific load instructions for future work.</p><p>We also simulate a futuristic up-scaled version of Tigerlake core, called Baseline-2x which is 10-wide, has all execution resources doubled and L1 bandwidth is increased, as compared to Tigerlake. Figure <ref type="figure" target="#fig_0">12</ref> shows that RFP gives 5.7% speedup and 53.7% coverage on the baseline-2x configuration. We find that owing to larger execution resources, baseline-2x exhibits more performance sensitivity to RFP compared to the baseline configuration. Additionally, baseline-2x has higher L1 bandwidth which allows more RFP requests to dispatch which increases the overall coverage. As shown, RFP performance continues to scale for a futuristic core, highlighting its viability for upcoming generations of processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Timeliness and Effectiveness of RFP</head><p>Figure <ref type="figure" target="#fig_11">13</ref> shows three bars for each category of workloads. The Prefetches Injected bar shows the fraction of all loads for which prefetch packets are injected. We observe that prefetch packets are injected for 72% of all loads on average. The Prefetches Executed bar shows the fraction of all loads for which prefetches are executed and data is prefetched in the register-file. It must be highlighted here, that a prefetch is only launched when L1 cache bandwidth is available and thus the execution of a prefetch might be delayed at the L1 cache controller. Further, it must be noted that a prefetch would be cancelled and dropped, if the corresponding load is launched before the prefetch, as described in section 3.2. Overall, it is observed that prefetches are executed for 48% of all loads. Thus for 24% of all loads, prefetch packets are injected, but the prefetch is not executed and is dropped. Most of these cancellations are due to the limited L1 cache bandwidth, where the prefetch gets delayed and the load gets launched before the prefetch.  The Prefetches Useful bar shows the fraction of all loads for which a prefetch was successfully executed and the load obtained the correct data which was prefetched in the register file. Overall, we observe that prefetches are successful for 43% of all loads on average. About 5% of all loads suffer from incorrect prefetches implying that a re-issue was needed for the load. Note that these prefetches to incorrect addresses don't have any impact on the latency of the corresponding loads, but do consume additional L1 bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.1</head><p>Impact of L1 cache bandwidth on timeliness of RFP. L1 cache bandwidth is critical for the timely execution of RFPs. In our baseline system, RFPs get the lowest priority for scheduling at the L1 cache controller. To highlight the impact of L1 cache bandwidth on the timely execution of RFPs, we model a system that doubles the number of L1 cache ports and dedicates half of the L1 cache ports  for executing RFPs. Figure <ref type="figure" target="#fig_12">14</ref> shows that the system with dedicated ports, delivers a speedup of 4%, while the baseline system with shared ports deliver a speedup of 3.1%. Further, this new system with dedicated ports executes 16.1% additional prefetches compared to the baseline system with shared ports. In the baseline system, these prefetches were getting cancelled due to the unavailability of ports which gets remedied in a dedicated port configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Effectiveness of RFP.</head><p>An RFP prefetch completely hides the latency of the corresponding load if it completes before the load is dispatched. In such cases, the load's dependents observe the load to have been effectively executed in a single cycle. We observe that on average for 34.2% of all loads, prefetch requests were able to complete before the corresponding loads were even dispatched. For these loads, the prefetches were able to hide the load's latency. Further, for 9.2% (= 43.4% -34.2%) of all loads, prefetches were delayed and were only able to partially save the load's latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Value Prediction (VP)</head><p>For the baseline processor, Figure <ref type="figure" target="#fig_3">15</ref> compares speedup and coverage of RFP against the state-of-the-art Composite value predictor proposed by Sheikh et al. <ref type="bibr" target="#b67">[68]</ref>. The Composite VP is essentially an intelligent fusion of EVES <ref type="bibr" target="#b64">[65]</ref> and DLVP <ref type="bibr" target="#b66">[67]</ref>   <ref type="bibr" target="#b1">[2]</ref> and the state-of-the-art Composite value predictor <ref type="bibr" target="#b67">[68]</ref>. The Composite VP is an intelligent fusion of EVES <ref type="bibr" target="#b64">[65]</ref> and DLVP <ref type="bibr" target="#b66">[67]</ref> predictors. gains (2.05%) for the EPP coupled system compared to the standalone Composite VP (2.20%) due to the extra load re-executions at retirement, that are introduced by false-positives from SSBF.</p><p>We also evaluated a new configuration that combines EVES <ref type="bibr" target="#b64">[65]</ref> and RFP, essentially demonstrating a fusion of VP and RFP. Here, an RFP is performed for a given load only if the load is not value predictable. This VP+RFP configuration gives 4.15% speedup with coverage of 54.6%, as shown in Figure <ref type="figure" target="#fig_3">15</ref>. We find that VP has constrained coverage due to its very high accuracy whereas RFP by virtue of being low-confidence, can cover many loads otherwise not targeted by VP. On the other hand, RFP is limited by L1 bandwidth and pipeline latencies which do not hinder VP. Therefore, RFP and VP work in a complementary manner as a result of which, their combined speedup is higher than their individual speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with Address Prediction</head><p>We study the DLVP predictor <ref type="bibr" target="#b66">[67]</ref> for this evaluation. DLVP predicts the address of loads in the Fetch Stage and probes the L1 cache to get the data using the predicted address. It then uses this fetched data to value predict the load. Our experiments with DLVP showed that, although it can address predict as many loads as RFP, the actual fraction of loads that get value predicted is only 11%. Further investigations reveal the following insights, also shown in Figure <ref type="figure" target="#fig_13">16</ref>:</p><p>(1) As mentioned above, the fraction of loads that are address predictable is similar to RFP. However, value predictors like DLVP use much higher confidence thresholds for checking load eligibility because of their significantly higher cost of misprediction. This high-confidence requirement reduces the fraction of eligible loads to 49% (APHC -AP High Confidence, Figure <ref type="figure" target="#fig_13">16</ref>). (2) Address predictors need to take into account the in-flight stores for data correctness. DLVP uses an additional predictor which prevents address prediction when there is a likelihood of the load getting store-forwarded. This no-FWD predictor  (3) Next, we note that DLVP only probes the caches when there is a port available. This means that all eligible loads may not probe the caches because of resource constraints. Our studies showed that port unavailability further drops DLVP coverage to 22%. (4) Lastly, a DLVP load only gets value predicted if the probed value is available by load allocation. Note that the L1 latency in modern processors is 5 cycles <ref type="bibr" target="#b21">[22]</ref>, and not 1 cycle as assumed in the DLVP proposal <ref type="bibr" target="#b66">[67]</ref>. Additionally, fetch latency optimizations like uop-caches, reduce the window available to finish the L1 cache probe before load allocation. Hence, only 11% probes were complete in time (ùëÉùëüùëúùëèùëíùëÜùë¢ùëêùëêùëíùë†ùë†).</p><p>As outlined above, prior address predictors are unable to convert a major fraction of eligible loads into useful value predictions whereas RFP converts almost 43% of loads (3.8X of DLVP coverage) into prefetches because of its unique design-First, RFP foregoes costly pipeline flushes on a misprediction, so it uses relaxed accuracy which boosts coverage. Secondly, RFP needs no additional L1 bandwidth which in turn gives it more opportunity to prefetch data. Finally, RFP continues to prefetch aggressively even in the presence of in-flight stores which is not possible with the prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">RFP Sensitivity</head><p>In this section, we show sensitivity to confidence thresholds, table sizes and other pipeline simplifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Impact of Confidence Counter</head><p>Width. The default RFP implementation uses 1-bit confidence counters for PT entries. Figure <ref type="figure" target="#fig_5">17</ref> shows the performance change with increasing width of confidence counters. As shown, widening confidence counters improve accuracy but drop prefetch coverage. For instance, 1-bit confidence counters have 5% incorrect prefetches but 4-bit counters have only 0.7% wrong prefetches. However, since RFP mispredictions are not very expensive, even 1-bit confidence counters are sufficient as   <ref type="table">entries</ref> 5.5.2 Impact of L1 cache latency. We observe that RFP's performance increases by 0.5% to 3.6% when the L1 data cache latency is increased from 5-cycles to 6-cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Impact of Context Prefetcher.</head><p>Our experimental configuration includes a context prefetcher based on the path-based address predictor (PAP) of DLVP <ref type="bibr" target="#b66">[67]</ref>. We find that the context prefetcher only gives a marginal 0.3% additional speedup over the stride prefetcher. Therefore, we conclude that to reduce storage, the stride prefetcher may be enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Impact of Page Address Table.</head><p>In Section 3.5, we had mentioned an area-efficient optimization of RFP wherein we do not record the full virtual address in the PT but only record a set-way pointer to a 64-entry PAT entry and a 12-bit page offset. This scheme works very well in most cases except when the load moves across to a different page. In this case, the recorded Page Address computes an incorrect virtual address causing an RFP misprediction. Additionally, the PAT pointers become stale and RFP needs to relearn a new PAT entry. Our analysis shows that such instances are infrequent. PAT table cuts our storage significantly and only causes a negligible 0.09% drop in performance.</p><p>5.5.5 Impact of Pipeline simplifications. In Section 3.2.2, we list two pipeline simplification techniques that help reduce hardware complexity. Our studies show that dropping an RFP request on a TLB miss has negligible performance impact. Performing RFP for L1 misses has a very small 0.02% performance upside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Power Analysis</head><p>RFP has a similar area overhead as traditional value and address predictors. Moreover, by using a separate Page Address Table, RFP reduces the storage requirements significantly which saves overall power. Additionally, if an RFP is correct, demand loads completely skip the L1 access, which means no extra L1 bandwidth and power are consumed for validation. Furthermore, RFP only has 5% incorrect prefetches, which cause a small addition to overall L1 cache power due to a re-lookup of the L1 but are still drastically cheaper than the costly pipeline flushes in value and address predictors. Overall, RFP blends easily into a modern OOO pipeline and does not entail significant design or power overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OTHER RELATED WORK</head><p>VP have been explored extensively in the past <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65]</ref>. Some recent works have focused on implementation details of VP. EOLE <ref type="bibr" target="#b56">[57]</ref> proposed implementation of VP without adding extra read ports to the register file for validation. FVP <ref type="bibr" target="#b6">[7]</ref> focused on only critical loads to manage the value predictor size. Recently, Perais <ref type="bibr" target="#b55">[56]</ref> proposed targeted VP that leverages register renaming optimizations to reduce hardware complexity, while still preserving most of the gains from VP. Unlike VP, RFP seamlessly blends into the traditional OOO pipeline and does not add significant hardware or pipeline changes. In Section 5, we demonstrated that VP and RFP are synergistic. Load address predictors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref> have also been proposed with the goal to early fetch the data from the caches using AP and use it for value predicting the load. As discussed in Section 2, these schemes need costly flushes on wrong speculation and have limited coverage. Furthermore, these prior works require two cache accesses -one for reading data for VP and a subsequent access for validation, thereby putting pressure on precious L1 cache bandwidth. In contrast, RFP does not take additional L1 bandwidth. We compared RFP with DLVP <ref type="bibr" target="#b66">[67]</ref> and Composite <ref type="bibr" target="#b67">[68]</ref> predictors in Section 5.</p><p>A recent work, CRISP <ref type="bibr" target="#b46">[47]</ref>, leverages criticality based scheduling to execute delinquent loads and their load slices as early as possible. Criticality based instruction scheduling policies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b63">64]</ref> reduce the scheduling pipeline latency for critical instructions, while RFP is complementary to these works and hides the L1 cache access latency. Intelligently combining RFP with criticality-based solutions can further reduce end-to-end pipeline latency for critical loads and their load slices, and result in even higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY</head><p>In this paper, we showed that to overcome the memory wall, future processors will have to mitigate the L1 data cache latency. We hence proposed RFP (Register File Prefetch) that successfully prefetches 43.4% of loads to the Register File, reducing effective L1 latency and providing a significant performance gain of 3.1%. We contrasted RFP with state-of-the-art value predictors and showed how RFP is synergistic with these proposals. We also distinguished RFP from existing load address predictors that rely on high confidence address prediction and early fetch of data from the processor front-end itself, but suffer from diminished opportunity because of advancements in front-end latency reduction techniques as well as constrained L1 cache bandwidth. Our proposal RFP blends easily into modern OOO pipeline, requiring small area addition, while providing significant performance upside for current as well as future processors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of demand load requests across the cache hierarchy. MSHR hits refer to loads that are waiting on prior in-flight prefetches or demand load requests that are being served by lower-level caches or memory.</figDesc><graphic url="image-2.png" coords="2,317.96,285.62,240.24,90.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example program with critical path of execution highlighted. The critical path is created by an LLC Miss, but all L1 hits on the dependence chain of the LLC Miss contribute to critical path length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pipeline stages in a typical OOO processor</figDesc><graphic url="image-4.png" coords="4,317.96,83.69,240.23,135.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: RFP structures in an OOO processor pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A typical OOO scheduling pipeline</figDesc><graphic url="image-5.png" coords="5,317.96,390.31,240.24,85.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: OOO schedule of an ADD instruction dependent on another ADD instruction. The ADD (A2) is woken when the ADD (A1) has been selected for execution to achieve back-toback contigious execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: OOO schedule of an ADD instruction dependent on a LOAD instruction. The ADD (A1) is woken 5-cycles after the LOAD (L1) is scheduled to achieve back-to-back contigious execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: RFP assisted OOO schedule of an ADD instruction dependent on a LOAD instruction highlighting reduction in Load latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance and Coverage of RFP on the baseline processor Figure 11 depicts a line graph for all workloads in our study list, showing a correlation between IPC gains and coverage for RFP.The leftmost side of the chart has workloads like spec06_tonto, spec06_gamess and spec06_milc, which have the lowest coverage for RFP and thus end up having the smallest IPC gains. Some workloads have negligible gains in IPC despite having high coverage, because they have performance bottlenecks elsewhere. For example, spec17_wrf, part of the FSPEC suite, shows negligible IPC gains from RFP. In general, we observed lower sensitivity for FSPEC17 because these applications are bottlenecked by AVX ports/FMA latency and not just by the L1/memory latency.Figure11also illustrates that baseline performance is not hindered in presence of RFP, as RFP prefetches have lower priority than demand loads. Further, the right side of the chart has workloads that have the largest IPC gain from RFP. These workloads generally have high coverage for RFP. However, we also observe that some workloads are more sensitive to RFP compared to others. Applications like lammps, spec06_namd, spec17_xalancbmk and hadoop gains more than 4% in IPC despite having less than 40% coverage. These results highlight that some prefetches are more critical for performance and that not all prefetches have a high impact on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: IPC and Coverage of RFP for all workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Timeliness and Accuracy of RFP on the baseline processor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Impact of L1 cache bandwidth on timeliness of RFP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Coverage of DLVP predictor under various constraints further reduces the fraction of eligible loads to 45% (shown in APHC + no-FWD).(3) Next, we note that DLVP only probes the caches when there is a port available. This means that all eligible loads may not probe the caches because of resource constraints. Our studies showed that port unavailability further drops DLVP coverage to 22%. (4) Lastly, a DLVP load only gets value predicted if the probed value is available by load allocation. Note that the L1 latency in modern processors is 5 cycles<ref type="bibr" target="#b21">[22]</ref>, and not 1 cycle as assumed in the DLVP proposal<ref type="bibr" target="#b66">[67]</ref>. Additionally, fetch latency optimizations like uop-caches, reduce the window available to finish the L1 cache probe before load allocation. Hence, only 11% probes were complete in time (ùëÉùëüùëúùëèùëíùëÜùë¢ùëêùëêùëíùë†ùë†).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: RFP performance and coverage sensitivity to Prefetch Table entries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Storage requirements for RFP</figDesc><table><row><cell></cell><cell>Tag (16b), Confidence (3b),</cell><cell></cell></row><row><cell></cell><cell>Utility (2b), Stride (5b),</cell><cell>6.5KB -</cell></row><row><cell>1024-2048 entries)</cell><cell>Inflight (7b), PAT Pointer (6b),</cell><cell>12KB</cell></row><row><cell></cell><cell>Page Offset (12b)</cell><cell></cell></row><row><cell>Page Address Table (64 entries)</cell><cell>Page Address 44b</cell><cell>352b</cell></row><row><cell>RFP-Inflight (128 entries)</cell><cell>1b</cell><cell>128b</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Core parameters for simulation</figDesc><table><row><cell>-bit virtual address</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Figure 17: Impact of confidence counter width used for Prefetch Tableentrieson RFP performance and coverage they provide significantly higher coverage at a marginally reduced accuracy. Figure18further shows that increasing entries in PT from 1K to 16K leads to minor improvements in performance and further scaling of entries shows no visible improvement.</figDesc><table><row><cell></cell><cell></cell><cell>5.6%</cell><cell>Speedup</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Prefetches Injected</cell><cell>90%</cell></row><row><cell></cell><cell></cell><cell>5.2%</cell><cell cols="3">Prefetches Executed</cell><cell></cell><cell cols="3">Prefetches Useful (Coverage)</cell><cell>80%</cell></row><row><cell cols="2">Speedup over baseline</cell><cell>2.8% 3.2% 3.6% 4.0% 4.4% 4.8%</cell><cell cols="2">57.5% 38.4% 37.7%</cell><cell cols="2">2.7% 61.4% 41.3% 39.9%</cell><cell cols="2">2.9% 66.2% 44.3% 41.6%</cell><cell>3.1% 72.4% 48.3% 43.4%</cell><cell>20% 30% 40% 50% 60% 70%</cell><cell>Coverage (Fraction of all Loads)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell></row><row><cell></cell><cell></cell><cell>2.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4-Bit</cell><cell></cell><cell>3-Bit</cell><cell></cell><cell>2-Bit</cell><cell></cell><cell>1-Bit</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Width of Confidence Counters used for Prefetch Table Entries</cell></row><row><cell></cell><cell cols="2">4.6%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Speedup</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Prefetches Injected</cell></row><row><cell></cell><cell cols="2">4.4%</cell><cell cols="3">Prefetches Executed</cell><cell></cell><cell cols="3">Prefetches Useful (Coverage)</cell><cell>80%</cell></row><row><cell>Speedup over baseline</cell><cell cols="2">3.4% 3.6% 3.8% 4.0% 4.2%</cell><cell>72.4% 48.3% 43.4%</cell><cell cols="2">3.2% 74.7% 50.1% 45.0%</cell><cell cols="2">3.3% 76.3% 51.3% 46.0%</cell><cell>3.4% 77.4% 52.2% 46.8%</cell><cell>3.5% 78.1% 52.7% 47.3%</cell><cell>30% 40% 50% 60% 70%</cell><cell>Coverage (Fraction of all Loads)</cell></row><row><cell></cell><cell cols="2">3.2%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">3.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1K</cell><cell>2K</cell><cell></cell><cell>4K</cell><cell></cell><cell>8K</cell><cell>16K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Number of Predictor Table Entries</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Delay and Bypass: Ready and Criticality Aware Instruction Scheduling in Outof-Order Processors</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Early Address Prediction: Efficient Pipeline Prefetch and Reuse</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">39</biblScope>
			<date type="published" when="2021-06">2021. June 2021</date>
		</imprint>
	</monogr>
	<note>22 pages</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Apache Software Foundation</title>
		<ptr target="https://hadoop.apache.org" />
	</analytic>
	<monogr>
		<title level="m">Hadoop</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-Cycle Loads: Microarchitecture Support for Reducing Load Latency</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Microarchitecture</title>
				<meeting>the 28th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing &apos;91:Proceedings of the 1991 ACM/IEEE Conference on Supercomputing</title>
				<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bingo Spatial Data Prefetcher</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focused Value Prediction</title>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Bandishte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeev</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihu</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Bapco</surname></persName>
		</author>
		<ptr target="https://bapco.com/products/sysmark-2018/" />
		<title level="m">SYSmark</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Correlated Load-Address Predictors</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Kirshenboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihu</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on Computer Architecture</title>
				<meeting>the 26th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DSPatch: Dual Spatial Pattern Prefetcher</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptron-Based Prefetch Filtering</title>
		<author>
			<persName><forename type="first">Eshan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gino</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim√©nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Select-Free Instruction Scheduling Logic</title>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 34th ACM/IEEE International Symposium on Microarchitecture</title>
				<meeting>34th ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">MICRO-34</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introducing Hierarchy-awareness in Replacement and Bypass Algorithms for Last-level Caches</title>
		<author>
			<persName><forename type="first">Mainak</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithiyanandan</forename><surname>Bashyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Nuzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Memory Dependence Prediction Using Store Sets</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Z</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 25th Annual International Symposium on Computer Architecture</title>
				<meeting>25th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speculative Precomputation: Longrange Prefetching of Delinquent Loads</title>
		<author>
			<persName><forename type="first">Jamison</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Fong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
				<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="https://www.spec.org/cpu2006/" />
	</analytic>
	<monogr>
		<title level="m">SPEC CPU</title>
				<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="https://www.spec.org/jEnterprise2010/" />
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="https://www.spec.org/jbb2015/" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="https://www.spec.org/cpu2017/" />
	</analytic>
	<monogr>
		<title level="j">SPEC CPU</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="http://www.tpc.org/tpcc/" />
		<title level="m">TPC-C</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://www.tpc.org/tpce/" />
		<title level="m">TPC-E</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intel&apos;s 11th Gen Core Tiger Lake SoC Detailed: SuperFin, Willow Cove and Xe-LP</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Cutress</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/15971/intels-11th-gen-core-tiger-lake-soc-detailed-superfin-willow-cove-and-xelp" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effectiveness of Hardware-Based Stride and Sequential Prefetching in Shared-Memory Nultiprocessors</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenstr√∂m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1995 1st IEEE Symposium on High Performance Computer Architecture</title>
				<meeting>1995 1st IEEE Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss</title>
		<author>
			<persName><forename type="first">James</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on Supercomputing</title>
				<meeting>the 11th international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A load-instruction unit for pipelined processors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassiliadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focusing Processor Policies via Critical-Path Prediction</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rastislav</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
				<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stride Directed Prefetching In Scalar Processors</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janak</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><forename type="middle">L</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Janssens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 25th Annual International Symposium on Microarchitecture MICRO 25</title>
				<meeting>the 25th Annual International Symposium on Microarchitecture MICRO 25</meeting>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BigBench: Towards an Industry Standard Benchmark for Big Data Analytics</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Ghazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilmann</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meikel</forename><surname>Poess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Crolotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Arno</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the 2013 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speculative Execution via Address Prediction and Data Prefetching</title>
		<author>
			<persName><forename type="first">Jos√©</forename><surname>Gonz√°lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gonz√°lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Supercomputing</title>
				<meeting>the 11th International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continuous Runahead: Transparent Hardware Acceleration for Memory Intensive Workloads</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving Branch Predictors by Correlating on Data Values</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">H</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zak</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-32. Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><surname>Patterson</surname></persName>
		</author>
		<title level="m">Computer Architecture: A Quantitative Approach</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Memory Prefetching Using Adaptive Stream Detection</title>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;06)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Access Map Pattern Matching for Data Cache Prefetch</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Supercomputing</title>
				<meeting>the 23rd international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Back to the Future: Leveraging Belady&apos;s Algorithm for Improved Cache Replacement</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on Computer architecture</title>
				<meeting>the 37th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prefetching using Markov Predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international symposium on Computer architecture</title>
				<meeting>the 24th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Helper Thread Prefetching for Loosely-Coupled Multiprocessor Systems</title>
		<author>
			<persName><forename type="first">Changhee</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daeseob</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 20th IEEE International Parallel Distributed Processing Symposium</title>
				<meeting>20th IEEE International Parallel Distributed Processing Symposium</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Instruction Criticality Based Energy-Efficient Hardware Data Prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Neelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biswabandan</forename><surname>Kalani</surname></persName>
		</author>
		<author>
			<persName><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="146" to="149" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">UC-Check: Characterizing Micro-operation Caches in x86 Processors and Implications in Security and Performance</title>
		<author>
			<persName><forename type="first">Joonsung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamin</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hunjun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jangwoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Path Confidence based Lookahead Prefetching</title>
		<author>
			<persName><forename type="first">Jinchun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Narasimha Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">T2: A Highly Accurate and Energy Efficient Stride Prefetcher</title>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Kondguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Design (ICCD)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Primate Labs</title>
		<ptr target="https://www.geekbench.com/" />
	</analytic>
	<monogr>
		<title level="m">Geekbench 5 CPU Benchmark</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<ptr target="https://www.lammps.org" />
		<title level="m">LAMMPS</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dead-Block Prediction &amp; Dead-Block Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">An-Chow</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Computer Architecture</title>
				<meeting>the 28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Value Locality and Load Value Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Paul</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CRISP: Critical Slice Prefetching</title>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic Helper Threaded Prefetching on the Sun UltraSPARC¬Æ CMP Processor</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khoa</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;05)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extended Histories: Improving Regularity and Performance in Correlation Prefetchers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manikantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers</title>
				<meeting>the 6th International Conference on High Performance and Embedded Architectures and Compilers</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Best-Offset Hardware Prefetching</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Delaying Physical Register Allocation Through Virtual-Physical Registers</title>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Monreal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos√©</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Vi√±als</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-32. Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data Cache Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Symposium on High Performance Computer Architecture (HPCA&apos;04)</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Criticality Aware Tiered Cache Hierarchy: A Fundamental Relook at Multi-level Cache Hierarchies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Anant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual International Symposium on Computer Architecture</title>
				<meeting>the 45th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evaluating Stream Buffers as a Secondary Cache Replacement</title>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 21 International Symposium on Computer Architecture</title>
				<meeting>21 International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reducing Design Complexity of the Load/Store Queue</title>
		<author>
			<persName><forename type="first">Il</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><forename type="middle">Liang</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 36th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Leveraging Targeted Value Prediction to Unlock New Hardware Strength Reduction Potential</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">EOLE: Paving the Way for an Effective Implementation of Value Prediction</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Practical Data Value Speculation for Future High-end Processors</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">BeBoP: A cost effective predictor infrastructure for superscalar value prediction</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sandbox Prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Seth H Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Fei</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><surname>Shih-Lien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kingsum</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Store Vulnerability Window (SVW): Re-Execution Filtering for Enhanced Load Optimization</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A decoupled predictor-directed stream prefetching architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="260" to="276" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The Predictability of Data Values</title>
		<author>
			<persName><forename type="first">Yiannakis</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th Annual International Symposium on Microarchitecture</title>
				<meeting>30th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Long Term Parking (LTP): Criticality-aware Resource Allocation in OOO Processors</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring value prediction with the EVES predictor</title>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Championship Value Prediction</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A case for (partially) TAgged GEometric history length branch prediction</title>
		<author>
			<persName><forename type="first">Andr√©</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-level Parallelism -JILP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">01</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Load Value Prediction via Path-based Address Prediction: Avoiding Mispredictions due to Conflicting Stores</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raguram</forename><surname>Damodaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Efficient Load Value Prediction Using Multiple Predictors and Filters</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatically Characterizing Large Scale Program Behavior</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 10th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficiently Prefetching Complex Address Patterns</title>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Applying Deep Learning to the Cache Replacement Problem</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sequential Program Prefetching in Memory Hierarchies</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Using a User-Level Memory Thread for Correlation Prefetching</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 29th Annual International Symposium on Computer Architecture</title>
				<meeting>29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Symposium on Computer Architecture (ISCA&apos;06)</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">PDede: Partitioned, Deduplicated, Delta Branch Target Buffer</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Niranjan K Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanvir</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Ahmed Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Kasikci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On Pipelining Dynamic Instruction Scheduling Logic</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 33rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>33rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">2000</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Memory Renaming: Fast, Early and Accurate Processing of Memory Communication</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Microarchitecture</forename><surname>Amd Zen</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/amd/microarchitectures/zen" />
		<imprint>
			<date>March 31</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sunnycove -Microarchitectures -Intel</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/sunny_cove" />
		<imprint>
			<date>March 31</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Speculation Techniques for Improving Load Related Instruction Scheduling</title>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on Computer Architecture</title>
				<meeting>the 26th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Apache Spark: A Unified Engine for Big Data Processing</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2016-10">2016. oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Accelerating and Adapting Precomputation Threads for Effcient Prefetching</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE 13th International Symposium on High Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">SIPT: Speculatively Indexed, Physically Tagged Caches</title>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haishan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
