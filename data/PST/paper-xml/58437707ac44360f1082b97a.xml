<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards an integration of deep learning and neuroscience</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-13">13 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adam</forename><forename type="middle">H</forename><surname>Marblestone</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT Media Lab Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
							<email>gregwayne@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Deepmind London</orgName>
								<address>
									<postCode>EC4A 3TW</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
							<email>koerding@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Rehabilitation Institute of Chicago Northwestern University Chicago</orgName>
								<address>
									<postCode>60611</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards an integration of deep learning and neuroscience</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-13">13 Jun 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1606.03813v1[q-bio.NC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cost Functions</term>
					<term>Neural Networks</term>
					<term>Neuroscience</term>
					<term>Cognitive Architecture Path nder e.g.</term>
					<term>Hippocampus Working memory slots e.g.</term>
					<term>PFC Gated relays e.g.</term>
					<term>Thalamus Multi-timescale predictive feedback e.g.</term>
					<term>Cerebellum Reinforcement learning e.g.</term>
					<term>Basal Ganglia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short-and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning and neuroscience speak different languages today. Brain science has discovered a dazzling array of brain areas, cell types, molecules, cellular states, and mechanisms for computation and information storage. Machine learning, in contrast, has largely focused on instantiations of a single principle: function optimization. It has found that simple optimization objectives, like minimizing classification error, can lead to the formation of rich internal representations and powerful algorithmic capabilities in multilayer and recurrent networks <ref type="bibr" target="#b178">(LeCun et al., 2015;</ref><ref type="bibr" target="#b283">Schmidhuber, 2015)</ref>.</p><p>Here we seek to connect these perspectives.</p><p>The artificial neural networks now prominent in machine learning were, of course, originally inspired by neuroscience <ref type="bibr" target="#b213">(McCulloch and Pitts, 1943)</ref>. While neuroscience has continued to play a role <ref type="bibr" target="#b60">(Cox and Dean, 2014)</ref>, many of the major developments were guided by insights into the mathematics of efficient optimization, rather than neuroscientific findings <ref type="bibr" target="#b313">(Sutskever and Martens, 2013)</ref>.</p><p>The field has advanced from simple linear systems <ref type="bibr" target="#b221">(Minsky and Papert, 1972)</ref>, to nonlinear networks <ref type="bibr" target="#b127">(Haykin, 1994)</ref>, to deep and recurrent networks <ref type="bibr" target="#b283">(Schmidhuber, 2015;</ref><ref type="bibr" target="#b178">LeCun et al., 2015)</ref>. Backpropagation of error <ref type="bibr" target="#b344">(Werbos, 1974</ref><ref type="bibr" target="#b344">(Werbos, , 1982;;</ref><ref type="bibr" target="#b276">Rumelhart et al., 1986)</ref> enabled neural networks to be trained efficiently, by providing an efficient means to compute the gradient with respect to the weights of a multi-layer network. Methods of training have improved to include momentum terms, better weight initializations, conjugate gradients and so forth, evolving to the current breed of networks optimized using batchwise stochastic gradient descent. These developments have little obvious connection to neuroscience.</p><p>We will argue here, however, that neuroscience and machine learning are, once again, ripe for convergence. Three aspects of machine learning are particularly important in the context of this paper. First, machine learning has focused on the optimization of cost functions (Figure <ref type="figure">1A</ref>).</p><p>Second, recent work in machine learning has started to introduce complex cost functions, those that are not uniform across layers and time, and those that arise from interactions between different parts of a network.</p><p>For example, introducing the objective of temporal coherence for lower layers (non-uniform cost function over space) improves feature learning <ref type="bibr" target="#b284">(Sermanet and Kavukcuoglu, 2013)</ref>, cost function schedules (non-uniform cost function over time) improve 1 generalization <ref type="bibr" target="#b281">(Saxe et al., 2013;</ref><ref type="bibr" target="#b106">Goodfellow et al., 2014b;</ref><ref type="bibr" target="#b112">Gülçehre and Bengio, 2016)</ref> and adversarial networks -an example of a cost function arising from internal interactions -allow gradient-based training of generative models <ref type="bibr" target="#b105">(Goodfellow et al., 2014a)</ref>.</p><p>Networks that are easier to train are being used to provide "hints" to help bootstrap the training of more powerful networks <ref type="bibr" target="#b272">(Romero et al., 2014)</ref>.</p><p>Third, machine learning has also begun to diversify the architectures that are subject to optimization.</p><p>It has introduced simple memory cells with multiple persistent states <ref type="bibr" target="#b138">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b56">Chung et al., 2014)</ref>, more complex elementary units such as "capsules" and other structures <ref type="bibr">(Hinton et al., 2011;</ref><ref type="bibr" target="#b192">Livni et al., 2013;</ref><ref type="bibr">Delalleau and Bengio, 2011;</ref><ref type="bibr" target="#b318">Tang et al., 2012)</ref>, content addressable <ref type="bibr" target="#b347">(Weston et al., 2014;</ref><ref type="bibr" target="#b108">Graves et al., 2014)</ref> and location addressable memories <ref type="bibr" target="#b108">(Graves et al., 2014)</ref>, as well as pointers <ref type="bibr">(Kurach et al., 2015)</ref> and hard-coded arithmetic operations <ref type="bibr" target="#b233">(Neelakantan et al., 2015)</ref>.</p><p>These three ideas have, so far, not received much attention in neuroscience. We thus formulate these ideas as three hypotheses about the brain, examine evidence for them, and sketch how experiments could test them. But first, let us state the hypotheses more precisely.</p><p>1.1 Hypothesis 1 -The brain optimizes cost functions.</p><p>The central hypothesis for linking the two fields is that biological systems, like many machinelearning systems, are able to optimize cost functions. The idea of cost functions means that neurons in a brain area can somehow change their properties, e.g., the properties of their synapses, so that they get better at doing whatever the cost function defines as their role. Human behavior sometimes approaches optimality in a domain, e.g., during movement <ref type="bibr" target="#b164">(Körding, 2007)</ref>, which suggests that the brain may have learned optimal strategies. Subjects minimize energy consumption of their movement system <ref type="bibr" target="#b320">(Taylor and Faisal, 2011)</ref>, and minimize risk and damage to their body, while maximizing financial and movement gains. Computationally, we now know that optimization of trajectories gives rise to elegant solutions for very complex motor tasks <ref type="bibr" target="#b230">(Mordatch et al., 2012;</ref><ref type="bibr" target="#b326">Todorov and Jordan, 2002;</ref><ref type="bibr" target="#b118">Harris and Wolpert, 1998)</ref>. We suggest that cost function optimization occurs much more generally in shaping the internal representations and processes used by the brain. We also suggest that this requires the brain to have mechanisms for efficient credit assignment in multilayer and recurrent networks.</p><p>1.2 Hypothesis 2 -Cost functions are diverse across areas and change over development.</p><p>A second realization is that cost functions need not be global. Neurons in different brain areas may optimize different things, e.g., the mean squared error of movements, surprise in a visual stimulus, or the allocation of attention. Importantly, such a cost function could be locally generated. For example, neurons could locally evaluate the quality of their statistical model of their inputs (Figure <ref type="figure">1B</ref>). Alternatively, cost functions for one area could be generated by another area. Moreover, cost functions may change over time, e.g., guiding young humans to understanding simple visual contrasts early on, and faces a bit later. This could allow the developing brain to bootstrap more complex knowledge based on simpler knowledge. Cost functions in the brain are likely to be complex and to be arranged to vary across areas and over development.</p><p>1.3 Hypothesis 3 -Specialized systems allow efficiently solving key computational problems.</p><p>A third realization is that structure matters. The patterns of information flow seem funda-mentally different across brain areas, suggesting that they solve distinct computational problems. Some brain areas are highly recurrent, perhaps making them predestined for short-term memory storage <ref type="bibr" target="#b339">(Wang, 2012)</ref>. Some areas contain cell types that can switch between qualitatively different states of activation, such as a persistent firing mode versus a transient firing mode, in response to particular neurotransmitters <ref type="bibr" target="#b121">(Hasselmo, 2006)</ref>. Other areas, like the thalamus appear to have the information from other areas flowing through them, perhaps allowing them to determine information routing <ref type="bibr" target="#b289">(Sherman, 2005)</ref>. Areas like the basal ganglia are involved in reinforcement learning and gating of discrete decisions <ref type="bibr">(Sejnowski and Poizner, 2014;</ref><ref type="bibr" target="#b74">Doya, 1999)</ref>. As every programmer knows, specialized algorithms matter for efficient solutions to computational problems, and the brain is likely to make good use of such specialization (Figure <ref type="figure">1C</ref>). These ideas are inspired by recent advances in machine learning, but we also propose that the brain has major differences from any of today's machine learning techniques. In particular, the world gives us a relatively limited amount of information that we could use for supervised learning <ref type="bibr" target="#b89">(Fodor and Crowther, 2002)</ref>. There is a huge amount of information available for unsupervised learning, but there is no reason to assume that a generic unsupervised algorithm, no matter how powerful, would learn the precise things that humans need to know, in the order that they need to know it. The evolutionary challenge of making unsupervised learning solve the "right" problems is, therefore, to find a sequence of cost functions that will deterministically build circuits and behaviors according to prescribed developmental stages, so that in the end a relatively small amount of information suffices to produce the right behavior. For example, a developing duck imprints <ref type="bibr" target="#b324">(Tinbergen, 1965)</ref> a template of its parent, and then uses that template to generate goal-targets that help it develop other skills like foraging.</p><p>Generalizing from this and from other studies <ref type="bibr" target="#b329">(Ullman et al., 2012;</ref><ref type="bibr">Minsky, 1977)</ref>, we propose that many of the brain's cost functions arise from such an internal bootstrapping process. Indeed, we propose that biological development and reinforcement learning can, in effect, program the emergence of a sequence of cost functions that precisely anticipates the future needs faced by the brain's internal subsystems, as well as by the organism as a whole. This type of developmentally programmed bootstrapping generates an internal infrastructure of cost functions which is diverse and complex, while simplifying the learning problems faced by the brain's internal processes. Beyond simple tasks like familial imprinting, this type of bootstrapping could extend to higher cognition, e.g., internally generated cost functions could train a developing brain to properly access its memory or to organize its actions in ways that will prove to be useful later on. The potential bootstrapping mechanisms that we will consider operate in the context of unsupervised and reinforcement learning, and go well beyond the types of curriculum learning ideas used in today's machine learning <ref type="bibr" target="#b28">(Bengio et al., 2009)</ref>.</p><p>In the rest of this paper, we will elaborate on these hypotheses. First, we will argue that both local and multi-layer optimization is, perhaps surprisingly, compatible with what we know about the brain. Second, we will argue that cost functions differ across brain areas and change over time and describe how cost functions interacting in an orchestrated way could allow bootstrapping of complex function. Third, we will list a broad set of specialized problems that need to be solved by neural computation, and the brain areas that have structure that seems to be matched to a particular computational problem. We then discuss some implications of the above hypotheses for research approaches in neuroscience and machine learning, and sketch a set of experiments to test these hypotheses. Finally, we discuss this architecture from the perspective of evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The brain can optimize cost functions</head><p>Much of machine learning is based on efficiently optimizing functions, and, as we will detail below, the ability to use backpropagation of error <ref type="bibr" target="#b344">(Werbos, 1974;</ref><ref type="bibr" target="#b276">Rumelhart et al., 1986)</ref> to calculate gradients of arbitrary parametrized functions has been a key breakthrough. In Hypothesis 1, we claim that the brain is also, at least in part, an optimization machine. But what exactly does it mean to say that the brain can optimize cost functions? After all, many processes can be viewed as optimizations. For example, the laws of physics are often viewed as minimizing an action functional, while evolution optimizes the fitness of replicators over a long timescale. To be clear, our main claims are: that a) the brain has powerful mechanisms for credit assignment during learning that allow it to optimize global functions in multi-layer networks by adjusting the properties of each neuron to contribute to the global outcome, and that b) the brain has mechanisms to specify exactly which cost functions it subjects its networks to, i.e., that the cost functions are highly tunable, shaped by evolution and matched to the animal's ethological needs. Thus, the brain uses cost functions as a key driving force of its development, much as modern machine learning systems do.</p><p>To understand the basis of these claims, we must now delve into the details of how the brain might efficiently perform credit assignment throughout large, multi-layered networks, in order to optimize complex functions. We argue that the brain uses several different types of optimization to solve distinct problems. In some structures, it may use genetic pre-specification of circuits for problems that require only limited learning based on data, or it may exploit local optimization to avoid the need to assign credit through many layers of neurons. It may also use a host of proposed circuit structures that would allow it to actually perform, in effect, backpropagation of errors through a multi-layer network, using biologically realistic mechanisms -a feat that had once been widely believed to be biologically implausible <ref type="bibr">(Crick, 1989;</ref><ref type="bibr" target="#b305">Stork, 1989)</ref>. Potential such mechanisms include circuits that literally backpropagate error derivatives in the manner of conventional backpropagation, as well as circuits that provide other efficient means of approximating the effects of backpropagation, i.e., of rapidly computing the approximate gradient of a cost function relative to any given connection weight in the network. Lastly, the brain may use algorithms that exploit specific aspects of neurophysiology -such as spike timing dependent plasticity, dendritic computation, local excitatory-inhibitory networks, or other proper- ties -as well as the integrated nature of higherlevel brain systems. Such mechanisms promise to allow learning capabilities that go even beyond those of current backpropagation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local self-organization and optimization without multi-layer credit assignment</head><p>Not all learning requires a general-purpose optimization mechanism like gradient descent 2 . Many theories of cortex <ref type="bibr" target="#b97">(George and Hawkins, 2009;</ref><ref type="bibr" target="#b139">Hoerzer et al., 2014;</ref><ref type="bibr" target="#b159">Kappel et al., 2014)</ref> emphasize potential self-organizing and unsupervised learning properties that may obviate the need for multi-layer backpropagation as such. Hebbian plasticity, which adjusts weights according to correlations in presynaptic and post-synaptic activity, is well established 3 . Various versions of Hebbian plasticity <ref type="bibr" target="#b218">(Miller and MacKay, 1994)</ref> can give rise to different forms of correlation and competition between neurons, leading to the self-organized formation of ocular dominance columns, self-organizing maps and orientation columns <ref type="bibr" target="#b83">(Ferster and Miller, 2003;</ref><ref type="bibr" target="#b217">Miller et al., 1989)</ref>. Often these types of local self-organization can also be viewed as optimizing a cost function: for example, certain forms of Hebbian plasticity can be viewed as extracting the principal components of the input, which minimizes a reconstruction error.</p><p>To generate complex temporal patterns, the brain may also implement other forms of learning that do not require any equivalent of full backpropagation through a multilayer network. For example, "liquid-" <ref type="bibr" target="#b196">(Maass et al., 2002)</ref> or "echostate machines" <ref type="bibr" target="#b151">(Jaeger and Haas, 2004</ref>) are randomly connected recurrent networks that form a basis set of random filters, which can be harnessed for learning with tunable readout weights.</p><p>Variants exhibiting chaotic, spontaneous dynamics can even be trained by feeding back readouts into the network and suppressing the chaotic activity <ref type="bibr" target="#b311">(Sussillo and Abbott, 2009)</ref>. Learning only the readout layer makes the optimization problem much simpler (indeed, equivalent to regression for supervised learning). Additionally, echo state networks can be trained by reinforcement learning as well as supervised learning <ref type="bibr" target="#b48">(Bush, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Biological implementation of optimization</head><p>We argue that the above mechanisms of local selforganization are insufficient to account for the brains powerful learning performance. To elaborate on the need for an efficient means of gradient computation in the brain, we will first place backpropagation into it's computational context <ref type="bibr" target="#b131">(Hinton, 1989;</ref><ref type="bibr">Baldi and Sadowski, 2015)</ref>. Then we will explain how the brain could plausibly implement approximations of gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">The need for efficient gradient descent in multi-layer networks</head><p>The simplest mechanism to perform cost function optimization is sometimes known as the "twiddle" algorithm or, more technically, as "serial perturbation". This mechanism works by perturbing (i.e., "twiddling"), with a small increment, a single weight in the network, and verifying improvement by measuring whether the cost function has decreased compared to the network's performance with the weight unperturbed. If improvement is noticeable, the perturbation is used as a direction of change to the weight; otherwise, the weight is changed in the opposite direction (or not changed at all). Serial perturbation is therefore a method of "coordinate 2. Of course, some circuits may also be heavily genetically pre-specified to minimize the burden on learning. For instance, particular cell adhesion molecules <ref type="bibr" target="#b123">(Hattori et al., 2007)</ref> expressed on particular parts of particular neurons defined by a genetic cell type <ref type="bibr" target="#b363">(Zeisel et al., 2015)</ref>, and the detailed shapes and placements of neuronal arbors, may constrain connectivity in some cases, though in other cases local connectivity is thought to be only weakly constrained <ref type="bibr" target="#b158">(Kalisman et al., 2005)</ref>. Genetics is sufficient to specify complex circuits involving hundreds of neurons, such as central pattern generators <ref type="bibr" target="#b361">(Yuste et al., 2005)</ref> which create complex self-stabilizing oscillations, or the entire nervous systems of small worms. Genetically guided wiring should not be thought of as fixed "hard-wiring" but rather as a programmatic construction process that can also accept external inputs and interact with learning mechanisms <ref type="bibr" target="#b203">(Marcus, 2004)</ref>. 3. Hebbian plasticity even has a well-understood biological basis in the form of the NMDA receptors, which are activated by the simultaneous occurrence of chemical transmitter delivered from the pre-synaptic neuron, and voltage depolarization of the post-synaptic neuron.</p><p>descent" on the cost, but it is slow and requires global coordination: each synapse in turn is perturbed while others remain fixed. Weight perturbation (or parallel perturbation) perturbs all of the weights in the network at once. It is able to optimize small networks to perform tasks but generally suffers from high variance. That is, the measurement of the gradient direction is noisy and changes drastically from perturbation to perturbation because a weight's influence on the cost is masked by the changes of all other weights, and there is only one scalar feedback signal indicating the change in the cost 4 . Weight perturbation is dramatically inefficient for large networks. In fact, parallel and serial perturbation learn at approximately the same rate if the time measure counts the number of times the network propagates information from input to output <ref type="bibr" target="#b346">(Werfel et al., 2005)</ref>.</p><p>Some efficiency gain can be achieved by perturbing neural activities instead of synaptic weights, acknowledging the fact that any longrange effect of a synapse is mediated through a neuron. Like weight perturbation and unlike serial perturbation, minimal global coordination is needed: each neuron only needs to receive a feedback signal indicating the global cost. The variance of node perturbation's gradient estimate is far smaller than that of weight perturbation under the assumptions that either all neurons or all weights, respectively, are perturbed and that they are perturbed at the same frequency. In this case, node perturbation's variance is proportional to the number of cells in the network, not the number of synapses.</p><p>All of these approaches are slow either due to the time needed for serial iteration over all weights or the time needed for averaging over low signal-to-noise ratio gradient estimates. To their credit however, none of these approaches requires more than knowledge of local activities and the single global cost signal. Real neural circuits in the brain have mechanisms (e.g., diffusible neuromodulators) that appear to code the signals relevant to implementing those algorithms. In many cases, for example in reinforcement learning, the cost function, which is computed based on interaction with an unknown environment, cannot be differentiated directly, and an agent has no choice but to deploy clever twiddling to explore at some level of the system <ref type="bibr" target="#b349">(Williams, 1992)</ref>.</p><p>Backpropagation, in contrast, works by computing the sensitivity of the cost function to each weight based on the layered structure of the system. The derivatives of the cost function with respect to the last layer can be used to compute the derivatives of the cost function with respect to the penultimate layer, and so on, all the way down to the earliest layers 5 . Backpropagation can be computed rapidly, and for a single inputoutput pattern, it exhibits no variance in its gradient estimate. The backpropagated gradient has no more noise for a large system than for a small system, so deep and wide architectures with great computational power can be trained efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Biologically plausible approximations of gradient descent</head><p>To permit biological learning with efficiency approaching that of machine learning methods, some provision for more sophisticated gradient propagation may be suspected. Contrary to what was once a common assumption, there are now many proposed "biologically plausible" mechanisms by which a neural circuit could implement optimization algorithms that, like backpropagation, can efficiently make use of the gradient. These include Generalized Recirculation <ref type="bibr" target="#b243">(O'Reilly, 1996)</ref>, Contrastive Hebbian Learning <ref type="bibr" target="#b355">(Xie and Seung, 2003)</ref>, random feedback weights together with synaptic homeostasis <ref type="bibr" target="#b190">(Lillicrap et al., 2014;</ref><ref type="bibr" target="#b189">Liao et al., 2015)</ref>, spike timing dependent plasticity (STDP) with iterative inference and target propagation <ref type="bibr" target="#b282">(Scellier and Bengio, 2016;</ref><ref type="bibr" target="#b29">Bengio et al., 2015a)</ref>, complex neurons with backpropagating action-potentials <ref type="bibr" target="#b166">(Körding and König, 2000)</ref>, and others <ref type="bibr" target="#b21">(Balduzzi et al., 2014)</ref>. While these mechanisms differ in detail, they all invoke feedback connections that carry error phasically. Learning occurs by comparing a prediction with 4. The variance can be mitigated by averaging out many perturbations before making a change to the baseline value of the weights, but this would take significant time for a network of non-trivial size as the variance of weight perturbation's estimates scales in proportion to the number of synapses in the network. 5. If the error derivatives of the cost function with respect to the last layer of unit activities are unknown, then they can be replaced with node-perturbation-like correlations, as is common in reinforcement learning.</p><p>a target, and the prediction error is used to drive top-down changes in bottom-up activity.</p><p>As an example, consider O'Reilly's temporally eXtended Contrastive Attractor Learning (XCAL) algorithm <ref type="bibr" target="#b248">(O'Reilly et al., 2012</ref><ref type="bibr" target="#b247">(O'Reilly et al., , 2014b))</ref>. Suppose we have a multilayer neural network with an input layer, an output layer, and a set of hidden layers in between. OReilly showed that the same functionality as backpropagation can be implemented by a bidirectional network with the same weights but symmetric connections. After computing the outputs using the forward connections only, we set the output neurons to the values they should have. The dynamics of the network then cause the hidden layers' activities to evolve toward a stable attractor state linking input to output. The XCAL algorithm performs a type of local modified Hebbian learning at each synapse in the network during this process <ref type="bibr" target="#b248">(O'Reilly et al., 2012)</ref>. The XCAL Hebbian learning rule compares the local synaptic activity (pre x post) during the early phase of this settling (before the attractor state is reached) to the final phase (once the attractor state has been reached), and adjusts the weights in a way that should make the early phase reflect the later phase more closely. These contrastive Hebbian learning methods even work when the connection weights are not precisely symmetric <ref type="bibr" target="#b243">(O'Reilly, 1996)</ref>. XCAL has been implemented in biologically plausible conductance-based neurons and basically implements the backpropagation of error approach.</p><p>Approximations to backpropagation could also be enabled by the millisecond-scale timing of of neural activities <ref type="bibr" target="#b247">(O'Reilly et al., 2014b)</ref>.</p><p>Spike timing dependent plasticity (STDP) <ref type="bibr">(Markram et al., 1997)</ref>, for example, is a feature of some neurons in which the sign of the synaptic weight change depends on the precise millisecond-scale relative timing of pre-synaptic and post-synaptic spikes. This is conventionally interpreted as Hebbian plasticity that measures the potential for a causal relationship between the pre-synaptic and post-synaptic spikes: a pre-synaptic spike could have contributed to causing a post-synaptic spike only if it occurs shortly beforehand 6 . To enable a backpropagation mechanism, Hinton has suggested an alternative interpretation: that neurons could encode the types of error derivatives needed for backpropagation in the temporal derivatives of their firing rates <ref type="bibr" target="#b129">(Hinton, 2007</ref><ref type="bibr" target="#b130">(Hinton, , 2016))</ref>. STDP then corresponds to a learning rule that is sensitive to these error derivatives <ref type="bibr" target="#b30">(Bengio et al., 2015b;</ref><ref type="bibr">Xie and Seung, 2000)</ref>. In other words, in an appropriate network context, STDP learning could give rise to a biological implementation of backpropagation 7 .</p><p>Another possible mechanism, by which biological neural networks could approximate backpropagation, is "feedback alignment" <ref type="bibr" target="#b190">(Lillicrap et al., 2014;</ref><ref type="bibr" target="#b189">Liao et al., 2015)</ref>. There, the feedback pathway in backpropagation, by which error derivatives at a layer are computed from error derivatives at the subsequent layer, is replaced by a set of random feed-6. Interestingly, STDP is not a unitary phenomenon, but rather a diverse collection of different rules with different timescales and temporal asymmetries <ref type="bibr" target="#b294">(Sjöström and Gerstner, 2010;</ref><ref type="bibr" target="#b222">Mishra et al., 2016)</ref>. Effects include STDP with the inverse temporal asymmetry, symmetric STDP, STDP with different temporal window sizes. STDP is also frequency dependent, which can be explained by rules that depend on triplets rather than pairs of spikes <ref type="bibr" target="#b257">(Pfister and Gerstner, 2006)</ref>. While STDP is often included explicitly in models, biophysical derivations of STDP from various underlying phenomena are also being attempted, some of which involve the post-synaptic voltage <ref type="bibr" target="#b58">(Clopath and Gerstner, 2010)</ref> or a local dendritic voltage <ref type="bibr" target="#b330">(Urbanczik and Senn, 2014)</ref>. Meanwhile, other theories suggest that STDP may enable the use of precise timing codes based on temporal coincidence of inputs, the generation and unsupervised learning of temporal sequences <ref type="bibr" target="#b88">(Fiete et al., 2010;</ref><ref type="bibr" target="#b0">Abbott and Blum, 1996)</ref>, enhancements to distal reward processing in reinforcement learning <ref type="bibr" target="#b147">(Izhikevich, 2007)</ref>, stabilization of neural responses <ref type="bibr" target="#b160">(Kempter et al., 2001)</ref>, or many other higher-level properties <ref type="bibr" target="#b234">(Nessler et al., 2013;</ref><ref type="bibr" target="#b159">Kappel et al., 2014)</ref>. 7. Hinton has suggested <ref type="bibr" target="#b129">(Hinton, 2007</ref><ref type="bibr" target="#b130">(Hinton, , 2016) )</ref> that this could take place in the context of autoencoders and recirculation <ref type="bibr" target="#b132">(Hinton and McClelland, 1988)</ref>. Bengio and colleagues have proposed <ref type="bibr" target="#b282">(Scellier and Bengio, 2016;</ref><ref type="bibr" target="#b27">Bengio and Fischer, 2015;</ref><ref type="bibr" target="#b26">Bengio, 2014)</ref> another context in which the connection between STDP and plasticity rules that depend on the temporal derivative of the post-synaptic firing rate can be exploited for biologically plausible multilayer credit assignment. This setting relies on clamping of outputs and stochastic relaxation in energy-based models <ref type="bibr" target="#b1">(Ackley et al., 1985)</ref>, which leads to a continuous network dynamics <ref type="bibr" target="#b142">(Hopfield, 1984)</ref> in which hidden units are perturbed towards target values <ref type="bibr" target="#b27">(Bengio and Fischer, 2015)</ref>, loosely similar to that which occurs in XCAL. This dynamics then allows the STDP-based rule to correspond to gradient descent on the energy function with respect to the weights <ref type="bibr" target="#b282">(Scellier and Bengio, 2016)</ref>. This scheme requires symmetric weights, but in an autoencoder context, Bengio notes that these can arise spontaneously <ref type="bibr" target="#b11">(Arora et al., 2015)</ref>.</p><p>back connections, with no dependence on the forward weights. Subject to the existence of a synaptic normalization mechanism and approximate sign-concordance between the feedforward and feedback connections <ref type="bibr" target="#b189">(Liao et al., 2015)</ref>, this mechanism of computing error derivatives works nearly as well as backpropagation on a variety of tasks. In effect, the forward weights are able to adapt to bring the network into a regime in which the random backwards weights actually carry the information that is useful for approximating the gradient. This is a remarkable and surprising finding, and is indicative of the fact that our understanding of gradient descent optimization, and specifically of the mechanisms by which backpropagation itself functions, are still incomplete. In neuroscience, meanwhile, we find feedback connections almost wherever we find feed-forward connections, and their role is the subject of diverse theories <ref type="bibr" target="#b51">(Callaway, 2004;</ref><ref type="bibr" target="#b197">Maass et al., 2007)</ref>. It should be noted that feedback alignment as such does not specify exactly how neurons represent and make use of the error signals; it only relaxes a constraint on the transport of the error signals. Thus, feedback alignment is more a primitive that can be used in fully biological (approximate) implementations of backpropagation, than a fully biological implementation in its own right. As such, it may be possible to incorporate it into several of the other schemes discussed here.</p><p>The above "biological" implementations of backpropagation still lack some key aspects of biological realism. For example, in the brain, neurons tend to be either excitatory or inhibitory but not both, whereas in artificial neural networks a single neuron may send both excitatory and inhibitory signals to its downstream neurons. Fortunately, this constraint is unlikely to limit the functions that can be learned <ref type="bibr" target="#b251">(Parisien et al., 2008;</ref><ref type="bibr" target="#b327">Tripp and Eliasmith, 2016)</ref>. Other biological considerations, however, need to be looked at in more detail: the highly recurrent nature of biological neural networks, which show rich dy-namics in time, and the fact that most neurons in mammalian brains communicate via spikes. We now consider these two issues in turn.</p><p>Temporal credit assignment: The biological implementations of backpropagation proposed above, while applicable to feedforward networks, do not give a natural implementation of "backpropagation through time" (BPTT) <ref type="bibr" target="#b345">(Werbos, 1990)</ref> for recurrent networks, which is widely used in machine learning for training recurrent networks on sequential processing tasks. BPTT "unfolds" a recurrent network across multiple discrete time steps and then runs backpropagation on the unfolded network to assign credit to particular units at particular time steps 8 . While the network unfolding procedure of BPTT itself does not seem biologically plausible, to our intuition, it is unclear to what extent temporal credit assignment is truly needed <ref type="bibr" target="#b237">(Ollivier and Charpiat, 2015)</ref> for learning particular temporally extended tasks.</p><p>If the system is given access to appropriate memory stores and representations <ref type="bibr" target="#b45">(Buonomano and Merzenich, 1995;</ref><ref type="bibr" target="#b99">Gershman et al., 2012</ref><ref type="bibr" target="#b99">Gershman et al., , 2014) )</ref> of temporal context, this could potentially mitigate the need for temporal credit assignment as such -in effect, memory systems could "spatialize" the problem of temporal credit assignment 9 . For example, memory networks <ref type="bibr" target="#b347">(Weston et al., 2014)</ref> store everything by default up to a certain buffer size, eliminating the need to perform credit assignment over the write-to-memory events, such that the network only needs to perform credit assignment over the read-from-memory events. In another example, certain network architectures that are superficially very deep, but which possess particular types of "skip connections", can actually be seen as ensembles of comparatively shallow networks <ref type="bibr" target="#b334">(Veit et al., 2016)</ref>; applied in the time domain, this could limit the need to propagate errors far backwards in time. Other, similar specializations or higher-levels of struc-8. Even BPTT has arguably not been completely successful in recurrent networks. The problems of vanishing and exploding gradients led to long short term memory networks with gated memory units. An alternative is to use optimization methods that go beyond first order derivatives <ref type="bibr" target="#b314">(Martens and Sutskever, 2011)</ref>. This suggests the need for specialized systems and structures in the brain to mitigate problems of temporal credit assignment. 9. Interestingly, the hippocampus seems to "time stamp" memories by encoding them into ensembles with cellular compositions and activity patterns that change gradually as a function of time on the scale of days <ref type="bibr" target="#b275">(Rubin et al., 2015;</ref><ref type="bibr" target="#b50">Cai et al., 2016)</ref>, and may use "time cells" to mark temporal positions within episodes on a timescale of seconds <ref type="bibr" target="#b167">(Kraus et al., 2013)</ref>.</p><p>ture could, potentially, further ease the burden on credit assignment. Can generic recurrent networks perform temporal credit assignment in in a way that is more biologically plausible than BPTT? Indeed, new discoveries are being made about the capacity for supervised learning in continuous-time recurrent networks with more realistic synapses and neural integration properties. In internal FORCE learning <ref type="bibr" target="#b311">(Sussillo and Abbott, 2009)</ref>, internally generated random fluctuations inside a chaotic recurrent network are adjusted to provide feedback signals that drive weight changes internal to the network while the outputs are clamped to desired patterns. This is made possible by a learning procedure that rapidly adjusts the network output to a state where it is close to the clamped values, and exerts continuous control to keep this difference small throughout the learning process 10 . This procedure is able to control and exploit the chaotic dynamical patterns that are spontaneously generated by the network. Werbos has proposed in his "error critic" that an online approximation to BPTT can be achieved by learning to predict the backward-through-time gradient signal (costate) in a manner analogous to the prediction of value functions in reinforcement learning <ref type="bibr" target="#b292">(Si, 2004)</ref>. Broadly, we are only beginning to understand how neural activity can itself represent the time variable <ref type="bibr">(Finnerty and Shadlen, 2015)</ref>, and how recurrent networks can learn to generate trajectories of population activity over time <ref type="bibr" target="#b191">(Liu and Buonomano, 2009)</ref>. Moreover, as we discuss below, a number of cortical models also propose means, other than BPTT, by which networks could be trained on sequential prediction tasks, even in an online fashion <ref type="bibr" target="#b63">(Cui et al., 2015;</ref><ref type="bibr" target="#b247">O'Reilly et al., 2014b)</ref>. A broad range of ideas can be used to approximate BPTT in more realistic ways.</p><p>Spiking networks: It has been difficult to apply gradient descent learning directly to spiking neural networks 1112 . A number of optimization procedures have been used to generate, indirectly, spiking networks which can perform complex tasks, by performing optimization on a continuous representation of the network dynamics and embedding variables into highdimensional spaces with many spiking neurons representing each variable <ref type="bibr">(Abbott et al., 2016;</ref><ref type="bibr" target="#b70">DePasquale et al., 2016;</ref><ref type="bibr" target="#b163">Komer and Eliasmith, 2016;</ref><ref type="bibr" target="#b323">Thalmeier et al., 2015)</ref>. The use of recurrent connections with multiple timescales can remove the need for backpropagation in the direct training of spiking recurrent networks <ref type="bibr" target="#b40">(Bourdoukan and Denève, 2015)</ref>. Fast connections maintain the network in a state where slow connections have local access to a global error signal. While the biological realism of these methods is still unknown, they all allow connection weights to be learned in spiking networks.</p><p>These and other novel learning procedures illustrate the fact that we are only beginning to understand the connections between the temporal dynamics of biologically realistic networks, and mechanisms of temporal and spatial credit assignment. Nevertheless, we argue here that existing evidence suggests that biologically plausible neural networks can solve these problems -in other words, it is possible to efficiently optimize complex functions of temporal history in the context of spiking networks of biologically realistic neurons. In any case, there is little doubt that spiking recurrent networks using realistic population coding schemes can, with an appropriate choice of connection weights, compute complicated, cognitively relevant functions 13 . The question is how the developing brain efficiently learns such complex functions.</p><p>10. Control theory concepts also appear to be useful for simplifying optimization problems in certain other settings <ref type="bibr" target="#b325">(Todorov, 2009)</ref>. 11. Analogs of weight perturbation and node perturbation are known for spiking networks <ref type="bibr">(Seung, 2003;</ref><ref type="bibr" target="#b86">Fiete and Seung, 2006)</ref>. <ref type="bibr">Seung (2003)</ref> also discusses implications of gradient based learning algorithms for neuroscience, echoing some of our considerations here. 12. A related, but more general, question is how to learn over many layers of non-differentiable structures. One option is to perform updates via finite-sized rather than infinitesimal steps, e.g., via target-propagation <ref type="bibr" target="#b26">(Bengio, 2014)</ref>. 13. Eliasmith and others have shown <ref type="bibr" target="#b76">(Eliasmith, 2013;</ref><ref type="bibr" target="#b77">Eliasmith and Anderson, 2004;</ref><ref type="bibr" target="#b79">Eliasmith et al., 2012)</ref> that complex functions and control systems can be compiled onto such networks, using nonlinear encoding and linear decoding of high-dimensional vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Alternative mechanisms for learning</head><p>The brain has mechanisms and structures that could support learning mechanisms different from typical gradient-based optimization algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Exploiting biological neural mechanisms</head><p>The complex physiology of individual biological neurons may not only help explain how some form of efficient gradient descent could be implemented within the brain, but also could provide mechanisms for learning that go beyond backpropagation. This suggests that the brain may have discovered mechanisms of credit assignment quite different from those dreamt up by machine learning.</p><p>One such biological primitive is dendritic computation, which could impact prospects for learning algorithms in several ways. First, real neurons are highly nonlinear, with the dendrites of each single neuron implementing 14 something computationally similar to a three-layer neural network <ref type="bibr">(Mel, 1992)</ref>.</p><p>Second, when a neuron spikes, its action potential propagates back from the soma into the dendritic tree. However, it propagates more strongly into the branches of the dendritic tree that have been active <ref type="bibr" target="#b351">(Williams and Stuart, 2000)</ref>, potentially simplifying the problem of credit assignment <ref type="bibr" target="#b166">(Körding and König, 2000)</ref>. Third, neurons can have multiple somewhat independent dendritic compartments, as well as a somewhat independent somatic compartment, which means that the neuron should be thought of as storing more than one variable. Thus, there is the possibility for a neuron to store both its activation itself, and the error derivative of a cost function with respect to its activation, as required in backpropagation, and biological implementations of backpropagation based on this principle have been proposed <ref type="bibr" target="#b166">(Körding and König, 2001)</ref> 15 . Overall, the implications of dendritic computation for credit assignment in deep networks are only beginning to be considered.</p><p>Beyond dendritic computation, diverse mechanisms (Marblestone and Boyden, 2014) like retrograde (post-synaptic to pre-synaptic) signals using cannabinoids <ref type="bibr" target="#b352">(Wilson and Nicoll, 2001)</ref>, or rapidly-diffusing gases such as nitric oxide <ref type="bibr" target="#b9">(Arancio et al., 1996)</ref>, are among many that could enable learning rules that go beyond backpropagation. Harris has suggested <ref type="bibr" target="#b119">(Harris, 2008;</ref><ref type="bibr" target="#b187">Lewis and Harris, 2014</ref>) how slow, retroaxonal (i.e., from the outgoing synapses back to the parent cell body) transport of molecules like neurotrophins could allow neural networks to implement an analog of an exchangeable currency in economics, allowing networks to self-organize to efficiently provide information to downstream "consumer" neurons that are trained via faster and more direct error signals. The existence of these diverse mechanisms may call into question traditional, intuitive notions of "biological plausibility" for learning algorithms.</p><p>Another biological primitive is neuromodulation. The same neuron or circuit can exhibit different input-output responses and plasticity depending on a global circuit state, as reflected by the concentrations of various neuromodulators like dopamine, serotonin, norepinephrine, acetylcholine, and hundreds of different neuropeptides such as opiods <ref type="bibr" target="#b23">(Bargmann and Marder, 2013;</ref><ref type="bibr" target="#b22">Bargmann, 2012)</ref>. These modulators interact in complex and cell-type-specific ways to influence circuit function. Interactions with glial cells also play a role in neural signaling and neuromodulation, leading to the concept of "tripartite" synapses that include a glial contribution <ref type="bibr" target="#b254">(Perea et al., 2009)</ref>. Modulation could have many implications for learning. First, modulators can be used to gate synaptic plasticity on and off selectively in different areas and at different times, allowing precise, rapidly updated orchestration of where and when cost functions are applied. Furthermore, it has been argued that a single neural circuit can be thought of as multiple overlapping circuits with modulation switching between them (Bargmann and <ref type="bibr" target="#b23">Marder, 2013;</ref><ref type="bibr" target="#b22">Bargmann, 2012)</ref>. In a learning context, this 14. Dendritic computation may also have other functions, e.g., competitive interactions between dendrites in a single neuron could also allow neurons to contribute to multiple different ensembles <ref type="bibr" target="#b182">(Legenstein and Maass, 2011)</ref>. 15. Interestingly, in the model of <ref type="bibr" target="#b166">(Körding and König, 2001)</ref>, single spikes are used to transmit activations and burst spikes are used to transmit error information. In other models, including the dendritic voltage in a plasticity rule leads to simple error-driven and predictive learning <ref type="bibr" target="#b330">(Urbanczik and Senn, 2014)</ref>. Single neurons with active dendrites and many synapses may embody learning rules of greater complexity, such as the storage and recall of temporal patterns <ref type="bibr" target="#b124">(Hawkins and Ahmad, 2015)</ref>.</p><p>could potentially allow sharing of synaptic weight information between overlapping circuits. <ref type="bibr" target="#b66">Dayan (2012)</ref> discusses further computational aspects of neuromodulation. Overall, neuromodulation seems to expand the range of possible algorithms that could be used for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Learning in the cortical sheet</head><p>A number of models attempt to explain cortical learning on the basis of specific architectural features of the 6-layered cortical sheet. These models generally agree that a primary function of the cortex is some form of unsupervised learning via prediction <ref type="bibr" target="#b247">(O'Reilly et al., 2014b)</ref>. Some cortical learning models are explicit attempts to map cortical structure onto the framework of message-passing algorithms for Bayesian inference <ref type="bibr" target="#b97">(George and Hawkins, 2009;</ref><ref type="bibr" target="#b67">Dean, 2005;</ref><ref type="bibr" target="#b180">Lee and Mumford, 2003)</ref>, while others start with particular aspects of cortical neurophysiology and seek to explain those in terms of a learning function. For example, the nonlinear and dynamical properties of cortical pyramidal neurons -the principal excitatory neuron type in cortex -are of particular interest here, especially because these neurons have multiple dendritic zones that are targeted by different kinds of projections, which may allow the pyramidal neuron to make comparisons of top-down and bottom-up inputs 16 . Other aspects of the laminar cortical architecture could be crucial to how the brain implements learning. Local inhibitory neurons targeting par-ticular dendritic compartments of the L5 pyramidal could be used to exert precise control over when and how the relevant feedback signals and associative mechanisms are utilized. Notably, local inhibitory networks could also give rise to competition <ref type="bibr" target="#b255">(Petrov et al., 2010)</ref> between different representations in the cortex, perhaps allowing one cortical column to suppress others nearby, or perhaps even to send more sophisticated messages to gate the state transitions of its neighbors <ref type="bibr" target="#b17">(Bach and Herger, 2015)</ref>. Moreover, recurrent connectivity with the thalamus, structured bursts of spiking, and cortical oscillations (not to mention other mechanisms like neuromodulation) could control the storage of information over time, to facilitate learning based on temporal prediction. These concepts begin to suggest preliminary, exploratory models for how the detailed anatomy and physiology of the cortex could be interpreted within a machine-learning framework that goes beyond backpropagation. But these are early days: we still lack detailed structural/molecular and functional maps of even a single local cortical microcircuit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">One-shot learning</head><p>Human learning is often one-shot: it can take just a single exposure to a stimulus to never forget it, as well as to generalize from it to new examples. One way of allowing networks to have such properties is what is described by I-theory, in the context of learning invariant representations for object recognition <ref type="bibr" target="#b8">(Anselmi et al., 2015)</ref>. In-16. This idea has been used by Hawkins and colleagues to suggest mechanisms for continuous online sequence learning <ref type="bibr" target="#b124">(Hawkins and Ahmad, 2015;</ref><ref type="bibr" target="#b63">Cui et al., 2015)</ref> and by Larkum and colleagues for comparison of top-down and bottom-up signals <ref type="bibr" target="#b175">(Larkum, 2013)</ref>. The Larkum model focuses on the layer 5 (L5) pyramidal neuron type. The cell body of this neuron lies in L5 but which extends its "apical" dendritic tree all the way up to a tuft at the top of the cortex in layer 1 (L1), which is a primary target of feedback projections. In the model, interactions between local spiking in these different dendritic zones, which are targeted by different kinds of projections, are crucial to the learning function. The model of Hawkins <ref type="bibr" target="#b124">(Hawkins and Ahmad, 2015;</ref><ref type="bibr" target="#b63">Cui et al., 2015)</ref> also focused on the unique dendritic structure of the L5 pyramidal neuron, and distinguishes internal states of the neuron, which impact its responsiveness to other inputs, from activation states, which directly translate into spike rates. Three integration zones in each neuron, and dendritic NMDA spikes <ref type="bibr" target="#b250">(Palmer et al., 2014)</ref> acting as local coincidence detectors, allow temporal patterns of dendritic input to impact the cells internal state. Intra-column inhibition is also used in this model. Other cortical models pay less attention to the details of dendritic computation, but still provide detailed interpretations of the inter-laminar projection patterns of the neocortex. For example, in <ref type="bibr" target="#b247">(O'Reilly et al., 2014b)</ref>, an architecture is presented for continuous learning based on prediction of the next input. Time is discretized into 100 millisecond bins via an alpha oscillation, and the deep vs. shallow layers maintain different information during these time bins, with deep layers maintaining a record of the previous time step, and shallow layers representing the current state. The stored information in the deep layers leads to a prediction of the current state, which is then compared with the actual current state. Periodic bursting locked to the oscillation provides a kind of clock that causes the current state to be shifted into the deep layers for maintenance during the subsequent time step, and recurrent loops with the thalamus allow this representation to remain stable for sufficiently long to be used to generate the prediction.</p><p>stead of training via gradient descent, image templates are stored in the weights of simple-complex cell networks while objects undergo transformations, similar to the use of stored templates in HMAX <ref type="bibr" target="#b285">(Serre et al., 2007)</ref>. The theories then aim to show that you can invariantly and discriminatively represent objects using a single sample, even of a new class <ref type="bibr" target="#b8">(Anselmi et al., 2015)</ref>.</p><p>Additionally, the nervous system may have a way of replaying reality over and over, allowing to move an item from episodic memory into a long-term memory in the neural network <ref type="bibr" target="#b154">(Ji and Wilson, 2007)</ref>. This solution effectively uses many iterations of weight updating to fully learn a single item, even if one has only been exposed to it once.</p><p>Finally, higher-level systems in the brain may be able to implement Bayesian learning of sequential programs, which is a powerful means of one-shot learning <ref type="bibr">(Lake et al., 2015)</ref>. This type of cognition likely relies on an interaction between multiple brain areas such as the prefrontal cortex and basal ganglia. Computer models, and neural network based models in particular <ref type="bibr" target="#b265">(Rezende et al., 2016)</ref>, have not yet reached fully human-like performance in this area, despite significant recent advances <ref type="bibr">(Lake et al., 2015)</ref>.</p><p>These potential substrates of one-shot learning rely on mechanisms other than simple gradient descent. It should be noted, though, that recent architectural advances, including specialized spatial attention and feedback mechanisms <ref type="bibr" target="#b265">(Rezende et al., 2016)</ref>, as well as specialized memory mechanism <ref type="bibr" target="#b280">(Santoro et al., 2016)</ref>, do allow some types of one-shot generalization to be driven by backpropagation-based learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Active learning</head><p>Human learning is often active and deliberate. It seems likely that, in human learning, actions are chosen so as to generate interesting training examples, and sometimes also to test specific hypotheses. Such ideas of active learning and "child as scientist" go back to Piaget and have been elaborated more recently <ref type="bibr" target="#b107">(Gopnik et al., 2000)</ref>. We want our learning to be based on maximally informative samples, and active querying of the environment (or of internal subsystems) provides a way route to this.</p><p>At some level of organization, of course, it would seem useful for a learning system to develop explicit representations of its uncertainty, since this can be used to guide the system to actively seek the information that would reduce its uncertainty most quickly. Moreover, there are population coding mechanisms that could support explicit probabilistic computations <ref type="bibr" target="#b195">(Ma et al., 2006;</ref><ref type="bibr" target="#b364">Zemel and Dayan, 1997;</ref><ref type="bibr" target="#b98">Gershman and Beck, 2016;</ref><ref type="bibr" target="#b78">Eliasmith and Martens, 2011;</ref><ref type="bibr">Rao, 2004;</ref><ref type="bibr" target="#b278">Sahani and Dayan, 2003</ref>). Yet it is unclear to what extent and at what levels the brain uses an explicitly probabilistic framework, or to what extent probabilistic computations are emergent from other learning processes (Orhan and <ref type="bibr" target="#b249">Ma, 2016)</ref>.</p><p>Standard gradient descent does not incorporate any such adaptive sampling mechanism, e.g., it does not deliberately sample data so as to maximally reduce its uncertainty. Interestingly, however, stochastic gradient descent can be used to generate a system that samples adaptively <ref type="bibr" target="#b39">(Bouchard et al., 2015;</ref><ref type="bibr" target="#b3">Alain et al., 2015)</ref>. In other words, a system can learn, by gradient descent, how to choose its own input data samples in order to learn most quickly from them by gradient descent.</p><p>Ideally, the learner learns to choose actions that will lead to the largest improvements in its prediction or data compression performance <ref type="bibr">(Schmidhuber, 2010)</ref>. In <ref type="bibr">(Schmidhuber, 2010)</ref>, this is done in the framework of reinforcement learning, and incorporates a mechanisms for the system to measure its own rate of learning. In other words, it is possible to reinforcementlearn a policy for selecting the most interesting inputs to drive learning. Adaptive sampling methods are also known in reinforcement learning that can achieve optimal Bayesian exploration of Markov Decision Process environments <ref type="bibr">(Guez et al., 2012;</ref><ref type="bibr" target="#b309">Sun et al., 2011)</ref>.</p><p>These approaches achieve optimality in an arbitrary, abstract environment. But of course, evolution may also encode its implicit knowledge of the organism's natural environment, the behavioral goals of the organism, and the developmental stages and processes which occur inside the organism, as priors or heuristics which would further constrain the types of adaptive sampling that are optimal in practice. For example, sim-ple heuristics like seeking certain perceptual signatures of novelty, or more complex heuristics like monitoring situations that other people seem to find interesting, might be good ways to bias sampling of the environment so as to learn more quickly. Other such heuristics might be used to give internal brain systems the types of training data that will be most useful to those particular systems at any given developmental stage.</p><p>We are only beginning to understand how active learning might be implemented in the brain. We speculate that multiple mechanisms, specialized to different brain systems and spatiotemporal scales, could be involved. The above examples suggest that at least some such mechanisms could be understood from the perspective of optimizing cost functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Differing biological requirements for supervised and reinforcement learning</head><p>We have described how the brain could implement learning mechanisms of comparable power to backpropagation. But in many cases, the system may be more limited by the available training signals than by the optimization process itself. In machine learning, one distinguishes supervised learning, reinforcement learning and unsupervised learning, and the training data limitation manifests differently in each case.</p><p>Both supervised and reinforcement learning require some form of teaching signal, but the nature of the teaching signal in supervised learning is different from that in reinforcement learning. In supervised learning, the trainer provides the entire vector of errors for the output layer and these are back-propagated to compute the gradient: a locally optimal direction in which to update all of the weights of a potentially multilayer and/or recurrent network. In reinforcement learning, however, the trainer provides a scalar evaluation signal, but this is not sufficient to derive a low-variance gradient. Hence, some form of trial and error twiddling must be used to discover how to increase the evaluation signal. Consequently, reinforcement learning is generally much less efficient than supervised learning.</p><p>Reinforcement learning in shallow networks is simple to implement biologically. For reinforcement learning of a deep network to be biologi-cally plausible, however, we need a more powerful learning mechanism, since we are learning based on a more limited evaluation signal than in the supervised case: we do not have the full target pattern to train towards. Nevertheless, approximations of gradient descent can be achieved in this case, and there are cases in which the scalar evaluation signal of reinforcement learning can be used to efficiently update a multi-layer network by gradient descent. The "attentiongated reinforcement learning" (AGREL) networks of <ref type="bibr" target="#b307">(Stnior et al., 2013;</ref><ref type="bibr" target="#b42">Brosch et al., 2015;</ref><ref type="bibr" target="#b268">Roelfsema and van Ooyen, 2005)</ref>, and variants like KickBack <ref type="bibr" target="#b20">(Balduzzi, 2014)</ref>, give a way to compute an approximation to the full gradient in a reinforcement learning context using a feedback-based attention mechanism for credit assignment within the multi-layer network. The feedback pathway, together with a diffusible reward signal, together gate plasticity. For networks with more than three layers, this gives rise to a model based on columns containing parallel feedforward and feedback pathways. The process is still not as efficient as backpropagation, but it seems that this form of feedback can make reinforcement learning in multi-layer networks more efficient than a naive node perturbation or weight perturbation approach.</p><p>The machine-learning field has recently been tackling the question of credit assignment in deep reinforcement learning.</p><p>Deep Qlearning <ref type="bibr" target="#b226">(Mnih et al., 2015)</ref> demonstrates reinforcement learning in a deep network, wherein most of the network is trained via backpropagation. In regular Q learning, we define a function Q, which estimates the best possible sum of future rewards (the return) if we are in a given state and take a given action. In deep Q learning, this function is approximated by a neural network that, in effect, estimates action-dependent returns in a given state. The network is trained using backpropagation of local errors in Q estimation, using the fact that the return decomposes into the current reward plus the discounted estimate of future return at the next moment.</p><p>During training, as the agent acts in the environment, a series of loss functions is generated at each step, defining target patterns that can be used as the supervision signal for backpropagation. As Q is a highly nonlinear function of the state, tricks are sometimes needed to make deep Q learning efficient and stable, including experience replay and a particular type of mini-batch training. It is also necessary to store the outputs from the previous iteration (or clone the entire network) in evaluating the loss function for the subsequent iteration 17 .</p><p>This process for generating learning targets provides a kind of bridge between reinforcement learning and efficient backpropagation-based gradient descent learning 18 . Importantly, only temporally local information is needed making the approach relatively compatible with what we know about the nervous system.</p><p>Even given these advances, a key remaining issue in reinforcement learning is the problem of long timescales, e.g., learning the many small steps needed to navigate from London to Chicago. Many of the formal guarantees of reinforcement learning <ref type="bibr" target="#b350">(Williams and Baird, 1993)</ref>, for example, suggest that the difference between an optimal policy and the learned policy becomes increasingly loose as the discount factor shifts to take into account reward at longer timescales. Although the degree of optimality of human behavior is unknown, people routinely engage in adaptive behaviors that can take hours or longer to carry out, by using specialized processes like prospective memory to "remember to remember" relevant variables at the right times, permitting extremely long timescales of coherent action. Machine learning has not yet developed methods to deal with such a wide range of timescales and scopes of hierarchical action. Below we discuss ideas of hierarchical reinforcement learning that may make use of callable procedures and sub-routines, rather than operating explicitly in a time domain.</p><p>As we will discuss below, some form of deep reinforcement learning may be used by the brain for purposes beyond optimizing global rewards, including the training of local networks based on diverse internally generated cost functions. Scalar reinforcement-like signals are easy to com-pute, and easy to deliver to other areas, making them attractive mechanistically. If the brain does employ internally computed scalar rewardlike signals as a basis for cost functions, it seems likely that it will have found an efficient means of reinforcement-based training of deep networks, but it is an open question whether an analog of deep Q networks, AGREL, or some other mechanism entirely, is used in the brain for this purpose. Moreover, as we will discuss further below, it is possible that reinforcement-type learning is made more efficient in the context of specialized brain systems like short term memories, replay mechanisms, and hierarchically organized control systems. These specialized systems could reduce reliance on a need for powerful credit assignment mechanisms for reinforcement learning. Finally, if the brain uses a diversity of scalar reward-like signals to implement different cost functions, then it may need to mediate delivery of those signals via a comparable diversity of molecular substrates. The great diversity of neuromodulatory signals, e.g., neuropeptides, in the brain <ref type="bibr" target="#b22">(Bargmann, 2012;</ref><ref type="bibr" target="#b23">Bargmann and Marder, 2013)</ref> makes such diversity quite plausible, and moreover, the brain may have found other, as yet unknown, mechanisms of diversifying reward-like signaling pathways and enabling them to act independently of one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The cost functions are diverse across brain areas and time</head><p>In the last section, we argued that the brain can optimize functions. This raises the question of what functions it optimizes. Of course, in the brain, a cost function will itself be created (explicitly or implicitly) by a neural network shaped by the genome. Thus, the cost function used to train a given sub-network in the brain is a key innate property that can be built into the system by evolution. It may be much cheaper in biological terms to specify a cost function that allows 17. Many other reinforcement learning algorithms, including REINFORCE <ref type="bibr" target="#b349">(Williams, 1992)</ref>, can be implemented as fully online algorithms using "eligibility traces", which accumulate the sensitivity of action distributions to parameters in a temporally local manner (Sutton and Barto, 1998). 18. <ref type="bibr" target="#b362">Zaremba and Sutskever (2015)</ref> also bridges reinforcement learning and backpropagation learning in the same system, in the context of a neural network controlling discrete interfaces, and illustrates some of the challenges of this approach: compared to an end-to-end backpropagation-trained Neural Turing Machine <ref type="bibr" target="#b108">(Graves et al., 2014)</ref>, reinforcement based training allows training of only relatively simple algorithmic tasks. Special measures need to be taken to make reinforcement efficient, including limiting the number of possible actions, subtracting a baseline reward, and training the network using a curriculum schedule.</p><p>the rapid learning of the solution to a problem than to specify the solution itself.</p><p>In Hypothesis 2, we proposed that the brain optimizes not a single "end-to-end" cost function, but rather a diversity of internally generated cost functions specific to particular functions. To understand how and why the brain may use a diversity of cost functions, it is important to distinguish the differing types of cost functions that would be needed for supervised, unsupervised and reinforcement learning. We can also seek to identify types of cost functions that the brain may need to generate from a functional perspective, and how each may be implemented as supervised, unsupervised, reinforcement-based or hybrid systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">How cost functions may be represented and applied</head><p>What additional circuitry is required to actually impose a cost function on an optimizing network?</p><p>In the most familiar case, supervised learning may rely on computing a vector of errors at the output of a network, which will rely on some comparator circuitry to compute the difference between the network outputs and the target values. This difference could then be backpropagated to earlier layers. An alternative way to impose a cost function is to "clamp" the output of the network, forcing it to occupy a desired target state. Such clamping is actually assumed in some of the putative biological implementations of backpropagation described above, such as XCAL and target propagation. Alternatively, as described above, scalar reinforcement signals are attractive as internally-computed cost functions, but using them in deep networks requires special mechanisms for credit assignment.</p><p>In unsupervised learning, cost functions may not take the form of externally supplied training or error signals, but rather can be built into the dynamics inherent to the network itself, i.e., there may be no need for a separate circuit to compute and impose a cost function on the network. Indeed, beginning with Hopfield's definition of an energy function for learning in certain classes of symmetric network <ref type="bibr" target="#b141">(Hopfield, 1982)</ref>, researchers have discovered networks with inherent learning dynamics that implicitly optimizes certain objectives, such as statistical reconstruction of the in-put (e.g., via stochastic relaxation in Boltzmann machines <ref type="bibr" target="#b1">(Ackley et al., 1985)</ref>), or the achievement of certain properties like temporally stable or sparse representations.</p><p>Alternatively, explicit cost functions could be computed, delivered to a network, and used for unsupervised learning, following a variety of principles being discovered in machine learning (e.g., <ref type="bibr" target="#b261">(Radford et al., 2015;</ref><ref type="bibr" target="#b193">Lotter et al., 2015)</ref>), which typically find a way to encode the cost function into the error derivatives which are backpropagated. For example, prediction errors naturally give rise to error signals for unsupervised learning, as do reconstruction errors in autoencoders, and these error signals can also be augmented with additional penalty or regularization terms that enforce objectives like sparsity or continuity, as described below. In the next sections, we elaborate on these and other means of specifying and delivering cost functions in different learning contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cost functions for unsupervised learning</head><p>There are many objectives that can be optimized in an unsupervised context, to accomplish different kinds of functions or guide a network to form particular kinds of representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Matching the statistics of the input data using generative models</head><p>In one common form of unsupervised learning, higher brain areas attempt to produce samples that are statistically similar to those actually seen in lower layers. For example, the wake-sleep algorithm <ref type="bibr" target="#b133">(Hinton et al., 1995)</ref>  Similarly, in target propagation <ref type="bibr" target="#b26">(Bengio, 2014)</ref>, a top-down circuit, together with lateral information, has to produce data that directs the local learning of a bottom-up circuit and vice-versa. Ladder autoencoders make use of lateral connections and local noise injection to introduce an unsupervised cost function, based on internal reconstructions, that can be readily combined with supervised cost functions defined on the networks top layer outputs <ref type="bibr" target="#b331">(Valpola, 2015)</ref>. Compositional generative models generate a scene from discrete combinations of template parts and their transformations <ref type="bibr" target="#b338">(Wang and Yuille, 2014)</ref>, in effect performing a rendering of a scene based on its structural description. Hinton and colleagues have also proposed cortical "capsules" <ref type="bibr" target="#b317">(Tang et al., 2013</ref><ref type="bibr" target="#b318">(Tang et al., , 2012;;</ref><ref type="bibr">Hinton et al., 2011)</ref> for compositional inverse rendering. The network can thus implement a statistical goal that embodies some understanding of the way that the world produces samples. Learning rules for generative models have historically involved local message passing of a form quite different from backpropagation, e.g., in a multi-stage process that first learns one layer at a time and then fine-tunes via the wake-sleep algorithm <ref type="bibr" target="#b135">(Hinton et al., 2006)</ref>. Message-passing implementations of probabilistic inference have also been proposed as an explanation and generalization of deep convolutional networks <ref type="bibr" target="#b253">(Patel et al., 2015;</ref><ref type="bibr" target="#b53">Chen et al., 2014)</ref>.</p><p>Various mappings of such processes onto neural circuitry have been attempted <ref type="bibr" target="#b298">(Sountsov and Miller, 2015;</ref><ref type="bibr" target="#b97">George and Hawkins, 2009;</ref><ref type="bibr" target="#b181">Lee and Yuille, 2011)</ref>. Feedback connections tend to terminate in distinct layers of cortex relative to the feedforward ones <ref type="bibr" target="#b51">(Callaway, 2004</ref>) making the idea of separate but interacting networks for recognition and generation potentially attractive. Interestingly, such sub-networks might even be part of the same neuron and map onto "apical" versus "basal" parts of the dendritic tree <ref type="bibr" target="#b166">(Körding and König, 2001;</ref><ref type="bibr" target="#b330">Urbanczik and Senn, 2014)</ref>.</p><p>Generative models can also be trained via backpropagation. Recent advances have shown how to perform variational approximations to Bayesian inference inside backpropagation-based neural networks (Kingma and Welling, 2013), and how to exploit this to create generative models <ref type="bibr">(Eslami et al., 2016;</ref><ref type="bibr" target="#b110">Gregor et al., 2015;</ref><ref type="bibr" target="#b105">Goodfellow et al., 2014a;</ref><ref type="bibr" target="#b261">Radford et al., 2015)</ref>. Through either explicitly statistical or gradient descent based learning, the brain can thus obtain a probabilistic model that simulates features of the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cost functions that approximate properties of the world</head><p>A perceiving system should exploit statistical regularities in the world that are not present in an arbitrary dataset or input distribution. For example, objects are sparse: there are far fewer objects than there are potential places in the world, and of all possible objects there is only a small subset visible at any given time.</p><p>As such, we know that the output of an object recognition system must have sparse activations. Building the assumption of sparseness into simulated systems replicates a number of representational properties of the early visual system <ref type="bibr" target="#b240">(Olshausen and Field, 1997;</ref><ref type="bibr" target="#b274">Rozell et al., 2008)</ref>, and indeed the original paper on sparse coding obtained sparsity by gradient descent optimization of a cost function <ref type="bibr" target="#b239">(Olshausen and Field, 1996)</ref>. A range of unsupervised machine learning techniques, such as the sparse autoencoders <ref type="bibr" target="#b176">(Le et al., 2011)</ref> used to discover cats in YouTube videos, build sparseness into neural networks. Building in such spatiotemporal sparseness priors should serve as an "inductive bias" (Mitchell, 1980) that can accelerate learning.</p><p>But we know much more about the regularities of objects. As young babies, we already know <ref type="bibr" target="#b41">(Bremner et al., 2015)</ref> that objects tend to persist over time. The emergence or disappearance of an object from a region of space is a rare event. Moreover, object locations and configurations tend to be coherent in time. We can formulate this prior knowledge as a cost function, for example by penalizing representations which are not temporally continuous. This idea of continuity is used in a great number of artificial neural networks and related models <ref type="bibr" target="#b227">(Mobahi et al., 2009;</ref><ref type="bibr" target="#b354">Wiskott and Sejnowski, 2002;</ref><ref type="bibr" target="#b90">Földiák, 2008)</ref>. Imposing continuity within certain models gives rise to aspects of the visual system including complex cells <ref type="bibr" target="#b165">(Körding et al., 2004)</ref>, specific properties of visual invariance <ref type="bibr" target="#b146">(Isik et al., 2012)</ref>, and even other representational properties such as the existence of place cells <ref type="bibr" target="#b354">(Wyss et al., 2006;</ref><ref type="bibr" target="#b94">Franzius et al., 2007)</ref>. Unsupervised learning mechanisms that maximize temporal coherence 19. Temporal continuity is exploited in Poggio (2015), which analyzes many properties of deep convolutional networks with respect to their biological plausibility, including their apparent need for large amounts of supervised training or slowness are increasingly used in machine learning 19 . We also know that objects tend to undergo predictable sequences of transformations, and it is possible to build this assumption into unsupervised neural learning systems <ref type="bibr" target="#b97">(George and Hawkins, 2009)</ref>. The minimization of prediction error explains a number of properties of the nervous system <ref type="bibr" target="#b95">(Friston and Stephan, 2007;</ref><ref type="bibr" target="#b144">Huang and Rao, 2011)</ref>, and biologically plausible theories are available for how cortex could learn using prediction errors by exploiting temporal differences <ref type="bibr" target="#b247">(O'Reilly et al., 2014b)</ref> or top-down feedback <ref type="bibr" target="#b97">(George and Hawkins, 2009)</ref>. In one implementation, a system can simply predict the next input delivered to the system and can then use the difference between the actual next input and the predicted next input as a full vectorial error signal for supervised gradient descent. Thus, rather than optimization of prediction error being implicitly implemented by the network dynamics, the prediction error is used as an explicit cost function in the manner of supervised learning, leading to error derivatives which can be back-propagated. Then, no special learning rules beyond simple backpropagation are needed. This approach has recently been advanced within machine learning <ref type="bibr" target="#b193">(Lotter et al., 2015</ref><ref type="bibr" target="#b194">(Lotter et al., , 2016))</ref>. Recently, combining such prediction-based learning with a specific gating mechanism has been shown to lead to unsupervised learning of disentangled representations <ref type="bibr" target="#b348">(Whitney et al., 2016)</ref>. Neural networks can also be designed to learn to invert spatial transformations <ref type="bibr" target="#b150">(Jaderberg et al., 2015b)</ref>. Statistically describing transformations or sequences is thus an unsupervised way of learning representations.</p><p>Furthermore, there are multiple modalities of input to the brain. Each sensory modality is pri-marily connected to one part of the brain 20 . But higher levels of cortex in each modality are heavily connected to the other modalities. This can enable forms of self-supervised learning: with a developing visual understanding of the world we can predict its sounds, and then test those predictions with the auditory input, and vice versa. The same is true about multiple parts of the same modality: if we understand the left half of the visual field, it tells us an awful lot about the right. Maximizing mutual information is a natural way of improving learning <ref type="bibr" target="#b25">(Becker and Hinton, 1992;</ref><ref type="bibr" target="#b229">Mohamed and Rezende, 2015)</ref>, and there are many other ways in which multiple modalities or processing streams could mutually train one another. Relatedly, we can use observations of one part of a visual scene to predict the contents of other parts (van den <ref type="bibr" target="#b332">Oord et al., 2016;</ref><ref type="bibr" target="#b235">Noroozi and Favaro, 2016)</ref>, and optimize a cost function that reflects the discrepancy. This way, each modality effectively produces training signals for the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cost functions for supervised learning</head><p>In what cases might the brain use supervised learning, given that it requires the system to "already know" the exact target pattern to train towards? One possibility is that the brain can store records of states that led to good outcomes. For example, if a baby reaches for a target and misses, and then tries again and successfully hits the target, then the difference in the neural representations of these two tries reflects the direction in which the system should change. The brain could potentially use a comparator circuit -a non-trivial task since neural activations are always positive, although different neuron types can be excitatory vs. inhibitory -to directly compute this vectorial difference in the neural data, and concludes that the environment may in fact provide a sufficient number of "implicitly", though not explicitly, labeled examples to train a deep convolutional network for object recognition. Implicit labeling of object identity, in this case, arises from temporal continuity: successive frames of a video are likely to have the same objects in similar places and orientations. This allows the brain to derive an invariant signature of object identity which is independent of transformations like translations and rotations, but which does not yet associate the object with a specific name or label. Once such an invariant signature is established, however, it becomes basically trivial to associate the signature with a label for classification <ref type="bibr" target="#b8">(Anselmi et al., 2015)</ref>. Poggio (2015) also suggests specific means, in the context of I-theory <ref type="bibr" target="#b8">(Anselmi et al., 2015)</ref>, by which this training could occur via the storage of image templates using Hebbian mechanisms among simple and complex cells in the visual cortex. Thus, in this model, the brain has used its implicit knowledge of the temporal continuity of object motion to provide a kind of minimal labeling that is sufficient to bootstrap object recognition. Although not formulated as a cost function, this shows how usefully the assumption of temporal continuity could be exploited by the brain. 20. Although, some multi-sensory integration appears to occur even in the early sensory cortices <ref type="bibr" target="#b231">(Murray et al., 2012)</ref>.</p><p>population codes and then apply this difference vector as an error signal.</p><p>Another possibility is that the brain uses supervised learning to implement a form of "chunking", i.e., a consolidation of something the brain already knows how to do: routines that are initially learned as multi-step, deliberative procedures could be compiled down to more rapid and automatic functions by using supervised learning to train a network to mimic the overall input-output behavior of the original multi-step process. Such a process is assumed to occur in cognitive models like ACT-R <ref type="bibr" target="#b286">(Servan-Schreiber and Anderson, 1990)</ref>, and methods for compressing the knowledge in neural networks into smaller networks are also being developed <ref type="bibr" target="#b15">(Ba and Caruana, 2014)</ref>. Thus supervised learning can be used to train a network to do in "one step" what would otherwise require long-range routing and sequential recruitment of multiple systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Repurposing reinforcement learning for diverse internal cost functions</head><p>Certain generalized forms of reinforcement learning may be ubiquitous throughout the brain. Such reinforcement signals may be repurposed to optimize diverse internal cost functions. These internal cost functions could be specified at least in part by genetics. Some brain systems such as in the striatum appear to learn via some form of temporal difference reinforcement learning <ref type="bibr">(Tesauro, 1995;</ref><ref type="bibr" target="#b91">Foster et al., 2000)</ref>. This is reinforcement learning based on a global value function <ref type="bibr" target="#b246">(O'Reilly et al., 2014a)</ref> that predicts total future reward or utility for the agent. Reward-driven signaling is not restricted to the striatum, and is present even in primary visual cortex <ref type="bibr" target="#b55">(Chubykin et al., 2013;</ref><ref type="bibr" target="#b307">Stnior et al., 2013)</ref>. Remarkably, the reward signaling in primary visual cortex is mediated in part by glial cells <ref type="bibr" target="#b316">(Takata et al., 2011)</ref>, rather than neurons, and involves the neurotransmitter acetylcholine <ref type="bibr" target="#b55">(Chubykin et al., 2013;</ref><ref type="bibr" target="#b117">Hangya et al., 2015)</ref>. On the other hand, some studies have suggested that visual cortex learns the basics of invariant object recognition in the absence of reward (Li and Dicarlo, 2012), perhaps using rein-forcement only for more refined perceptual learning <ref type="bibr" target="#b269">(Roelfsema et al., 2010)</ref>.</p><p>But beyond these well-known global reward signals, we argue that the basic mechanisms of reinforcement learning may be widely re-purposed to train local networks using a variety of internally generated error signals. These internally generated signals may allow a learning system to go beyond what can be learned via standard unsupervised methods, effectively guiding or steering the system to learn specific features or computations <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Cost functions for bootstrapping learning in the human environment</head><p>Special, internally-generated signals are needed specifically for learning problems where standard unsupervised methods -based purely on matching the statistics of the world, or on optimizing simple mathematical objectives like temporal continuity or sparsity -will fail to discover properties of the world which are statistically weak in an objective sense but nevertheless have special significance to the organism <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>. Indigo bunting birds, for example, learn a template for the constellations of the night sky long before ever leaving the nest to engage in navigation-dependent tasks <ref type="bibr" target="#b80">(Emlen, 1967)</ref>. This memory template is directly used to determine the direction of flight during migratory periods, a process that is modulated hormonally so that winter and summer flights are reversed. Learning is therefore a multi-phase process in which navigational cues are memorized prior to the acquisition of motor control.</p><p>In humans, we suspect that similar multistage bootstrapping processes are arranged to occur. Humans have innate specializations for social learning. We need to be able to read their expressions as indicated with hands and faces. Hands are important because they allow us to learn about the set of actions that can be produced by agents <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>. Faces are important because they give us insight into what others are thinking. People have intentions and personalities that differ from one another, and their feelings are important. How could we hack together cost functions, built on simple genetically specifiable mechanisms, to make it easier for a learning system to discover such behaviorally relevant variables? Some preliminary studies are beginning to suggest specific mechanisms and heuristics that humans may be using to bootstrap more sophisticated knowledge. In a groundbreaking study, <ref type="bibr" target="#b329">Ullman et al. (2012)</ref> asked how could we explain hands, to a system that does not already know about them, in a cheap way, without the need for labeled training examples? Hands are common in our visual space and have special roles in the scene: they move objects, collect objects, and caress babies. Building these biases into an area specialized to detect hands could guide the right kind of learning, by providing a downstream learning system with many likely positive examples of hands on the basis of innately-stored, heuristic signatures about how hands tend to look or behave <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>. Indeed, an internally supervised learning algorithm containing specialized, hard-coded biases to detect hands, on the basis of their typical motion properties, can be used to bootstrap the training of an image recognition module that learns to recognize hands based on their appearance. Thus, a simple, hard-coded module bootstraps the training of a much more complex algorithm for visual recognition of hands. <ref type="bibr" target="#b329">Ullman et al. (2012)</ref> then further exploits a combination of hand and face detection to bootstrap a predictor for gaze direction, based on the heuristic that faces tend to be looking towards hands. Of course, given a hand detector, it also becomes much easier to train a system for reaching, crawling, and so forth. Efforts are underway in psychology to determine whether the heuristics discovered to be useful computationally are, in fact, being used by human children during learning <ref type="bibr" target="#b81">(Fausey et al., 2016;</ref><ref type="bibr" target="#b361">Yu and Smith, 2013)</ref>.</p><p>Ullman refers to such primitive, inbuilt detectors as innate "proto-concepts" <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>. Their broader claim is that such pre-specification of mutual supervision signals can make learning the relevant features of the world far easier, by giving an otherwise unsupervised learner the right kinds of hints or heuristic biases at the right times. Here we call these approximate, heuristic cost functions "bootstrap cost functions". The purpose of the bootstrap cost functions is to reduce the amount of data required to learn a specific feature or task, but at the same time to avoid a need for fully unsupervised learning.</p><p>Could the neural circuitry for such a bootstrap hand-detector be pre-specified genetically? The precedent from other organisms is strong: for example, it is famously known that the frog retina contains circuitry sufficient to implement a kind of "bug detector" <ref type="bibr" target="#b185">(Lettvin et al., 1959</ref>). Ullman's hand detector, in fact, operates via a simple local optical flow calculation to detect "mover" events. This type of simple, local calculation could potentially be implemented in genetically-specified and/or spontaneously selforganized neural circuitry in the retina or early dorsal visual areas <ref type="bibr" target="#b34">(Biilthoff et al., 1989)</ref>, perhaps similarly to the frog's "bug detector".</p><p>How could we explain faces without any training data? Faces tend to have two dark dots in their upper half, a line in the lower half and tend to be symmetric about a vertical axis. Indeed, we know that babies are very much attracted to things with these generic features of upright faces starting from birth, and that they will acquire face-specific cortical areas<ref type="foot" target="#foot_0">21</ref> in their first few years of life if not earlier <ref type="bibr" target="#b215">(McKone et al., 2009)</ref>. It is easy to define a local rule that produces a kind of crude face detector (e.g., detecting two dots on top of a horizontal line), and indeed some evidence suggests that the brain can rapidly detect faces without even a single feed-forward pass through the ventral visual stream (Crouzet and Thorpe, 2011). The crude detection of human faces used together with statistical learning should be analogous to semi-supervised learning <ref type="bibr" target="#b308">(Sukhbaatar et al., 2014)</ref> and could allow identifying faces with high certainty.</p><p>Humans have areas devoted to emotional processing, and the brain seems to embody prior knowledge about the structure of emotions: emotions should have specific types of strong couplings to various other higher-level variables, should be expressed through the face, and so on. This prior knowledge, encoded into brain structure via evolution, could allow learning signals to come from the right places and to appear developmentally at the right times. What about agency? It makes sense to describe, when dealing with high-level thinking, other beings as optimizers of their own goal functions. It appears that heuristically specified notions of goals and agency are infused into human psychological development from early infancy and that notions of agency are used to bootstrap heuristics for ethical evaluation <ref type="bibr" target="#b295">(Skerry and Spelke, 2014;</ref><ref type="bibr" target="#b116">Hamlin et al., 2007)</ref>. Algorithms for establishing more complex, innately-important social relationships such as joint attention are under study <ref type="bibr">(Gao et al., 2014)</ref>, building upon more primitive proto-concepts like face detectors and Ullman's hand detectors <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>. The brain can thus use innate detectors to create cost functions and training procedures to train the next stages of learning.</p><p>It is intuitive to ask whether this type of bootstrapping poses a kind of "chicken and egg" problem: if the brain already has an inbuilt heuristic hand detector, how can it be used to train a detector that performs any better than those heuristics? After all, isn't a trained system only as good as its training data? The work of <ref type="bibr" target="#b329">Ullman et al. (2012)</ref> illustrates why this is not the case. First, the "innate detector" can be used to train a downstream detector that operates based on different cues: for example, based on the spatial and body context of the hand, rather than its motion. Second, once multiple such pathways of detection come into existence, they can be used to improve each other. In <ref type="bibr" target="#b329">Ullman et al. (2012)</ref>, appearance, body context, and mover motion are all used to bootstrap off of one another, creating a detector that is better than any of it training heuristics. In effect, the innate detectors are used not as supervision signals per se, but rather to guide or steer the learning process, enabling it to discover features that would otherwise be difficult. If such affordances can be found in other domains, it seems likely that the brain would make extensive use of them to ensure that developing animals learn the precise patterns of perception and behavior needed to ensure their later survival and reproduction.</p><p>Thus, generalizing previous ideas <ref type="bibr" target="#b329">(Ullman et al., 2012;</ref><ref type="bibr" target="#b189">Poggio, 2015)</ref>, we suggest that the brain uses optimization with respect to internally generated heuristic detection signals to bootstrap learning of biologically relevant features which would otherwise be missed by an unsupervised learner. In one possible implementation, such bootstrapping may occur via reinforcement learning, using the outputs of the innate detectors as local reinforcement signals, and perhaps using mechanisms similar to <ref type="bibr" target="#b307">(Stnior et al., 2013;</ref><ref type="bibr" target="#b271">Rombouts et al., 2015;</ref><ref type="bibr" target="#b42">Brosch et al., 2015;</ref><ref type="bibr" target="#b268">Roelfsema and van Ooyen, 2005)</ref> to perform reinforcement learning through a multi-layer network. It is also possible that the brain could use such internally generated heuristic detectors in other ways, for example to bias the inputs delivered to an unsupervised learning network towards entities of interest to humans (Joscha Bach, personal communication), or to directly train simple classifiers <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Cost functions for story generation and understanding</head><p>It has been widely noticed in cognitive science and AI that the generation and understanding of stories are crucial to human cognition. Researchers such as Winston have framed story understanding as the key to human-like intelligence <ref type="bibr" target="#b353">(Winston, 2011)</ref>. Stories consist of a linear sequence of episodes, in which one episode refers to another through cause and effect relationships, with these relationships often involving the implicit goals of agents. Many other cognitive faculties, such as conceptual grounding of language, could conceivably emerge from an underlying internal representation in terms of stories.</p><p>Perhaps the ultimate series of bootstrap cost functions would be those which would direct the brain to utilize its learning networks and specialized systems so as to construct representations that are specifically useful as components of sto-ries, to spontaneously chain these representations together, and to update them through experience and communication. How could such cost functions arise? One possibility is that they are bootstrapped through imitation and communication, where a child learns to mimic the storytelling behavior of others. Another possibility is that useful representations and primitives for stories emerge spontaneously from mechanisms for learning state and action chunking in hierarchical reinforcement learning and planning. Yet another is that stories emerge from learned patterns of saliency-directed memory storage and recall (e.g., <ref type="bibr" target="#b356">(Xiong et al., 2016)</ref>). In addition, priors that direct the developing child's brain to learn about and attend to social agency seem to be important for stories. These systems will be discussed in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization occurs in the context of specialized structures</head><p>Optimization of initially unstructured "blank slate" networks is not sufficient to generate complex cognition in the brain, we argue, even given a diversity of powerful genetically-specified cost functions and local learning rules, as we have posited above. Instead, in Hypothesis 3, we suggest that specialized, pre-structured architectures are needed for at least two purposes. First, pre-structured architectures are needed to allow the brain to find efficient solutions to certain types of problems. When we write computer code, there are a broad range of algorithms and data structures employed for different purposes: we may use dynamic programming to solve planning problems, trees to efficiently implement nearest neighbor search, or stacks to implement recursion. Having the right kind of algorithm and data structure in place to solve a problem allows it to be solved efficiently, robustly and with a minimum amount of learning or optimization needed. This observation is concordant with the increasing use of pre-specialized architectures and specialized computational components in machine learning <ref type="bibr" target="#b108">(Graves et al., 2014;</ref><ref type="bibr" target="#b347">Weston et al., 2014;</ref><ref type="bibr" target="#b233">Neelakantan et al., 2015)</ref>. In particular, to enable the learning of efficient computational solutions, the brain may need pre-specialized systems for planning and execut-ing sequential multi-step processes, for accessing memories, and for forming and manipulating compositional and recursive structures.</p><p>Second, the training of optimization modules may need to be coordinated in a complex and dynamic fashion, including delivering the right training signals and activating the right learning rules in the right places and at the right times. To allow this, the brain may need specialized systems for storing and routing data, and for flexibly routing training signals such as target patterns, training data, reinforcement signals, attention signals, and modulatory signals. These mechanisms may need to be at least partially in place in advance of learning.</p><p>Looking at the brain, we indeed seem to find highly conserved structures, e.g., cortex, where it is theorized that a similar type of learning and/or computation is happening in multiple places <ref type="bibr" target="#b73">(Douglas and Martin, 2004;</ref><ref type="bibr">Braitenberg and Schutz, 1991)</ref>. But we also see a large number of specialized structures, including thalamus, hippocampus, basal ganglia and cerebellum <ref type="bibr" target="#b297">(Solari and Stoner, 2011)</ref>. Some of these structures evolutionarily predate <ref type="bibr" target="#b179">(Lee et al., 2015)</ref> the cortex, and hence the cortex may have evolved to work in the context of such specialized mechanisms. For example, the cortex may have evolved as a trainable module for which the training is orchestrated by these older structures.</p><p>Even within the cortex itself, microcircuitry within different areas may be specialized: tinkered variations on a common ancestral microcircuit scaffold could potentially allow different cortical areas, such as sensory areas vs. prefrontal areas, to be configured to adopt a number of qualitatively distinct computational and learning configurations <ref type="bibr">(Marcus et al., 2014a,b;</ref><ref type="bibr" target="#b361">Yuste et al., 2005)</ref>, even while sharing a common gross physical layout and communication interface. Within cortex, over forty distinct cell types differing in such aspects as dendritic organization, distribution throughout the six cortical layers, connectivity pattern, gene expression, and electrophysiological properties have already been found <ref type="bibr" target="#b363">(Zeisel et al., 2015;</ref><ref type="bibr" target="#b208">Markram et al., 2015)</ref>. Central pattern generator circuits provide an example of the kinds of architectures that can be pre-wired into neural microcircuitry, and may have evolutionary relationships with cor-tical circuits <ref type="bibr" target="#b361">(Yuste et al., 2005)</ref>. Thus, while the precise degree of architectural specificity of particular cortical regions is still under debate <ref type="bibr">(Marcus et al., 2014a,b)</ref>, various mechanism could offer pre-specified heterogeneity.</p><p>In this section, we explore the kinds of computational problems for which specialized structures may be useful, and attempt to map these to putative elements within the brain. Our preliminary sketch of a functional decomposition can be viewed as a summary of suggestions for specialized functions that have been made throughout the computational neuroscience literature, and is influenced strongly by the models of O'Reilly, <ref type="bibr">Eliasmith, Grossberg, Marcus, Hayworth and others (O'Reilly, 2006;</ref><ref type="bibr" target="#b79">Eliasmith et al., 2012;</ref><ref type="bibr">Marcus, 2001;</ref><ref type="bibr" target="#b111">Grossberg, 2013;</ref><ref type="bibr">Hayworth, 2012)</ref>. The correspondence between these models and actual neural circuitry is, of course, still the subject of extensive debate.</p><p>Many of the computational and neural concepts sketched here are preliminary and will need to be made more rigorous through future study. Our knowledge of the functions of particular brain areas, and thus our proposed mappings of certain computations onto neuroanatomy, also remains tentative. Finally, it is still far from established which processes in the brain emerge from optimization of cost functions, which emerge from other forms of selforganization, which are pre-structured through genetics and development, and which rely on an interplay of all these mechanisms. Our discussion here should therefore be viewed as a sketch of potential directions for further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Structured forms of memory</head><p>One of the central elements of computation is memory. Importantly, multiple different kinds of memory are needed <ref type="bibr" target="#b300">(Squire, 2004)</ref>. For example, we need memory that is stored for a long period of time and that can be retrieved in a number of ways, such as in situations similar to the time when the memory was first stored (content addressable memory). We also need memory that we can keep for a short period of time and that we can rapidly rewrite (working memory). Lastly, we need the kind of implicit memory that we cannot explicitly recall, similar to the kind of memory that is classically learned using gradient descent on errors, i.e., sculpted into the weight matrix of a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Content addressable memories</head><p>Content addressable memories 22 are classic models in neuroscience <ref type="bibr" target="#b141">(Hopfield, 1982)</ref>. Most simply, they allow us to recognize a situation similar to one that we have seen before, and to "fill in" stored patterns based on partial or noisy information, but they may also be put to use as sub-components of many other functions. Recent research has shown that including such memories allows deep networks to learn to solve problems that previously were out of reach, even of LSTM networks that already have a simpler form of local memory and are already capable of learning long-term dependencies <ref type="bibr" target="#b347">(Weston et al., 2014;</ref><ref type="bibr" target="#b108">Graves et al., 2014)</ref>. Hippocampal area CA3 may act as an auto-associative memory 23 capable of content-addressable pattern completion, with pattern separation occurring in the dentate gyrus <ref type="bibr" target="#b270">(Rolls, 2013)</ref>. Such systems could permit the retrieval of complete memories from partial cues, enabling networks to perform operations similar to database retrieval or to instantiate lookup tables of historical stimulus-response mappings, among numerous other possibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Working memory buffers</head><p>Cognitive science has long characterized properties of the working memory. It is somewhat limited, with the old idea being that it can represent "seven plus or minus two" elements <ref type="bibr">(Miller,</ref><ref type="bibr">22.</ref> Attractor models of memory in neuroscience tend to have the property that only one memory can be accessed at a time. Yet recent machine learning systems have constructed differentiable addressable memory <ref type="bibr" target="#b108">(Graves et al., 2014)</ref> and gating <ref type="bibr" target="#b348">(Whitney et al., 2016)</ref> systems by allowing weighted superpositions of memory registers or gates to be queried. It is unclear whether the brain uses such mechanisms. 23. Computational analogies have also been drawn between associative memory storage and object recognition <ref type="bibr" target="#b183">(Leibo et al., 2015a)</ref>, suggesting the possibility of closely related computations occurring in parts of neocortex and hippocampus. Indeed both areas have some apparent anatomical similarities such as the presence of pyramidal neurons, and it has been suggested that the hippocampus can be thought of as the top of the cortical hierarchy <ref type="bibr" target="#b125">(Hawkins and Blakeslee, 2007)</ref>, responsible for handling and remembering information that could not be fully explained by lower levels of the hierarchy. These connections are still tentative.</p><p>1956). There are many models of working memory <ref type="bibr" target="#b339">(Wang, 2012;</ref><ref type="bibr">Singh and Eliasmith, 2006;</ref><ref type="bibr" target="#b245">O'Reilly and Frank, 2006;</ref><ref type="bibr" target="#b47">Buschman and Miller, 2014;</ref><ref type="bibr" target="#b340">Warden and Miller, 2007)</ref>, some of which attribute it to persistent, self-reinforcing patterns of neural activation <ref type="bibr" target="#b102">(Goldman et al., 2003)</ref> in the recurrent networks of the prefrontal cortex. Prefrontal working memory appears to be made up of multiple functionally distinct subsystems <ref type="bibr" target="#b207">(Markowitz et al., 2015)</ref>. Neural models of working memory can store not only scalar variables <ref type="bibr" target="#b287">(Seung, 1998)</ref>, but also highdimensional vectors <ref type="bibr" target="#b77">(Eliasmith and Anderson, 2004;</ref><ref type="bibr" target="#b79">Eliasmith et al., 2012)</ref> or sequences of vectors (Choo and <ref type="bibr" target="#b321">Eliasmith, 2010)</ref>. Working memory buffers seem crucial for human-like cognition, e.g., reasoning, as they allow short-term storage while also -in conjunction with other mechanisms -enabling generalization of operations across anything that can fill the buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Storing state in association with saliency</head><p>Saliency, or interestingness, measures can be used to tag the importance of a memory (Gonzalez Andino and Grave de Peralta <ref type="bibr" target="#b103">Menendez, 2012)</ref>. This can allow removal of the boring data from the training set, allowing a mechanism that is more like optimal experimentation. Moreover, saliency can guide memory replay or sampling from generative models, to generate more training data drawn from a distribution useful for learning <ref type="bibr" target="#b226">(Mnih et al., 2015;</ref><ref type="bibr" target="#b154">Ji and Wilson, 2007)</ref>. Conceivably, hippocampal replay could allow a batch-like training process, similar to how most machine learning systems are trained, rather than requiring all training to occur in an online fashion. Plasticity mechanisms in memory systems which are gated by saliency are starting to be uncovered in neuroscience <ref type="bibr" target="#b75">(Dudman et al., 2007)</ref>. Importantly, the notions of "saliency" computed by the brain could be quite intricate and multi-faceted, potentially leading to complex schemes by which specific kinds of memories would be tagged for later context-dependent retrieval. As a hypothetical example, representations of both timing and importance associated with memories could perhaps allow retrieval only of important memories that happened within a certain window of time <ref type="bibr" target="#b198">(MacDonald et al., 2011;</ref><ref type="bibr" target="#b167">Kraus et al., 2013;</ref><ref type="bibr" target="#b275">Rubin et al., 2015)</ref>. Storing and retrieving information selectively based on specific properties of the information itself, or of "tags" appended to that information, is a powerful computational primitive that could enable learning of more complex tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Structured routing systems</head><p>To use its information flexibly, the brain needs structured systems for routing data. Such systems need to address multiple temporal and spatial scales, and multiple modalities of control. Thus, there are several different kinds of information routing systems in the brain which operate by different mechanisms and under different constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Attention</head><p>If we can focus on one thing at a time, we may be able to allocate more computational resources to processing it, make better use of scarce data to learn about it, and more easily store and retrieve it from memory<ref type="foot" target="#foot_1">24</ref> . Notably in this context, attention allows improvements in learning: if we can focus on just a single object, instead of an entire scene, we can learn about it more easily using limited data. Formal accounts in a Bayesian framework talk about attention reducing the sample complexity of learning <ref type="bibr" target="#b54">(Chikkerur et al., 2010)</ref>. Likewise, in models, the processes of applying attention, and of effectively making use of incoming attentional signals to appropriately modulate local circuit activity, can themselves be learned by optimizing cost functions <ref type="bibr" target="#b225">(Mnih et al., 2014;</ref><ref type="bibr" target="#b152">Jaramillo and Pearlmutter, 2004)</ref>. The right kinds of attention make processing and learning more efficient, and also allow for a kind of programmatic control over multi-step perceptual tasks. How does the brain determine where to allocate attention, and how is the attentional signal physically mediated? Answering this question is still an active area of neuroscience. Higher-level cortical areas may be specialized in allocating attention. The problem is made complex by the fact that there seem to be many different types of attention -such as object-based, feature-based and spatial attention in vision -that may be mediated by interactions between different brain areas. The frontal eye fields (area FEF), for example, are important in visual attention, specifically for controlling saccades of the eyes to attended locations. Area FEF contains "retinotopic" spatial maps whose activation determines the saccade targets in the visual field. Other prefrontral areas such as the dorsolateral prefrontal cortex and inferior frontal junction are also involved in maintaining representations that specify the targets of certain types of attention. Certain forms of attention may require a complex interaction between brain areas, e.g., to determine targets of attention based on higher-level properties that are represented across multiple areas, like the identity and spatial location of a specific face <ref type="bibr" target="#b19">(Baldauf and Desimone, 2014)</ref>.</p><p>There are many proposed neural mechanisms of attention, including the idea that synchrony plays a role <ref type="bibr" target="#b19">(Baldauf and Desimone, 2014)</ref>, perhaps by creating resonances that facilitate the transfer of information between synchronously oscillating neural populations in different areas. Other proposed mechanisms include specific circuits for attention-dependent signal routing (Anderson and <ref type="bibr" target="#b5">Van Essen, 1987;</ref><ref type="bibr" target="#b238">Olshausen et al., 1993)</ref>. Various forms of attention also have specific neurophysiological signatures, such as enhancements in synchrony among neural spikes and with the ambient local field potential, changes in the sharpness of neural tuning curves, and other properties. These diverse effects and signatures of attention may be consequences of underlying pathways that wire up to particular elements of cortical microcircuits to mediate different attentional effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Buffers</head><p>One possibility is that the brain uses distinct groups of neurons, which we can call "buffers", to store distinct variables, such as the subject or object in a sentence <ref type="bibr" target="#b93">(Frankland and Greene, 2015)</ref>. Having memory buffers allows the abstraction of a variable. As is ubiquitously used in computer science, this comes with the ability to generalize operations across any variable that could meaningfully fill the buffer and makes computation flexible.</p><p>Once we establish that the brain has a number of memory buffers, we need ways for those buffers to interact. We need to be able to take a buffer, do a computation on its contents and store the output into another buffer. But if the representations in each of two groups of neurons are learned, and hence are coded differently, how can the brain "copy and paste" information between these groups of neurons? Malsburg argued that such a system of separate buffers is impossible because the neural pattern for "chair" in buffer 1 has nothing in common with the neural pattern for "chair" in buffer 2 -any learning that occurs for the contents of buffer 1 would not automatically be transferable to buffer 2. Various mechanisms have been proposed to allow such transferability, which focus on ways in which all buffers could be trained jointly and then later separated so that they can work independently when they need to 25 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discrete gating of information flow between buffers</head><p>Dense connectivity is only achieved locally, but it would be desirable to have a way for any two cortical units to talk to one another, if needed, re-25. One idea for achieving such transferability is that of a partitionable <ref type="bibr">(Hayworth, 2012)</ref> or annexable <ref type="bibr" target="#b36">(Bostrom, 1996)</ref> network. These models posit that a large associative memory network links all the different buffers. This large associative memory network has a number of stable attractor states. These are called "global" attractor states since they link across all the buffers. Forcing a given buffer into an activity pattern resembling that of its corresponding "piece" of an attractor state will cause the entire global network to enter that global attractor state.</p><p>During training, all of the connections between buffers are turned on, so that their learned contents, though not identical, are kept in correspondence by being part of the same attractor. Later, the connections between specific buffers can be turned off to allow them to store different information. Copy and paste is then implemented by turning on the connections between a source buffer and a destination buffer <ref type="bibr">(Hayworth, 2012)</ref>. Copying between a source and destination buffer can also be implemented, i.e., learned, in a deep learning system using methods similar to the addressing mechanisms of the Neural Turing Machine <ref type="bibr" target="#b108">(Graves et al., 2014)</ref>. 26. Micro-stimulation experiments, in which an animal learns to behaviorally report stimulation of electrode channels located in diverse cortical regions, suggest that many areas can be routed or otherwise linked to behavioral "outputs" <ref type="bibr" target="#b137">(Histed et al., 2013)</ref>, although the mechanisms behind this -e.g., whether this stimulation gives rise to a high-level percept that the animal then uses to make a decision -are unclear. Likewise, it is possible to reinforcement-train an animal to control the activity of individual neurons <ref type="bibr" target="#b84">(Fetz, 1969</ref><ref type="bibr" target="#b85">(Fetz, , 2007))</ref>.</p><p>gardless of their distance from one another, and without introducing crosstalk 26 . It is therefore critical to be able to dynamically turn on and off the transfer of information between different source and destination regions, in much the manner of a switchboard. Together with attention, such dedicated routing systems can make sure that a brain area receives exactly the information it needs. Such a discrete routing system is, of course, central to cognitive architectures like <ref type="bibr">ACT-R (Anderson, 2007)</ref>. The key feature of ACT-R is the ability to evaluate the IF clauses of tens of thousands of symbolic rules (called "productions"), in parallel, approximately every 50 milliseconds. Each rule requires equality comparisons between the contents of many constant and variable memory buffers, and the execution of a rule leads to the conditional routing of information from one buffer to another. What controls which long-range routing operations occur when, i.e., where is the switchboad and what controls it?</p><p>Several models, including ACT-R, have attributed such parallel rule-based control of routing to the action selection circuitry <ref type="bibr" target="#b114">(Gurney et al., 2001;</ref><ref type="bibr" target="#b321">Terrence C. Stewart, Xuan Choo, 2010)</ref> of the basal ganglia (BG) <ref type="bibr" target="#b304">(Stocco et al., 2010;</ref><ref type="bibr" target="#b245">O'Reilly and Frank, 2006)</ref>, and its interaction with working memory buffers in the prefrontal cortex. In conventional models of thalamocortico-striatal loops, competing actions of the direct and indirect pathways through the basal ganglia can inhibit or disinhibit an area of motor cortex, thereby gating a motor action 27 . Models like <ref type="bibr" target="#b304">(Stocco et al., 2010;</ref><ref type="bibr" target="#b245">O'Reilly and Frank, 2006)</ref> propose further that the basal ganglia can gate not just the transfer of information from motor cortex to downstream actuators, but also the transfer of information between cortical areas. To do so, the basal ganglia would dis-inhibit a thalamic relay <ref type="bibr" target="#b289">(Sherman, 2005</ref><ref type="bibr" target="#b290">(Sherman, , 2007) )</ref> linking two cortical areas. Dopamine-related activity is thought to lead to temporal difference reinforcement learning of such gating policies in the basal ganglia <ref type="bibr" target="#b92">(Frank and Badre, 2012)</ref>. Beyond the basal ganglia, there are also other, separate pathways involved in action selection, e.g., in the prefrontal cortex <ref type="bibr" target="#b65">(Daw et al., 2006)</ref>. Thus, multiple systems including basal ganglia and cortex could control the gating of long-range information transfer between cortical areas, with the thalamus perhaps largely constituting the switchboard itself.</p><p>How is such routing put to use in a learning context? One possibility is that the basal ganglia acts to orchestrate the training of the cortex. The basal ganglia may exert tight control 28 over the cortex, helping to determine when and how it is trained. Indeed, because the basal ganglia pre-dates the cortex evolutionarily, it is possible that the cortex evolved as a flexible, trainable resource that could be harnessed by existing basal ganglia circuitry. All of the main regions and circuits of the basal ganglia are conserved from our common ancestor with the lamprey more than five hundred million years ago. The major part of the basal ganglia even seems to be conserved from our common ancestor with insects <ref type="bibr" target="#b306">(Strausfeld and Hirth, 2013)</ref>. Thus, in addition to its real-time action selection and routing functions, the basal ganglia may sculpt how the cortex learns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Structured state representations to enable efficient algorithms</head><p>Certain algorithmic problems benefit greatly from particular types of representation and transformation, such as a grid-like representation of space. In some cases, rather than just waiting for 27. Conventionally, models of the basal ganglia involve all or none gating of an action, but recent evidence suggests that the basal ganglia may also have continuous, analog outputs <ref type="bibr" target="#b360">(Yttri and Dudman, 2016)</ref>. 28. It has been suggested that the basic role of the BG is to provide tonic inhibition to other circuits <ref type="bibr" target="#b110">(Grillner et al., 2005)</ref>. Release of this inhibition can then activate a "discrete" action, such as a motor command. A core function of the BG is thus to choose, based on patterns detected in its input, which of a finite set of actions to initiate via such release of inhibition. In many models of the basal ganglias role in cognitive control, the targets of inhibition are thalamic relays <ref type="bibr" target="#b289">(Sherman, 2005)</ref>, which are set in a default "off" state by tonic inhibition from the basal ganglia. Upon disinhibition of a relay, information is transferred from one cortical location to another -a form of conditional "gating" of information transfer. For example, the BG might be able to selectively "clamp" particular groups of cortical neurons in a fixed state, while leaving others free to learn and adapt. It could thereby enforce complex training routines, perhaps similar to those used to force the emergence of disentangled representations in <ref type="bibr" target="#b171">(Kulkarni et al., 2015)</ref>. The idea that the basal ganglia can train the cortex is not new, and already appears to have considerable experimental and anatomical support <ref type="bibr" target="#b12">(Ashby et al., 2007</ref><ref type="bibr" target="#b13">(Ashby et al., , 2010;;</ref><ref type="bibr" target="#b252">Pasupathy and Miller, 2005)</ref>.</p><p>them to emerge via gradient descent optimization of appropriate cost functions, the brain may be pre-structured to facilitate their creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Continuous predictive control</head><p>We often have to plan and execute complicated sequences of actions on the fly, in response to a new situation. At the lowest level, that of motor control, our body and our immediate environment change all the time. As such, it is important for us to maintain knowledge about this environment in a continuous way. The deviations between our planned movements and those movements that we actually execute continuously provide information about the properties of the environment. Therefore it seems important to have a specialized system that takes all our motor errors and uses them to update a dynamical model of our body and our immediate environment that can predict the delayed sensory results of our motor actions <ref type="bibr" target="#b214">(McKinstry et al., 2006</ref>). It appears that the cerebellum is such a structure, and lesions to it abolish our way of dealing successfully with a changing body. Incidentally, the cerebellum has more connections than the rest of the brain taken together, apparently in a largely feedforward architecture, and the tiny cerebellar granule cells, which may form a randomized high-dimensional input representation <ref type="bibr" target="#b209">(Marr, 1969;</ref><ref type="bibr" target="#b148">Jacobson and Friedrich, 2013)</ref>, outnumber all other neurons. The brain clearly needs a way of continuously correcting movements to minimize errors.</p><p>Newer research shows that the cerebellum is involved in a broad range of cognitive problems <ref type="bibr" target="#b228">(Moberget et al., 2014)</ref> as well, potentially because they share computational problems with motor control. For example, when subjects estimate time intervals, which are naturally important for movement, it appears that the brain uses the cerebellum even if no movements are involved <ref type="bibr" target="#b104">(Gooch et al., 2010)</ref>. Even individual cerebellar Purkinjie cells may learn to generate precise timings of their outputs <ref type="bibr" target="#b156">(Johansson et al., 2014)</ref>. The brain also appears to use inverse models to rapidly predict motor activity that would give rise to a given sensory target <ref type="bibr" target="#b100">(Giret et al., 2014;</ref><ref type="bibr" target="#b117">Hanuschkin et al., 2013)</ref>. Such mechanisms could be put to use far beyond motor control, in bootstrapping the training of a larger architecture by exploiting continuously changing error signals to update a real-time model of the system state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Hierarchical control</head><p>Importantly, many of the control problems we appear to be solving are hierarchical. We have a spinal cord, which deals with the fast signals coming from our muscles and proprioception. Within neuroscience, it is generally assumed that this system deals with fast feedback loops and that this behavior is learned to optimize its own cost function. The nature of cost functions in motor control is still under debate. In particular, the timescale over which cost functions operate remains unclear: motor optimization may occur via real-time responses to a cost function that is computed and optimized online, or via policy choices that change over time more slowly in response to the cost function <ref type="bibr" target="#b164">(Körding, 2007)</ref>. Nevertheless, the effect is that central processing in the brain has an effectively simplified physical system to control, e.g., one that is far more linear. So the spinal cord itself already suggests the existence of two levels of a hierarchy, each trained using different cost functions.</p><p>However, within the computational motor control literature (see e.g., <ref type="bibr" target="#b71">(DeWolf and Eliasmith, 2011)</ref>), this idea can be pushed far further, e.g., with a hierarchy including spinal cord, M1, PMd, frontal, prefrontal areas. A low level may deal with muscles, the next level may deal with getting our limbs to places or moving objects, a next layer may deal with solving simple local problems (e.g., navigating across a room) while the highest levels may deal with us planning our path through life. This factorization of the problem comes with multiple aspects: First, each level can be solved with its own cost functions, and second, every layer has a characteristic timescale. Some levels, e.g., the spinal cord, must run at a high speed. Other levels, e.g., high-level planning, only need to be touched much more rarely. Converting the computationally hard optimal control problem into a hierarchical approximation promises to make it dramatically easier.</p><p>Does the brain solve control problems hierarchically? There is evidence that the brain uses such a strategy <ref type="bibr" target="#b37">(Botvinick and Weinstein, 2014;</ref><ref type="bibr" target="#b38">Botvinick et al., 2009)</ref>, beside neural network demonstrations <ref type="bibr" target="#b343">(Wayne and Abbott, 2014)</ref>. The brain may use specialized structures at each hierarchical level to ensure that each operates efficiently given the nature of its problem space and available training signals. At higher levels, these systems may use an abstract syntax for combining sequences of actions in pursuit of goals <ref type="bibr" target="#b4">(Allen et al., 2010)</ref>. Subroutines in such processes could be derived by a process of chunking sequences of actions into single actions <ref type="bibr" target="#b37">(Botvinick and Weinstein, 2014;</ref><ref type="bibr" target="#b109">Graybiel, 1998)</ref>. Some brain areas like Broca's area, known for its involvement in language, also appear to be specifically involved in processing the hierarchical structure of behavior, as such, as opposed to its detailed temporal structure <ref type="bibr" target="#b162">(Koechlin and Jubault, 2006)</ref>.</p><p>At the highest level of the decision making and control hierarchy, human reward systems reflect changing goals and subgoals, and we are only beginning to understand how goals are actually coded in the brain, how we switch between goals, and how the cost functions used in learning depend on goal state <ref type="bibr" target="#b247">(O'Reilly et al., 2014b;</ref><ref type="bibr" target="#b47">Buschman and Miller, 2014;</ref><ref type="bibr" target="#b256">Pezzulo et al., 2014)</ref>. Goal hierarchies are beginning to be incorporated into deep learning <ref type="bibr">(Kulkarni et al., 2016)</ref>.</p><p>Given this hierarchical structure, the optimization algorithms can be fine-tuned. For the low levels, there is sheer unlimited training data. For the high levels, a simulation of the world may be simple, with a tractable number of high-level actions to choose from. Finally, each area needs to give reinforcement to other areas, e.g., high levels need to punish lower levels for making planning complicated. Thus this type of architecture can simplify the learning of control problems.</p><p>Progress is being made in both neuroscience and machine learning on finding potential mechanisms for this type of hierarchical planning and goal-seeking.</p><p>This is beginning to reveal mechanisms for chunking goals and actions and for searching and pruning decision trees <ref type="bibr" target="#b18">(Balaguer et al., 2016;</ref><ref type="bibr" target="#b169">Krishnamurthy et al., 2016;</ref><ref type="bibr" target="#b317">Tamar et al., 2016;</ref><ref type="bibr" target="#b246">O'Reilly et al., 2014a;</ref><ref type="bibr" target="#b145">Huys et al., 2015)</ref>. The study of model-based hierarchical reinforcement learning and prospective optimiza-tion (Sejnowski and Poizner, 2014), which concerns the planning and evaluation of nested sequences of actions, implicates a network coupling the dorsolateral prefontral and orbitofrontal cortex, and the ventral and dorsolateral striatum <ref type="bibr" target="#b38">(Botvinick et al., 2009)</ref>. Hierarchical RL relies on a hierarchical representation of state and action spaces, and it has been suggested that error-driven learning of an optimal such representation in the hippocampus<ref type="foot" target="#foot_2">29</ref> gives rise to place and grid cell properties (Stachenfeld, 2014), with goal representations themselves emerging in the amygdala, prefrontal cortex and other areas <ref type="bibr" target="#b246">(O'Reilly et al., 2014a)</ref>.</p><p>The question of how control problems can be successfully divided into component problems remains one of the central questions in neuroscience and machine learning, and the cost functions involved in learning to create such decompositions are still unknown. These considerations may begin to make plausible, however, how the brain could not only achieve its remarkable feats of motor learning -such as generating complex "innate" motor programs, like walking in the newborn gazelle almost immediately after birth -but also the kind of planning that allows a human to prepare a meal or travel from London to Chicago.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Spatial planning</head><p>Spatial planning requires solving shortest-path problems subject to constraints. If we want to get from one location to another, there are an arbitrarily large number of simple paths that could be taken. Most naive implementations of such shortest paths problems are grossly inefficient. It appears that, in animals, the hippocampus aids -at least in part through place cell and grid cell systems -in efficient learning about new environments and in targeted navigation in such environments <ref type="bibr" target="#b43">(Brown et al., 2016)</ref>. In some simple models, targeted navigation in the hippocampus is achieved via the dynamics of "bump attractors" or propagating waves in a place cell network with Hebbian plasticity and adaptation <ref type="bibr" target="#b49">(Buzsáki and Moser, 2013;</ref><ref type="bibr" target="#b143">Hopfield, 2009;</ref><ref type="bibr" target="#b260">Ponulak and Hopfield, 2013)</ref>, which allows the network to effectively chart out a path in the space of place cell representations.</p><p>Higher-level cognitive tasks such as prospective planning appear to share computational sub-problems with pathfinding <ref type="bibr" target="#b120">(Hassabis and Maguire, 2009)</ref> 30 . Interaction between hippocampus and prefrontal cortex could perhaps support a more abstract notion of "navigation" in a space of goals and sub-goals. Having specialized structures for path-finding simplifies these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Variable binding</head><p>Language and reasoning appear to present a problem for neural networks <ref type="bibr" target="#b220">(Minsky, 1991;</ref><ref type="bibr" target="#b115">Hadley, 2009;</ref><ref type="bibr">Marcus, 2001)</ref>: we seem to be able to apply common grammatical rules to sentences regardless of the content of those sentences, and regardless of whether we have ever seen even remotely similar sentences in the training data. While this is achieved automatically in a computer with fixed registers, location addressable memories, and hard-coded operations, how it could be achieved in a biological brain, or emerge from an optimization algorithm, has been under debate for decades.</p><p>As the putative key capability underlying such operations, variable binding has been defined as "the transitory or permanent tying together of two bits of information: a variable (such as an X or Y in algebra, or a placeholder like subject or verb in a sentence) and an arbitrary instantiation of that variable (say, a single number, symbol, vector, or word)" <ref type="bibr">(Marcus et al., 2014a,b)</ref>.</p><p>A number of potential biologically plausible binding mechanisms <ref type="bibr">(Hayworth, 2012;</ref><ref type="bibr" target="#b168">Kriete et al., 2013;</ref><ref type="bibr" target="#b79">Eliasmith et al., 2012;</ref><ref type="bibr" target="#b101">Goertzel, 2014)</ref> are reviewed in <ref type="bibr">(Marcus et al., 2014a,b)</ref>. Some, such as vector symbolic architectures 31 , which were proposed in cognitive science <ref type="bibr" target="#b76">(Eliasmith, 2013;</ref><ref type="bibr" target="#b259">Plate, 1995;</ref><ref type="bibr" target="#b303">Stewart and Eliasmith, 2009)</ref>, are also being considered in the context of efficiently-trainable ar-tificial neural networks <ref type="bibr" target="#b64">(Danihelka et al., 2016)</ref> in effect, these systems learn how to use variable binding.</p><p>Variable binding could potentially emerge from simpler memory systems.</p><p>For example, the Scrub-Jay can remember the place and time of last visit for hundreds of different locations, e.g., to determine whether highquality food is currently buried at any given location (Clayton and <ref type="bibr" target="#b57">Dickinson, 1998)</ref>. It is conceivable that such spatially-grounded memory systems enabled a more general binding mechanism to emerge during evolution, perhaps through integration with routing systems or other content-addressable or working memory systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Hierarchical syntax</head><p>Fixed, static hierarchies (e.g., the hierarchical organization of cortical areas (Felleman and <ref type="bibr" target="#b82">Van Essen, 1991)</ref>) only take us so far: to deal with long chains of arbitrary nested references, we need dynamic hierarchies that can implement recursion on the fly. Human language syntax has a hierarchical structure, which Berwick et al described as "composition of smaller forms like words and phrases into larger ones" <ref type="bibr" target="#b31">(Berwick et al., 2012;</ref><ref type="bibr" target="#b224">Miyagawa et al., 2013)</ref>. Specific fronto-temporal networks may be involved in representing and generating such hierarchies <ref type="bibr" target="#b68">(Dehaene et al., 2015)</ref> 32 .</p><p>Little is known about the underlying circuit mechanisms for such dynamic hierarchies, but it is clear that specific affordances for representing such hierarchies in an efficient way would be beneficial. This may be closely connected with the issue of variable binding, and it is possible that operations similar to pointers could be useful in this context, in both the brain and artificial neural networks <ref type="bibr" target="#b168">(Kriete et al., 2013;</ref><ref type="bibr">Kurach et al., 2015)</ref>. Augmenting neural networks with a dif-30. Other spatial problems such as mental rotation may require learning architectures specialized for geometric coordinate transformations <ref type="bibr">(Hinton et al., 2011;</ref><ref type="bibr" target="#b149">Jaderberg et al., 2015a)</ref> or binding mechanisms that support structural, compositional, parametric descriptions of a scene <ref type="bibr" target="#b128">(Hayworth et al., 2011)</ref>. 31. There is some direct fMRI evidence for anatomically separate registers representing the contents of different sentence roles in the human brain <ref type="bibr" target="#b93">(Frankland and Greene, 2015)</ref>, which is suggestive of a possible anatomical binding mechanism, but also consistent with other mechanisms like vector symbolic architectures. More generally, the substrates of symbolic processing in the brain may bear an intimate connection with the representation of objects in working memory in the prefrontal cortex, and specifically with the question of how the PFC represents multiple objects in working memory simultaneously. This question is undergoing extensive study in primates <ref type="bibr">(Warden and</ref><ref type="bibr">Miller, 2007, 2010;</ref><ref type="bibr" target="#b293">Siegel et al., 2009;</ref><ref type="bibr" target="#b266">Rigotti et al., 2013)</ref>. 32. There is controversy around claims that recursive syntax is also present in songbirds <ref type="bibr" target="#b333">(Van Heijningen et al., 2009)</ref>.</p><p>ferentiable analog of a push-down stack is another such affordance being pursued in machine learning <ref type="bibr" target="#b157">(Joulin and Mikolov, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Mental programs and imagination</head><p>Humans excel at stitching together sub-actions to form larger actions <ref type="bibr">(Sejnowski and Poizner, 2014;</ref><ref type="bibr" target="#b2">Acuna et al., 2014;</ref><ref type="bibr" target="#b336">Verwey, 1996)</ref>. Structured, serial, hierarchical probabilistic programs have recently been shown to model aspects of human conceptual representation and compositional learning <ref type="bibr">(Lake et al., 2015)</ref>. In particular, sequential programs were found to enable one-shot learning of new geometric/visual concepts <ref type="bibr">(Lake et al., 2015)</ref>, a key capability that deep learning networks for object recognition seem to fundamentally lack. Generative programs have also been proposed in the context of scene understanding <ref type="bibr" target="#b24">(Battaglia et al., 2013)</ref>. The ability to deal with problems in terms of subproblems is central both in human thought and in many successful algorithms.</p><p>One possibility is that the hippocampus supports the construction and learning of sequential programs. The hippocampus appears to explore, in simulation, possible future trajectories to a goal, even those involving previously unvisited locations <ref type="bibr">( Ólafsdóttir et al., 2015)</ref>. Hippocampalprefrontal interaction has been suggested to allow rapid, subconscious evaluation of potential action sequences during decision-making, with the hippocampus in effect simulating the expected outcomes of potential actions that are generated and evaluated in the prefrontal <ref type="bibr" target="#b337">(Wang et al., 2015;</ref><ref type="bibr" target="#b232">Mushiake et al., 2006)</ref>.</p><p>The role of the hippocampus in imagination, concept generation <ref type="bibr" target="#b173">(Kumaran et al., 2009)</ref>, scene construction <ref type="bibr" target="#b120">(Hassabis and Maguire, 2007)</ref>, mental exploration and goal-directed path planning <ref type="bibr" target="#b236">( Ólafsdóttir et al., 2015;</ref><ref type="bibr" target="#b43">Brown et al., 2016;</ref><ref type="bibr" target="#b143">Hopfield, 2009)</ref> suggests that it could help to create generative models to underpin more complex inference such as program induction <ref type="bibr">(Lake et al., 2015)</ref> or common-sense world simulation <ref type="bibr" target="#b24">(Battaglia et al., 2013)</ref>  33 .</p><p>Another related possibility is that the cortex itself intrinsically supports the construction and learning of sequential programs <ref type="bibr" target="#b17">(Bach and Herger, 2015)</ref>. Recurrent neural networks have been used for image generation through a sequential, attention-based process <ref type="bibr" target="#b110">(Gregor et al., 2015)</ref>, although their correspondence with the brain is unclear 34 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Other specialized structures</head><p>Importantly, there are many other specialized structures known in neuroscience, which arguably receive less attention than they deserve, even for those interested in higher cognition. In the above, in addition to the hippocampus, basal ganglia and cortex, we emphasized the key roles of the thalamus in routing, of the cerebellum as a rapidly trainable control and modeling system, of the amygdala and other areas as a potential source of utility functions, of the retina or early visual areas as a means to generate detectors for motion and other features to bootstrap more complex visual learning, and of the frontal eye fields and other areas as a possible source of attention control. We ignored other structures entirely, whose functions are only beginning to be uncovered, such as the claustrum <ref type="bibr" target="#b61">(Crick and Koch, 2005)</ref>, which has been speculated to be important for rapidly binding together information from many modalities. Our overall understanding of the functional decomposition of brain circuitry still seems very preliminary.</p><p>33. One common idea is that the hippocampus plays a key role in certain processes where learning must occur quickly, whereas the cortex learns more slowly <ref type="bibr" target="#b183">(Leibo et al., 2015a;</ref><ref type="bibr" target="#b128">Herd et al., 2013)</ref>. For example, a sequential, programmatic process, mediated jointly by the basal ganglia, hippocampus and prefrontal cortex might allow one-shot learning of a new concept, as in the sequential computations underlying Bayesian Program <ref type="bibr">Learning (Lake et al., 2015)</ref>. 34. The above mechanisms are spontaneous and subconscious. In conscious thought, too, the brain can clearly visit the multiple layers of a program one after the other. We make high-level plans that we fill with lower-level plans.</p><p>Humans also have memory for their own thought processes. We have some ability to put "on hold" our current state of mind, start a new train of thought, and then come back to our original thought. We also are able to ask, introspectively, whether we have had a given thought before. The neural basis of these processes is unclear, although one may speculate that the hippocampus is involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Relationships with other cognitive frameworks involving specialized systems</head><p>A recent analysis <ref type="bibr" target="#b174">(Lake et al., 2016)</ref> suggested directions by which to modify and enhance existing neural-net-based machine learning towards more powerful and human-like cognitive capabilities, particularly by introducing new structures and systems which go beyond data-driven optimization. This analysis emphasized that systems should construct generative models of the world that incorporate compositionality (discrete construction from re-usable parts), inductive biases reflecting causality, intuitive physics and intuitive psychology, and the capacity for probabilistic inference over discrete structured models (e.g., structured as graphs, trees, or programs) <ref type="bibr" target="#b322">(Tervo et al., 2016)</ref> to harness abstractions and enable transfer learning.</p><p>We view these ideas as consistent with and complementary to the framework of cost functions, optimization and specialized systems discussed here. One might seek to understand how optimization and specialized systems could be used to implement some of the mechanisms proposed in <ref type="bibr" target="#b174">(Lake et al., 2016)</ref> inside neural networks. Lake et al. ( <ref type="formula">2016</ref>) emphasize how incorporating additional structure into trainable neural networks can potentially give rise to systems that use compositional, causal and intuitive inductive biases and that "learn to learn" using structured models and shared data structures.</p><p>For example, sub-dividing networks into units that can be modularly and dynamically combined, where representations can be copied and routed, may present a path towards improved compositionality and transfer learning <ref type="bibr" target="#b226">(Andreas et al., 2015)</ref>. The control flow for recombining pre-existing modules and representations could be learned via reinforcement learning <ref type="bibr" target="#b334">(Andreas et al., 2016)</ref>. How to implement the broad set of mechanisms discussed in <ref type="bibr" target="#b174">(Lake et al., 2016)</ref> is a key computational problem, and it remains open at which levels (e.g., cost functions and training procedures vs. specialized computational structures vs. underlying neural primitives) architectural innovations will need to be introduced to capture these phenomena.</p><p>Primitives that are more complex than those used in conventional neural networks -for instance, primitives that act as state machines with complex message passing <ref type="bibr" target="#b17">(Bach and Herger, 2015)</ref> or networks that intrinsically implement Bayesian inference <ref type="bibr" target="#b97">(George and Hawkins, 2009)</ref> -could potentially be useful, and it is plausible that some of these may be found in the brain. Recent findings on the power of generic optimization also do not rule out the idea that the brain may explicitly generate and use particular types of structured representations to constrain its inferences; indeed, the specialized brain systems discussed here might provide a means to enforce such constraints. It might be possible to further map the concepts of Lake et al. ( <ref type="formula">2016</ref>) onto neuroscience via an infrastructure of interacting cost functions and specialized brain systems under rich genetic control, coupled to a powerful and generic neurally implemented capacity for optimization. For example, it was recently shown that complex probabilistic population coding and inference can arise automatically from backpropagation-based training of simple neural networks (Orhan and <ref type="bibr" target="#b249">Ma, 2016)</ref>, without needing to be built in by hand. The nature of the underlying primitives in the brain, on top of which learning can operate, is a key question for neuroscience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Machine learning inspired neuroscience</head><p>Hypotheses are primarily useful if they lead to concrete, experimentally testable predictions. As such, we now want to go through the hypotheses and see to which level they can be directly tested, as well as refined, through neuroscience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hypothesis 1-Existence of cost functions</head><p>There are multiple general strategies for addressing whether and how the brain optimizes cost functions. A first strategy is based on observing the endpoint of learning. If the brain uses a cost function, and we can guess its identity, then the final state of the brain should be close to optimal for the cost function. If we know the statistics of natural environments, and know the cost function, we can compare receptive fields that are optimized in a simulation with the measured ones. This strategy is only beginning to be used at the moment because it has been difficult to measure the receptive fields or other representational properties across a large population of neurons, but this situation is beginning to improve technologically with the emergence of large-scale recording methods.</p><p>A second strategy could directly quantify how well a cost function describes learning. If the dynamics of learning minimize a cost function then the underlying vector field should have a strong gradient descent type component and a weak rotational component. If we could somehow continuously monitor the synaptic strengths, while externally manipulating them, then we could, in principle, measure the vector field in the space of synaptic weights, and calculate its divergence as well as its rotation. For at least the subset of synapses that are being trained via some approximation to gradient descent, the divergence component should be strong relative to the rotational component. This strategy has not been developed yet due to experimental difficulties with monitoring large numbers of synaptic weights <ref type="foot" target="#foot_3">35</ref> .</p><p>A third strategy is based on perturbations: cost function based learning should undo the effects of perturbations which disrupt optimality, i.e., the system should return to local minima after a perturbation, and indeed perhaps to the same local minimum after a sufficiently small perturbation. If we change synaptic connections, e.g., in the context of a brain machine interface, we should be able to produce a reorganization that can be predicted based on a guess of the relevant cost function. This strategy is starting to be feasible in motor areas.</p><p>Lastly, if we knew structurally which cell types and connections mediated the delivery of error signals vs. input data or other types of connections, then we could stimulate specific connections so as to impose a user-defined cost function. In effect, we would use the brain's own networks as a trainable deep learning substrate, and then study how the network responds to training. Brain machine interfaces can be used to set up specific local learning problems, in which the brain is asked to create certain user-specified rep-resentations, and the dynamics of this process can be monitored <ref type="bibr" target="#b277">(Sadtler et al., 2014)</ref>. In order to do this properly, we must first understand more about the system is wired to deliver cost signals. Much of the structure that would be found in connectomic circuit maps, for example, would not just be relevant for short-timescale computing, but also for creating the infrastructure that supports cost functions and their optimization.</p><p>Many of the learning mechanisms that we have discussed in this paper make specific predictions about connectivity or dynamics. For example, the "feedback alignment" approach to biological backpropagation suggests that cortical feedback connections should, at some level of neuronal grouping, be largely sign-concordant with the corresponding feedforward connections, although not necessarily of concordant weight <ref type="bibr" target="#b189">(Liao et al., 2015)</ref>, and feedback alignment also makes predictions for synaptic normalization mechanisms <ref type="bibr" target="#b189">(Liao et al., 2015)</ref>. The Kickback model for biologically plausible backpropagation has a specific role for NMDA receptors <ref type="bibr" target="#b21">(Balduzzi et al., 2014)</ref>.</p><p>Some models that incorporate dendritic coincidence detection for learning temporal sequences predict that a given axon should make only a small number of synapses on a given dendritic segment <ref type="bibr" target="#b124">(Hawkins and Ahmad, 2015)</ref>. Models that involve STDP learning will make predictions about the dynamics of changing firing rates <ref type="bibr" target="#b129">(Hinton, 2007</ref><ref type="bibr" target="#b130">(Hinton, , 2016;;</ref><ref type="bibr" target="#b29">Bengio et al., 2015a;</ref><ref type="bibr" target="#b27">Bengio and Fischer, 2015;</ref><ref type="bibr" target="#b30">Bengio et al., 2015b)</ref>, as well as about the particular network structures, such as those based on autoencoders or recirculation, in which STDP can give rise to a form of backpropagation.</p><p>It is critical to establish the unit of optimization. We want to know the scale of the modules that are trainable by some approximation of gradient descent optimization. How large are the networks which share a given error signal or cost function? On what scales can appropriate training signals be delivered? It could be that the whole brain is optimized end-to-end, in principle. In this case we would expect to find connections that carry training signals from each layer to the preceding ones. On successively smaller scales, optimization could be within a brain area, a microcircuit 36 , or an individual neuron <ref type="bibr" target="#b164">(Körding and</ref><ref type="bibr" target="#b166">König, 2000, 2001;</ref><ref type="bibr">Mel, 1992;</ref><ref type="bibr" target="#b124">Hawkins and Ahmad, 2015)</ref>. Importantly, optimization may co-exist across these scales. There may be some slow optimization end-to-end, with stronger optimization within a local area and very efficient algorithms within each cell. Careful experiments should be able to identify the scale of optimization, e.g., by quantifying the extent of learning induced by a local perturbation.</p><p>The tightness of the structure-function relationship is the hallmark of molecular and to some extent cellular biology, but in large connectionist learning systems, this relationship can become difficult to extract: the same initial network can be driven to compute many different functions by subjecting it to different training 3738 . It can be hard to understand the way a neural network solves its problems.</p><p>How could one tell the difference, then, between a gradient-descent trained network vs. untrained or random networks vs. a network that has been trained against a different kind of task? One possibility would be to train artificial neural networks against various candidate cost functions, study the resulting neural tuning properties <ref type="bibr" target="#b326">(Todorov, 2002)</ref>, and compare them with those found in the circuit of interest <ref type="bibr" target="#b365">(Zipser and Andersen, 1988)</ref>. This has already been done to aid the interpretation of the neural dynamics underlying decision making in the PFC <ref type="bibr" target="#b310">(Sussillo, 2014)</ref>, working memory in the posterior parietal cortex <ref type="bibr" target="#b262">(Rajan et al., 2016)</ref> and object representation in the visual system <ref type="bibr">(Yamins and DiCarlo, 2016b,a)</ref>. Some have gone on to suggest a direct correspondence between cortical circuits and optimized, appropriately regularized <ref type="bibr" target="#b312">(Sussillo et al., 2015)</ref>, recurrent neural networks <ref type="bibr" target="#b188">(Liao and Poggio, 2016)</ref>. In any case, effective analytical methods to reverse engineer complex machine learning systems <ref type="bibr">(Jonas and Kording, 2016)</ref>, and methods to reverse engineer biological brains, may have some commonalities.</p><p>Does this emphasis on function optimization and trainable substrates mean that we should give up on reverse engineering the brain based on detailed measurements and models of its specific connectivity and dynamics? On the contrary: we should use large-scale brain maps to try to better understand a) how the brain implements optimization, b) where the training signals come from and what cost functions they embody, and c) what structures exist, at different levels of organization, to constrain this optimization to efficiently find solutions to specific kinds of problems. The answers may be influenced by diverse local properties of neurons and networks, such as homeostatic rules of neural structure, gene expression and function <ref type="bibr" target="#b206">(Marder and Goaillard, 2006)</ref>, the diversity of synapse types, cell-type-specific connectivity <ref type="bibr" target="#b155">(Jiang et al., 2015)</ref>, patterns of interlaminar projection, distributions of inhibitory neuron types, dendritic targeting and local dendritic physiology and plasticity <ref type="bibr" target="#b208">(Markram et al., 2015;</ref><ref type="bibr" target="#b35">Bloss et al., 2016;</ref><ref type="bibr" target="#b279">Sandler et al., 2016)</ref> or local glial networks <ref type="bibr" target="#b254">(Perea et al., 2009)</ref>. They may also be influenced by the integrated machine learning in a network of spiking neurons using conventional plasticity rules <ref type="bibr" target="#b298">(Sountsov and Miller, 2015;</ref><ref type="bibr" target="#b273">Roudi and Taylor, 2015)</ref>. As a simpler example, the classical problem of how neurons with only one output axon could communicate both activation and error derivatives for backpropagation ceases to be a problem if the unit of optimization is not a single neuron. Similar considerations hold for the issue of weight symmetry, or approximate sign-concordance in the case of feedback alignment <ref type="bibr" target="#b189">(Liao et al., 2015)</ref>. 37. Within this framework, networks that adhere to the basic statistics of neural connectivity, electrophysiology and morphology, such as the initial cortical column models from the Blue Brain Project <ref type="bibr" target="#b208">(Markram et al., 2015)</ref>, would recapitulate some properties of the cortex, but -just like untrained neural networks -would not spontaneously generate complex functional computation without being subjected to a multi-stage training process, naturalistic sensory data, signals arising from other brain areas and action-driven reinforcement signals. 38. Not only in applied machine learning, but also in today's most advanced neuro-cognitive models such as SPAUN <ref type="bibr" target="#b76">(Eliasmith, 2013;</ref><ref type="bibr" target="#b79">Eliasmith et al., 2012)</ref>, the detailed local circuit connectivity is obtained through an optimization process of some kind to achieve a particular functionality. In the case of modern machine learning, training is often done via end-to-end backpropagation through an architecture that is only structured at the level of higher-level "blocks" of units, whereas in SPAUN each block is optimized <ref type="bibr" target="#b77">(Eliasmith and Anderson, 2004)</ref> separately according to a procedure that allows the blocks to subsequently be stitched together in a coherent way. Technically, the Neural Engineering Framework <ref type="bibr" target="#b77">(Eliasmith and Anderson, 2004)</ref> used in SPAUN uses singular value decomposition, rather than gradient descent, to compute the connections weights as optimal linear decoders. This is possible because of a nonlinear mapping into a high-dimensional space, in which approximating any desired function can be done via a hyperplane regression <ref type="bibr" target="#b319">(Tapson and van Schaik, 2013)</ref>.</p><p>nature of higher-level brain systems, including mechanisms for developmental bootstrapping <ref type="bibr" target="#b329">(Ullman et al., 2012)</ref>, information routing <ref type="bibr" target="#b114">(Gurney et al., 2001;</ref><ref type="bibr">Stocco et al., 2010), attention (Buschman and</ref><ref type="bibr">Miller, 2010)</ref> and hierarchical decision making <ref type="bibr" target="#b179">(Lee et al., 2015)</ref>. Mapping these systems in detail is of paramount importance to understanding how the brain works, down to the nanoscale dendritic organization of ion channels and up to the real-time global coordination of cortex, striatum and hippocampus, all of which are computationally relevant in the framework we have explicated here. We thus expect that large-scale, multi-resolution brain maps would be useful in testing these framework-level ideas, in inspiring their refinements, and in using them to guide more detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hypothesis 2-Biological fine-structure of cost functions</head><p>Clearly, we can map differences in structure, dynamics and representation across brain areas. When we find such differences, the question remains as to whether we can interpret these as resulting from differences in the internallygenerated cost functions, as opposed to differences in the input data, or from differences that reflect other constraints unrelated to cost functions. If we can directly measure aspects of the cost function in different areas, then we can also compare them across areas. For example, methods from inverse reinforcement learning 39 might allow backing out the cost function from observed plasticity <ref type="bibr">(Ng and Russell, 2000)</ref>. Moreover, as we begin to understand the "neural correlates" of particular cost functions -perhaps encoded in particular synaptic or neuromodulatory learning rules, genetically-guided local wiring patterns, or patterns of interaction between brain areas -we can also begin to understand when differences in observed neural circuit architecture reflect differences in cost functions.</p><p>We expect that, for each distinct learning rule or cost function, there may be specific molecularly identifiable types of cells and/or synapses. Moreover, for each specialized system there may be specific molecularly identifiable developmental programs that tune it or otherwise set its parameters. This would make sense if evolution has needed to tune the parameters of one cost function without impacting others.</p><p>How many different types of internal training signals does the brain generate? When thinking about error signals, we are not just talking about dopamine and serotonin, or other classical reward-related pathways. The error signals that may be used to train specific sub-networks in the brain, via some approximation of gradient descent or otherwise, are not necessarily equivalent to reward signals. It is important to distinguish between cost functions that may be used to drive optimization of specific sub-circuits in the brain, and what are referred to as "value functions" or "utility functions", i.e., functions that predict the agents aggregate future reward. In both cases, similar reinforcement learning mechanisms may be used, but the interpretation of the cost functions is different. We have not emphasized global utility functions for the animal here, since they are extensively studied elsewhere (e.g., <ref type="bibr" target="#b246">(O'Reilly et al., 2014a;</ref><ref type="bibr" target="#b16">Bach, 2015)</ref>), and since we argue that, though important, they are only a part of the picture, i.e., that the brain is not solely an end-to-end reinforcement trained system.</p><p>Progress in brain mapping could soon allow us to classify the types of reward signals in the brain, follow the detailed anatomy and connectivity of reward pathways throughout the brain, and map in detail how reward pathways are integrated into striatal, cortical, hippocampal and cerebellar microcircuits. This program is beginning to be carried out in the fly brain, in which twenty specific types of dopamine neuron project to distinct anatomical compartments of the mushroom body to train distinct odor classifiers operating on a set of high-dimensional odor representations <ref type="bibr">(Aso et al., 2014a,b;</ref><ref type="bibr" target="#b52">Caron et al., 2013;</ref><ref type="bibr" target="#b59">Cohn et al., 2015)</ref>. It is known that, even within the same system, such as the fly olfactory pathway, some neuronal wiring is highly specific and molecularly programmed <ref type="bibr" target="#b140">(Hong and Luo, 2014;</ref><ref type="bibr"></ref> 39. There is a rich tradition of trying to estimate the cost function used by human beings <ref type="bibr">(Ng and Russell, 2000)</ref>.</p><p>The idea is that we observe (by stipulation) behavior that is optimal for the human's cost function. We can then search for the cost function that makes the observed behavior most probable and simultaneously makes the behaviors that could have been observed, but were not, least probable. Extensions of such approaches could perhaps be used to ask which cost functions the brain is optimizing. <ref type="bibr" target="#b123">Hattori et al., 2007)</ref>, while other wiring is effectively random <ref type="bibr" target="#b52">(Caron et al., 2013)</ref>, and yet other wiring is learned <ref type="bibr" target="#b14">(Aso et al., 2014a)</ref>. The interplay between such design principles could give rise to many forms of "division of labor" between genetics and learning. Likewise, it is believed that birdsong learning is driven by reinforcement learning using a specialized cost function that relies on comparison with a memorized version of a tutor's song <ref type="bibr" target="#b87">(Fiete et al., 2007)</ref>, and also that it involves specialized structures for controlling song variability during learning <ref type="bibr" target="#b10">(Aronov et al., 2011)</ref>. These detailed pathways underlying the construction of cost functions for vocal learning are beginning to be mapped <ref type="bibr" target="#b200">(Mandelblat-Cerf et al., 2014)</ref>. Starting with simple systems, it should become possible to map the reward pathways and how they evolved and diversified, which would be a step on the way to understanding how the system learns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hypothesis 3-Embedding within a pre-structured architecture</head><p>If different brain structures are performing distinct types of computations with a shared goal, then optimization of a joint cost function will take place with different dynamics in each area. If we focus on a higher level task, e.g., maximizing the probability of correctly detecting something, then we should find that basic feature detection circuits should learn when the features were insufficient for detection, that attentional routing structures should learn when a different allocation of attention would have improved detection and that memory structures should learn when items that matter for detection were not remembered. If we assume that multiple structures are participating in a joint computation, which optimizes an overall cost function (but see Hypothesis 2), then an understanding of the computational function of each area leads to a prediction of the measurable plasticity rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Neuroscience inspired machine learning</head><p>Machine learning may be equally transformed by neuroscience. Within the brain, a myriad of subsystems and layers work together to produce an agent that exhibits general intelligence. The brain is able to show intelligent behavior across a broad range of problems using only relatively small amounts of data. As such, progress at understanding the brain promises to improve machine learning. In this section, we review our three hypotheses about the brain and discuss how their elaboration might contribute to more powerful machine learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Hypothesis 1-Existence of cost functions</head><p>A good practitioner of machine learning should have a broad range of optimization methods at their disposal as different problems ask for different approaches. The brain, we have argued, is an implicit machine learning mechanism which has been evolved over millions of years. Consequently, we should expect the brain to be able to optimize cost functions efficiently, across many domains and kinds of data. Indeed, across different animal phyla, we even see convergent evolution of certain brain structures <ref type="bibr" target="#b291">(Shimizu and Karten, 2013;</ref><ref type="bibr" target="#b113">Güntürkün and Bugnyar, 2016)</ref>, e.g., the bird brain has no cortex yet has developed homologous structures which -as the linguistic feats of the African Grey Parrot demonstrate -can give rise to quite complex intelligence. It seems reasonable to hope to learn how to do truly generalpurpose optimization by looking at the brain. Indeed, there are multiple kinds of optimization that we may expect to discover by looking at the brain. At the hardware level, the brain clearly manages to optimize functions efficiently despite having slow hardware subject to molecular fluctuations, suggesting directions for improving the hardware of machine learning to be more energy efficient. At the level of learning rules, the brain solves an optimization problem in a highly nonlinear, non-differentiable, temporally stochastic, spiking system with massive numbers of feedback connections, a problem that we arguably still do not know how to efficiently solve for neural networks. At the architectural level, the brain can optimize certain kinds of functions based on very few stimulus presentations, operates over diverse timescales, and clearly uses advanced forms of active learning to infer causal structure in the world.</p><p>While we have discussed a range of theories <ref type="bibr" target="#b129">(Hinton, 2007</ref><ref type="bibr" target="#b130">(Hinton, , 2016;;</ref><ref type="bibr" target="#b29">Bengio et al., 2015a;</ref><ref type="bibr" target="#b21">Balduzzi et al., 2014;</ref><ref type="bibr" target="#b269">Roelfsema et al., 2010;</ref><ref type="bibr" target="#b243">O'Reilly, 1996;</ref><ref type="bibr" target="#b246">O'Reilly et al., 2014a;</ref><ref type="bibr" target="#b166">Körding and König, 2001;</ref><ref type="bibr" target="#b190">Lillicrap et al., 2014)</ref> for how the brain can carry out optimization, these theories are still preliminary. Thus, the first step is to understand whether the brain indeed performs multi-layer credit assignment in a manner that approximates full gradient descent, and if so, how it does this. Either way, we can expect that answer to impact machine learning. If the brain does not do some form of backpropagation, this suggests that machine learning may benefit from understanding the tricks that the brain uses to avoid having to do so. If, on the other hand, the brain does do backpropagation, then the underlying mechanisms clearly can support a very wide range of efficient optimization processes across many domains, including learning from rich temporal data-streams and via unsupervised mechanisms, and the architectures behind this will likely be of long-term value to machine learning 40 . Moreover, the search for biologically plausible forms of backpropagation has already led to interesting insights, such as the possibility of using random feedback weights (feedback alignment) in backpropagation <ref type="bibr" target="#b190">(Lillicrap et al., 2014)</ref>, or the unexpected power of internal FORCE learning in chaotic, spontaneously active recurrent networks <ref type="bibr" target="#b311">(Sussillo and Abbott, 2009)</ref>. This and other findings discussed here suggest that there are still fundamental things we don't understand about backpropagation -which could potentially lead not only to more biologically plausible ways to train recurrent neural networks, but also to fundamentally simpler and more powerful ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hypothesis 2-Biological fine-structure of cost functions</head><p>A good practitioner of machine learning has access to a broad range of learning techniques and thus implicitly is able to use many different cost functions. Some problems ask for clustering, others for extracting sparse variables, and yet others for prediction quality to be maximized. The brain also needs to be able to deal with many different kinds of datasets. As such, it makes sense for the brain to use a broad range of cost functions appropriate for the diverse set of tasks it has to solve to thrive in this world. Many of the most notable successes of deep learning, from language modeling <ref type="bibr" target="#b314">(Sutskever et al., 2011)</ref>, to vision <ref type="bibr" target="#b170">(Krizhevsky et al., 2012)</ref>, to motor control <ref type="bibr">(Levine et al., 2015)</ref>, have been driven by end-to-end optimization of single task objectives. We have highlighted cases where machine learning has opened the door to multiplicities of cost functions that shape network modules into specialized roles. We expect that machine learning will increasingly adopt these practices in the future.</p><p>In computer vision, we have begun to see researchers re-appropriate neural networks trained for one task (e.g., ImageNet classification) and then deploy them on new tasks other than the ones they were trained for or for which more limited training data is available <ref type="bibr" target="#b359">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b242">Oquab et al., 2014;</ref><ref type="bibr" target="#b235">Noroozi and Favaro, 2016)</ref>. We imagine this procedure will be gen-40. Successes of deep learning are already being used, speculatively, to rationalize features of the brain. It has been suggested that large networks, with many more neurons available than are strictly needed for the target computation, make learning easier <ref type="bibr" target="#b106">(Goodfellow et al., 2014b)</ref>. In concordance with this, visual cortex appears to be a 100-fold over-complete representation of the retinal output <ref type="bibr" target="#b186">(Lewicki and Sejnowski, 2000)</ref>. Likewise, it has been suggested that biological neurons stabilized <ref type="bibr" target="#b328">(Turrigiano, 2012)</ref> to operate far below their saturating firing rates mirror the successful use of rectified linear units in facilitating the training of artificial neural networks <ref type="bibr" target="#b273">(Roudi and Taylor, 2015)</ref>. Hinton and others have also suggested a biological motivation <ref type="bibr" target="#b273">(Roudi and Taylor, 2015)</ref> for "dropout" regularization <ref type="bibr" target="#b301">(Srivastava et al., 2014)</ref>, in which a fraction of hidden units is stochastically set to zero during each round of training: such a procedure may correspond to the noisiness of neural spike trains, although other theories interpret spikes as sampling in probabilistic inference <ref type="bibr" target="#b44">(Buesing et al., 2011)</ref>, or in many other ways.</p><p>Randomness of spiking has some support in neuroscience <ref type="bibr" target="#b296">(Softky and Koch, 1993)</ref>, although recent experiments suggest that spike trains in certain areas may be less noisy than previously thought <ref type="bibr" target="#b136">(Hires et al., 2015)</ref>. The key role of proper initialization in enabling effective gradient descent is an important recent finding <ref type="bibr" target="#b281">(Saxe et al., 2013;</ref><ref type="bibr" target="#b313">Sutskever and Martens, 2013)</ref> which may also be reflected by biological mechanisms of neural homeostasis or self-organization that would enforce appropriate initial conditions for learning. But making these speculative claims of biological relevance more rigorous will require researchers to first evaluate whether biological neural circuits are performing multi-layer optimization of cost functions in the first place.</p><p>eralized, whereby, in series and in parallel, diverse training problems, each with an associated cost function, are used to shape visual representations. For example, visual data streams can be segmented into elements like foreground vs. background, objects that can move of their own accord vs. those that cannot, all using diverse unsupervised criteria <ref type="bibr" target="#b329">(Ullman et al., 2012;</ref><ref type="bibr" target="#b189">Poggio, 2015)</ref>. Networks so trained can then be shared, augmented, and retrained on new tasks. They can be introduced as front-ends for systems that perform more complex objectives or even serve to produce cost functions for training other circuits <ref type="bibr" target="#b342">(Watter et al., 2015)</ref>. As a simple example, a network that can discriminate between images of different kinds of architectural structures (pyramid, staircase, etc.) could act as a critic for a building-construction network. Scientifically, determining the order in which cost functions are engaged in the biological brain will inform machine learning about how to construct systems with intricate and hierarchical behaviors via divide-and-conquer approaches to learning problems, active learning, and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hypothesis 3-Embedding within a pre-structured architecture</head><p>A good practitioner of machine learning should have a broad range of algorithms at their disposal. Some problems are efficiently solved through dynamic programming, others through hashing, and yet others through multi-layer backpropagation. The brain needs to be able to solve a broad range of learning problems without the luxury of being reprogrammed. As such, it makes sense for the brain to have specialized structures that allow it to rapidly learn to approximate a broad range of algorithms.</p><p>The first neural networks were simple singlelayer systems, either linear or with limited non-linearities <ref type="bibr" target="#b263">(Rashevsky, 1939)</ref>.</p><p>The explosion of neural network research in the 1980s <ref type="bibr" target="#b212">(McClelland et al., 1986)</ref> saw the advent of multilayer networks, followed by networks with layer-wise specializations as in convolutional networks <ref type="bibr" target="#b96">(Fukushima, 1980;</ref><ref type="bibr" target="#b177">LeCun and Bengio, 1995)</ref>.</p><p>In the last two decades, architectures with specializations for holding variables stable in memory like the LSTM <ref type="bibr" target="#b138">(Hochreiter and Schmidhuber, 1997)</ref>, the control of content-addressable memory <ref type="bibr" target="#b347">(Weston et al., 2014;</ref><ref type="bibr" target="#b108">Graves et al., 2014)</ref>, and game playing by reinforcement learning <ref type="bibr" target="#b226">(Mnih et al., 2015)</ref> have been developed. These networks, though formerly exotic, are now becoming mainstream algorithms in the toolbox of any deep learning practitioner. There is no sign that progress in developing new varieties of structured architectures is halting, and the heterogeneity and modularity of the brain's circuitry suggests that diverse, specialized architectures are needed to solve the diverse challenges that confront a behaving animal.</p><p>The brain combines a jumble of specialized structures in a way that works. Solving this problem de novo in machine learning promises to be very difficult, making it attractive to be inspired by observations about how the brain does it. An understanding of the breadth of specialized structures, as well as the architecture that combines them, should be quite useful. We hypothesize that the brain also acquired such a separation between optimization mechanisms and cost functions. If neural circuits, such as in cortex, implement a general-purpose optimization algorithm, then any improvement to that algorithm will improve function across the cortex. At the same time, different cortical areas solve different problems, so tinkering with each area's cost function is likely to improve its per-formance. As such, functionally and evolutionarily separating the problems of optimization and cost function generation could allow evolution to produce better computations, faster. For example, common unsupervised mechanisms could be combined with area-specific reinforcementbased or supervised mechanisms and error signals, much as recent advances in machine learning have found natural ways to combine supervised and unsupervised objectives in a single system <ref type="bibr">(Rasmus and Berglund, 2015)</ref>.</p><p>This suggests interesting questions 41 : When did the division between cost functions and optimization algorithms occur? How is this separation implemented? How did innovations in cost functions and optimization algorithms evolve? And how do our own cost functions and learning algorithms differ from those of other animals?</p><p>There are many possibilities for how such a separation might be achieved in the brain. Perhaps the six-layered cortex represents a common optimization algorithm, which in different cortical areas is supplied with different cost functions. This claim is different from the claim that all cortical areas use a single unsupervised learning algorithm and achieve functional specificity by tuning the inputs to that algorithm. In that case, both the optimization mechanism and the implicit unsupervised cost function would be the same across areas (e.g., minimization of prediction error), with only the training data differing between areas, whereas in our suggestion, the optimization mechanism would be the same across areas but the cost function, as well as the training data, would differ. Thus the cost function itself would be like an ancillary input to a cortical area, in addition to its input and output data. Some cortical microcircuits could then, perhaps, compute the cost functions that are to be delivered to other cortical microcircuits.</p><p>Another possibility is that, within the same circuitry, certain aspects of the wiring and learning rules specify an optimization mechanism and are relatively fixed across areas, while others specify the cost function and are more variable. This latter possibility would be similar to the notion of cortical microcircuits as molecularly and structurally configurable elements, akin to the cells in a field-programmable gate array (FPGA) <ref type="bibr">(Marcus et al., 2014a,b)</ref>, rather than a homogenous substrate. The biological nature of such a separation, if any exists, remains an open question. For example, individual parts of a neuron may separately deal with optimization and with the specification of the cost function, or different parts of a microcircuit may specialize in this way, or there may be specialized types cells, some of which deal with signal processing and others with cost functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>Due to the complexity and variability of the brain, pure "bottom up" analysis of neural data faces potential challenges of interpretation <ref type="bibr" target="#b267">(Robinson, 1992;</ref><ref type="bibr">Jonas and Kording, 2016)</ref>. Theoretical frameworks can potentially be used to constrain the space of hypotheses being evaluated, allowing researchers to first address higherlevel principles and structures in the system, and 41. It would be interesting to study these questions in specific brain systems. The primary visual cortex, for example, is still only understood very incompletely <ref type="bibr" target="#b241">(Olshausen and Field, 2004)</ref>. It serves as a key input modality to both the ventral and dorsal visual pathways, one of which seems to specialize in object identity and the other in motion and manipulation. Higher-level areas like STP draw on both streams to perform tasks like complex action recognition. In some models (e.g., <ref type="bibr" target="#b153">(Jhuang et al., 2007)</ref>), both ventral and dorsal streams are structured hierarchically, but the ventral stream primarily makes use of the spatial filtering properties of V1, whereas the dorsal stream primarily makes use of its spatio-temporal filtering properties, e.g., temporal frequency filtering by the space-time receptive fields of V1 neurons. Given this, we can ask interesting questions about V1. Within a framework of multilayer optimization, do both dorsal and ventral pathways impose cost functions that help to shape V1's response properties? Or is V1 largely pre-structured by genetics and local self-organization, with different optimization principles in the ventral and dorsal streams only having effects at higher levels of the hierarchy? Or, more likely, is there some interplay between pre-structuring of the V1 circuitry and optimization according to multiple cost functions? Relatedly, what establishes the differing roles of the downstream ventral vs. dorsal cortical areas, and can their differences be attributed to differing cost functions? This relates to ongoing questions about the basic nature of cortical circuitry. For example, <ref type="bibr">DiCarlo et al. (2012)</ref> suggests that visual cortical regions containing on the order of 10000 neurons are locally optimized to perform disentangling of the manifolds corresponding to their local views of the transformations of an object, allowing these manifolds to be linearly separated by readout areas. Yet, DiCarlo et al. ( <ref type="formula">2012</ref>) also emphasizes the possibility that certain computations such as normalization are pre-initialized in the circuitry prior to learning-based optimization.</p><p>then "zoom in" to address the details. Proposed "top down" frameworks for understanding neural computation include entropy maximization, efficient encoding, faithful approximation of Bayesian inference, minimization of prediction error, attractor dynamics, modularity, the ability to subserve symbolic operations, and many others <ref type="bibr">(Bialek, 2002;</ref><ref type="bibr" target="#b33">Bialek et al., 2006;</ref><ref type="bibr">Friston, 2010;</ref><ref type="bibr">Knill and Pouget, 2004;</ref><ref type="bibr">Marcus, 2001;</ref><ref type="bibr" target="#b258">Pinker, 1999)</ref>. Interestingly, many of the "top down" frameworks boil down to assuming that the brain simply optimizes a single, given cost function for a single computational architecture. We generalize these proposals assuming both a heterogeneous combination of cost functions unfolding over development, and a diversity of specialized sub-systems.</p><p>Much of neuroscience has focused on the search for "the neural code", i.e., it has asked which stimuli are good at driving activity in individual neurons, regions, or brain areas. But, if the brain is capable of generic optimization of cost functions, then we need to be aware that rather simple cost functions can give rise to complicated stimulus responses. This potentially leads to a different set of questions. Are differing cost functions indeed a useful way to think about the differing functions of brain areas? How does the optimization of cost functions in the brain actually occur, and how is this different from the implementations of gradient descent in artificial neural networks? What additional constraints are present in the circuitry that remain fixed while optimization occurs? How does optimization interact with a structured architecture, and is this architecture similar to what we have sketched? Which computations are wired into the architecture, which emerge through optimization, and which arise from a mixture of those two extremes? To what extent are cost functions explicitly computed in the brain, versus implicit in its local learning rules? Did the brain evolve to separate the mechanisms involved in cost function generation from those involved in the optimization of cost functions, and if so how? What kinds of meta-level learning might the brain apply, to learn when and how to invoke different cost functions or specialized systems, among the diverse options available, to solve a given task? What crucial mechanisms are left out of this framework? A more in-depth dialog between neuroscience and machine learning could help elucidate some of these questions.</p><p>Much of machine learning has focused on finding ever faster ways of doing end-to-end gradient descent in neural networks. Neuroscience may inform machine learning at multiple levels. The optimization algorithms in the brain have undergone a couple of hundred million years of evolution. Moreover, the brain may have found ways of using heterogeneous cost functions that interact over development so as to simplify learning problems by guiding and shaping the outcomes of unsupervised learning. Lastly, the specialized structures evolved in the brain may inform us about ways of making learning efficient in a world that requires a broad range of computational problems to be solved over multiple timescales. Looking at the insights from neuroscience may help machine learning move towards general intelligence in a structured heterogeneous world with access to only small amounts of supervised data.</p><p>In some ways our proposal is opposite to many popular theories of neural computation. There is not one mechanism of optimization but (potentially) many, not one cost function but a host of them, not one kind of a representation but a representation of whatever is useful, and not one homogeneous structure but a large number of them. All these elements are held together by the optimization of internally generated cost functions, which allows these systems to make good use of one another. Rejecting simple unifying theories is in line with a broad range of previous approaches in AI. For example, Minsky and Papert's work on the Society of Mind <ref type="bibr" target="#b219">(Minsky, 1988)</ref> -and more broadly on ideas of genetically staged and internally bootstrapped development in connectionist systems <ref type="bibr">(Minsky, 1977)</ref> -emphasizes the need for a system of internal monitors and critics, specialized communication and storage mechanisms, and a hierarchical organization of simple control systems.</p><p>At the time these early works were written, it was not yet clear that gradient-based optimization could give rise to powerful feature representations and behavioral policies. One can view our proposal as a renewed argument against simple end-to-end training and in favor of a heterogeneous approach. In other words, this framework could be viewed as proposing a kind of "society" of cost functions and trainable networks, permit-ting internal bootstrapping processes reminiscent of the Society of Mind <ref type="bibr" target="#b219">(Minsky, 1988)</ref>. In this view, intelligence is enabled by many computationally specialized structures, each trained with its own developmentally regulated cost function, where both the structures and the cost functions are themselves optimized by evolution like the hyperparameters in neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label></label><figDesc>Hypothesis 1 -The brain optimizes cost functions. . . . . . . . . . . . . . . . . . . . 3 1.2 Hypothesis 2 -Cost functions are diverse across areas and change over development. 4 1.3 Hypothesis 3 -Specialized systems allow efficiently solving key computational problems. 4 The brain can optimize cost functions 5 2.1 Local self-organization and optimization without multi-layer credit assignment . . . 7 2.2 Biological implementation of optimization . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2.1 The need for efficient gradient descent in multi-layer networks . . . . . . . . . 7 2.2.2 Biologically plausible approximations of gradient descent . . . . . . . . . . . 8 2.3 Alternative mechanisms for learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.1 Exploiting biological neural mechanisms . . . . . . . . . . . . . . . . . . . . . 12 2.3.2 Learning in the cortical sheet . . . . . . . . . . . . . . . . . . . . . . . . . . . 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Deep learning methods have taken the field of machine learning by storm. Driving the success is the separation of the problem of learning into two pieces: (1) An algorithm, backpropagation, that allows efficient distributed optimization, and (2) Approaches to turn any given problem into an optimization problem, by designing a cost function and training procedure which will result in the desired computation. If we want to apply deep learning to a new domain, e.g., playing Jeopardy, we do not need to change the optimization algorithm -we just need to cleverly set up the right cost function. A lot of work in deep learning, perhaps the majority, is now focused on setting up the right cost functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>In the brain, supervised training of networks can still occur via gradient descent on an error signal, but this error signal must arise from internally generated cost functions. These cost functions are themselves computed by neural modules specified by both genetics and learning. Internally generated cost functions create heuristics that are used to bootstrap more complex learning. For example, an area which recognizes faces might first be trained to detect faces using simple heuristics, like the presence of two dots above a line, and then further trained to discriminate salient facial expressions using representations arising from unsupervised learning and error signals from other brain areas related to social reward processing. C) Internally generated cost functions and error-driven training of cortical deep networks form part of a larger architecture containing several specialized systems. Although the trainable cortical areas are schematized as feedforward neural networks here, LSTMs or other types of recurrent networks may be a more accurate analogy, and many neuronal properties such as spiking, dendritic com-</figDesc><table><row><cell></cell><cell>Cortical</cell></row><row><cell></cell><cell>Area</cell></row><row><cell>Other</cell><cell></cell></row><row><cell>inputs to</cell><cell></cell></row><row><cell>cost function</cell><cell></cell></row><row><cell>Inputs</cell><cell></cell></row><row><cell>Data</cell><cell>Training</cell></row><row><cell>Sensory Inputs</cell><cell>Motor Outputs</cell></row><row><cell>Specialized subsystems</cell><cell></cell></row></table><note>InputsFig 1: Putative differences between conventional and brain-like neural network designs. A) In conventional deep learning, supervised training is based on externally-supplied, labeled data. B) putation, neuromodulation, adaptation, timing-dependent plasticity, direct electrical connections, transient synaptic dynamics, spontaneous activity, and others, will influence what and how such networks learn.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_0">. In the visual system, it is still unknown why a clustered spatial pattern of representational categories arises, e.g., a physically localized "area" that seems to correspond to representations of faces<ref type="bibr" target="#b158">(Kanwisher et al., 1997)</ref>, another area for representations of visual word forms<ref type="bibr" target="#b211">(McCandliss et al., 2003)</ref>, and so on. It is also unknown why this spatial pattern seems to be largely reproducible across individuals. Some theories are based on bottom-up correlationbased clustering or neuronal competition mechanisms, which generate category-selective regions as a byproduct. Other theories suggest a computational reason for this organization, in the context of I-theory<ref type="bibr" target="#b8">(Anselmi et al., 2015)</ref>, involving the limited ability to generalize transformation-invariances learned for one class of objects to other classes<ref type="bibr" target="#b184">(Leibo et al., 2015b)</ref>. Areas for abstract culture-dependent concepts, like the visual word form area, suggest that the decomposition cannot be "purely genetic". But it is conceivable that these areas could at least in part reflect different local cost functions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_1">. Attention also arguably solves certain types of perceptual binding problem<ref type="bibr" target="#b264">(Reynolds and Desimone, 1999)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_2">. Like many brain areas, the hippocampus is richly innervated by a variety of reward-related and other neuromodulatory systems<ref type="bibr" target="#b122">(Hasselmo and Wyble, 1997;</ref><ref type="bibr" target="#b335">Verney et al., 1985;</ref> Colino and Halliwell, 1987).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35" xml:id="foot_3">. Fluorescent techniques like<ref type="bibr" target="#b126">(Hayashi-Takagi et al., 2015)</ref> might be helpful.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="36" xml:id="foot_4">. The use of structured microcircuits rather than individual neurons as the units of learning can ease the burden on the learning rules possessed by individual neurons, as exemplified by a study implementing Helmholtz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ken Hayworth for key discussions that led to this paper. We thank Ed Boyden, Chris Eliasmith, Gary Marcus, Shimon Ullman, Tomaso Poggio, Josh Tenenbaum, Dario Amodei, Tom Dean, Joscha Bach, Mohammad Gheshlaghi Azar, Joshua Glaser, Ali Hummos, David Markowitz, David Rolnick, Sam Rodriques, Nick Barry, Darcy Wayne, Lyra and Neo Marblestone, and all of the participants of a Kavli Salon on Cortical Computation (Feb/Oct 2015) for helpful discussions. We thank Miles Brundage for an excellent Twitter feed of deep learning papers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Functional significance of long-term potentiation for sequence learning and prediction</title>
		<author>
			<persName><forename type="first">Abbott</forename><surname>Lf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Blum</surname></persName>
		</author>
		<ptr target="http://www.neurotheory.columbia.edu/Larry/SpikingNetworkReview.pdf" />
	</analytic>
	<monogr>
		<title level="m">LF Abbott, B DePasquale, and RM Memmesheimer. Building Functional Networks of Spiking Model Neurons. neurotheory.columbia.edu</title>
				<imprint>
			<date type="published" when="1996">1996. 2016</date>
		</imprint>
	</monogr>
	<note>Cerebral Cortex</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A learning algorithm for Boltzmann machines</title>
		<author>
			<persName><surname>Dh Ackley</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Sejnowski</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0364021385800124" />
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multifaceted aspects of chunking enable robust algorithms</title>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">F</forename><surname>Daniel E Acuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><forename type="middle">A</forename><surname>Wymbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">T</forename><surname>Strick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Grafton</surname></persName>
		</author>
		<author>
			<persName><surname>Kording</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1849" to="1856" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variance reduction in sgd by distributed importance sampling</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06481</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract structural representations of goal-directed behavior</title>
		<author>
			<persName><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Ibara</surname></persName>
		</author>
		<author>
			<persName><surname>Seymour</surname></persName>
		</author>
		<ptr target="http://pss.sagepub.com/content/21/10/1518.short" />
	</analytic>
	<monogr>
		<title level="j">Psychological</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shifter circuits: a computational strategy for dynamic aspects of visual processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="6297" to="6301" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How Can the Human Mind Occur in the Physical Universe?</title>
		<author>
			<persName><surname>John R Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02799</idno>
		<ptr target="http://books.google.com/books?id=eYZXEtfplyAC&amp;pgis=1" />
	</analytic>
	<monogr>
		<title level="m">Deep compositional question answering with neural module networks</title>
				<editor>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</editor>
		<meeting><address><addrLine>Jacob Andreas, Marcus Rohrbach</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2007">2007. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01705</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Andrea Tacchetti, and Tomaso Poggio. Unsupervised learning of invariant representations</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Mutch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tcs.2015.06.048</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0304397515005587" />
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<date type="published" when="2015-06">jun 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nitric oxide acts directly in the presynaptic neuron to produce long-term potentiation in cultured hippocampal neurons</title>
		<author>
			<persName><surname>Arancio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C J</forename><surname>Kiebler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lev-Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E R</forename><surname>Y Tsien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R D</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName><surname>Hawkins</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/8978607" />
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<idno type="ISSN">0092-8674</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1025" to="1035" />
			<date type="published" when="1996-12">dec 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two distinct modes of forebrain circuit dynamics underlie temporal patterning in the vocalizations of young songbirds</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Aronov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michale</forename><forename type="middle">S</forename><surname>Fee</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3009-11</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<idno type="ISSN">1529-2401</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="16353" to="16368" />
			<date type="published" when="2011-11">nov 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Why are deep nets reversible: A simple theory, with implications for training</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.05653" />
		<imprint>
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neurobiological theory of automaticity in perceptual categorization</title>
		<author>
			<persName><forename type="first">Ashby</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Ennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Spiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">632</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cortical and basal ganglia contributions to habit learning and automaticity</title>
		<author>
			<persName><forename type="first">Ashby</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">O</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">C</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2010</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<idno type="ISSN">1879-307X</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="2010-05">may 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mushroom body output neurons encode valence and guide memory-based action selection in Drosophila</title>
		<author>
			<persName><forename type="first">Yoshinori</forename><surname>Aso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirmala</forename><forename type="middle">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teri-T B</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Dionne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L F</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Axel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiromu</forename><surname>Tanimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><forename type="middle">M</forename><surname>Rubin ; Aso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Sitaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiharu</forename><surname>Ichinose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Karla R Kaun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">M</forename><surname>Rowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teri-T B</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wyatt</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Korff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Michael N Nitabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Heberlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">M</forename><surname>Preat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiromu</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><forename type="middle">M</forename><surname>Tanimoto</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.04577</idno>
		<idno>doi: 10.7554/ eLife.04580</idno>
		<ptr target="http://elifesciences.org/content/3/e04580.abstract" />
	</analytic>
	<monogr>
		<title level="j">Ghislain Belliart-Guérin</title>
		<idno type="ISSN">2050-084X</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e04580</biblScope>
			<date type="published" when="2014-01">jan 2014a. jan 2014b</date>
		</imprint>
	</monogr>
	<note>The neuronal architecture of the mushroom body provides a logic for associative learning. eLife</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Do deep nets really need to be deep? Advances in neural information processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Caruana</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Modeling Motivation in MicroPsi 2. Artificial General Intelligence</title>
		<author>
			<persName><forename type="first">Joscha</forename><surname>Bach</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-21365-1{_}1</idno>
		<ptr target="http://link.springer.com/chapter" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Request confirmation networks for neuro-symbolic script execution</title>
		<author>
			<persName><forename type="first">Joscha</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priska</forename><surname>Herger</surname></persName>
		</author>
		<ptr target="http://www.neural-symbolic.org/CoCo2015/" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches at NIPS</title>
				<editor>
			<persName><forename type="first">Tarek</forename><surname>Besold</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Artur</forename><surname>Davila Garcez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural Mechanisms of Hierarchical Planning in a Virtual Subway Network</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Spiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="893" to="903" />
			<date type="published" when="2016-05">may 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Baldi and Peter Sadowski. The Ebb and Flow of Deep Learning: a Theory of Local Learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Baldauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Desimone</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1247003</idno>
		<ptr target="http://arxiv.org/abs/1506.06472" />
	</analytic>
	<monogr>
		<title level="m">Science</title>
				<meeting><address><addrLine>New York, N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04">apr 2014. jun 2015</date>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="424" to="427" />
		</imprint>
	</monogr>
	<note>Neural mechanisms of object-based attention</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Balduzzi</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2617449" />
		<title level="m">Cortical prediction markets. Proceedings of the 2014 international conference on</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kickback cuts Backprop&apos;s redtape: Biologically plausible credit assignment in neural networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hastagiri</forename><surname>Vanchinathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Buhmann</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.6191" />
		<imprint>
			<date type="published" when="2014-11">nov 2014</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond the connectome: how neuromodulators shape neural circuits</title>
		<author>
			<persName><surname>Cornelia I Bargmann</surname></persName>
		</author>
		<idno type="DOI">10.1002/bies</idno>
	</analytic>
	<monogr>
		<title level="m">BioEssays : news and reviews in molecular, cellular and developmental biology</title>
				<imprint>
			<date type="published" when="2012-06">jun 2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="458" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From the connectome to brain function</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cornelia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eve</forename><surname>Bargmann</surname></persName>
		</author>
		<author>
			<persName><surname>Marder</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.2451</idno>
		<ptr target="http://dx.doi.org/10.1038/nmeth.2451" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<idno type="ISSN">1548-7091</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="483" to="490" />
			<date type="published" when="2013-05">may 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1306572110</idno>
		<ptr target="http://www.pnas.org/content/110/45/18327.short" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<idno type="ISSN">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013-11">nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selforganizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName><forename type="first">Suzanna</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1407.7906" />
		<imprint>
			<date type="published" when="2014-07">jul 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Early Inference in Energy-Based Models Approximates Back-Propagation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1510.02777" />
		<imprint>
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards Biologically Plausible Deep Learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorg</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1502.04156" />
		<imprint>
			<date type="published" when="2015-02">feb 2015a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">STDP as presynaptic activity times rate of change of postsynaptic activity</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.05936" />
		<imprint>
			<date type="published" when="2015-09">sep 2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Bird&apos;s Eye View of Human Language Evolution</title>
		<author>
			<persName><surname>Robert C Berwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J L</forename><surname>Gabriël</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuo</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><forename type="middle">J</forename><surname>Okanoya</surname></persName>
		</author>
		<author>
			<persName><surname>Bolhuis</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnevo.2012</idno>
	</analytic>
	<monogr>
		<title level="m">Frontiers in evolutionary neuroscience</title>
				<imprint>
			<date type="published" when="2012-01">jan 2012</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Thinking about the brain. Physics of bio-molecules and cells</title>
		<idno type="DOI">10.1007/3-540-45701-1{_}12</idno>
		<ptr target="http://link.springer.com/chapter/10.1007/3-540-45701-1_12" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Physique des . . .</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient representation as a design principle for neural coding and computation</title>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>De Ruyter Van Steveninck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISIT.2006.261867</idno>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4036045" />
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Symposium on Information Theory</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006-07">jul 2006</date>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A parallel algorithm for real-time computation of optical flow</title>
		<author>
			<persName><surname>Biilthoff</surname></persName>
		</author>
		<author>
			<persName><surname>Little</surname></persName>
		</author>
		<author>
			<persName><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="issue">6207</biblScope>
			<biblScope unit="page" from="549" to="553" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured Dendritic Inhibition Supports Branch-Selective Integration in CA1 Pyramidal Cells</title>
		<author>
			<persName><surname>Erikb</surname></persName>
		</author>
		<author>
			<persName><surname>Bloss</surname></persName>
		</author>
		<author>
			<persName><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Cembrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Karsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richardd</forename><surname>Colonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Fetter</surname></persName>
		</author>
		<author>
			<persName><surname>Spruston</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1016" to="1030" />
			<date type="published" when="2016-02">feb 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cortical integration: Possible solutions to the binding and linking problems in perception, reasoning and long term memory</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bostrom</surname></persName>
		</author>
		<ptr target="http://philpapers.org/rec/BOSCIP" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Model-based hierarchical reinforcement learning and human action control</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Weinstein</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2013.0480</idno>
		<ptr target="http://rstb.royalsocietypublishing.org/content/369/1655/20130480" />
	</analytic>
	<monogr>
		<title level="j">Biological sciences</title>
		<idno type="ISSN">1471-2970</idno>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<date type="published" when="1655">1655. 20130480-, nov 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective. Cognition</title>
		<author>
			<persName><surname>Mm Botvinick</surname></persName>
		</author>
		<author>
			<persName><surname>Niv</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0010027708002059" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Accelerating stochastic gradient descent via online learning to sample</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.09016</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enforcing balance allows local supervised learning in spiking recurrent networks</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Bourdoukan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Denève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Braitenberg and A Schutz. Anatomy of the cortex: studies of brain function</title>
				<imprint>
			<date type="published" when="1991">2015. 1991</date>
			<biblScope unit="page" from="982" to="990" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perception of Object Persistence: The Origins of Object Permanence in Infancy</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">P</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1111/cdep.12098</idno>
		<ptr target="http://doi.wiley.com/10.1111/cdep.12098" />
	</analytic>
	<monogr>
		<title level="j">Child Development Perspectives</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="13" />
			<date type="published" when="2015-03">mar 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reinforcement Learning of Linking and Tracing Contours in Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1004489</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e1004489</biblScope>
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prospective representation of navigational goals in the human hippocampus</title>
		<author>
			<persName><forename type="first">I</forename><surname>Thackery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">F</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serra</forename><forename type="middle">E</forename><surname>Larocque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Favila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">N</forename><surname>Bowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaf0784</idno>
		<ptr target="http://science.sciencemag.org/content/352/6291/1323" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="issue">6291</biblScope>
			<biblScope unit="page" from="1323" to="1326" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1002211</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e1002211</biblScope>
			<date type="published" when="2011-11">nov 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal information transformed into a spatial code by a neural network with realistic properties</title>
		<author>
			<persName><forename type="first">D V</forename><surname>Buonomano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Merzenich</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/7863330" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">5200</biblScope>
			<biblScope unit="page" from="1028" to="1030" />
			<date type="published" when="1995-02">feb 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Shifting the spotlight of attention: evidence for discrete computations in cognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Buschman</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2010.00194</idno>
	</analytic>
	<monogr>
		<title level="m">Frontiers in human neuroscience</title>
				<imprint>
			<date type="published" when="2010-01">jan 2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">194</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Goaldirection and top-down control. Philosophical transactions of the Royal Society of London</title>
		<author>
			<persName><forename type="first">J</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Buschman</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2013.0471</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/25267814" />
	</analytic>
	<monogr>
		<title level="j">Series B, Biological sciences</title>
		<idno type="ISSN">1471-2970</idno>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<date type="published" when="1655-11">1655. nov 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An echo state model of non-markovian reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><surname>Bush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Memory, navigation and theta rhythm in the hippocampalentorhinal system</title>
		<author>
			<persName><forename type="first">György</forename><surname>Buzsáki</surname></persName>
		</author>
		<author>
			<persName><surname>Edvard</surname></persName>
		</author>
		<author>
			<persName><surname>Moser</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn.3304</idno>
		<ptr target="http://dx.doi.org/10.1038/nn.3304" />
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<idno type="ISSN">1546-1726</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="138" />
			<date type="published" when="2013-02">feb 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A shared neural ensemble links distinct contextual memories encoded close in time</title>
		<author>
			<persName><forename type="first">Denise</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Shobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Biane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Veshkini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mimi</forename><surname>La-Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">534</biblScope>
			<biblScope unit="issue">7605</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feedforward, feedback and inhibitory connections in primate visual cortex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Callaway</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608004000887" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random convergence of olfactory inputs in the Drosophila mushroom body</title>
		<author>
			<persName><forename type="first">J C</forename><surname>Sophie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L F</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><surname>Axel</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature12063</idno>
		<ptr target="http://dx.doi.org/10.1038/nature12063" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="issue">7447</biblScope>
			<biblScope unit="page" from="113" to="117" />
			<date type="published" when="2013-05">may 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning Deep Structured Models</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1407.2538" />
		<imprint>
			<date type="published" when="2014-07">jul 2014</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">What and where: a Bayesian inference theory of attention</title>
		<author>
			<persName><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2010.05.013</idno>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.353.1190" />
	</analytic>
	<monogr>
		<title level="m">Spiking Neuron Model of Serial-Order Recall. 32nd Annual Conference of the Cognitive Science Society</title>
				<imprint>
			<date type="published" when="2010-10">oct 2010. 2010</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2233" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A cholinergic mechanism for reward timing within primary visual cortex</title>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">B</forename><surname>Alexander A Chubykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">F</forename><surname>Roach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall G Hussain</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName><surname>Shuler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2012.12.039</idno>
		<ptr target="http://www.cell.com/article/S0896627313000470/fulltext" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<idno type="ISSN">1097-4199</idno>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="723" to="735" />
			<date type="published" when="2013-02">feb 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.3555" />
		<imprint>
			<date type="published" when="2014-12">dec 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Episodiclike memory during cache recovery by scrub jays</title>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="issue">6699</biblScope>
			<biblScope unit="page" from="272" to="274" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Voltage and spike timing interact in stdp-a unified model. Spike-timing dependent plasticity</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wulfram</forename><surname>Gerstner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">294</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Colino and JV Halliwell. Differential modulation of three separate k-conductances in hippocampal ca1 neurons by serotonin</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ianessa</forename><surname>Morantte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Ruta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2015.11.019</idno>
		<ptr target="http://www.cell.com/article/S0092867415014993/fulltext.A" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="73" to="77" />
			<date type="published" when="1987">dec 2015. 1987</date>
		</imprint>
	</monogr>
	<note>Cell</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Neural networks and neuroscience-inspired computer vision</title>
		<author>
			<persName><forename type="first">David</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cox</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1038/337129a0</idno>
		<idno>doi: 10.1038/337129a0</idno>
		<ptr target="http://dx.doi.org/10.1038/337129a0" />
	</analytic>
	<monogr>
		<title level="j">Current biology : CB</title>
		<idno type="ISSN">1879-0445</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="129" to="132" />
			<date type="published" when="1989-01">sep 2014. jan 1989</date>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">What is the function of the claustrum?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Crick</surname></persName>
		</author>
		<author>
			<persName><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical Transactions of the Royal Society of London B: Biological Sciences</title>
				<imprint>
			<date type="published" when="1458">1458. 2005</date>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="1271" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Low-level cues and ultra-fast face detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sébastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">J</forename><surname>Crouzet</surname></persName>
		</author>
		<author>
			<persName><surname>Thorpe</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2011.00342</idno>
	</analytic>
	<monogr>
		<title level="m">Frontiers in psychology</title>
				<imprint>
			<date type="published" when="2011-01">jan 2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">342</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Continuous online sequence learning with an unsupervised neural network model</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chetan</forename><surname>Surpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subutai</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hawkins</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.05463" />
		<imprint>
			<date type="published" when="2015-12">dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Associative Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1602.03032" />
		<imprint>
			<date type="published" when="2016-02">feb 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Actions, policies, values and the basal ganglia</title>
		<author>
			<persName><forename type="first">Yael</forename><surname>Nathaniel D Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent breakthroughs in basal ganglia research</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="91" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Twenty-five lessons from computational neuromodulation</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2012.09.027</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0896627312008628" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<idno type="ISSN">1097-4199</idno>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="240" to="256" />
			<date type="published" when="2012-10">oct 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A computational model of the cerebral cortex</title>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Papers/AAAI/2005/AAAI05-148.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial</title>
				<meeting>the National Conference on Artificial</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The Neural Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees</title>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Meyniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wacongne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Pallier</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2015</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="19" />
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Delalleau and Yoshua Bengio. Shallow vs. Deep Sum-Product Networks</title>
		<ptr target="http://papers.nips.cc/paper/4350-shallow-vs-deep-sum-product-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="666" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Using firing-rate dynamics to train recurrent networks of spiking model neurons</title>
		<author>
			<persName><surname>Depasquale</surname></persName>
		</author>
		<author>
			<persName><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><surname>Abbott</surname></persName>
		</author>
		<idno>arXiv: . . .</idno>
		<ptr target="http://arxiv.org/abs/1601.07620" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">The neural optimal control hierarchy for motor control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName><surname>Eliasmith</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2560/8/6/065009</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/22056418" />
		<imprint>
			<date type="published" when="2011-12">dec 2011</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">65009</biblScope>
		</imprint>
	</monogr>
	<note>Journal of neural engineering</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>James J Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">C</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Neuronal circuits of the neocortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevan A C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.neuro.27.070203</idno>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<idno type="ISSN">0147- 006X</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="419" to="451" />
			<date type="published" when="2004-01">jan 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="961" to="974" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A role for synaptic inputs at distal dendrites: instructive signals for hippocampal longterm plasticity</title>
		<author>
			<persName><forename type="first">David</forename><surname>Joshua T Dudman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Tsay</surname></persName>
		</author>
		<author>
			<persName><surname>Siegelbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="866" to="879" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">How to Build a Brain: A Neural Architecture for Biological Cognition</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=BK0YRJPmuzgC&amp;pgis=1" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=J6jz9s4kbfIC&amp;pgis=1" />
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Normalization for probabilistic inference with neurons</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="251" to="262" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A largescale model of the functioning brain</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Bekolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rasmussen</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1225266</idno>
		<ptr target="http://www.sciencemag.org/content/338/6111/1202" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">1095-9203</idno>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="issue">6111</biblScope>
			<biblScope unit="page" from="1202" to="1205" />
			<date type="published" when="2012-11">nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training. International</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Stephen T Emlen ; D Erhan</surname></persName>
		</author>
		<author>
			<persName><surname>Manzagol</surname></persName>
		</author>
		<idno>arXiv: . . .</idno>
		<ptr target="http://arxiv.org/abs/1603.08575" />
	</analytic>
	<monogr>
		<title level="m">Migratory orientation in the indigo bunting, passerina cyanea: part i: evidence for use of celestial cues</title>
				<imprint>
			<date type="published" when="1967">1967. 2009. 2016</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="309" to="342" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SM Eslami, N Heess, and T Weber. Attend, Infer, Repeat: Fast Scene Understanding with Generative Models</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">From faces to hands: Changing visual input in the first two years</title>
		<author>
			<persName><forename type="first">Caitlin</forename><forename type="middle">M</forename><surname>Fausey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swapnaa</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="101" to="107" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Distributed hierarchical processing in the primate cerebral cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Felleman</surname></persName>
		</author>
		<author>
			<persName><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Neural Mechanisms of Orientation Selectivity in the Visual Cortex</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ferster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.neuro.23.1.441</idno>
		<ptr target="http://www.annualreviews.org/doi/abs/10.1146/annurev.neuro.23.1.441" />
		<imprint>
			<date type="published" when="2003-11">nov 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Operant conditioning of cortical unit activity</title>
		<author>
			<persName><forename type="first">Eberhard</forename><forename type="middle">E</forename><surname>Fetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="955" to="958" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Volitional control of neural activity: implications for brain-computer interfaces</title>
		<author>
			<persName><forename type="first">Eberhard</forename><forename type="middle">E</forename><surname>Fetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">579</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="571" to="579" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Gradient learning in spiking neural networks by dynamic perturbation of conductances</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ila R Fiete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Sebastian</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.97.048104</idno>
		<ptr target="http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.97.048104" />
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<idno type="ISSN">0031-9007</idno>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">48104</biblScope>
			<date type="published" when="2006-07">jul 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</title>
		<author>
			<persName><forename type="first">Michale</forename><forename type="middle">S</forename><surname>Ila R Fiete</surname></persName>
		</author>
		<author>
			<persName><surname>Fee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Sebastian</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.01311.2006</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/17652414" />
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<idno type="ISSN">0022-3077</idno>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2038" to="2057" />
			<date type="published" when="2007-10">oct 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Spike-time-dependent plasticity and heterosynaptic competition organize networks to produce long scale-free sequences of neural activity</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Ila R Fiete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude Z H</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard H R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hahnloser</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2010.02.003</idno>
		<ptr target="https://www.jneurosci.org/content/35/41/13912.full" />
	</analytic>
	<monogr>
		<title level="j">Time in Cortical Circuits. The Journal</title>
		<idno type="ISSN">1097-4199</idno>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="563" to="576" />
			<date type="published" when="2010-02">feb 2010. 2015</date>
		</imprint>
	</monogr>
	<note>Neuron</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Understanding stimulus poverty arguments. The Linguistic Review</title>
		<author>
			<persName><forename type="first">Janet</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fodor</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Crowther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="105" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Learning Invariance from Transformation Sequences</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Földiák</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1991.3.2.194{#}.VofznO8rKHo</idno>
		<ptr target="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1991.3.2.194#.VofznO8rKHo" />
		<imprint>
			<date type="published" when="2008-03">mar 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Models of Hippocampally Dependent Navigation, Using The Temporal Difference Learning Rule</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G M</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Centre</forename><surname>For</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neuroscience</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000-09">sep 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><surname>Badre</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhr114</idno>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<idno type="ISSN">1460-2199</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="509" to="526" />
			<date type="published" when="1991-03">1991. mar 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">An architecture for encoding sentence meaning in left mid-superior temporal cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Frankland</surname></persName>
		</author>
		<author>
			<persName><surname>Greene</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1421236112</idno>
		<ptr target="http://www.pnas.org/content/112/37/11732.full" />
		<imprint>
			<date type="published" when="2015-08">aug 2015</date>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="11732" to="11737" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Slowness and sparseness lead to place, head-direction, and spatial-view cells</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Franzius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Sprekeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<idno type="DOI">http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030166</idno>
		<ptr target="http://www.nature.com/nrn/journal/v11/n2/abs/nrn2787.html" />
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2007-08">aug 2007. 2010</date>
		</imprint>
	</monogr>
	<note>PLoS computational biology</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Free-energy and the brain. Synthese</title>
		<author>
			<persName><surname>Kj Friston</surname></persName>
		</author>
		<author>
			<persName><surname>Stephan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11229-007-9237-y</idno>
		<ptr target="http://link.springer.com/article/10.1007/s11229-007-9237-y" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Neocognitron: A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">Kunihiko</forename><surname>Fukushima</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.2672" />
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980-12">1980. dec 2014</date>
		</imprint>
	</monogr>
	<note>Tao Gao, Daniel Harari, Joshua Tenenbaum, and Shimon Ullman. When Computer Vision Gazes at Cognition</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Towards a mathematical theory of cortical micro-circuits</title>
		<author>
			<persName><forename type="first">Dileep</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1000532</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e1000532</biblScope>
			<date type="published" when="2009-10">oct 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Complex probabilistic inference: From cognition to neural computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Time representation in reinforcement learning models of the basal ganglia</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><forename type="middle">B</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">A</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName><surname>Ludvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1553" to="1568" />
			<date type="published" when="2012">2012. 2014</date>
		</imprint>
	</monogr>
	<note>The successor representation and temporal context</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Evidence for a causal inverse model in an avian cortico-basal ganglia circuit</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Giret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joergen</forename><surname>Kornfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard H R</forename><surname>Hahnloser</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1317087111</idno>
		<ptr target="http://www.pnas.org/content/111/16/6063.full" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<idno type="ISSN">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="6063" to="6068" />
			<date type="published" when="2014-04">apr 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">How might the brain represent complex symbolic knowledge? Neural Networks (IJCNN), 2014 International Joint</title>
		<author>
			<persName><surname>Goertzel</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6889662" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Robust persistent neural activity in a model integrator with multiple hysteretic dendrites per neuron</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">H</forename><surname>Mark S Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1185" to="1195" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Coding of saliency by ensemble bursting in the amygdala of primates</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalez</forename><surname>Andino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R Grave De Peralta</forename><surname>Menendez</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnbeh.2012.00038/abstract</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fnbeh.2012.00038/abstract" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in behavioral neuroscience</title>
		<idno type="ISSN">1662-5153</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2012-01">jan 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Interval timing disruptions in subjects with cerebellar lesions</title>
		<author>
			<persName><forename type="first">Cynthia</forename><forename type="middle">M</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wiener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elaine</forename><forename type="middle">B</forename><surname>Wencil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coslett</forename><surname>Branch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2009.11.028</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<idno type="ISSN">1873-3514</idno>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1022" to="1031" />
			<date type="published" when="2010-03">mar 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1406.2661" />
		<imprint>
			<date type="published" when="2014-06">jun 2014a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6544" />
		<imprint>
			<date type="published" when="2014-12">dec 2014b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">The scientist in the crib: What early learning tells us about the mind</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Meltzoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Harper Paperbacks</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.5401" />
	</analytic>
	<monogr>
		<title level="j">Neural Turing Machines. ArXiv</title>
		<imprint>
			<date type="published" when="2014-10">oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The basal ganglia and chunking of action repertoires</title>
		<author>
			<persName><forename type="first">Ann</forename><forename type="middle">M</forename><surname>Graybiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of learning and memory</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="136" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Mechanisms for selection of basic motor programs-roles for the striatum and pallidum. Trends in neurosciences</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra ; Sten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanette</forename><surname>Grillner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Hellgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Ménard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Saitoh</surname></persName>
		</author>
		<author>
			<persName><surname>Wikström</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tins.2005.05.004</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/15935487" />
		<imprint>
			<date type="published" when="2005-07">feb 2015. jul 2005</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="364" to="370" />
		</imprint>
	</monogr>
	<note>DRAW: A Recurrent Neural Network For Image Generation</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Adaptive Resonance Theory: how a brain learns to consciously attend, learn, and recognize a changing world</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Grossberg</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2012.09.017</idno>
		<ptr target="http://www.sciencedirect" />
	</analytic>
	<monogr>
		<title level="m">Neural networks : the official journal of the International Neural Network Society</title>
				<editor>
			<persName><forename type="first">/</forename><forename type="middle">S0893608012002584 Arthur</forename><surname>Guez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">jan 2013. 2012</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1025" to="1033" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Knowledge Matters: Importance of Prior Information for Optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>¸alar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gülçehre</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v17/gulchere16a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Cognition without cortex</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Güntürkün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bugnyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="303" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A computational model of action selection in the basal ganglia. I. A new functional anatomy</title>
		<author>
			<persName><forename type="first">T J</forename><surname>Gurney</surname></persName>
		</author>
		<author>
			<persName><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><surname>Redgrave</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/11417052" />
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<idno type="ISSN">0340-1200</idno>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="401" to="410" />
			<date type="published" when="2001-06">jun 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">The problem of rapid variable creation</title>
		<author>
			<persName><forename type="first">Hadley</forename><surname>Robert</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2008.07-07-572</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/19431268" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="510" to="532" />
			<date type="published" when="2009-03">mar 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Social evaluation by preverbal infants</title>
		<author>
			<persName><forename type="first">Kiley</forename><surname>Hamlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Wynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bloom</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature06288</idno>
		<ptr target="http://dx.doi.org/10.1038/nature06288" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="issue">7169</biblScope>
			<biblScope unit="page" from="557" to="559" />
			<date type="published" when="2007-11">nov 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A Hebbian learning rule gives rise to mirror neurons and links them to control theoretic inverse models</title>
		<author>
			<persName><surname>Balázs Hangya</surname></persName>
		</author>
		<author>
			<persName><surname>Sachinp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lorenc</surname></persName>
		</author>
		<author>
			<persName><surname>Kepecs ; Hanuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R H R</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Hahnloser</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2015.07.057</idno>
		<idno>doi: 10.3389/fncir.2013.00106</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0092867415009733.A" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in neural circuits</title>
		<idno type="ISSN">1662-5110</idno>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2013-01">aug 2015. jan 2013</date>
		</imprint>
	</monogr>
	<note>Cell</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Signaldependent noise determines motor planning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="issue">6695</biblScope>
			<biblScope unit="page" from="780" to="784" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Stability of the fittest: organizing learning through retroaxonal signals</title>
		<author>
			<persName><surname>Harris</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0166223608000180" />
	</analytic>
	<monogr>
		<title level="m">Trends in neurosciences</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Demis Hassabis and Eleanor A Maguire. Deconstructing episodic memory with construction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><surname>Maguire</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2007.05.001</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1364661307001258" />
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<idno type="ISSN">1364-6613</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="299" to="306" />
			<date type="published" when="2007">2009. jul 2007</date>
		</imprint>
	</monogr>
	<note>The construction system of the brain</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">The role of acetylcholine in learning and memory. Current opinion in neurobiology</title>
		<author>
			<persName><surname>Michael E Hasselmo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2006.09.002</idno>
		<imprint>
			<date type="published" when="2006-12">dec 2006</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="710" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Free recall and recognition in a network model of the hippocampus: simulating effects of scopolamine on human memory function</title>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">P</forename><surname>Hasselmo</surname></persName>
		</author>
		<author>
			<persName><surname>Wyble</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0166-4328(97)00048-X</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S016643289700048X" />
	</analytic>
	<monogr>
		<title level="j">Behavioural Brain Research</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1997-12">dec 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Dscam diversity is essential for neuronal wiring and self-recognition</title>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebru</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Won</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erika</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Viragh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">J</forename><surname>Zipursky</surname></persName>
		</author>
		<author>
			<persName><surname>Dickson</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature06099</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="issue">7159</biblScope>
			<biblScope unit="page" from="223" to="227" />
			<date type="published" when="2007-09">sep 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subutai</forename><surname>Ahmad</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.00083" />
		<imprint>
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">On Intelligence. Henry Holt and Company</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Blakeslee</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=Qg2dmntfxmQC&amp;pgis=1" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Labelling and optical erasure of synaptic memory traces in the motor cortex</title>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Hayashi-Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sho</forename><surname>Yagishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayumi</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fukutoshi</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Loshbaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kuhlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruo</forename><surname>Kasai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Dynamically partitionable autoassociative networks as a solution to the neural binding problem</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Haykin</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2012.00073</idno>
		<ptr target="https://books.google.com/books/about/Neural_networks.html?id=OZJKAQAAIAAJ&amp;pgis=1" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<idno type="ISSN">1662-5188</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="1994-01">1994. jan 2012</date>
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
	<note>Kenneth J Hayworth</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Strategic cognitive sequencing: a computational cognitive neuroscience approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hayworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irving</forename><surname>Lescroart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Sa Herd</surname></persName>
		</author>
		<author>
			<persName><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><surname>Kriete</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0022338</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2537972" />
	</analytic>
	<monogr>
		<title level="m">Neural encoding of relative position</title>
				<imprint>
			<date type="published" when="2011-08">aug 2011. 2013</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1032" to="1050" />
		</imprint>
	</monogr>
	<note>Computational . . .</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">How to do backpropagation in a brain. Invited talk at the NIPS&apos;2007 Deep Learning Workshop</title>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.utoronto.ca/~hinton/backpropincortex2014.pdf" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Can the brain do back-propagation?</title>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=VIRCybGgHts" />
	</analytic>
	<monogr>
		<title level="m">vited talk at Stanford University Colloquium on Computer Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Connectionist learning procedures</title>
		<author>
			<persName><surname>Ge Hinton</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/" />
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Learning representations by recirculation. Neural information processing</title>
		<author>
			<persName><surname>Ge Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">The&quot; wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName><surname>Ge Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<ptr target="http://www.sciencemag.org/content/268/5214/1158.short" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Transforming auto-encoders. Artificial Neural Networks and</title>
		<author>
			<persName><surname>Ge Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-21735-7{_}6</idno>
		<ptr target="http://link.springer.com/chapter" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/16764513" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07">jul 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Lownoise encoding of active touch by layer 4 in the somatosensory cortex. eLife, 4</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><forename type="middle">A</forename><surname>Hires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Gutnisky</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H O'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Svoboda</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.06619</idno>
		<imprint>
			<date type="published" when="2015-01">jan 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Insights into cortical mechanisms of behavior from microstimulation experiments</title>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">M</forename><surname>Mark H Histed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Hr</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><surname>Maunsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in neurobiology</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="115" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/9377276" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">nov 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gregor M Hoerzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhs348</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/23146969" />
	</analytic>
	<monogr>
		<title level="m">Cerebral cortex</title>
				<meeting><address><addrLine>New York, N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-03">1991. mar 2014</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="677" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Genetic control of wiring specificity in the fly olfactory system</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1534/genetics.113.154336</idno>
	</analytic>
	<monogr>
		<title level="m">Genetics</title>
				<imprint>
			<date type="published" when="2014-01">jan 2014</date>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="17" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J J</forename><surname>Hopfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982-04">apr 1982</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Neurons with graded response have collective computational properties like those of two-state neurons</title>
		<author>
			<persName><forename type="first">J J</forename><surname>Hopfield</surname></persName>
		</author>
		<ptr target="http://www.pnas.org/content/81/10/3088.abstract" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<idno type="ISSN">0027- 8424</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3088" to="3092" />
			<date type="published" when="1984-05">may 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Neurodynamics of mental exploration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0913991107</idno>
		<ptr target="http://www.pnas.org/content/107/4/1648.abstract" />
		<imprint>
			<date type="published" when="2009-12">dec 2009</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1648" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.142/pdf</idno>
		<ptr target="http://onlinelibrary.wiley.com/doi/10.1002/wcs.142/pdf" />
		<title level="m">Predictive coding. Wiley Interdisciplinary Reviews: Cognitive</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Interplay of approximate planning strategies</title>
		<author>
			<persName><forename type="first">Quentin Jm</forename><surname>Huys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Níall</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neir</forename><surname>Eshel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Seifritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><surname>Roiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3098" to="3103" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Learning and disrupting invariance in visual recognition with a temporal association rule. Frontiers in computational neuroscience</title>
		<author>
			<persName><forename type="first">Leyla</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2012.00037/abstract</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fncom.2012.00037/abstract" />
		<imprint>
			<date type="published" when="2012-01">jan 2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Solving the distal reward problem through linkage of STDP and dopamine signaling</title>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">M</forename><surname>Izhikevich</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhl152</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/17220510" />
	</analytic>
	<monogr>
		<title level="m">Cerebral cortex</title>
				<meeting><address><addrLine>New York, N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-10">1991. oct 2007</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Neural circuits: random design of a higher-order olfactory projection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><forename type="middle">W</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2013.04.016</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0960982213004247" />
	</analytic>
	<monogr>
		<title level="j">Current biology : CB</title>
		<idno type="ISSN">1879-0445</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="R448" to="R451" />
			<date type="published" when="2013-05">may 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5854-spatial-transformer-networks" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural</title>
		<imprint>
			<date type="published" when="2015">2015a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Spatial Transformer Networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1506.02025" />
		<imprint>
			<date type="published" when="2015-06">jun 2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Haas</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1091277</idno>
		<ptr target="http://www.sciencemag.org/content/304/5667/78.abstract" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">1095-9203</idno>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5667</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2004-04">apr 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A normative model of attention: Receptive field modulation</title>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Jaramillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="613" to="618" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Coordinated memory replay in the visual cortex and hippocampus during sleep</title>
		<author>
			<persName><forename type="first">Daoyun</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1825</idno>
		<ptr target="http://dx.doi.org/10.1038/nn1825" />
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<idno type="ISSN">1097-6256</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="2007-01">jan 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Tolias. Principles of connectivity among morphologically defined cell types in adult neocortex</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Cadwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1126/science.aac9462</idno>
		<ptr target="http://science.sciencemag.org/content/350/6264/aac9462.abstract" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6264</biblScope>
			<biblScope unit="page" from="9462" to="9462" />
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Memory trace and timing mechanism localized to cerebellar Purkinje cells. Proceedings of the National Academy of Sciences of the United States of America</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan-Anders</forename><surname>Jirenhed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zucca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germund</forename><surname>Hesslow</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1415371111</idno>
		<idno>doi: 10.1101/055624</idno>
		<ptr target="http://biorxiv.org/content/early/2016/05/26/055624" />
	</analytic>
	<monogr>
		<title level="m">Eric Jonas and Konrad Kording</title>
				<imprint>
			<date type="published" when="2014-10">oct 2014. 2016</date>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="14930" to="14934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">The fusiform face area: a module in human extrastriate cortex specialized for face perception</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Kalisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Silberberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry Markram ; Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0407088102</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4302" to="4311" />
			<date type="published" when="1997">jan 2005. 1997</date>
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
	<note>The neocortical microcircuit as a tabula rasa</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">STDP installs in Winner-Take-All circuits an online approximation to hidden Markov model learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003511</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e1003511</biblScope>
			<date type="published" when="2014-03">mar 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Intrinsic stabilization of output rates by spikebased Hebbian learning</title>
		<author>
			<persName><surname>Kempter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J L</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><surname>Van Hemmen</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976601317098501</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/11705408" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2709" to="2741" />
			<date type="published" when="2001-12">dec 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">DC Knill and A Pouget. The Bayesian brain: the role of uncertainty in neural coding and computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0166223604003352" />
	</analytic>
	<monogr>
		<title level="j">TRENDS in Neurosciences</title>
		<imprint>
			<date type="published" when="2004">dec 2013. 2004</date>
		</imprint>
	</monogr>
	<note>Auto-Encoding Variational Bayes</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Broca&apos;s area and the hierarchical organization of human behavior</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Koechlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Jubault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="963" to="974" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">A unified theoretical approach for biological cognition and learning</title>
		<author>
			<persName><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2016.03.006</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S2352154616300651" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="14" to="20" />
			<date type="published" when="2016-03">mar 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Decision theory: what&quot; should&quot; the nervous system do?</title>
		<author>
			<persName><surname>Körding</surname></persName>
		</author>
		<ptr target="http://science.sciencemag.org/content/318/5850/606.short" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">How are complex cell properties adapted to the statistics of natural stimuli</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Konrad P Körding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName><surname>König</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00149.2003</idno>
		<ptr target="http://jn.physiology.org/content/91/1/206.short" />
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<idno type="ISSN">0022-3077</idno>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="212" />
			<date type="published" when="2004-01">jan 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Körding and P König. Supervised and unsupervised learning with two sites of synaptic integration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName><surname>König</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1013776130161</idno>
		<ptr target="http://link.springer.com/article/10.1023/A:1013776130161" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
				<imprint>
			<date type="published" when="2000-01">jan 2000. 2001</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>A learning rule for dynamic recruitment and decorrelation</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Eichenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Hasselmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hippocampal time cells: time versus path integration</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1090" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Indirection and symbollike processing in the prefrontal cortex and basal ganglia</title>
		<author>
			<persName><forename type="first">Trenton</forename><surname>Kriete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">D</forename><surname>David C Noelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall C O'</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Reilly</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1303547110</idno>
		<ptr target="http://www.pnas.org/content/110/41/16390.short" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<idno type="ISSN">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page" from="16390" to="16395" />
			<date type="published" when="2013-10">oct 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Ramnandan</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><forename type="middle">S</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peeyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.05359" />
		<imprint>
			<date type="published" when="2016-05">may 2016</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Deep Convolutional Inverse Graphics Network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.03167" />
		<imprint>
			<date type="published" when="2015-03">mar 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.06057" />
		<imprint>
			<date type="published" when="2016-04">apr 2016</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Tracking the emergence of conceptual knowledge during human decision making</title>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">J</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleanor</forename><forename type="middle">A</forename><surname>Maguire</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2009.07.030</idno>
		<idno>doi: 10.1126/science.aab3050</idno>
		<ptr target="http://www.sciencemag.org/content/350/6266/1332.full" />
	</analytic>
	<monogr>
		<title level="m">Marcin Andrychowicz, and Ilya Sutskever. Neural Random-Access Machines</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Brenden</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Tenenbaum</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">sep 2009. nov 2015. dec 2015</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
		</imprint>
	</monogr>
	<note>Neuron</note>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00289</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Larkum</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tins.2012.11.006</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/23273272" />
	</analytic>
	<monogr>
		<title level="j">Trends in neurosciences</title>
		<idno type="ISSN">1878-108X</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2013-03">mar 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1112.6209" />
		<imprint>
			<date type="published" when="2011-12">dec 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="http://dx.doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">may 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Between the primate and &apos;reptilian&apos; brain: Rodent models demonstrate the role of corticostriatal circuits in decision making</title>
		<author>
			<persName><forename type="first">A M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-H</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><surname>Zador</surname></persName>
		</author>
		<author>
			<persName><surname>Wilbrecht</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroscience.2014.12.042</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0306452214010914" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<idno type="ISSN">1873- 7544</idno>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="page" from="66" to="74" />
			<date type="published" when="2015-06">jun 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian inference in the visual cortex</title>
		<author>
			<persName><forename type="first">Tai</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mumford</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/12868647" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America. A, Optics, image science, and vision</title>
		<idno type="ISSN">1084-7529</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1434" to="1448" />
			<date type="published" when="2003-07">jul 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Efficient coding of visual scenes by grouping and segmentation: theoretical predictions and biological evidence. Department of Statistics</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Ts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://escholarship.org/uc/item/1mc5v1b6.pdf" />
	</analytic>
	<monogr>
		<title level="j">UCLA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Branchspecific plasticity enables self-organization of nonlinear computation in single neurons. The Journal of neuroscience : the official journal of the Society for</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.5684-10</idno>
		<ptr target="http://www.jneurosci.org.libproxy.mit.edu/content/31/30/10787.short" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<idno type="ISSN">1529-2401</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="10787" to="10802" />
			<date type="published" when="2011-07">jul 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">Approximate Hubel-Wiesel Modules and the Data Structures of Neural Computation</title>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1512.08457" />
		<imprint>
			<date type="published" when="2015-12">dec 2015a</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">The invariance hypothesis implies domain-specific regions in visual cortex</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Joel Z Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e1004390</biblScope>
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">What the Frog&apos;s Eye Tells the Frog&apos;s Brain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lettvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
		<idno type="DOI">10.1109/JRPROC.1959.287207</idno>
		<idno type="arXiv">arXiv:1504.00702</idno>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4065609.SergeyLevine" />
	</analytic>
	<monogr>
		<title level="m">Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies</title>
				<meeting><address><addrLine>Chelsea Finn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1959-11">nov 1959. 2015</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1940" to="1951" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IRE</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Learning Overcomplete Representations</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015826</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<idno type="ISSN">0899- 7667</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000-02">feb 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Neuronal learning of invariant object representation in the ventral visual stream is not dependent on reward</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3786-11.2012</idno>
		<ptr target="http://www.jneurosci.org/content/32/19/6611.full" />
	</analytic>
	<monogr>
		<title level="m">The Neural Marketplace: I. General Formalismand Linear Theory</title>
				<imprint>
			<date type="published" when="2012-05">dec 2014. may 2012</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6611" to="6620" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.03640" />
		<imprint>
			<date type="published" when="2016-04">apr 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">How Important is Weight Symmetry in Backpropagation?</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1510.05067" />
		<imprint>
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Random feedback weights support learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cownden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">B</forename><surname>Tweed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.0247" />
		<imprint>
			<date type="published" when="2014-11">nov 2014</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Embedding multiple trajectories in simulated recurrent neural networks in a self-organizing manner</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Buonomano</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2358-09</idno>
		<ptr target="http://www.jneurosci.org/content/29/42/13172.short" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<idno type="ISSN">1529-2401</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page" from="13172" to="13181" />
			<date type="published" when="2009-10">oct 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">An Algorithm for Training Polynomial Networks</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1304.7045" />
		<imprint>
			<date type="published" when="2013-04">apr 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Visual Structure using Predictive Generative Networks</title>
		<author>
			<persName><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gabriel Kreiman</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06380" />
		<imprint>
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gabriel Kreiman</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Bayesian inference with probabilistic population codes</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName><surname>Pouget</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1790</idno>
		<ptr target="http://dx.doi.org/10.1038/nn1790" />
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<idno type="ISSN">1097-6256</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1432" to="1438" />
			<date type="published" when="2006-11">nov 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Real-time computing without stable states: a new framework for neural computation based on perturbations</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Markram</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976602760407955</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2531" to="2560" />
			<date type="published" when="2002-11">nov 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Computational aspects of feedback in neural circuits</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="DOI">http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0020165</idno>
		<ptr target="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0020165" />
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e165</biblScope>
			<date type="published" when="2007-01">jan 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Hippocampal time cells bridge the gap in memory for discontiguous events</title>
		<author>
			<persName><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">Q</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><forename type="middle">T</forename><surname>Lepage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName><surname>Eichenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="737" to="749" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03492</idno>
		<ptr target="http://arxiv.org/abs/1502.03492" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">A role for descending auditory cortical projections in songbird vocal learning</title>
		<author>
			<persName><forename type="first">Yael</forename><surname>Mandelblat-Cerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liora</forename><surname>Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Denisenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michale</forename><forename type="middle">S</forename><surname>Fee</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.02152</idno>
		<ptr target="http://elifesciences.org/content/3/e02152.abstract" />
		<imprint>
			<date type="published" when="2014-01">jan 2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e02152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Designing tools for assumption-proof brain mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">S</forename><surname>Marblestone</surname></persName>
		</author>
		<author>
			<persName><surname>Boyden</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2014.09</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<idno type="ISSN">1097-4199</idno>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1239" to="1241" />
			<date type="published" when="2014-09">sep 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">The Algebraic Mind: Integrating Connectionism and Cognitive Science</title>
		<ptr target="http://books.google.com/books?id=7YpuRUlFLm8C&amp;pgis=1" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">The birth of the mind: How a tiny number of genes creates the complexities of human thought</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Frequently asked question for: the atoms of neural computation</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Marblestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.8826" />
		<imprint>
			<date type="published" when="2014-10">oct 2014a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">The atoms of neural computation</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Marblestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1261661</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">6209</biblScope>
			<biblScope unit="page" from="551" to="552" />
			<date type="published" when="2014">2014b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Variability, compensation and homeostasis in neuron and network function</title>
		<author>
			<persName><forename type="first">Eve</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Goaillard</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn1949</idno>
		<ptr target="http://dx.doi.org/10.1038/nrn1949" />
	</analytic>
	<monogr>
		<title level="j">Nature reviews. Neuroscience</title>
		<idno type="ISSN">1471-003X</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="563" to="574" />
			<date type="published" when="2006-07">jul 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Proceedings of the National Academy of Sciences of the United States of America</title>
		<author>
			<persName><forename type="first">Clayton</forename><forename type="middle">E</forename><surname>David A Markowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><surname>Markram</surname></persName>
		</author>
		<author>
			<persName><surname>Lübke</surname></persName>
		</author>
		<author>
			<persName><surname>Frotscher</surname></persName>
		</author>
		<author>
			<persName><surname>Sakmann</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1504172112</idno>
		<idno>ISSN 0036-8075</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/8985014" />
	</analytic>
	<monogr>
		<title level="j">APs and EPSPs. Science</title>
		<idno type="ISSN">1091-6490</idno>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="213" to="215" />
			<date type="published" when="1997-01">aug 2015. jan 1997</date>
		</imprint>
	</monogr>
	<note>Regulation of synaptic efficacy by coincidence of postsynaptic</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Rodrigo Perin, Rajnish Ranjan, Imad Riachi, José-Rodrigo Rodríguez, JuanLuis Riquelme, Christian Rössert</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eilif</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaelw</forename><surname>Reimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwan</forename><surname>Abdellah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Car-Losaguado</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><surname>Alonso-Nanclares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Antille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selim</forename><surname>Arsever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guyantoineatenekeng</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><surname>Thomask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Bilgili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanassia</forename><surname>Buncic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Chalimourda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Denis</forename><surname>Chindemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Courcol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Delalondre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaul</forename><surname>Delattre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Druckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Dumusc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Dynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Eilemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaelemiel</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>Gevaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Ghobril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joew</forename><surname>Gidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etay</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><surname>Heinis</surname></persName>
		</author>
		<author>
			<persName><surname>Juanb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lida</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georges</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihwa</forename><surname>Khazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamesg</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pramod</forename><surname>Kisvarday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Kumbhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Vincent</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brunor</forename><forename type="middle">C</forename><surname>Lebé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Merchán-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjaminroy</forename><surname>Meystre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Morrice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Muñoz-Céspedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keerthan</forename><surname>Muralidhar</surname></persName>
		</author>
		<author>
			<persName><surname>Muthurasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylorh</forename><surname>Daniel Nachbaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Ovcharenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Palacios</surname></persName>
		</author>
		<author>
			<persName><surname>Pastor</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2015.09.029</idno>
		<ptr target="http://www.cell.com/article/S0092867415011915/fulltext" />
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<editor>Konstantinos Sfyrakis, Ying Shi, Ju-lianC. Shillcock, Gilad Silberberg, Ricardo Silva, Farhan Tauheed, Martin Telefont, Maria Toledo-Rodriguez, Thomas Tränkler, Werner VanGeit, JafetVillafranca Díaz, Richard Walker, Yun Wang, StefanoM. Zaninetta, Javier DeFelipe, SeanL. Hill, Idan Segev, and Felix Schürmann</editor>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="456" to="492" />
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
	<note>Reconstruction and Simulation of Neocortical Microcircuitry</note>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">A theory of cerebellar cortex. The Journal of physiology</title>
		<author>
			<persName><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969-06">jun 1969</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="437" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th . . . , 2011</title>
				<meeting>the 28th . . . , 2011</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">The visual word form area: expertise for reading in the fusiform gyrus</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Bruce D Mccandliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="293" to="299" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">Feature Discovery by Competitive Learning</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>James L Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Rumelhart</surname></persName>
		</author>
		<editor>DE Rumel hart-JL McClelland</editor>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="163" />
			<pubPlace>voi</pubPlace>
		</imprint>
	</monogr>
	<note>Parallel distributed processing</note>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Pitts</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02478259</idno>
		<ptr target="http://link.springer.com/10.1007/BF02478259" />
	</analytic>
	<monogr>
		<title level="j">The Bulletin of Mathematical Biophysics</title>
		<idno type="ISSN">0007-4985</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943-12">dec 1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">A cerebellar model for predictive motor control tested in a brain-based device</title>
		<author>
			<persName><forename type="first">Gerald</forename><forename type="middle">M</forename><surname>Jeffrey L Mckinstry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><surname>Krichmar</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0511281103</idno>
		<ptr target="http://www.pnas.org/content/103/9/3387.full#ref-4" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3387" to="3392" />
			<date type="published" when="2006-02">feb 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">The cognitive and neural development of face recognition in humans. The cognitive neurosciences</title>
		<author>
			<persName><forename type="first">Elinor</forename><surname>Mckone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1992">2009. 1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
	<note>BW Mel. The clusteron: toward a simple abstraction for a complex neuron</note>
</biblStruct>

<biblStruct xml:id="b216">
	<monogr>
		<title level="m" type="main">The magical number seven, plus or minus two: some limits on our capacity for processing information</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Ocular dominance column development: analysis and simulation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stryker</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.2762813</idno>
		<ptr target="http://www.sciencemag.org/content/245/4918/605.short" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="issue">4918</biblScope>
			<biblScope unit="page" from="605" to="615" />
			<date type="published" when="1989-08">aug 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Minsky. Plain talk about neurodevelopmental epistemology</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1994.6.1.100</idno>
		<ptr target="http://dspace.mit.edu/handle/1721.1/5763" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="126" />
			<date type="published" when="1977">jan 1994. 1977</date>
		</imprint>
	</monogr>
	<note>The Role of Constraints in Hebbian Learning</note>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Society Of Mind</title>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Minsky</surname></persName>
		</author>
		<ptr target="http://books.google.com/books/about/Society_Of_Mind.html?id=bLDLllfRpdkC&amp;pgis=1" />
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Simon and Schuster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<title level="m" type="main">Logical Versus Analogical or Symbolic Versus Connectionist or Neat Versus Scruffy</title>
		<author>
			<persName><forename type="first">Marvin</forename><forename type="middle">L</forename><surname>Minsky</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ojs/index.php/aimagazine/article/view/894" />
		<imprint>
			<date type="published" when="1991-06">jun 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<title level="m" type="main">Perceptrons: An Introduction to Computational Geometry</title>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsky</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Seymour</forename><surname>Papert</surname></persName>
		</author>
		<ptr target="https://books.google.com/books/about/Perceptrons.html?id=Ow1OAQAAIAAJ&amp;pgis=1" />
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Symmetric spike timingdependent plasticity at ca3-ca3 synapses optimizes storage and recall in autoassociative networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rajiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sooyun</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segundo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><surname>Jonas</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms11552</idno>
		<ptr target="http://dx.doi.org/10.1038/ncomms11552.Article" />
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Tom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research, Rutgers Univ. New Jersey</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">The emergence of hierarchical structure in human language</title>
		<author>
			<persName><forename type="first">Shigeru</forename><surname>Miyagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Berwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuo</forename><surname>Okanoya</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00071</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<idno type="ISSN">1664-1078</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2013-01">jan 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<ptr target="http://dx.doi.org/10.1038/nature14236" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02">feb 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553469</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=1553374.1553469" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning -ICML &apos;09</title>
				<meeting>the 26th Annual International Conference on Machine Learning -ICML &apos;09<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009-06">jun 2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Generalized role for the cerebellum in encoding internal models: evidence from semantic processing</title>
		<author>
			<persName><forename type="first">Torgeir</forename><surname>Moberget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Hilland Gullesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stein</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">B</forename><surname>Ivry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><surname>Endestad</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2264-13</idno>
		<ptr target="http://www.jneurosci.org/content/34/8/2871.short" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<idno type="ISSN">1529-2401</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2871" to="2878" />
			<date type="published" when="2014-02">feb 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.08731" />
		<imprint>
			<date type="published" when="2015-09">sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Discovery of complex behaviors through contact-invariant optimization</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main">Cortical and thalamic pathways for multisensory and sensorimotor interplay</title>
		<author>
			<persName><forename type="first">Micah</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">T</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Céline</forename><surname>Cappe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Rouiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Barone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Activity in the lateral prefrontal cortex reflects multiple steps of future events in action plans</title>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Mushiake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naohiro</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhiro</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuto</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tanji</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2006.03.045</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/16701212" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<idno type="ISSN">0896-6273</idno>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="631" to="641" />
			<date type="published" when="2006-05">may 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<title level="m" type="main">Neural Programmer: Inducing Latent Programs with Gradient Descent</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.04834" />
		<imprint>
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003037</idno>
		<ptr target="http://ai.stanford.edu/~ang/papers/icml00-irl.pdf" />
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553- 7358</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e1003037</biblScope>
			<date type="published" when="2000">apr 2013. 2000</date>
		</imprint>
	</monogr>
	<note>Icml</note>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09246</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b236">
	<monogr>
		<title level="m" type="main">Hippocampal place cells construct reward related sequences through unexplored space. eLife, 4: e06063</title>
		<author>
			<persName><forename type="first">Caswell</forename><surname>H Freyja Ólafsdóttir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><forename type="middle">B</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><forename type="middle">J</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><surname>Spiers</surname></persName>
		</author>
		<idno>doi: 10</idno>
		<imprint>
			<date type="published" when="2015-06">jun 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<monogr>
		<title level="m" type="main">Training recurrent networks online without backtracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><surname>Charpiat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07680</idno>
		<ptr target="http://arxiv.org/abs/1507.07680" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</title>
		<author>
			<persName><forename type="first">C H</forename><surname>B A Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Van Essen</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/8229193" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<idno type="ISSN">0270-6474</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4700" to="4719" />
			<date type="published" when="1993-11">nov 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<idno type="DOI">10.1038/381607a0</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/8637596" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996-06">jun 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research</title>
		<author>
			<persName><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0042-6989(97)00169-7</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0042698997001697" />
		<imprint>
			<date type="published" when="1997-12">dec 1997</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">What is the other 85% of v1 doing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Problems in Systems Neuroscience</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="182" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><surname>O'reilly</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1996.8.5.895</idno>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6796552" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="895" to="938" />
			<date type="published" when="1996-07">jul 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Biologically based computational models of high-level cognition</title>
		<author>
			<persName><forename type="first">C O'</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><surname>Reilly</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1127242</idno>
		<ptr target="http://www.sciencemag.org/content/314/5796/91" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">1095-9203</idno>
		<imprint>
			<biblScope unit="volume">314</biblScope>
			<biblScope unit="issue">5796</biblScope>
			<biblScope unit="page" from="91" to="94" />
			<date type="published" when="2006-10">oct 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</title>
		<author>
			<persName><forename type="first">C O'</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976606775093909</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/16378516" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="328" />
			<date type="published" when="2006-03">mar 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">Goal-Driven Cognition in the Brain: A Computational Framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hazy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prescott</forename><surname>Mollick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Mackie</surname></persName>
		</author>
		<author>
			<persName><surname>Herd</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1404.7591" />
		<imprint>
			<date type="published" when="2014-04">apr 2014a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<monogr>
		<title level="m" type="main">Learning Through Time in the Thalamocortical Loops</title>
		<author>
			<persName><forename type="first">C</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wyatte</surname></persName>
		</author>
		<author>
			<persName><surname>Rohrlich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1407.3432" />
		<imprint>
			<date type="published" when="2014-07">jul 2014b</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<title level="m" type="main">Computational cognitive neuroscience</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rc O'reilly</surname></persName>
		</author>
		<author>
			<persName><surname>Munakata</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><surname>Hazy</surname></persName>
		</author>
		<ptr target="grey.colorado.edu/mediawiki/sites/CompCogNeuro/images/a/a2/Cecn_oreilly_intro.pdf" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">The Inevitability of Probability: Probabilistic Inference in Generic Neural Networks Trained with Non-Probabilistic Feedback</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Emin</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1601.03060" />
		<imprint>
			<date type="published" when="2016-01">jan 2016</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">NMDA spikes enhance action potential generation during sensory input</title>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Shai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Harry L Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Paulsen</surname></persName>
		</author>
		<author>
			<persName><surname>Larkum</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn.3646</idno>
		<ptr target="http://dx.doi.org/10.1038/nn.3646" />
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<idno type="ISSN">1546-1726</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="383" to="390" />
			<date type="published" when="2014-03">mar 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Solving the problem of negative synaptic weights in cortical models</title>
		<author>
			<persName><surname>Parisien</surname></persName>
		</author>
		<author>
			<persName><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Eliasmith</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6796691" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Different time courses of learning-related activity in the prefrontal cortex and striatum</title>
		<author>
			<persName><forename type="first">Anitha</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature03287</idno>
		<ptr target="http://dx.doi.org/10.1038/nature03287" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="issue">7028</biblScope>
			<biblScope unit="page" from="873" to="876" />
			<date type="published" when="2005-02">feb 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<monogr>
		<title/>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00641</idno>
		<ptr target="http://arxiv.org/abs/1504.00641" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">A Probabilistic Theory of Deep Learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b254">
	<monogr>
		<title level="m" type="main">Tripartite synapses: astrocytes process and control synaptic information</title>
		<author>
			<persName><forename type="first">Gertrudis</forename><surname>Perea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Navarrete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfonso</forename><surname>Araque</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tins.2009.05.001</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/19615761" />
		<imprint>
			<date type="published" when="2009-08">aug 2009</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="421" to="431" />
		</imprint>
	</monogr>
	<note>Trends in neurosciences</note>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">The Leabra architecture: Specialization without modularity</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Jilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><forename type="middle">C</forename><surname>O'reilly</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X10001160</idno>
		<ptr target="http://journals.cambridge.org/abstract_S0140525X10001160" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<idno type="ISSN">0140-525X</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="286" to="287" />
			<date type="published" when="2010-10">oct 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">The principles of goal-directed decision-making: from neural mechanisms to computation and robotics</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Pezzulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F M J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Verschure</surname></persName>
		</author>
		<author>
			<persName><surname>Balkenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M A</forename><surname>Cyriel</surname></persName>
		</author>
		<author>
			<persName><surname>Pennartz</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2013.0470</idno>
		<ptr target="http://rstb.royalsocietypublishing.org/content/369/1655/20130470.short" />
	</analytic>
	<monogr>
		<title level="m">Philosophical transactions of the Royal Society of London. Series B, Biological sciences</title>
				<imprint>
			<date type="published" when="1655">1655. 20130470-, nov 2014</date>
			<biblScope unit="volume">369</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Triplets of spikes in a model of spike timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">Jean-Pascal</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wulfram</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="9673" to="9682" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">How the mind works</title>
		<author>
			<persName><surname>Pinker</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1749-6632.1999.tb08538.x/full</idno>
		<ptr target="http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.1999.tb08538.x/full" />
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName><forename type="first">T A</forename><surname>Plate</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.377968</idno>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=377968" />
	</analytic>
	<monogr>
		<title level="j">IEEE Neural Networks Council</title>
		<idno type="ISSN">1045-9227</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995-01">jan 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Rapid, parallel path planning by propagating wavefronts of spiking neural activity</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ponulak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2013.00098</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<idno type="ISSN">1662-5188</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2013-01">jan 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06434" />
		<imprint>
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Rajesh PN Rao. Bayesian computation in recurrent neural circuits</title>
		<author>
			<persName><forename type="first">Kanaka</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2004">2016. 2004</date>
		</imprint>
	</monogr>
	<note>Neuron</note>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">A Rasmus and M Berglund. Semi-Supervised Learning with Ladder Networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Rashevsky</surname></persName>
		</author>
		<idno>S0002-9904- 1939-06963-2 PII</idno>
		<ptr target="cc/paper/5947-semi-supervised-learning-with-ladder-networks" />
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2" to="9904" />
			<date type="published" when="1939">1939. 1939. 2015</date>
		</imprint>
	</monogr>
	<note>Advances in Neural .</note>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">The Role of Neural Mechanisms of Attention in Solving the Binding Problem</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">H</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Desimone</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0896-6273(00)80819-3</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0896627300808193" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="1999-09">sep 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<monogr>
		<title level="m" type="main">One-Shot Generalization in Deep Generative Models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1603.05106" />
		<imprint>
			<date type="published" when="2016-03">mar 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">The importance of mixed selectivity in complex cognitive tasks</title>
		<author>
			<persName><forename type="first">Mattia</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><forename type="middle">R</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Fusi</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature12160</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="issue">7451</biblScope>
			<biblScope unit="page" from="585" to="590" />
			<date type="published" when="2013-05">may 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Implications of neural networks for how we think about brain function</title>
		<author>
			<persName><surname>Da Robinson</surname></persName>
		</author>
		<ptr target="http://journals.cambridge.org/abstract_S0140525X00072563" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Attentiongated reinforcement learning of internal representations for classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pieter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjen</forename><surname>Roelfsema</surname></persName>
		</author>
		<author>
			<persName><surname>Van Ooyen</surname></persName>
		</author>
		<idno type="DOI">10.1162/0899766054615699</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/16105222" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2176" to="2214" />
			<date type="published" when="2005-10">oct 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Perceptual learning rules based on reinforcers and attention</title>
		<author>
			<persName><surname>Pr Roelfsema</surname></persName>
		</author>
		<author>
			<persName><surname>Van Ooyen</surname></persName>
		</author>
		<author>
			<persName><surname>Watanabe</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1364661309002617" />
	</analytic>
	<monogr>
		<title level="m">Trends in cognitive sciences</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">The mechanisms for pattern completion and pattern separation in the hippocampus</title>
		<author>
			<persName><forename type="first">Edmund</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnsys.2013.00074/abstract</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fnsys.2013.00074/abstract" />
	</analytic>
	<monogr>
		<title level="m">Frontiers in systems neuroscience</title>
				<imprint>
			<date type="published" when="2013-01">jan 2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">How attention can create synaptic tags for the learning of working memories in sequential tasks</title>
		<author>
			<persName><forename type="first">Sander</forename><forename type="middle">M</forename><surname>Jaldert O Rombouts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">R</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName><surname>Roelfsema</surname></persName>
		</author>
		<idno type="DOI">http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004060</idno>
		<ptr target="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004060" />
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e1004060</biblScope>
			<date type="published" when="2015-03">mar 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b273">
	<monogr>
		<title level="m" type="main">Learning with hidden variables. Current opinion in neurobiology</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Roudi</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0959438815001245" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Sparse coding via thresholding and local competition in neural circuits</title>
		<author>
			<persName><forename type="first">Don</forename><forename type="middle">H</forename><surname>Christopher J Rozell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><surname>Olshausen</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2008.03-07-486</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/18439138" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2526" to="2563" />
			<date type="published" when="2008-10">oct 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitzan</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liron</forename><surname>Sheintuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Ziv</surname></persName>
		</author>
		<title level="m">Hippocampal ensemble dynamics timestamp events in long-term memory. eLife</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">e12247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
		<ptr target="http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986-10">oct 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Neural constraints on learning</title>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">M</forename><surname>Patrick T Sadtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Quick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">C</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Tyler-Kabara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Batista</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature13665</idno>
		<ptr target="http://dx.doi.org/10.1038/nature13665" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="issue">7515</biblScope>
			<biblScope unit="page" from="423" to="426" />
			<date type="published" when="2014-08">aug 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Doubly distributional population codes: simultaneous representation of uncertainty and multiplicity</title>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2255" to="2279" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">A novel form of local plasticity in tuft dendrites of neocortical somatosensory layer 5 pyramidal neurons</title>
		<author>
			<persName><forename type="first">Maya</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Schiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<monogr>
		<title level="m" type="main">One-shot Learning with Memory-Augmented Neural Networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.06065" />
		<imprint>
			<date type="published" when="2016-05">may 2016</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6120" />
		<imprint>
			<date type="published" when="2013-12">dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Formal theory of creativity, fun, and intrinsic motivation</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Scellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5508364" />
	</analytic>
	<monogr>
		<title level="m">Autonomous Mental Development, IEEE</title>
				<imprint>
			<date type="published" when="2010">feb 2016. 19902010. 2010</date>
		</imprint>
	</monogr>
	<note>Towards a Biologically Plausible Backprop</note>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6803897" />
	</analytic>
	<monogr>
		<title level="m">TJ Sejnowski and H Poizner</title>
		<title level="s">Prospective Optimization. Proceedings of the</title>
		<imprint>
			<date type="published" when="2014">2015. 2014</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings of the</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">A feedforward architecture accounts for rapid categorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0700622104</idno>
		<ptr target="http://www.pnas.org/content/104/15/6424.long" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="6424" to="6429" />
			<date type="published" when="2007-04">apr 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Chunking as a mechanism of implicit learning</title>
		<author>
			<persName><forename type="first">-</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Experimental Psychology: Learning</title>
				<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Continuous attractors and oculomotor control</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(98)00064-1</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608098000641" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="1253" to="1258" />
			<date type="published" when="1998-10">oct 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0896-6273(03)</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1063" to="1073" />
			<date type="published" when="2003-12">dec 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Thalamic relays and cortical functioning</title>
		<author>
			<persName><forename type="first">Sherman</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0079-6123(05)49009-3</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/16226580" />
	</analytic>
	<monogr>
		<title level="j">Progress in brain research</title>
		<idno type="ISSN">1875-7855</idno>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="2005-01">jan 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">The thalamus is more than just a relay</title>
		<author>
			<persName><forename type="first">Sherman</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2007.07.003</idno>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<idno type="ISSN">0959-4388</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="422" />
			<date type="published" when="2007-08">aug 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<monogr>
		<title level="m" type="main">Multiple origins of neocortex: Contributions of the dorsal. The Neocortex: Ontogeny and Phylogeny</title>
		<author>
			<persName><forename type="first">Toru</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harvey</forename><forename type="middle">J</forename><surname>Karten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<monogr>
		<title level="m" type="main">Handbook of learning and approximate dynamic programming</title>
		<author>
			<persName><forename type="first">Jennie</forename><surname>Si</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<monogr>
		<title level="m" type="main">Ray Singh and Chris Eliasmith. Higher-dimensional neurons explain the tuning and dynamics of working memory cells. The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><forename type="middle">R</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0908193106</idno>
		<idno>doi: 10.1523/JNEUROSCI. 4864-05</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/16597721" />
		<imprint>
			<date type="published" when="2006-04">dec 2009. apr 2006. 2006</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="3667" to="3678" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
	<note>Phase-dependent neuronal coding of objects in short-term memory</note>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Spike-timing dependent plasticity</title>
		<author>
			<persName><forename type="first">Jesper</forename><surname>Sjöström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wulfram</forename><surname>Gerstner</surname></persName>
		</author>
		<idno type="DOI">10.4249/scholarpedia.1362</idno>
		<ptr target="http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity" />
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<idno type="ISSN">1941-6016</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1362</biblScope>
			<date type="published" when="2010-02">feb 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Preverbal infants identify emotional reactions that are incongruent with goal outcomes</title>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">E</forename><surname>Skerry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">S</forename><surname>Spelke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.11.002</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<idno type="ISSN">1873-7838</idno>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="204" to="216" />
			<date type="published" when="2014-02">feb 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<monogr>
		<title level="m" type="main">The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs. The Journal of Neuroscience</title>
		<author>
			<persName><surname>Wr Softky</surname></persName>
		</author>
		<author>
			<persName><surname>Koch</surname></persName>
		</author>
		<ptr target="http://www.jneurosci.org/content/13/1/334.short" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Cognitive consilience: primate non-primary neuroanatomical circuits underlying cognition</title>
		<author>
			<persName><forename type="first">Soren</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hout</forename><surname>Solari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Stoner</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnana.2011.00065/abstract</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fnana.2011.00065/abstract" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroanatomy</title>
		<idno type="ISSN">1662-5129</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2011-01">jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">Spiking neuron network Helmholtz machine</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2015</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<idno type="ISSN">1662-5188</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2015-01">jan 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<monogr>
		<title/>
		<idno type="DOI">10.3389/fncom.2015.00046/abstract</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fncom.2015.00046/abstract" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Memory systems of the brain: a brief history and current perspective</title>
		<author>
			<persName><surname>Larry R Squire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of learning and memory</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="177" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2670313" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">jan 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">Design Principles of the Hippocampal Cognitive Map</title>
		<author>
			<persName><surname>Kl Stachenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<monogr>
		<title level="m" type="main">Compositionality and biologically plausible models</title>
		<author>
			<persName><forename type="first">Terry</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<ptr target="http://philpapers.org/rec/STECAB-2" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Conditional routing of information to the cortex: a model of the basal ganglia&apos;s role in cognitive coordination</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Stocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0019077</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<idno type="ISSN">1939-1471</idno>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="541" to="574" />
			<date type="published" when="2010-04">apr 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Is backpropagation biologically plausible?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.1989.118705</idno>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=118705" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Deep homology of arthropod central complex and vertebrate basal ganglia</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Strausfeld</surname></persName>
		</author>
		<author>
			<persName><surname>Hirth</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1231828</idno>
		<ptr target="http://www.sciencemag.org/content/340/6129/157.short" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">1095-9203</idno>
		<imprint>
			<biblScope unit="issue">6129</biblScope>
			<biblScope unit="page" from="157" to="161" />
			<date type="published" when="2013-04">apr 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<monogr>
		<title level="m" type="main">A unified selection signal for attention and reward in primary visual cortex</title>
		<author>
			<persName><forename type="first">Liviu</forename><surname>Stnior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Van Der Togt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M A</forename><surname>Cyriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">R</forename><surname>Pennartz</surname></persName>
		</author>
		<author>
			<persName><surname>Roelfsema</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1300117110</idno>
		<imprint>
			<date type="published" when="2013-05">may 2013</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="9136" to="9141" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<monogr>
		<title level="m" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Planning to be surprised: Optimal bayesian exploration in dynamic environments</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial General Intelligence</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<monogr>
		<title level="m" type="main">Neural circuits as computational dynamical systems. Current opinion in neurobiology</title>
		<author>
			<persName><surname>Sussillo</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0959438814000166" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Generating coherent patterns of activity from chaotic neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><surname>Abbott</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0896627309005479" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">A neural network that finds a naturalistic solution for the production of muscle activity</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Mark M Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn.4042</idno>
		<ptr target="http://dx.doi.org/10.1038/nn.4042" />
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<idno type="ISSN">1546-1726</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1025" to="1033" />
			<date type="published" when="2015-07">jul 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<ptr target="http://machinelearning.wustl.edu/mlpapers/papers/icml2013_sutskever13" />
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
				<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Astrocyte calcium signaling transforms cholinergic modulation to cortical plasticity in vivo</title>
		<author>
			<persName><forename type="first">Norio</forename><surname>Takata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuneko</forename><surname>Mishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chihiro</forename><surname>Hisatsune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terumi</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ebisui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhiko</forename><surname>Mikoshiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Hirase</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<idno type="ISSN">1529-2401</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="18155" to="18165" />
			<date type="published" when="2011-12">dec 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02867</idno>
		<ptr target="http://jmlr.org/proceedings/papers/v28/tang13.html" />
	</analytic>
	<monogr>
		<title level="m">Tensor analyzers. Proceedings of The 30th International</title>
				<imprint>
			<date type="published" when="2013">2016. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b318">
	<monogr>
		<title level="m" type="main">Deep Mixtures of Factor Analysers</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1206.4635" />
		<imprint>
			<date type="published" when="2012-06">jun 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Learning the pseudoinverse solution to network weights</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Does the cost function of human motor control depend on the internal metabolic state?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aldo</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Faisal</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2202-12-S1-P99</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3240571/" />
	</analytic>
	<monogr>
		<title level="j">BMC Neuroscience</title>
		<idno type="ISSN">1471-2202</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">P99</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Suppl 1</note>
</biblStruct>

<biblStruct xml:id="b321">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Choo</surname></persName>
		</author>
		<title level="m">Symbolic reasoning in spiking neurons: A model of the cortex/basal ganglia/thalamus loop. 32nd Annual Meeting of the Cognitive Science Society</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">Toward the neural implementation of structure learning. Current opinion in neurobiology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gowanlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tervo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2016.01.014</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/26874471.GTesauro" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<idno type="ISSN">1873-6882</idno>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="99" to="105" />
			<date type="published" when="1995">feb 2016. 1995</date>
		</imprint>
	</monogr>
	<note>Temporal difference learning and TD-Gammon</note>
</biblStruct>

<biblStruct xml:id="b323">
	<monogr>
		<title level="m" type="main">Learning universal computations with spikes</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Thalmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Uhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilbert</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raoul-Martin</forename><surname>Memmesheimer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1505.07866" />
		<imprint>
			<date type="published" when="2015-05">may 2015</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">Emanuel Todorov. Cosine tuning minimizes motor errors</title>
		<author>
			<persName><surname>Tinbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1233" to="1260" />
			<date type="published" when="1965">1965. 2002</date>
		</imprint>
	</monogr>
	<note>Behavior and natural selection</note>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Efficient computation of optimal actions</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="11478" to="11483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">Optimal feedback control as a theory of motor coordination</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1226" to="1235" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">Function approximation in inhibitory networks</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2016</idno>
	</analytic>
	<monogr>
		<title level="m">Neural networks : the official journal of the International Neural Network Society</title>
				<imprint>
			<date type="published" when="2016-05">may 2016</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">Homeostatic synaptic plasticity: local and global mechanisms for stabilizing neuronal function</title>
		<author>
			<persName><forename type="first">Gina</forename><surname>Turrigiano</surname></persName>
		</author>
		<idno type="DOI">10.1101/cshperspect.a005736</idno>
	</analytic>
	<monogr>
		<title level="j">Cold Spring Harbor perspectives in biology</title>
		<idno type="ISSN">1943-0264</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5736</biblScope>
			<date type="published" when="2012-01">jan 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<monogr>
		<title level="m" type="main">From simple innate biases to complex visual concepts</title>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Harari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Dorfman</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1207690109</idno>
		<ptr target="http://www.pnas.org/content/109/44/18215.full" />
		<imprint>
			<date type="published" when="2012-10">oct 2012</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="18215" to="18220" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Learning by the dendritic prediction of somatic spiking</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Urbanczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Senn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2013.11.030</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/24507189" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<idno type="ISSN">1097-4199</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="528" />
			<date type="published" when="2014-02">feb 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">From neural PCA to deep unsupervised learning</title>
		<author>
			<persName><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. in Independent Component Analysis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<monogr>
		<title level="m" type="main">Pixel Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1601.06759" />
		<imprint>
			<date type="published" when="2016-01">jan 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">Simple rules can explain discrimination of putative recursive syntactic structures by a songbird species</title>
		<author>
			<persName><forename type="first">Caroline Aa</forename><surname>Van Heijningen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><forename type="middle">De</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carel</forename><surname>Ten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cate</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="20538" to="20543" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<monogr>
		<title level="m" type="main">Residual Networks are Exponential Ensembles of Relatively Shallow Networks</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.06431" />
		<imprint>
			<date type="published" when="2016-05">may 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Morphological evidence for a dopaminergic terminal field in the hippocampal formation of young and adult rat</title>
		<author>
			<persName><surname>Verney</surname></persName>
		</author>
		<author>
			<persName><surname>Baulac</surname></persName>
		</author>
		<author>
			<persName><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><surname>Vigny</surname></persName>
		</author>
		<author>
			<persName><surname>Helle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1039" to="1052" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main">Buffer loading and chunking in sequential keypressing</title>
		<author>
			<persName><forename type="first">Willem</forename><forename type="middle">B</forename><surname>Verwey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">544</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">Covert rapid action-memory simulation (CRAMS): a hypothesis of hippocampal-prefrontal interactions for adaptive behavior</title>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">L</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.nlm.2014.04.003</idno>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of learning and memory</title>
		<idno type="ISSN">1095-9564</idno>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="22" to="33" />
			<date type="published" when="2015-01">jan 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<monogr>
		<title level="m" type="main">Semantic Part Segmentation using Compositional Model combining Shape and Appearance</title>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6124" />
		<imprint>
			<date type="published" when="2014-12">dec 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<monogr>
		<author>
			<persName><forename type="first">Xiao-Jing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1093/med/9780199837755.003.0018</idno>
		<ptr target="http://oxfordindex.oup.com/view/10.1093/med/9780199837755.003.0018" />
		<title level="m">The Prefrontal Cortex as a Quintessential Cognitive-Type Neural Circuit : Principles of Frontal Lobe Function -oi</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<analytic>
		<title level="a" type="main">The representation of multiple objects in prefrontal neuronal delay activity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Melissa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhm070</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/17726003" />
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<idno type="ISSN">1047- 3211</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="1991-09">1991. sep 2007</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b341">
	<monogr>
		<title level="m" type="main">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<author>
			<persName><forename type="first">R</forename><surname>Melissa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">K</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1569-10</idno>
		<imprint>
			<date type="published" when="2010-11">nov 2010</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="15801" to="15810" />
		</imprint>
	</monogr>
	<note>Taskdependent changes in short-term memory in the prefrontal cortex</note>
</biblStruct>

<biblStruct xml:id="b342">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joschka</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2728" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<analytic>
		<title level="a" type="main">Hierarchical control using networks trained with higher-level forward models</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L F</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2163" to="2193" />
			<date type="published" when="2014-10">oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b344">
	<monogr>
		<title level="m" type="main">PJ Werbos. Applications of advances in nonlinear sensitivity analysis. System modeling and optimization</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Werbos</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0006203</idno>
		<ptr target="http://link.springer.com/chapter/10.1007/BFb0006203" />
		<imprint>
			<date type="published" when="1974">1974. 1982</date>
		</imprint>
	</monogr>
	<note>Beyond regression: New tools for prediction and analysis in the behavioral sciences</note>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName><surname>Pj Werbos</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=58337" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main">Learning curves for stochastic gradient descent in linear feedforward networks</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Werfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Sebastian</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976605774320539</idno>
		<ptr target="http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6790355" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2699" to="2718" />
			<date type="published" when="2005-12">dec 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<monogr>
		<title level="m" type="main">Memory Networks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.3916" />
		<imprint>
			<date type="published" when="2014-10">oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<monogr>
		<title level="m" type="main">Understanding Visual Concepts with Continuation Learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1602.06822" />
		<imprint>
			<date type="published" when="2016-02">feb 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
		<ptr target="http://link.springer.com/10.1007/BF00992696" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992-05">may 1992</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<monogr>
		<title level="m" type="main">Tight performance bounds on greedy policies based on imperfect value functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leemon C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Baird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Backpropagation of physiological spike trains in neocortical pyramidal neurons: implications for temporal coding in dendrites</title>
		<author>
			<persName><forename type="first">S R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G J</forename><surname>Stuart</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/11069929" />
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<idno type="ISSN">1529-2401</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="8238" to="8246" />
			<date type="published" when="2000-11">nov 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Endogenous cannabinoids mediate retrograde signalling at hippocampal synapses</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nicoll</surname></persName>
		</author>
		<idno>doi: 10.1038</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="issue">6828</biblScope>
			<biblScope unit="page" from="588" to="592" />
			<date type="published" when="2001-03">mar 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<monogr>
		<title level="m" type="main">The strong story hypothesis and the directed perception hypothesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winston</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Xie and HS Seung. Spike-based learning rules and stabilization of persistent neural activity</title>
		<author>
			<persName><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence J Sejnowski ; Reto</forename><surname>Wyss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul F M J</forename><surname>Verschure</surname></persName>
		</author>
		<idno type="DOI">http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0040120</idno>
		<idno>doi: 10.1371/journal.pbio.0040120</idno>
		<ptr target="http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0040120.X" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing</title>
				<imprint>
			<date type="published" when="2000">apr 2002. may 2006. 2000</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">e120</biblScope>
		</imprint>
	</monogr>
	<note>A model of the ventral visual system based on temporal stability and local memory</note>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Equivalence of backpropagation and contrastive Hebbian learning in a layered network</title>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename></persName>
		</author>
		<idno type="DOI">10.1162/089976603762552988</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/12590814" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="2003-02">feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<monogr>
		<title level="m" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01417</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Eight open questions in the computational modeling of higher sensory cortex</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="114" to="120" />
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<analytic>
		<title level="a" type="main">Using goal-driven deep learning models to understand sensory cortex</title>
		<author>
			<persName><surname>Dlk Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Dicarlo</surname></persName>
		</author>
		<ptr target="http://www.nature.com/neuro/journal/v19/n3/abs/nn.4244.html" />
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b360">
	<analytic>
		<title level="a" type="main">Opponent and bidirectional control of movement velocity in the basal ganglia</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Yttri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Dudman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="issue">7603</biblScope>
			<biblScope unit="page" from="402" to="406" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Joint attention without gaze following: Human infants and their parents coordinate visual attention to objects through eyehand coordination</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Yuste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">N</forename><surname>Maclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Lansner</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn1686</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/15928717" />
	</analytic>
	<monogr>
		<title level="j">Nature reviews. Neuroscience</title>
		<idno type="ISSN">1471-003X</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="477" to="483" />
			<date type="published" when="2005">2013. jun 2005</date>
		</imprint>
	</monogr>
	<note>PloS one</note>
</biblStruct>

<biblStruct xml:id="b362">
	<monogr>
		<title level="m" type="main">Reinforcement learning neural turing machines</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">Cell types in the mouse cortex and hippocampus revealed by single-cell RNAseq</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B M</forename><surname>Manchado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Codeluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lonnerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Manno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jureus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Munguba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Betsholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rolny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castelo-Branco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hjerling-Leffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Linnarsson</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaa1934</idno>
		<ptr target="http://science.sciencemag.org/content/347/6226/1138.abstract" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="issue">6226</biblScope>
			<biblScope unit="page" from="1138" to="1142" />
			<date type="published" when="2015-02">feb 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<analytic>
		<title level="a" type="main">Combining probabilistic population codes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1114" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b365">
	<analytic>
		<title level="a" type="main">A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<author>
			<persName><surname>Andersen</surname></persName>
		</author>
		<ptr target="https://cortex.vis.caltech.edu/Papers/PDFsofjournalarticles/Nature/V331_88.pdf" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
