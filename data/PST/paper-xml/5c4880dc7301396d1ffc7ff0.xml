<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intelligent character recognition using fully convolutional neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-17">17 December 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raymond</forename><surname>Ptucha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suhas</forename><surname>Pillai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frank</forename><surname>Brockler</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kodak Alaris</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vatsala</forename><surname>Singh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kodak Alaris</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Hutkowski</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kodak Alaris</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intelligent character recognition using fully convolutional neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-17">17 December 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">CE33136B31BF26C86D7DB2200FAE300F</idno>
					<idno type="DOI">10.1016/j.patcog.2018.12.017</idno>
					<note type="submission">Received 2 May 2018 Revised 4 December 2018 Accepted 15 December 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Handwriting recognition Fully convolutional neural networks Deep learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recognition of handwritten text is challenging as there are virtually infinite ways a human can write the same message. Deep learning approaches for handwriting analysis have recently demonstrated breakthrough performance using both lexicon-based architectures and recurrent neural networks. This paper presents a fully convolutional network architecture which outputs arbitrary length symbol streams from handwritten text. A preprocessing step normalizes input blocks to a canonical representation which negates the need for costly recurrent symbol alignment correction. When a lexicon is known, we further introduce a probabilistic character error rate to correct errant word blocks. Our multi-state convolutional method is the first to demonstrate state-of-the-art results on both lexicon-based and arbitrary symbol based handwriting recognition benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite a general shift towards a paperless world, there are still many instances where handwritten communications are prevalent. Be it traditional pen onto paper or stylus onto tablet, handwritten input is often either more convenient, efficient, or cost effective. Further, many documents such as invoices, taxes, and questionnaires often have fill-in-the-blank fields which need to be digitized. Different from online handwriting recognition which records sequences of strokes, offline handwriting recognition does not have temporal context.</p><p>Intelligent Character Recognition (ICR) is the task of deciphering digitized handwritten text. Although Optical Character Recognition (OCR) formally includes the optical scanning and deciphering of machine and human generated text, many researchers refer to OCR as the conversion of only machine generated imagery to text. Using this restricted definition of OCR, ICR is more difficult than OCR because no two handwritten symbols are identical. Like OCR, ICR systems first extract lines of text. Because of difficulty recognizing white space between handwritten blocks as well as the additional benefit of context, some ICR systems prefer to process entire lines of text at once. Other ICR systems first segment lines of text into word blocks. Each word block of symbols, often in con-text with surrounding lines of text, can be fed into lexicon-based recognition systems or split into slices for sequence recognition. Lexicon-based systems are very accurate, but constrain output to words in a training set. Sequence recognition has the difficult task of converting a string of symbol and blank predictions into a word without skipping or duplicating symbols. When extra context is known (surrounding symbols, fixed lexicon, phone number field), post processing can improve results significantly.</p><p>Convolutional Neural Networks (CNNs) have revolutionized the computer vision and pattern recognition community, and specifically offline handwriting recognition <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> . Jaderberg et al. <ref type="bibr" target="#b3">[4]</ref> used CNNs on OCR tasks in natural images. Poznanski and Wolf <ref type="bibr" target="#b4">[5]</ref> used deep CNNs for word recognition with fixed lexicons. Although this approach performs well in applications with fixed vocabulary, increasing the size of the vocabulary impacts performance. More importantly, applications with phone numbers, surnames, street addresses, etc. could have unbounded dictionaries.</p><p>Recurrent Neural Networks (RNNs), such as Long Short Term Memory (LSTM) units <ref type="bibr" target="#b5">[6]</ref> , split an image into segments and read it as a sequence of inputs <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> . Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b10">[11]</ref> further eliminates the need for precise alignment. Sun et al. <ref type="bibr" target="#b11">[12]</ref> , Shi et al. <ref type="bibr" target="#b12">[13]</ref> , Shi et al. <ref type="bibr" target="#b13">[14]</ref> , Wigington et al. <ref type="bibr" target="#b14">[15]</ref> , Wu et al. <ref type="bibr" target="#b15">[16]</ref> , Dutta et al. <ref type="bibr" target="#b16">[17]</ref> and Sueiras et al. <ref type="bibr" target="#b17">[18]</ref> use convolution and RNNs layers. Voigtlaender et al. <ref type="bibr" target="#b18">[19]</ref> additionally performed ICR at the paragraph level to include language context. Chen et al. <ref type="bibr" target="#b19">[20]</ref> use a variation of LSTM to do simultaneous handwriting recognition and writer recognition. Kumar et al. <ref type="bibr" target="#b20">[21]</ref> feed depth information from Leap Motion depth sensors into bidirectional LSTMs for 3D handwriting recognition.</p><p>Fully Convolutional neural Network (FCN) methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> take in arbitrary size images and output region level classification for simultaneous detection and classification. Handwritten word blocks have arbitrary length and can benefit from FCN methods. Concurrent with this research, Bai et al. <ref type="bibr" target="#b23">[24]</ref> has shown that fully convolutional methods can outperform recurrent networks on sequence modeling problems. Our research further shows that by using an initial CNN to calculate the number of symbols in a word block, word blocks can be resized to a canonical representation tuned to a FCN architecture. Knowing the average symbol width, this FCN can then perform accurate symbol prediction without CTC post processing.</p><p>This paper proposes a method to obtain character based classification without relying on predefined dictionaries or contextual information. We believe this to be the first FCN method that can reliably predict both arbitrary symbols as well as words from a lexicon. The novel contributions of this paper are: 1) Usage of a common CNN architecture for word identification, number of symbols in word block, and accurate symbol prediction; 2) Introduction of a probabilistic character error rate that calculates a word probability from a sequence of character probabilities; 3) Creation of a realistic block based dataset derived from the recently released NIST single character dataset; and 4) First Fully Convolutional method to demonstrate state-of-the-art results on both lexicon-based and arbitrary symbol based handwriting recognition benchmarks.</p><p>The paper is organized as follows: Section 2 outlines some of the related work, Section 3 describes the proposed fully convolutional approach, Section 4 discusses the results, and Section 5 contains concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In offline handwriting recognition, input features have traditionally been extracted from image data, then a classifier like Artificial Neural Network (ANN) or Gaussian Mixture Model (GMM), are used to estimate posterior probabilities <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> . These posterior probabilities are given as an input to a Hidden Markov Model (HMM) to generate transcriptions. One of the major disadvantages of HMMs is that they fail to model long term dependencies in input data. However, RNNs such as LSTM units <ref type="bibr" target="#b5">[6]</ref> can help to resolve this drawback. LSTMs have shown remarkable abilities in sequence learning tasks like speech recognition <ref type="bibr" target="#b27">[28]</ref> , machine translation <ref type="bibr" target="#b28">[29]</ref> , video summarization <ref type="bibr" target="#b29">[30]</ref> , and more.</p><p>Given a 2D image, a naive approach for offline recognition would be to take every column of an image as a 1D vector and feed it as an input to a RNN. Another way to tackle this problem is to use multidimensional RNNs, which take contextual information from several directions, e.g. left, right, top and bottom. The idea is to use both spatial and temporal information. To avoid issues related to text detection and segmentation, Wigington et al. <ref type="bibr" target="#b30">[31]</ref> uses a region proposal network to find regions of handwriting text on a page and then uses a line-follower algorithm to trace the handwriting across the page. These handwriting sequences are fed into a CNN-LSTM network. The use of CTC enables the use of inputs without any prior segmentation as opposed to forcefully aligning inputs in previous approaches <ref type="bibr" target="#b10">[11]</ref> . One of the major advantages of the CTC algorithm is that you do not need properly segmented labeled data. The CTC algorithm takes care of the alignment of input with the output. Doetsch et al. <ref type="bibr" target="#b9">[10]</ref> proposed hybrid RNN-HMM for English offline handwriting recognition. In order to get frame-wise labeling they applied HMM to the training data. These frames were used as an input to an RNN, with corresponding target labels. The system was trained to get posterior probabilities which generated emission probabilities for an HMM, which were used to generate transcription for a given input. They introduced a new technique of scaling gates of a LSTM memory cell by using a scalar multiple for every gate in each layer of the RNN. Bluche et al. <ref type="bibr" target="#b31">[32]</ref> compared Convolutional Neural Network (CNN) and traditional feature extraction techniques along with HMM for transcription.</p><p>One of the difficulties using a sliding window CNN approach is assigning labels to input sliding windows. GMM-HMM trained on handcrafted features are used to assign a label to the sliding window portion. Thus, the system can be trained end to end and the posterior probabilities can be used to estimate emission probabilities for the HMM, which outputs the final labeling sequence.</p><p>Almazan et al. <ref type="bibr" target="#b32">[33]</ref> described a common latent space where images of similar words and text strings lie close, while dissimilar words and strings are far apart. Almazan et al. <ref type="bibr" target="#b32">[33]</ref> used word attributes such as bigrams and trigrams to feed a pyramid histogram of characters. Motivated by Almazán et al. <ref type="bibr" target="#b32">[33]</ref> , CNNs can be used to extract n -grams which can be used for word-level recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> . Poznanski and Wolf <ref type="bibr" target="#b4">[5]</ref> used deep CNNs to extract n -grams which feed Canonical Correlation Analysis (CCA) for final word recognition. Although capable of state-of-the-art results, this method has limited symbol understanding and only works with fixed lexicons.</p><p>Wu et al. <ref type="bibr" target="#b15">[16]</ref> used character level convolutional methods in feed forward and and recurrent neural network configurations for handwritten Chinese character recognition, showing substantial improvements when combining neural networks with back-off Ngram language models. Liu et al. <ref type="bibr" target="#b34">[35]</ref> further benchmarked several methods on large Chinese datasets and Xie et al. <ref type="bibr" target="#b35">[36]</ref> used CNNs to feed a multi-layer LSTM network for handwritten Chinese character recognition. Several works <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b36">37]</ref> have shown that using CNNs to do feature extraction, followed by RNNs give superior results. Shi et al. <ref type="bibr" target="#b12">[13]</ref> , 14 ], Yin et al. <ref type="bibr" target="#b37">[38]</ref> and Su and Lu <ref type="bibr" target="#b38">[39]</ref> used CNN and RNN variations for text recognition in natural imagery. Wigington et al. <ref type="bibr" target="#b14">[15]</ref> used bidirectional LSTMs along with normalization and augmentation. Dutta et al. <ref type="bibr" target="#b16">[17]</ref> used similar a similar approach to study the impact of synthetic data for augmentation, domain specific transfer learning, and slant correction, showing all three have strong impact on resulting error reduction. Voigtlaender et al. <ref type="bibr" target="#b18">[19]</ref> used alternating layers of convolution followed by multidimensional RNNs.</p><p>Pham et al. <ref type="bibr" target="#b8">[9]</ref> proposed Multidimensional RNN using dropout to improve offline handwriting recognition performance. RNNs with dropout prevent over fitting on the training set, similar to regularization. Yadav et al. <ref type="bibr" target="#b39">[40]</ref> and Ahmed et al. <ref type="bibr" target="#b40">[41]</ref> use convolutional neural networks and data augmentation to predict characters for text recognition in images. Similarly, Balci et al. <ref type="bibr" target="#b41">[42]</ref> use CNNs for character classification and LSTMs for character segmentation. Deep CNNs for offline handwriting recognition have also been used for languages other than English. Dewan and Chakravarthy <ref type="bibr" target="#b42">[43]</ref> and Xie et al. <ref type="bibr" target="#b35">[36]</ref> used CNNs for offline character recognition of Telugu and Chinese recognition respectively. Dewan and Chakravarthy <ref type="bibr" target="#b42">[43]</ref> used auto encoders, where the model was trained in a greedy layer wise fashion to learn weights in an unsupervised fashion, then fine-tuned with supervised data. Xie et al. <ref type="bibr" target="#b35">[36]</ref> used fully convolutional CNNs to feed a multi-layer LSTM network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This work focuses on extracted handwritten symbol blocks. An input is defined as a tightly cropped grayscale image of an arbitrary 1D sequence of symbols. We use the word symbol to emphasize that the model is not limited to Latin based characters. Neither spaces between characters, nor the need to predict space symbols between characters are required in this work. Although preprocessing such as contrast normalization and deslanting have shown to be effective for handwriting recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44]</ref> , we can achieve on par or better results by omitting these sometimes costly operations.</p><p>Our method is similar to sliding window approaches used by Menasri et al. <ref type="bibr" target="#b6">[7]</ref> , Bluche et al. <ref type="bibr" target="#b7">[8]</ref> , Pham et al. <ref type="bibr" target="#b8">[9]</ref> , Doetsch et al. <ref type="bibr" target="#b9">[10]</ref> and Sueiras et al. <ref type="bibr" target="#b17">[18]</ref> , while striving to remove the need for multi-direction scanning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> . Further, our method removes the need for a pre-defined lexicon <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> , or segmentation boundaries for symbol cleanup <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> .</p><p>The algorithm consists of four consecutive stages which will be described next:</p><p>1. An optional Lexicon CNN ( Section 3.4 ) may be used to detect common words (such as "the", "her", "and", ...); 2. If a common word is not found, or step '1.' is skipped, a Block Length CNN ( Section 3.1 ) predicts the number of symbols in a block and resamples the block to a canonical width × height; 3. Symbol prediction is made using a FCN called Symbol Prediction FCN ( Section 3.2 ); 4. An optional vocabulary matching ( Section 3.3 ) is done if word blocks are from a known vocabulary.</p><p>All CNNs utilize the feature extraction portion of the common architecture as shown in Fig. <ref type="figure" target="#fig_0">1</ref> . The details of these CNNs will be described in Sections 3.1 -3.4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Symbol alignment</head><p>Early versions of the model were inspired by prior CTC approaches, but models proved surprisingly difficult to converge, often achieving poor accuracies ( &gt; 50% CER). Ideally, a model could predict symbols equally distributed across the word block, each symbol separated by a blank. However, word block samples have arbitrary width, causing an inconsistent distribution of the blanks during training. Additionally, some dataset samples are so small, the number of possible predictions are smaller than the number of symbols in the label. Inspired by Jaderberg et al. <ref type="bibr" target="#b3">[4]</ref> , both problems are solved by resizing input word blocks to 32 × 128, then passing this resized image to a Block Length CNN which estimates the number of symbols N in a word block. Using N , each sample is resized to 32 × 16 N , then passed to the Symbol Prediction FCN. This allows (2 N + 1) predictions per sample, where each symbol prediction is interleaved with blank predictions. To accommodate  <ref type="bibr" target="#b31">(32)</ref>. Batch-Norm and ReLU activation are used after every layer. Maxout is used over two inputs (C(64) requires computation of 128 filters before the Maxout). The network has 32 classes, enabling word length prediction lengths of 1-32. Empirical studies showed this discrete approach has a small improvement over a linear regression based network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Symbol prediction</head><p>The core of the algorithm lies in the character prediction model called the Symbol Prediction FCN. This model consists of a FCN model similar to the Block Length CNN, but with symbols as classification labels. The output of the Block Length CNN step creates word blocks of size 32 × 16 N . These resampled blocks are passed into the Symbol Prediction FCN where the fully convolutional nature results in the number of output predictions being dependent on the predicted block length N . Both the ground truth labels and outputs of the Symbol Prediction FCN  prediction, where 111 includes 110 unique symbols shown in Table <ref type="table" target="#tab_0">1</ref> (which includes upper and lower English and French alphabets, digits, and special characters) plus blank symbol. From these (2 N + 1) predictions, the N desired outputs are interleaved by N + 1 blank predictions which can be discarded.</p><formula xml:id="formula_0">j k l m n o p q r s t u v w x y z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ! " # $ % &amp; ' ( ) * + , - . / : ; &lt; = &gt; ? @ [ \ ] ^ _ ' { | } ˜ É à â ç è é ê ë î ï ð ò ô û ù × × × × × × × × ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Even filter intuition</head><p>The Symbol Prediction FCN outputs a string of symbols and blanks. Aligning the output of the Symbol Prediction FCN with ground truth symbols is critical to performance. After the last max pool operation, the filtered bank of images is 4 × 2 N × 512 . We want to extract stroke information across the word block in the 2 N direction. Fig. <ref type="figure" target="#fig_2">3</ref> (top) pictorially demonstrates the layout of a word block at this layer using a word block with 4 symbols. Fig. <ref type="figure" target="#fig_3">4</ref> (left) demonstrates the receptive field of a two tap even filter as it steps across the input word block of width 2 N . On average, the filter is either centered on a symbol or a blank.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> (right) demonstrates the receptive field of a four tap even filter as it steps across the same word block. Once again, on average, the filter is centered on a symbol or a blank. Padding is needed here to allow filtering on left-most and right-most symbols. In general for even tap filters, a pad of F w / 2 -1 is required on either side of the block word, where F w is the width of the filter.</p><p>Fig. <ref type="figure" target="#fig_5">5</ref> (left) demonstrates the receptive field of the more common three tap odd convolution filter as it steps across the same input word block. The filter is never perfectly centered on a symbol. Fig. <ref type="figure" target="#fig_2">3</ref> (bot) shows an alternate representation for input images. If we scale input images to 24 N instead of 16 N , for a N = 4 word block as in Fig. <ref type="figure" target="#fig_2">3</ref> , we get a word block width of 3 N . Using this word block width of 3 N as input, Fig. <ref type="figure" target="#fig_5">5</ref> (right) demonstrates the receptive field of a three tap odd filter as it steps across the word block. This does allow perfect centering of the filter on top of the symbol, but now there are two blanks (instead of one blank for even filters) between each symbol. We choose the even tap configuration to center the filter over symbols and minimize blank predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Filter receptive field</head><p>The symbols in the example word block in Fig. <ref type="figure" target="#fig_2">3</ref> are not of equivalent width (for example, the letter "m" takes up twice as much real estate as the letter "e"). Although, on average, even tap convolution filters alternately align on symbol and blanks, it can be seen perfect alignment requires each symbol to be of identical width. Wider filters ensure the complete symbol is in the receptive field as the filter steps across the word block. With regard to the Symbol Prediction FCN, the activation maps passed into the dotted box of Fig. <ref type="figure" target="#fig_0">1</ref>  The input to fc_2 is 3 × (2 N + 1) × 1024 . The 3 × 9 filter both reduces the output height to 1, while having a receptive field that is tolerant to left-to-right blank alignment errors. Similar to the processing of fc_final, if no padding were used, each of the 1024 filters in fc_2 would generate a 1 × (2 N -7) × 1 image. To maintain the desirable width of 2 N + 1 , a 0 × 4 pad is used, adding 0 rows to top/bottom and 4 columns to both left/right. The output of fc_2 is 1 × (2 N + 1) × 1024 which is passed into FC_classify which is a 1024 × 111 layer, giving 2 N + 1 predictions, or N symbol predictions, each being surrounded by a blank. Each symbol predictions has 111 output nodes, corresponding to 110 symbols and a blank symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">CER and vocabulary matching</head><p>The results are reported using normalized character error rate defined in <ref type="bibr" target="#b0">(1)</ref> .</p><formula xml:id="formula_1">CER = R + D + I R + D + I + C (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Where R is the number of characters replaced, D is the number of characters deleted I is the number of characters inserted, and C is the number of correct characters. With respect to an actual vs. predicted word, a dynamic programming grid, (2) describes the CER computation where C 0 , 0 = 0 and CER = C h, where h is the length of the prediction and is the length of the label. p i is the i th character of the prediction and L j is the jth character of the label.</p><formula xml:id="formula_3">C i, j = min (C i -1 , j + 1 , C i, j-1 + 1 , Diag)<label>(2)</label></formula><p>where:</p><formula xml:id="formula_4">Diag = C i -1 , j-1 , if p i = L j C i -1 , j-1 + 1 , otherwise</formula><p>To improve performance in applications that have a knownlimited vocabulary, we applied a CER-based vocabulary matching system using dynamic programming along with (3) . We define V as the vocabulary set and W ( p ) as the word prediction based on the sequence prediction p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W (p) = arg min</head><formula xml:id="formula_5">L ∈ V CER (p, L ) (3)</formula><p>The above method improved CER significantly, but discards most of the information computed from the neural network. An improvement of the above algorithm, we refer to as probabilistic CER, uses character probabilities instead of simply the top character prediction. Eq. ( <ref type="formula">4</ref>) describes the probabilistic CER based on (3) . P (p i = L j ) is the probability that the character at p i is equal to L j .</p><p>C i, j = min C i -1 , j + 1 -P (p i = L j ) ,</p><formula xml:id="formula_6">C i, j-1 + 1 -P (p i = blank ) , C i -1 , j-1 + 1 -P (p i = L j ) (4)</formula><p>Given this method which computes word probabilities from sequence probabilities, we can combine multiple predictions into the same system. We use the frequency of occurrence of a given word ( C ( L )) to further improve the vocabulary matching using <ref type="bibr" target="#b4">(5)</ref> .</p><formula xml:id="formula_7">W (p) = arg min L ∈ V CER (p, L ) + 1 1 + C(L ) (5)</formula><p>As this method computes a probability of matched words in a vocabulary, it is possible to limit the word replacement only if the probability exceeds a certain threshold. This allows the system to predict words it has never seen before. Along with the CER we also report the Word Error Rate (WER) which determines the average word-level accuracy of a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Lexicon based prediction</head><p>The described system is susceptible to low prediction accuracy when short common words, or special characters (",", ".", ...) are input. To improve performance, we optionally allow a Lexicon CNN that predicts a word from a lexicon. The input to this network is a 32 × 128 sample. The network produces a confidence level for each one of the words present in the lexicon. Starting with the input layer, the vocabulary based prediction architecture is C(6 4)-C(6 4)-C(6 4)-P(2)-C(128)-C(128)-P(2)-C(256)-C(256)-P(2)-C(512)-C(512)-Prediction VP , where Prediction VP is -FC(2048)-Dropout(0.5)-FC( V ), where V is the lexicon size. Since some entries on the lexicon only appear once on each dataset, we limit predictions from this model by limiting lexicon size and requiring a minimum confidence for a prediction to be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We used Caffe <ref type="bibr" target="#b46">[47]</ref> to perform all experiments. Custom layers were implemented in Python to handle labels. We used momentum optimization using a learning rate of 0.01 decreasing by 10 every 25,0 0 0 iterations, where an iteration is a mini-batch. The batch size of each iteration was 64. We also used L2-regularization with λ = 0 . 0025 . The Block Length CNN and the Symbol Prediction FCN were trained in a combined dataset before fine-tuning in the reported datasets. A batch from this combined dataset contains 24 samples from the IAM dataset, 24 samples from the RIMES dataset, and 16 samples generated from the NIST Special Database 19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Results are demonstrated on the IAM <ref type="bibr" target="#b45">[46]</ref> , RIMES <ref type="bibr" target="#b47">[48]</ref> , and NIST <ref type="bibr" target="#b48">[49]</ref> offline handwritten datasets. The IAM dataset contains 115,320 English words, mostly cursive, by 500 authors. This dataset includes training, validation, and test splits, where an author contributing to a training set, cannot occur in the validation or test split. The RIMES dataset contains 60,0 0 0 French words, by over 10 0 0 authors. There are several versions of the RIMES dataset, where each newer release is a super-set of prior releases. We utilize the ICDAR 2011 release. Published train and test splits are used for all tests.</p><p>The NIST Handprinted Forms and Characters Database, Special Database 19, contains NIST's entire corpus of training materials for handprinted document and character recognition. Each participant filled out one or more pages of the NIST Form-based Handprint Recognition System. It publishes Handprinted Sample Forms from 810,0 0 0 character images, by 3600 participants. We used a lexicon of 150k words and 250k numbers (3-16 digits) as labels for samples generated. To generate a sample we concatenate individual characters from the NIST dataset to create a sequence of characters. Gaussian noise as well as random padding and margins between characters was added to increase variations. Specifically, characters were bottom aligned with a random vertical offset (Between -10 and 10 pixels); 0-16 white pixels were randomly added be-  Predictions obtained with the symbol sequence prediction model on the IAM dataset. The third example has a questionable ground truth and a prediction that could be considered valid out of context. To obtain a fair comparison with other methods we did not alter the ground truth in anyway.</p><p>tween characters; Gaussian Blur with radius 1 was added after the word has been assembled; and finally we pad each sample with 0-3 white pixels in each dimension. The participants were divided into training and test set in a 9/1 ratio. Samples of this generated dataset can be seen in Fig. <ref type="figure" target="#fig_6">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">IAM Results</head><p>We first test our system on the IAM English handwritten dataset. Our model achieves CER of 4.70% (8.22% WER) on IAM. Table <ref type="table" target="#tab_1">2</ref> shows that our generic model is quite competitive against the current leaders of this dataset. Table <ref type="table" target="#tab_2">3</ref> shows randomly selected samples. In general, we note most errors are on lower case letters as numbers and upper case letters are close to 99% accuracy.</p><p>Kozielski et al. <ref type="bibr" target="#b43">[44]</ref> used HMMs. Dreuw et al. <ref type="bibr" target="#b49">[50]</ref> and Boquera et al. <ref type="bibr" target="#b50">[51]</ref> use a hybrid neural network and Hidden Markov Model HMM approach. Dreuw et al. <ref type="bibr" target="#b49">[50]</ref> showed that Gaussian HMMs outperform HMMs. Bluche et al. <ref type="bibr" target="#b7">[8]</ref> used Gaussian HMMs to initialize neural networks and showed that both deep CNNs and RNNs could produce state of the art results. Doetsch et al. <ref type="bibr" target="#b9">[10]</ref> uses a custom LSTM topology along with CTC alignment. Bluche and Messina <ref type="bibr" target="#b36">[37]</ref> use a CNN encoder along with a novel bidirectional LSTM which uses convolutional gates. Doetsch et al. <ref type="bibr" target="#b9">[10]</ref> and Bluche and Messina <ref type="bibr" target="#b36">[37]</ref> used all words in a sentence and paragraph respectively to provide word context. Doetsch et al. <ref type="bibr" target="#b9">[10]</ref> results are based upon the more difficult full line recognition, rather than single word block recognition. Poznanski and Wolf <ref type="bibr" target="#b51">[52]</ref> used deep CNNs to extract n -gram attributes which feed CCA word recognition. Dutta et al. <ref type="bibr" target="#b16">[17]</ref> uses convolutional features fed into a bidirectional LSTM and CTC. Their method uses synthetic data for pretraining, image normalization, deslanting and train and test time augmentation. Doetsch et al. <ref type="bibr" target="#b43">[44]</ref> , Poznanski and Wolf <ref type="bibr" target="#b51">[52]</ref> , Doetsch et al. <ref type="bibr" target="#b9">[10]</ref> , Espana-Boquera et al. <ref type="bibr" target="#b50">[51]</ref> and Dutta et al. <ref type="bibr" target="#b16">[17]</ref> use deslanting, training augmentation, and an ensemble of test samples.</p><p>Our work uses a lexicon CNN of 1100 words and a minimum confidence of 70%. The symbol CNN uses 110 symbols, and we use probabilistic CER correction. We did not preprocess with a deslanting algorithm, and no train or test sample augmentation was used, but we did pretrain the model with samples from RIMES and NIST. Aside from the probabilistic CER correction, no CTC alignment or CCA post correction was applied. Although our competitive results are not ranked the best, our processing path can work at both the symbol (i.e. will work just as well on street address or phone number) and lexicon (words from a dictionary) level, and we include substantially more symbols (110) than prior methods (e.g. Poznanski and Wolf <ref type="bibr" target="#b51">[52]</ref> can only recognize upper and lower case Latin alphabet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">RIMES results</head><p>On RIMES dataset our model obtained a 2.46% CER which is among the state of the art on the RIMES challenge. Table <ref type="table" target="#tab_3">4</ref> shows the performance of our model against the current leaders of this dataset. Stunner et al. <ref type="bibr" target="#b52">[53]</ref> uses multiple character prediction models along with multiple language models.</p><p>For the RIMES dataset we used a vocabulary of the top 500 words from the training set with a minimum confidence level of 70%. Table <ref type="table" target="#tab_4">5</ref> shows examples of predictions obtained on the RIMES dataset using the symbol sequence prediction. An interesting result of our vocabulary independent symbol CNN model is that the model reads characters literally. In general, errors can be attributed to character ambiguity, segmentation artifacts (Sample "effet" contains a comma even though it isn't part of the label), or character   overlap (Sample "vous"). The last three rows of Table <ref type="table" target="#tab_4">5</ref> contain examples of good predictions. For these examples the sequence predictions obtained perfect predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>To gain insight into which portions of our model contribute most to overall performance, ablation studies were performed as shown in Table <ref type="table" target="#tab_5">6</ref> . For example, our generically trained models, while not as good as those fine-tuned for a particular dataset performed quite well. We also see the vocabulary matching with probabilistic CER is more important than pre-processing with a Lexicon CNN, but using both performs best results. All models are capable of detecting 110 unique symbols and output words are not constrained to a pre-defined lexicon.</p><p>We have found the size of the lexicon used by the Lexicon CNN to be application dependent. For example, insurance forms require large dictionaries, where tax forms use much smaller dictionaries. In our tests, we have restricted our Lexicon CNN to less than 20 0 0 words as the fully convolutional Symbol Prediction FCN is performing very well. The exploration of larger lexicons, such as the 90k size used by Jaderberg et al. <ref type="bibr" target="#b3">[4]</ref> is an area of future research.</p><p>To gain insight as to what the filters in the CNN are learning, Fig. <ref type="figure" target="#fig_7">7</ref> shows randomly selected activation maps before and after the first pooling operation. We see several variants of edge based filtering operations and note the abstraction of content after pooling. We have performed limited studies using higher resolution, and find that increasing the height of words from 32 to 64 gains minor improvement.</p><p>This work demonstrates how to replace recurrent neural networks with fully convolutional methods when processing variable length temporal streams of offline handwriting imagery. These streams are firstly broken into their constituent parts, where each part is measured in length and resampled to a canonical representation compatible with a fully convolutional network. This divide and conquer fully convolutional approach is input length agnostic, and does not suffer from exploding or vanishing gradients. For example, if we contrast our method to Voigtlaender et al. <ref type="bibr" target="#b18">[19]</ref> , which uses deslanting preprocessing, followed by alternating layers of convolution and multidimensional RNNs, then by CTC, our method requires fewer computational resources, yet yields similar results. The method of Poznanski and Wolf <ref type="bibr" target="#b4">[5]</ref> uses simple CNNs to achieve better results than our method, but can only predict words from a predetermined lexicon, only recognizes letters A-Z (as compared to our 110 symbols), and uses compute intensive deslanting preprocessing and CCA post processing. The top performing Dutta et al. <ref type="bibr" target="#b16">[17]</ref> uses both convolutional and bidirectional LSTM. They use test time augmentation along with deslanting preprocessing and CTC post processing. To avoid the usage of CTC, Battenberg et al. <ref type="bibr" target="#b53">[54]</ref> and Sueiras et al. <ref type="bibr" target="#b17">[18]</ref> utilize the intuitively simple encoderdecoder sequence-to-sequence <ref type="bibr" target="#b28">[29]</ref> models which firstly encode an entire stream of data, then decode one character at a time until the end of the utterance. Although extremely versatile, when these sequence-to-sequence methods process long streams of data, they are subject to gradient propagation issues.</p><p>Other applications that may benefit from our approach include speech recognition <ref type="bibr" target="#b27">[28]</ref> , image tagging <ref type="bibr" target="#b54">[55]</ref> , video captioning <ref type="bibr" target="#b55">[56]</ref> , sign language translation <ref type="bibr" target="#b56">[57]</ref> , music composition <ref type="bibr" target="#b57">[58]</ref> , and genome sequencing <ref type="bibr" target="#b58">[59]</ref> . For example, in the DeepSpeech speech recognition method <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60]</ref> , a recurrent neural network converts raw speech into character streams. These character streams use CTC to reduce streams of characters to plausible words. Our alternate approach to ASR would eliminate the need for CTC. A CNN similar to our Lexicon CNN can decipher common words. If a common word is not detected, a character length CNN similar to our Block Length CNN resamples the stream to a fixed number of samples per character. Once in this canonical representation, a length agnostic fully convolutional character prediction network similar to our Symbol Prediction FCN converts the audio stream to a stream of characters which represent the spoken words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a novel offline handwriting recognition algorithm by utilizing a standard convolutional network to firstly measure and then resample an input stream to a canonical representation. This canonical representation is suitable for processing by a custom fully convolutional network for precise character prediction. This character prediction embraces a large symbol set with no lexicon constraints.</p><p>Our method provides an alternate approach to the popular strategy of using recurrent neural networks followed by connectionist temporal classification for character cleanup. Although connectionist temporal classification avoids the need for properly segmented labeled data, it can be difficult to tune. Unlike lexiconbased methods, our method can recognize common words as well as infinite symbol blocks such as surnames, phone numbers, and acronyms. The pairing of word block length prediction along with a family of even convolution filters enable accurate symbol alignment. Although popular in signal processing, the usage of even convolution filters in convolutional neural networks is unusual. For sequences separated by blank spaces, we demonstrate even tap filters align output predictions precisely with input sequences without the need for extra padding or special treatment of edge pixels.</p><p>Rather than constrain predictions to simple alphanumeric or even only lower-case alphanumeric symbols, our method is shown to be robust to large symbol sets. We utilize a set of 110 symbols, which includes virtually all punctuation as well as special Latin characters to recognize both common words as well as virtually any symbol block. Despite avoiding the usage of recurrent neural networks, connectionist temporal classification, small symbol sets, and dependencies on fixed lexicons, our method achieves state-ofthe-art results on the English-based IAM and French-based RIMES lexicon datasets. We further demonstrate how our method can be used to advance the related field of automatic speech recognition, as well as several other recurrent neural network applications which generate sequences of symbols. Future work includes experimentation with larger contextual filters, addition of sentence-level features such as sent2vec, introduction of hierarchical processing to gain knowledge at the symbol, word, sentence, and paragraph level, usage of fully convolutional recurrent networks, as well as the demonstration of these methods to more languages, resource constrained languages, and other recurrent applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Baseline deep CNN used. Input image is 32 × 128 for Lexicon and Block Length CNNs. Input image is resampled to 32 × 16 N for the fully convolutional Symbol Prediction FCN, where N is the estimated number of symbols in the word block from the Block Length CNN. Figure best viewed at 200% magnification.</figDesc><graphic coords="3,35.03,57.21,516.00,174.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Sample label alignment and correct prediction for the label "meet". Sample from the IAM<ref type="bibr" target="#b45">[46]</ref> dataset. Note that the FCN has overlapping filters so this does not indicate receptive field of each prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Input sample representations (left) and their corresponding sizes after last pooling step (right) for input image representations of 32 × 16 N (top) and 32 × 24 N (bot) for a 4-symbol word block.</figDesc><graphic coords="4,44.84,155.51,516.00,349.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (left) 2-wide even filter on 16 N original. (right) 4-wide even filter on 16 N original.</figDesc><graphic coords="5,35.03,57.71,516.00,307.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>are 4 × 2 N</head><label>2</label><figDesc>× 512. fc_final receives the 4 × 2 N × 512 buffer and applies 1024 filters, each being 4 × 4 × 512. If applied without padding, each of the 1024 filters would generate a 1 × (2 N -3) × 1 activation map. By adding a 1 × 2 pad, each of the 1024 filters generates a 3 × (2 N + 1) activation map. The single row pad on top and bottom provides robustness to vertical registration error. The double column pad to left and right ensure the even filters are centered on symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (left) 3-wide odd filter on 16 N original word block; (right) 3-wide odd filter on 24 N original word block.</figDesc><graphic coords="6,44.84,57.32,516.24,258.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Sample word blocks generated from the NIST derived dataset. Reading from left-right, top-bottom: 1189940230, Symbiotic, 6790, 3420, 4495897556, Specialists, Limpopo, 6866026774, BLASTED, DEPRESSIVE.</figDesc><graphic coords="7,35.03,57.72,516.00,115.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Sample activation maps from the CNN. Top shows input 32 × 128 image. Next two rows show sample activations before the first pooling. Bottom two rows show sample activation maps after the first pooling operation (magnified by 2 × ).</figDesc><graphic coords="8,104.84,57.66,396.48,191.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>110 symbols used in models.</figDesc><table><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>a</cell><cell>b</cell><cell>c</cell><cell>d</cell><cell>e</cell><cell>f</cell><cell>g</cell></row><row><cell>h</cell><cell>i</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparison of results on IAM dataset to previous methods.</figDesc><table><row><cell>Model</cell><cell>WER</cell><cell>CER</cell></row><row><cell>Dreuw et al. [50]</cell><cell>18.8</cell><cell>10.1</cell></row><row><cell>Boquera et al. [51]</cell><cell>15.5</cell><cell>6.90</cell></row><row><cell>Kozielski et al. [44]</cell><cell>13.30</cell><cell>5.10</cell></row><row><cell>Bluche et al. [8]</cell><cell>11.90</cell><cell>4.90</cell></row><row><cell>Doetsch et al. [10]</cell><cell>12.20</cell><cell>4.70</cell></row><row><cell>Bluche and Messina [37]</cell><cell>10.5</cell><cell>3.2</cell></row><row><cell>Our work</cell><cell>8.22</cell><cell>4.70</cell></row><row><cell>Voigtlaender et al. [19]</cell><cell>9.3</cell><cell>3.5</cell></row><row><cell>Poznanski and Wolf [52]</cell><cell>6.45</cell><cell>3.44</cell></row><row><cell>Dutta et al. [17]</cell><cell>4.80</cell><cell>2.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Comparison of results on RIMES dataset to previous methods.</figDesc><table><row><cell>Database</cell><cell>RIMES</cell><cell></cell></row><row><cell>Model</cell><cell>WER</cell><cell>CER</cell></row><row><cell>Kozielski et al. [44]</cell><cell>13.70</cell><cell>4.60</cell></row><row><cell>Doetsch et al. [10]</cell><cell>12.90</cell><cell>4.30</cell></row><row><cell>Bluche et al. [8]</cell><cell>11.80</cell><cell>3.70</cell></row><row><cell>Voigtlaender et al. [19]</cell><cell>9.6</cell><cell>2.8</cell></row><row><cell>Bluche and Messina [37]</cell><cell>7.9</cell><cell>1.9</cell></row><row><cell>Stunner et al. [53]</cell><cell>7.84</cell><cell>2.53</cell></row><row><cell>Our work</cell><cell>5.68</cell><cell>2.46</cell></row><row><cell>Poznanski and Wolf [52]</cell><cell>3.90</cell><cell>1.90</cell></row><row><cell>Dutta et al. [17]</cell><cell>1.86</cell><cell>0.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Predictions obtained with the symbol sequence prediction model on the RIMES dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Comparison of results on RIMES and IAM dataset. First column indicates if the model was fine tuned. Second column indicates if the model included the Lexicon CNN. Third column indicates if the model included vocabulary matching using probabilistic CER. WER values are in parenthesis. All values are represented as a percentage.</figDesc><table><row><cell>Fine-tuned</cell><cell>Lex. CNN</cell><cell>Prob. CER</cell><cell>IAM</cell><cell>RIMES</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>4.70(8.22)</cell><cell>2.46(5.68)</cell></row><row><cell>X</cell><cell></cell><cell>X</cell><cell>5.05(8.62)</cell><cell>2.55(5.98)</cell></row><row><cell>X</cell><cell>X</cell><cell></cell><cell>6.50(18.30)</cell><cell>4.15(15.91)</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell>7.09(17.77)</cell><cell>4.74(19.91)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8.86(21.80)</cell><cell>5.03(20.05)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDAR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">Convolutional neural network committees for handwritten character classification, in: 2011 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1135" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1406.2227</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cnn-n-gram for handwriting word recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Poznanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2305" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The a2ia french handwriting recognition system at the rimes-icdar2011 competition, IS&amp;T/SPIE Electronic Imaging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Menasri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Bianne-Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<idno>82970Y-82970Y</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison of sequence-trained deep neural networks and recurrent neural networks optical modeling for handwriting recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and robust training of recurrent neural networks for offline handwriting recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="279" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional multi-directional recurrent network for offline handwritten text recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2016 15th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="240" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data augmentation for recognition of handwritten words and lines using a cnn-lstm network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving handwritten chinese text recognition using neural network language models and convolutional neural network shape models</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving CNN-RNN hybrid networks for handwriting recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Krishnan Praveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2018 18th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Offline continuous handwriting recognition using sequence to sequence neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sueiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">289</biblScope>
			<biblScope unit="page" from="119" to="128" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Handwriting recognition with large multidimensional long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous script identification and handwriting recognition via multi-task learning of recurrent neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2017 14th IAPR International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="525" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A lexicon-free approach for 3d handwriting recognition using classifier combination</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">More than twenty years of advancements on frontiers in handwriting recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Impedovo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="916" to="928" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online and off-line handwriting recognition: a comprehensive survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural network language models for off-line handwriting recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zamora-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>España-Boquera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Castro-Bleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1642" to="1652" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep speech: scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>2015 . abs/1502.04681</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Start, follow, read: end-to-end full-page handwriting recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="367" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature extraction with convolutional neural networks for handwritten word recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="285" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep feature embedding for accurate recognition and retrieval of handwritten text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2016 15th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online and offline handwritten Chinese character recognition: benchmarking on new databases</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional recurrent network for handwritten chinese text recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4011" to="4016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gated convolutional recurrent neural networks for multilingual handwriting recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Messina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="646" to="651" />
		</imprint>
		<respStmt>
			<orgName>IC-DAR</orgName>
		</respStmt>
	</monogr>
	<note>Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01727</idno>
		<title level="m">Scene text recognition with sliding convolutional character models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate recognition of words in scenes without character segmentation using recurrent neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="397" to="405" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deep learning based character recognition system from multimedia document</title>
		<author>
			<persName><forename type="first">U</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Xaxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mahobiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Power and Advanced Computing Technologies (i-PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning based isolated arabic scene character recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yousaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arabic Script Analysis and Recognition (ASAR), 2017 1st International Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Handwritten text recognition using deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shiferaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS231n: Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University Project</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A system for offline character recognition using auto-encoder networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakravarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improvements in rwth&apos;s system for off-line handwriting recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="935" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The iam-database: an english sentence database for offline handwriting recognition</title>
		<author>
			<persName><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recogn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rimes evaluation campaign for handwritten mail processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Brodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Geoffrois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Prêteux</surname></persName>
		</author>
		<idno>IWFHR&apos;06</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<ptr target="https://www.nist.gov/srd/nist-special-database-19" />
	</analytic>
	<monogr>
		<title level="j">Nist special database</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical hybrid mlp/hmm or rather mlp features for a discriminatively trained gaussian hmm: a comparison for offline handwriting recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2011.6116480</idno>
	</analytic>
	<monogr>
		<title level="m">2011 18th IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3541" to="3544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving offline handwritten text recognition with hybrid hmm/ann models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Espana-Boquera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Castro-Bleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gorbe-Moya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zamora-Martinez</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.141</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cnn-n-gram for handwriting word recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Poznanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Stuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07432</idno>
		<title level="m">Lv-rover: lexicon verified recognizer output voting error reduction</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantic text summarization of long videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulhare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prud'hommeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
	<note>Neural sign language translation</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking recurrent latent variable model for music composition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00810</idno>
		<title level="m">Deep learning for genomics: a concise overview</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep speech 2: end-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">His research specializes in machine learning, computer vision, and robotics. Ray was a research scientist with Eastman Kodak Company where he worked on computational imaging algorithms and was awarded 31 U.S. patents with another 19 applications on file</title>
	</analytic>
	<monogr>
		<title level="m">He graduated from SUNY/Buffalo with a B.S. in Computer Science and a B.S. in Electrical Engineering. He earned a M.S. in Image Science from RIT. He earned a Ph.D. in Computer Science from RIT in 2013. Ray was awarded an NSF Graduate Research Fellowship in 2010 and his Ph</title>
		<imprint/>
		<respStmt>
			<orgName>Raymond Ptucha is an Assistant Professor in Computer Engineering and Director of the Machine Intelligence Laboratory at Rochester Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>D. research earned the 2014 Best RIT Doctoral Dissertation Award. Ray is a passionate supporter of STEM education and is an active member of his local IEEE chapter and FIRST robotics organizations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
