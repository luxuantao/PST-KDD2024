<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Objective Quality Assessment of Tone-Mapped Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Yeganeh</forename><surname>Hojatollah</surname></persName>
							<email>hyeganeh@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
							<email>zhouwang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Objective Quality Assessment of Tone-Mapped Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E8A689A1092ED6CB3002C059BBA1DBF7</idno>
					<idno type="DOI">10.1109/TIP.2012.2221725</idno>
					<note type="submission">received August 22, 2011; revised September 7, 2012; accepted September 7, 2012. Date of publication October 2, 2012; date of current version January 10, 2013.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High dynamic range image</term>
					<term>image fusion</term>
					<term>image quality assessment</term>
					<term>naturalness</term>
					<term>perceptual image processing</term>
					<term>structural similarity</term>
					<term>tone mapping operator</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tone-mapping operators (TMOs) that convert high dynamic range (HDR) to low dynamic range (LDR) images provide practically useful tools for the visualization of HDR images on standard LDR displays. Different TMOs create different tonemapped images, and a natural question is which one has the best quality. Without an appropriate quality measure, different TMOs cannot be compared, and further improvement is directionless. Subjective rating may be a reliable evaluation method, but it is expensive and time consuming, and more importantly, is difficult to be embedded into optimization frameworks. Here we propose an objective quality assessment algorithm for tonemapped images by combining: 1) a multiscale signal fidelity measure on the basis of a modified structural similarity index and 2) a naturalness measure on the basis of intensity statistics of natural images. Validations using independent subject-rated image databases show good correlations between subjective ranking score and the proposed tone-mapped image quality index (TMQI). Furthermore, we demonstrate the extended applications of TMQI using two examples-parameter tuning for TMOs and adaptive fusion of multiple tone-mapped images. 1   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HERE has been a growing interest in recent years in high dynamic range (HDR) images, where the range of intensity levels could be on the order of 10,000 to 1 <ref type="bibr" target="#b0">[1]</ref>. This allows for accurate representations of the luminance variations in real scenes, ranging from direct sunlight to faint starlight <ref type="bibr" target="#b0">[1]</ref>. With recent advances in imaging and computer graphics technologies, HDR images are becoming more widely available. A common problem that is often encountered in practice is how to visualize HDR images on standard display devices that are designed to display low dynamic range (LDR) images. To overcome this problem, an increasing number of tone mapping operators (TMOs) that convert HDR to LDR images have been developed, for examples <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Because of the reduction in dynamic range, tone mapping procedures inevitably cause information loss. With multiple TMOs available, one would ask which TMO faithfully preserves the structural information in the original HDR images, and which TMO produces natural-looking realistic LDR images.</p><p>TMO assessment in the past mostly relied on human subjective evaluations. In <ref type="bibr" target="#b5">[6]</ref>, perceptual evaluations of 6 TMOs were conducted with regard to similarity and preferences. An overview and a subjective comparison of 8 TMOs were reported in <ref type="bibr" target="#b6">[7]</ref>. HDR capable monitor was employed in <ref type="bibr" target="#b7">[8]</ref> to compare 6 TMOs in a subjective experiment using a paired comparison method. In <ref type="bibr" target="#b8">[9]</ref>, 14 subjects were asked to rate 2 architectural interior scenes produced by 7 TMOs based on basic image attributes as well as the naturalness of the LDR images. A more comprehensive subjective evaluation was carried out in <ref type="bibr" target="#b9">[10]</ref>, where tone mapped images generated by 14 TMOs were shown to 2 groups of 10 human observers to rate LDR images, concerning overall quality, brightness, contrast, detail reproduction and color. In <ref type="bibr" target="#b10">[11]</ref>, subjects were asked to choose the best LDRs derived from 2 TMOs with different parameter settings to optimally tune the algorithms. The value of subjective testing cannot be overestimated. However, they have fundamental limitations. First, it is expensive and time consuming. Second, it is difficult to be incorporated into an optimization framework to automatically improve TMOs and adjust their parameter settings. Furthermore, important image structures contained in HDR images may be missing in tone mapped images, but human observers may not be aware of their existence. In this sense, subjective evaluation should not be regarded as a golden standard for the quality of tone mapped images.</p><p>Typical objective image quality assessment (IQA) approaches assume the reference and test images to have the same dynamic range <ref type="bibr" target="#b11">[12]</ref>, and thus cannot be directly applied to evaluate tone mapped images. Only a few objective assessment methods have been proposed for HDR images. The HDR visible differences predictor (HDR-VDP) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref> is a human visual system (HVS) based fidelity metric that aims to distinguish between visible (suprathreshold) and invisible (subthreshold) distortions. The metric reflects the perception of distortions in terms of detection probability. Since HDR-VDP is designed to predict the visibility of differences between two HDR images of the same dynamic range, it is not applicable to compare an HDR image with an LDR image. A dynamic range independent approach was proposed in <ref type="bibr" target="#b13">[14]</ref>, which improves upon HDR-VDP and produces three types of quality maps that indicate the loss of visible features, the amplification of invisible features, and reversal of contrast polarity, respectively. These quality maps show good correlations with subjective classifications of image degradation types including blur, sharpening, contrast reversal, and no distortion. However, it does not provide a single quality score for an entire image, making it impossible to be validated with subjective evaluations of overall image quality.</p><p>The purpose of the current work is to develop an objective IQA model for tone mapped LDR images using their corresponding HDR images as references. Our work is inspired by the success of two design principles in IQA literature. The first is the structural similarity (SSIM) approach <ref type="bibr" target="#b14">[15]</ref> and its multi-scale derivations <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which asserts that the main purpose of vision is to extract structural information from the visual scene and thus structural fidelity is a good predictor of perceptual quality. The second is the natural scene statistics (NSS) approach, which maintains that the visual system is highly adapted to the natural visual environment and uses the departure from natural image statistics as a measure of perceptual quality <ref type="bibr" target="#b17">[18]</ref>. Here we propose a method that combines a multi-scale structural fidelity measure and a statistical naturalness measure, leading to Tone Mapped image Quality Index (TMQI). Moreover, we demonstrate that TMQI can be employed for optimizing parameters in TMOs and for adaptively fusing multiple tone mapped images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. QUALITY ASSESSMENT METHOD</head><p>Due to the reduction in dynamic range, TMOs cannot preserve all information in HDR images, and human observers of the LDR versions of these images may not be aware of this. Therefore, structural fidelity plays an important role in assessing the quality of tone-mapped images <ref type="bibr" target="#b18">[19]</ref>. On the other hand, structural fidelity alone does not suffice to provide an overall quality evaluation. A good quality tone mapped image should achieve a good compromise between structural fidelity preservation and statistical naturalness, which are sometimes competing factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structural Fidelity</head><p>The SSIM approach provides a useful design philosophy as well as a practical method for measuring structural fidelities between images <ref type="bibr" target="#b19">[20]</ref>. The original SSIM algorithm is applied locally and contains three comparison componentsluminance, contrast and structure. Since TMOs are meant to change local intensity and contrast, direct comparisons of local and contrast are inappropriate. Let x and y be two local image patches extracted from the HDR and the tone-mapped LDR images, respectively. We define our local structural fidelity measure as</p><formula xml:id="formula_0">S local (x, y) = 2σ x σ y + C 1 σ x 2 + σ y 2 + C 1 • σ xy + C 2 σ x σ y + C 2<label>(1)</label></formula><p>where σ x , σ y and σ xy are the local standard deviations and cross correlation between the two corresponding patches in HDR and LDR images, respectively, and C 1 and C 2 are positive stabilizing constants. Compared with the SSIM definition <ref type="bibr" target="#b14">[15]</ref>, the luminance comparison component is missing, and the structure comparison component (the second term in (1)) is exactly the same. The first term in (1) compares signal strength and is modified from that of the SSIM definition based on two intuitive considerations. First, the difference of signal strength between HDR and LDR image patches should not be penalized when their signal strengths are both significant (above visibility threshold) or both insignificant (below visibility threshold). Second, the algorithm should penalize the cases that the signal strength is significant in one of the image patches but insignificant in the other. This is different from the corresponding term in the original SSIM definition where any change in signal strength is penalized.</p><p>To distinguish between significant and insignificant signal strength, we pass the local standard deviation σ through a nonlinear mapping, which results in the σ value employed in <ref type="bibr" target="#b0">(1)</ref>. The nonlinear mapping should be designed so that significant signal strength is mapped to 1 and insignificant signal strength to 0, with a smooth transition in-between. Therefore, the nonlinear mapping is related to the visual sensitivity of contrast, which has been an extensively studied subject in the literature of visual psychophysics <ref type="bibr" target="#b20">[21]</ref>. Practically, the HVS does not have a fixed threshold of contrast detection, but typically follows a gradual increasing probability in observing contrast variations. Psychometric functions describing the detection probability of signal strength have been employed to model the data taken from psychophysical experiments. Generally, the psychometric function resembles a sigmoid shape <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and the sensory threshold is usually defined at the level of 50% of detection probability. A commonly adopted psychometric function is known as Galton's ogive <ref type="bibr" target="#b20">[21]</ref>, which takes the form of a cumulative normal distribution function given by</p><formula xml:id="formula_1">p(s) = 1 √ 2πθ s s -∞ exp - (x -τ s ) 2 2θ 2 s dx (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where p is the detection probability density, s is the amplitude of the sinusoidal stimulus, τ s is the modulation threshold, and θ s is the standard deviation of the normal distribution that controls the slope of detection probability variation. It was found that the ratio</p><formula xml:id="formula_3">k = τ s θ s (3)</formula><p>is roughly a constant, known as Crozier's law <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Typical values of k ranges between 2.3 and 4, and k = 3 makes the probability of false alarm considerably small <ref type="bibr" target="#b20">[21]</ref>.</p><p>The reciprocal of the modulation threshold τ s is often used to quantify visual contrast sensitivity, which is a function of spatial frequency, namely the contrast sensitivity function (CSF) <ref type="bibr" target="#b20">[21]</ref>. A CSF formula that fits well with data collected in various psychological experiments is given by <ref type="bibr" target="#b24">[25]</ref> </p><formula xml:id="formula_4">A( f ) ≈ 2.6[0.0192 + 0.114 f ] exp[-(0.114 f ) 1.1 ] (4)</formula><p>where f denotes spatial frequency. This function is normalized to have peak value 1, and thus only provides relative sensitivity across the frequency spectrum. In practice, it needs to be scaled by a constant λ to fit psychological data. In our implementation, we follow Kelly's CSF measurement <ref type="bibr" target="#b25">[26]</ref>. Combining this with (4), we obtain</p><formula xml:id="formula_5">τ s ( f ) = 1 λ A( f ) . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>This threshold value is calculated based on contrast sensitivity measurement assuming pure sinusoidal stimulus. To convert it to a signal strength threshold measured using the standard deviation of the signal, we need to take into account that signal amplitude scales with both contrast and mean signal inensity, and there is a √ 2 factor between the amplitude and standard deviation of a sinusoidal signal. As a result, a threshold value defined on signal standard deviation can be computed as</p><formula xml:id="formula_7">τ σ ( f ) = μ √ 2 λ A( f ) (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where μ is the mean intensity value. Based on Crozier's law <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we have</p><formula xml:id="formula_9">θ σ ( f ) = τ σ ( f ) k . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>We can then define the mapping between σ and σ as</p><formula xml:id="formula_11">σ = 1 √ 2πθ σ σ -∞ exp - (x -τ σ ) 2 2θ 2 σ dx<label>(8)</label></formula><p>In ( <ref type="formula" target="#formula_0">1</ref>), σ x and σ y are the mapped versions of σ x and σ y , respectively. They are bounded between 0 and 1, where 0 and 1 represent completely insignificant and completely significant signal strengths, respectively. The local structural fidelity measure S local is applied to an image using a sliding window that runs across the image space. This results in a map that reflects the variation of structural fidelity across space. The visibility of image details depends on the sampling density of the image, the distance between the image and the observer, the resolution of the display, and the perceptual capability of the observer's visual system. A singlescale method cannot capture such variations. Following the idea used in multi-scale <ref type="bibr" target="#b15">[16]</ref> and information-weighted SSIM <ref type="bibr" target="#b16">[17]</ref>, we adopt a multi-scale approach, where the images are iteratively low-pass filtered and downsampled to create an image pyramid structure <ref type="bibr" target="#b26">[27]</ref>, as illustrated in Fig. <ref type="figure">1</ref>. The local structural fidelity map is generated at each scale. Fig. <ref type="figure" target="#fig_0">2</ref> shows two examples of such maps computed at multiple scales for the LDR images created from two different TMOs. It is interesting to observe these fidelity maps and examine how they correlate with perceived image fidelity. For example, the structural details of the brightest window regions are missing in Image (b), but are more visible in Image (a). For another example, there are detailed structures in the top-right dark regions that are not easily discerned in Image (a), but are better visualized in Image (b). All of these are clearly reflected in the structural fidelity maps.</p><p>At each scale, the map is pooled by averaging to provide a single score:</p><formula xml:id="formula_12">S l = 1 N l N l i=1 S local (x i , y i ) (9)</formula><p>where x i and y i are the i -th patches in the HDR and LDR images being compared, respectively, and N l is the number of</p><formula xml:id="formula_13">L 2 S 1 HDR image L 2 L 2 L 2 LDR image L 2 L 2 S 2 S L S Fig. 1.</formula><p>Framework of multiscale structural fidelity assessment.</p><p>patches in the l-th scale. In the literature, advanced pooling strategies such as information content based pooling <ref type="bibr" target="#b16">[17]</ref> have been shown to improve the performance of IQA algorithms. However, in our current experiment, these advanced pooling methods did not result in notable performance gain in the proposed structural fidelity measure. The overall structural fidelity is calculated by combining scale level structural fidelity scores using the method in <ref type="bibr" target="#b15">[16]</ref> </p><formula xml:id="formula_14">S = L l=1 S β l l (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where L is the total number of scales and β l is the weight assigned to the l-th scale.</p><p>There are several parameters in the implementation of our structural fidelity model. First, when computing S local , we set C 1 = 0.01 and C 2 = 10, and we find that the overall performance of the structural fidelity model is insensitive to these parameters within an order of magnitude. Second, to create the fidelity map at each scale, we adopt the same setting as in the SSIM algorithm <ref type="bibr" target="#b14">[15]</ref> by employing a Gaussian sliding window of size 11×11 with standard deviation 1.5. Third, as in <ref type="bibr" target="#b15">[16]</ref>, we assume a viewing distance of 32 cycles/degree, which can represent signals up to 16 cycles/degree of resolution without aliasing, and thus we use 16 cycles/degree as the spatial frequency parameter when applying the CSF in (4) to the finest scale measurement. The spatial frequency parameters applied to the subsequent finer scales are then 8, 4, 2, 1 cycles/degree, respectively. Fourth, the mean intensity value in ( <ref type="formula" target="#formula_7">6</ref>) is set to be the mean of the dynamic range of LDR images, i.e., μ = 128. Fifth, when combining the measures across scales, we set L = 5 and {β l } = {0.0448, 0.2856, 0.3001, 0.2363, 0.1333}, which follows the psychophysical experiment results reported in <ref type="bibr" target="#b15">[16]</ref>. Finally, in order to assess the quality of color images, we first convert them from RGB color space to Yxy space and then apply the proposed structural fidelity measure on the Y component only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Statistical Naturalness</head><p>A high quality tone mapped LDR image should not only faithfully preserve the structural fidelity of the HDR image, but also look natural. Nevertheless, naturalness is a subjective quantity that is difficult to define quantitatively. A large literature has been dedicated to the statistics of natural images which have important significance to both image processing applications and the understanding of biological vision <ref type="bibr" target="#b27">[28]</ref>. An interesting study of naturalness in the context of subjective evaluation of tone mapped images was carried out in <ref type="bibr" target="#b28">[29]</ref>, which provided useful information regarding the correlations between image naturalness and different image attributes such as brightness, contrast, color reproduction, visibility and reproduction of details. The results showed that among all attributes being tested, brightness and contrast have more correlation with perceived naturalness. This motivates us to build our statistical naturalness model based on these two attributes. This choice may be oversimplifying in defining the general concept of statistical image naturalness (and may not generalize to other image processing applications that uses the concept of naturalness), but it provides an ideal compromise between the simplicity of our model and the capability of capturing the most important ingredients of naturalness that are related to the tone mapping evaluation problem we are trying to solve, where brightness mapping is an inevitable issue in all tone mapping operations. It also best complements the structural fidelity measure described in Section II-A, where brightness modeling and evaluation are missing.</p><p>Our statistical naturalness model is built upon statistics conducted on about 3,000 8bits/pixel gray-scale images obtained from [30], [31] that represent many different types of natural scenes. Fig. <ref type="figure" target="#fig_2">3</ref> shows the histograms of the means and standard deviations of these images, which are useful measures that reflect the global intensity and contrast of images. We found that these histograms can be well fitted using a Gaussian and a Beta probability density functions given by</p><formula xml:id="formula_16">P m (m) = 1 √ 2πσ m exp - m -μ m 2σ 2 m (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>and Recent studies suggested that brightness and contrast are largely independent quantities in terms of both natural image statistics and biological computation <ref type="bibr" target="#b29">[32]</ref>. As a result, their joint probability density function would be the product of the two. Therefore, we define our statistical naturalness measure as</p><formula xml:id="formula_18">P d (d) = (1 -d) β d -1 d α d -1 B(α d , β d ) (<label>12</label></formula><formula xml:id="formula_19">N = 1 K P m P d (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where K is a normalization factor given by K = max{P m P d }.</p><p>This constrains the statistical naturalness measure to be bounded between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quality Assessment Model</head><p>The structural fidelity measure S introduced in Section II-A and the statistical naturalness measure N described in Section II-B characterizes different aspects of the quality of tone mapped images. They may be used individually or jointly as a vector valued measure. In many practical applications, however, users prefer a single score that indicates the overall quality of the image. Therefore these parameters should be combined in some manner. In the literature of IQA, there had been earlier work that combines image statistics and measures of structure and contrast <ref type="bibr" target="#b30">[33]</ref>, though in a different context. Here we define a three-parameter function to scalarize the joint measure, resulting in a Tone Mapped image Quality Index (TMQI)</p><formula xml:id="formula_21">Q = aS α + (1 -a)N β (14)</formula><p>where 0 ≤ a ≤ 1 adjusts the relative importance of the two components, and α and β determine their sensitivities, respectively. Since both S and N are upper-bounded by 1, the overall quality measure is also upper-bounded by 1.</p><p>The parameters in ( <ref type="formula">14</ref>) are left to be determined. In our implementation, they are tuned to best fit the subjective evaluation data provided by the authors of <ref type="bibr" target="#b31">[34]</ref>. In their experiments, the subjects were instructed to look simultaneously at two LDR images created by two different TMOs applied upon the same HDR image, and then pick the one with better overall quality. Two studies have been done, involving two groups of subjects. The first study was carried out at Zheijang University, where 59 naive volunteers were invited to do the pair-wise comparison task and fill the preference matrix. The second study was conducted using Amazon Mechanical Turk, an online service of subjective evaluation. Each paired comparison was assigned to 150 anonymous subjects. The database includes 6 data sets, each of which contains images generated by 5 well-known TMOs, introduced by Drago et. al. <ref type="bibr" target="#b3">[4]</ref>, Durand &amp; Dorsey <ref type="bibr" target="#b32">[35]</ref>, Fattal et. al. <ref type="bibr" target="#b4">[5]</ref>, Reinhard et. al. <ref type="bibr" target="#b1">[2]</ref> and Mertens et. al. <ref type="bibr" target="#b33">[36]</ref>. The subjective ranking scores in each folder are then computed using the preference matrix.</p><p>Finding the best parameters in ( <ref type="formula">14</ref>) using subjective data is essentially a regression problem. The major difference from traditional regression problems is that here we are provided with relative ranking data between images only, but not quality scores associated with individual images. We developed a learning method where the parameters are learnt from an iterative method. At each iteration, one pair of images is randomly selected from one randomly selected data set. If the model generates objective scores that give the same order of the pair as the subjective rank order, then there is no change to the model parameters; Otherwise, each parameter is updated towards the direction of correcting the model error by a small step. The iteration continues until convergence. In our experiment, we observe good convergence property of this iterative learning process. Furthermore, to ensure the robustness of our approach, we conducted a leave-one-out cross validation procedure, where the database (of 6 data sets) was divided into 5 training sets and 1 testing set, and the same process was repeated 6 times, each with a different division between training and testing sets. Although each time ends up with a different set of parameters, they are fairly close to each other and result in the same ranking orders for all the training and testing sets. In the end, we select a = 0.8012, α = 0.3046, and β = 0.7088 as our final model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VALIDATION OF QUALITY ASSESSMENT METHOD</head><p>The validation process is conducted by comparing our objective quality assessment results with subjective data. Two evaluation metrics are employed which are given as follows.</p><p>1) Spearman's rank-order correlation coefficient (SRCC) is defined as</p><formula xml:id="formula_22">SRCC = 1 - 6 N i=1 d 2 i N(N 2 -1)<label>(15)</label></formula><p>where d i is the difference between the i -th image's ranks in subjective and objective evaluations. SRCC is a nonparametric rank-order based correlation metric, independent of any monotonic nonlinear mapping between subjective and objective scores. 2) Kendall's rank-order correlation coefficient (KRCC) is another non-parametric rank correlation metric computed as</p><formula xml:id="formula_23">KRCC = N c -N d 1 2 N(N -1)<label>(16)</label></formula><p>where N c and N d are the numbers of concordant (of consistent rank order) and discordant (of inconsistent rank order) pairs in the data set, respectively. The proposed TMQI is the only objective quality measure being tested. To the best of our knowledge, almost no other method has been proposed to compare images with different dynamic ranges. The only exception is the method proposed in <ref type="bibr" target="#b13">[14]</ref>, which creates probability maps to distinguish between visible (suprathreshold) and invisible (subthreshold) degradations. The probability maps are shown to be useful in classifying image distortion types but are not meant to be pooled to produce an overall quality score of a tone mapped image. As a result, direct comparison with the proposed method is not possible.</p><p>Three experiments have been carried out in our validation process, each uses a different subject-ranked database. The first database is from <ref type="bibr" target="#b31">[34]</ref>, which was also used in the parameter training step discussed in Section II-C. Our leave-one-out cross validation method described in Section II-C creates SRCC and KRCC values for each of the six testing data sets, where for each data set, the parameters were trained using the other five data sets. Table <ref type="table">I</ref> shows the means and standard deviations of KRCC and SRCC values between subjective rankings and our model predictions, respectively.</p><p>In the second experiment, we use the database introduced in [10], <ref type="bibr" target="#b34">[37]</ref>, from which we employ the overall quality ranking data by 10 naive subjects of 14 tone mapped images created from the same HDR image. The KRCC and SRCC values between subjective rankings of the images and our structural fidelity, statistical naturalness and overall quality scores are given in Table <ref type="table">II</ref>, where we observe that the structural fidelity measure alone can provide reasonable predictions of subjective rankings. The statistical naturalness measure by itself is not a good predictor of the overall quality ranking, but it complements the structural fidelity measure. When the two measures are combined, better prediction of the overall image quality is achieved. It is worth mentioning that the test data here is not used in the training process, but the resulting KRCC and SRCC values are comparable with those obtained in the test using the first database, which is used for training. This implies good generalization capability of the training method described in Section II-C.  <ref type="bibr" target="#b35">[38]</ref> and Pattanaik et. al. <ref type="bibr" target="#b36">[39]</ref> are computed using the publicly available software Qtpfsgui <ref type="bibr" target="#b37">[40]</ref>. In addition, three other images were created using the built-in TMOs in Adobe Photoshop, namely "Exposure and Gamma," "Equalize Histogram," and "Local Adaptation," respectively. The parameters used in all 8 TMOs are set as their default values and are not optimized. The reference HDR images are selected to represent different indoor and outdoor scenes and are all available online <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b38">[41]</ref>- <ref type="bibr" target="#b40">[43]</ref>. In the subjective test, each of the 20 observers was asked to rank the 8 images in each image set from the best to the worst. The subjective rankings for each image is then averaged, resulting in its mean ranking score within the set.</p><p>To evaluate the TMQI method, we calculate the KRCC and SRCC values between the mean ranking scores and the objective quality measures for each image set. The results are given in Table <ref type="table">III</ref>. To provide an anchor in evaluating the performance of TMQI, we compare it with the behavior of an average subject. To do this, we first compute the KRCC and SRCC values between the mean ranking scores and the ranking scores given by each individual subject for each image set. We then compute the mean and standard deviation of these KRCC and SRCC values over subjects, which are shown in Table <ref type="table">III</ref>. The average KRCC and SRCC values over all 15 image set are given in the last row. It can be seen that for all image sets, the KRCC and SRCC values of TMQI are well within the range of ±1 standard deviation from the KRCC and SRCC values of the mean over all subjects. This indicates that TMQI behaves quite similarly to an average subject.</p><p>Since the TMQI algorithm does not involve any expensive search or iterative procedure, it is computationally efficient. Our unoptimized MATLAB implementation on an Intel Quad-Core 2.67 GHz computer takes on average around 0.75 and 2.7 seconds to evaluate images of sizes 512×512 and 1024×1024, respectively. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates the scatter plot of runtime versus the number of image pixels for 20 HDR-LDR comparisons. It shows that the computational complexity of the TMQI algorithm is approximately linear with respect to the number  of pixels in the image. The relatively low computational cost makes it easily adapted to practical applications that involve iterative optimization processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATIONS OF QUALITY ASSESSMENT METHOD</head><p>The application scope of objective IQA measures is beyond evaluating images and comparing algorithms. A wider range of applications extends to developing novel image processing algorithms optimized for the novel IQA measures. In this section, we use two examples to demonstrate the potentials of TMQI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Parameter Tuning in TMO Algorithm</head><p>Many TMOs contain one or more parameters whose optimal values are often image-dependent. Without human interference, it is often a challenging task to choose these parameters, which could lead to drastically different results. An objective quality measure provides a useful tool to pick these parameters automatically. Here we use the TMO proposed in <ref type="bibr" target="#b3">[4]</ref> as an example, which uses logarithmic function with varying bases in different locations to change the dynamic range adaptively. The algorithm is given by </p><formula xml:id="formula_24">L d = L dmax • 0.01 log 10 (L wmax +1) • log(L w + 1) log 2+ L w L wmax log(b) log(0.5) • 8 (17)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Fusion of Tone-Mapped Images</head><p>When experimenting with different TMOs on different HDR images, we often find it difficult to pick a single TMO that produces the best results for all HDR images. Furthermore, within a single HDR image, the best TMO may also vary when different regions in the image are under consideration. To take the advantages of multiple TMOs, image fusion techniques may be employed to combine multiple tone mapped images and an objective quality measure can play an important role in this process.</p><p>Given multiple tone mapped images created by different TMOs, we first apply a Laplacian pyramid transform that decomposes these images into different scales. In the pyramid domain, this results in multiple coefficients at the same scale and the same spatial location, each corresponds to a different TMO. Examples are given in the first two rows in Fig. <ref type="figure" target="#fig_6">8</ref>, which demonstrate four-scale Laplacian pyramid decompositions, where the fine scale coefficients (Scales 1-3) represent image details and the coarsest scale coefficients (Scale 4) preserve local mean intensities across space. A fusion strategy can then be applied to combine multiple coefficients into one at each location in each scale before an inverse Laplacian pyramid transform is employed to reconstruct a fused image. Typical fusion schemes aim to locally select the most salient image features <ref type="bibr" target="#b41">[44]</ref>. The most widely adopted approaches include averaging the coefficients or picking one of the coefficients with the largest absolute value.</p><p>Here we propose a different fusion scheme. The general idea is to use the TMQI as the weighting factors in the fusion process. Let S j and c j be the local structural fidelity measure and the Laplacian pyramid transform coefficient computed from the j -th tone mapped image being fused, respectively. The fused coefficient is computed as</p><formula xml:id="formula_25">c ( f used) = j S j c j j S j . (<label>18</label></formula><formula xml:id="formula_26">)</formula><p>This is applied to all scales except for the coarsest scale, for which we use the statistical naturalness measure as the  weighting factor:</p><formula xml:id="formula_27">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o)</formula><formula xml:id="formula_28">c ( f used) = j N j c j j N j (<label>19</label></formula><formula xml:id="formula_29">)</formula><p>where N j denotes the statistical naturalness score of the j -th tone mapped image. The proposed Laplacian pyramid domain fusion method is demonstrated in the bottom row of Fig. <ref type="figure" target="#fig_6">8</ref>, where the fused image preserves the details in the brightest region (light area on the top) as in (f), while at the same time maintains higher contrast in relatively darker regions, as in (a). Fig. <ref type="figure" target="#fig_7">9</ref> provides an example with natural scene, where one tone mapped image (a) better preserves structural details, and another (b) gives more natural overall appearance (but loses structural information, especially at the brightest areas). Three fused images created by three different image fusion algorithms are given in (c), (d) and (e), respectively. The image created by the proposed method achieves the best balance between structure preserving and statistical naturalness, and also results in the best quality score using TMQI.</p><p>To further validate the proposed fusion scheme, we have conducted an additional subjective experiment, where ten subjects were invited to rank five sets of tone-mapped images, each of which includes eight images. Seven of these images are generated using the TMOs employed in the third experiment in Section III. Two of these seven TMOs are chosen to produce the eighth image using the proposed fusion method. Table IV compares average subjective rankings of the source images and their corresponding fused images, where lower ranking scores correspond to better quality. It can be seen that the fused image is almost always ranked significantly higher than the two source images being fused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We develop an objective model to assess the quality of tone mapped images by combining a multi-scale structural fidelity measure and a statistical naturalness measure. The proposed measure not only provides an overall quality score of an image, but also creates multi-scale quality maps that reflect the structural fidelity variations across scale and space. Our experiments show that TMQI is reasonably correlated with subjective evaluations of image quality. Moreover, we demonstrate the usefulness of TMQI in automatic parameter tuning of tone mapping algorithms and in fusing multiple tone mapped images.</p><p>As one of the first attempts on the research topic, our method has several limitations that may be resolved or improved in the future. First, TMQI is designed to evaluate grayscale images only, but most HDR images of natural scenes are captured in color. One simple method to evaluate tone mapped color images is to apply the TMQI to each color channel independently and then combine them. Color fidelity and color naturalness measures may be developed to improve the quality measure.</p><p>Second, simple averaging is used in the current pooling method of the structural fidelity map. Advanced pooling method that incorporate visual attention models may be employed to improve the quality prediction performance.</p><p>Third, the current statistical naturalness measure is based on intensity statistics only. There is a rich literature on natural image statistics <ref type="bibr" target="#b27">[28]</ref> and advanced statistical models (that reflects the structural regularities in space, scale and orientation in natural images) may be included to improve the statistical naturalness measure.</p><p>Fourth, using TMQI as a new optimization goal, many existing TMOs may be redesigned to achieve better image quality. Novel TMOs may also be developed by taking advantage of the construction of the proposed quality assessment approach.</p><p>Finally, the current method is applied and tested using natural images only. The application scope of HDR images and TMOs is beyond natural images. For example, modern medical imaging devices often capture HDR medical images that need to be tone-mapped before visualization. The TMQI and optimization methods may be adapted to these extended applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Tone-mapped LDR images and their structural fidelity maps in five scales. The images were created using Adobe Photoshop "Highlight compression" and "Exposure and Gamma" methods (not optimized for quality), respectively. (a) S = 0.9152 (S 1 = 0.8940; S 2 = 0.9341; S 3 = 0.9428; S 4 = 0.9143; S 5 = 0.8277). (b) S = 0.8614 (S 1 = 0.9161; S 2 = 0.9181; S 3 = 0.8958; S 4 = 0.8405; S 5 = 0.7041).</figDesc><graphic coords="4,124.43,248.21,111.74,167.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) where B(•, •) is the Beta function. The fitting curves are shown in Fig. 3, where the model parameters are estimated by regression, and the best values we found are μ m = 115.94 and σ m = 27.99 in (11), and α d = 4.4 and β d = 10.1 in (12), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Histograms of (a) means (fitted by Gaussian PDF) and (b) standard deviations (fitted by Beta PDF) of natural images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Run time versus the number of image pixels of the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overall quality measure Q versus parameter b for (a) Desk and (b) Bristol Bridge images. The tone-mapped images corresponding to selected b values are shown in Figs. 6 and 7, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. LDR images generated with different parameter b in (17). (a) b = 0.1, S = 0.8344, N = 0.4599, and Q = 0.8959. (b) b = 0.8, S = 0.8448, N = 0.4874, and Q = 0.8998. (c) b = 1.0, S = 0.8337, N = 0.1423, and Q = 0.8485.</figDesc><graphic coords="8,89.99,211.01,138.02,103.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Image fusion in Laplacian pyramid domain. Top row: first tone-mapped image (a) created by TMO proposed in [38], and its (b)-(e) Laplacian pyramid subbands, S = 0.5034, N = 0.1263, Q = 0.6937. Middle row: second tone-mapped image (f) using "Exposure and Gamma" method in Adobe Photoshop, and its (g)-(j) Laplacian pyramid subbands, S = 0.6642, N = 0.0786, and Q = 0.7386. Bottom row: fused image by (k) the proposed method, and its (l)-(o) Laplacian pyramid domain representation, S = 0.7419, N = 0.3080, and Q = 0.8167.</figDesc><graphic coords="9,370.47,380.57,58.14,58.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Fusion of tone-mapped images. (a) First tone-mapped image using TMO proposed in [35], S = 0.8168, N = 0.1631, and Q = 0.8075. (b) Second tone-mapped image using the "Exposure and Gamma" method in Adobe Photoshop, S = 0.6315, N = 0.8657, and Q = 0.8744. (c) Fused image by coefficient averaging in Laplacian pyramid domain, S = 0.7561, N = 0.7409, and Q = 0.8955. (d) Fused image by selecting coefficient of maximal absolute value in Laplacian pyramid domain, S = 0.7685, N = 0.9428, and Q = 0.9290. (e) Fused image by the proposed method, S = 0.7836, N = 0.9970, and Q = 0.9413.</figDesc><graphic coords="10,79.43,172.49,143.90,97.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TMOs from the same HDR image. The results created by five of the TMOs developed by Reinhard et al. [2], Drago et. al. [4], Durand &amp; Dorsey [35], Mantiuk et. al.</figDesc><table><row><cell></cell><cell>TABLE I</cell><cell></cell></row><row><cell cols="4">CROSS-VALIDATION RESULTS USING DATA FROM [34]</cell></row><row><cell></cell><cell>KRCC</cell><cell>SRCC</cell></row><row><cell>Mean</cell><cell>0.7333</cell><cell>0.8333</cell></row><row><cell>Std</cell><cell>0.1632</cell><cell>0.1211</cell></row><row><cell cols="2">TABLE II</cell><cell></cell></row><row><cell cols="4">PERFORMANCE EVALUATION USING DATA FROM [10], [37]</cell></row><row><cell></cell><cell>KRCC</cell><cell></cell><cell>SRCC</cell></row><row><cell>Structural Fidelity</cell><cell cols="2">0.6923</cell><cell>0.7912</cell></row><row><cell>Statistical Naturalness</cell><cell cols="2">0.3846</cell><cell>0.5385</cell></row><row><cell>Overall Quality</cell><cell cols="2">0.7179</cell><cell>0.8187</cell></row><row><cell cols="4">The third experiment is conducted using a database devel-</cell></row><row><cell cols="4">oped by ourselves. Twenty subjects were provided with 15 sets</cell></row><row><cell cols="4">of tone mapped images, each of which includes 8 images</cell></row><row><cell>generated by 8</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>RANKING SCORES MADE BY 10 SUBJECTS FOR EACH SET</figDesc><table><row><cell>Image Set</cell><cell>Source 1</cell><cell>Source 2</cell><cell>Fused Image</cell></row><row><cell>1</cell><cell>4.3</cell><cell>7</cell><cell>1.8</cell></row><row><cell>2</cell><cell>5.2</cell><cell>4</cell><cell>1.5</cell></row><row><cell>3</cell><cell>3.7</cell><cell>5.9</cell><cell>2.3</cell></row><row><cell>4</cell><cell>4.1</cell><cell>6.1</cell><cell>2.2</cell></row><row><cell>5</cell><cell>2.7</cell><cell>6.9</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A MATLAB code of the proposed algorithm is available online at https://ece.uwaterloo.ca/∼z70wang/research/tmqi/. Partial preliminary results of this work were presented at International Conference on Image Analysis and Recognition, Burnaby, BC, Canada, June</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2011" xml:id="foot_1"><p></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank M. Song, D. Tao, C. Chen, J. Bu, J. Luo, and C. Zhang, for providing us with their subjective test data from Zhejiang University, Hangzhou, China, and Amazon Mechanical Turk.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<title level="m">High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annu. Conf</title>
		<meeting>29th Annu. Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A visibility matching tone reproduction operator for high dynamic range scenes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="306" />
			<date type="published" when="1997-12">Oct.-Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive logarithmic mapping for displaying high contrast scenes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Drago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Annen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="426" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient domain high dynamic range compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annu. Conf</title>
		<meeting>29th Annu. Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of tone mapping operators</title>
		<author>
			<persName><forename type="first">F</forename><surname>Drago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Conf. Sketches Appl</title>
		<meeting>SIGGRAPH Conf. Sketches Appl</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Testing HDR image rendering algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS T/SID Color Imag. Conf</title>
		<meeting>IS T/SID Color Imag. Conf</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="315" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of tone mapping operators using a high dynamic range display</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seetzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="640" to="648" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of tone mapping operators with real-world scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Human Vis</title>
		<meeting>SPIE, Human Vis</meeting>
		<imprint>
			<date type="published" when="2005-01">Jan. 2005</date>
			<biblScope unit="volume">5666</biblScope>
			<biblScope unit="page" from="192" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image attributes and quality for evaluation of tone mapping operators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Čadík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Pacific Conf</title>
		<meeting>14th Pacific Conf</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the perceptual similarity of realistic looking tone mapped high dynamic range images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3245" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<title level="m">Modern Image Quality Assessment</title>
		<meeting><address><addrLine>San Rafael, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting visible differences in high dynamic range images-model and its calibration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2005-12">Dec. 2005</date>
			<biblScope unit="volume">5666</biblScope>
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic range independent image quality assessment</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput</title>
		<meeting>Int. Conf. Comput</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-scale structural similarity for image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Asilomar Conf. Signals, Syst., Comput</title>
		<meeting>IEEE Asilomar Conf. Signals, Syst., Comput<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-11">Nov. 2003</date>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information content weighting for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reduced-and no-reference image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Objective assessment of tone mapping algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="2477" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G J</forename><surname>Barten</surname></persName>
		</author>
		<title level="m">Contrast Sensitivity of the Human Eye and Its Effects on Image Quality</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Guilford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Psychometric</forename><surname>Methods</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Colour and Vision</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName><surname>Light</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the variability of critical illumination for flicker fusion and intensity discrimination</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Crozier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. General Physiol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="522" />
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The effects of a visual fidelity criterion on the encoding of images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1974-07">Jul. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effects of sharp edges on the visibility of sinusoidal gratings</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="103" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983-04">Apr. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The naturalness of reproduced high dynamic range images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Čadík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Slavík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Inf. Visual</title>
		<meeting>9th Int. Conf. Inf. Visual</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="920" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Independence of luminance and contrast in natural scenes and in the early visual system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frazor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1690" to="1697" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Most apparent distortion: Fullreference image quality assessment and the role of strategy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11006" to="11007" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic exposure fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="357" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast bilateral filtering for the display of highdynamic-range images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exposure fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific Conf</title>
		<meeting>Pacific Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="382" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Evaluation of Tone Mapping Operators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cadik</surname></persName>
		</author>
		<ptr target="http://www.cgg.cvut.cz/members/cadikm/tmo" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A perceptual framework for contrast processing of high dynamic range images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Symp</title>
		<meeting>2nd Symp</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Timedependent visual adaptation for fast realistic image display</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH Conf</title>
		<meeting>ACM SIGGRAPH Conf</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<ptr target="http://qtpfsgui.sourceforge.net/index.php" />
	</analytic>
	<monogr>
		<title level="j">Open Source Community</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reinhard&apos;s High Dynamic Range Data</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<ptr target="http://www.cs.utah.edu/∼reinhard/cdrom/hdr/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ward&apos;s High Dynamic Range Data</title>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<ptr target="http://www.anyhere.com/gward/pixformat/tiffluvimg.html" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Debevec&apos;s High Dynamic Range Data</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<ptr target="http://www.debevec.org/Research/HDR/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Multi-Sensor Image Fusion and Its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Taylor &amp; Francis</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
