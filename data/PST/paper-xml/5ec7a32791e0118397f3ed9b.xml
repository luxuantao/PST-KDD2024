<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-01">1 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
							<email>&lt;tongzhou@mit.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab (CSAIL)</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab (CSAIL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab (CSAIL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-01">1 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.10242v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ssnl</term>
					<term>github</term>
					<term>io/hypersphere Similar samples have similar features Alignment: Similar samples have similar features Preserve maximal information Uniformity: Preserve maximal information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A vast number of recent empirical works learn representations with a unit 2 norm constraint, effectively restricting the output space to the unit hypersphere <ref type="bibr" target="#b37">(Parkhi et al., 2015;</ref><ref type="bibr" target="#b40">Schroff et al., 2015;</ref><ref type="bibr" target="#b32">Liu et al., 2017;</ref><ref type="bibr" target="#b19">Hasnat et al., 2017;</ref><ref type="bibr" target="#b49">Wang et al., 2017;</ref><ref type="bibr" target="#b4">Bojanowski &amp; Joulin, 2017;</ref><ref type="bibr" target="#b35">Mettes et al., 2019;</ref><ref type="bibr" target="#b24">Hou et al., 2019;</ref><ref type="bibr" target="#b11">Davidson et al., 2018;</ref><ref type="bibr" target="#b53">Xu &amp; Durrett, 2018)</ref>, including many recent unsupervised contrastive representation learning methods <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>.</p><p>Intuitively, having the features live on the unit hypersphere leads to several desirable traits. Fixed-norm vectors are known to improve training stability in modern machine learning where dot products are ubiquitous <ref type="bibr">(Xu &amp; Durrett,</ref> Figure <ref type="figure">1</ref>: Illustration of alignment and uniformity of feature distributions on the output unit hypersphere. <ref type="bibr">STL-10 (Coates et al., 2011)</ref> images are used for demonstration. 2018; <ref type="bibr" target="#b49">Wang et al., 2017)</ref>. Moreover, if features of a class are sufficiently well clustered, they are linearly separable with the rest of feature space (see Figure <ref type="figure" target="#fig_0">2</ref>), a common criterion used to evaluate representation quality.</p><p>While the unit hypersphere is a popular choice of feature space, not all encoders that map onto it are created equal. Recent works argue that representations should additionally be invariant to unnecessary details, and preserve as much information as possible <ref type="bibr" target="#b36">(Oord et al., 2018;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b23">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>. Let us call these two properties alignment and uniformity (see Figure <ref type="figure">1</ref>). Alignment favors encoders that assign similar features to similar samples. Uniformity prefers a feature distribution that preserves maximal information, i.e., the uniform distribution on the unit hypersphere.</p><p>In this work, we analyze the alignment and uniformity properties. We show that a currently popular form of contrastive representation learning in fact directly optimizes for these two properties in the limit of infinite negative samples. Empirically, we propose theoretically-motivated metrics for alignment and uniformity, and observe strong agreement between them and downstream task performance. Remarkably, directly optimizing for these two metrics leads to comparable or better performance than contrastive learning.</p><p>Our main contributions are:</p><p>• We propose quantifiable metrics for alignment and uniformity as two measures of representation quality, with theoretical motivations.</p><p>• We prove that the contrastive loss optimizes for alignment and uniformity asymptotically.</p><p>• Empirically, we find strong agreement between both metrics and downstream task performance.</p><p>• Despite being simple in form, our proposed metrics, when directly optimized with no other loss, empirically lead to comparable or better performance at downstream tasks than contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Contrastive Representation Learning has seen remarkable success in learning representations for image and sequential data <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018;</ref><ref type="bibr" target="#b52">Wu et al., 2018;</ref><ref type="bibr" target="#b36">Oord et al., 2018;</ref><ref type="bibr" target="#b22">Hénaff et al., 2019;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b23">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>. The com-mon motivation behind these work is the InfoMax principle <ref type="bibr" target="#b31">(Linsker, 1988)</ref>, which we here instantiate as maximizing the mutual information (MI) between two views <ref type="bibr" target="#b46">(Tian et al., 2019;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b51">Wu et al., 2020)</ref>. However, this interpretation is known to be inconsistent with the actual behavior in practice, e.g., optimizing a tighter bound on MI can lead to worse representations <ref type="bibr" target="#b48">(Tschannen et al., 2019)</ref>.</p><p>What the contrastive loss exactly does remains largely a mystery. Analysis based on the assumption of latent classes provides nice theoretical insights <ref type="bibr" target="#b39">(Saunshi et al., 2019)</ref>, but unfortunately has a rather large gap with empirical practices: the result that representation quality suffers with a large number of negatives is inconsistent with empirical observations <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>. In this paper, we analyze and characterize the behavior of contrastive learning from the perspective of alignment and uniformity properties, and empirically verify our claims with standard representation learning tasks.</p><p>Representation learning on the unit hypersphere. Outside contrastive learning, many other representation learning approaches also normalize their features to be on the unit hypersphere. In variational autoencoders, the hyperspherical latent space has been shown to perform better than the Euclidean space <ref type="bibr" target="#b53">(Xu &amp; Durrett, 2018;</ref><ref type="bibr" target="#b11">Davidson et al., 2018)</ref>.</p><p>Directly matching uniformly sampled points on the unit hypersphere is known to provide good representations <ref type="bibr" target="#b4">(Bojanowski &amp; Joulin, 2017)</ref>, agreeing with our intuition that uniformity is a desirable property. <ref type="bibr" target="#b35">Mettes et al. (2019)</ref> optimizes prototype representations on the unit hypersphere for classification. Hyperspherical face embeddings greatly outperform the unnormalized counterparts <ref type="bibr" target="#b37">(Parkhi et al., 2015;</ref><ref type="bibr" target="#b32">Liu et al., 2017;</ref><ref type="bibr" target="#b49">Wang et al., 2017;</ref><ref type="bibr" target="#b40">Schroff et al., 2015)</ref>.</p><p>Its empirical success suggests that the unit hypersphere is indeed a nice feature space. In this work, we formally investigate the interplay between the hypersphere geometry and the popular contrastive representation learning.</p><p>Distributing points on the unit hypersphere. The problem of uniformly distributing points on the unit (hyper)sphere is a well-studied one. It is often defined as minimizing the total pairwise potential w.r.t. a certain potential function <ref type="bibr" target="#b5">(Borodachov et al., 2019;</ref><ref type="bibr" target="#b30">Landkof, 1972)</ref>, e.g., the Thomson problem of finding the minimal electrostatic potential energy configuration of electrons <ref type="bibr" target="#b44">(Thomson, 1904)</ref>, and minimization of the Riesz s-potential <ref type="bibr" target="#b13">(Götz &amp; Saff, 2001;</ref><ref type="bibr" target="#b18">Hardin &amp; Saff, 2005;</ref><ref type="bibr" target="#b33">Liu et al., 2018)</ref>. The uniformity metric we propose is based on the Gaussian potential, which can be used to represent a very general class of kernels and is closely related to the universally optimal point configurations <ref type="bibr" target="#b5">(Borodachov et al., 2019;</ref><ref type="bibr" target="#b10">Cohn &amp; Kumar, 2007)</ref>. Additionally, the best-packing problem on hyperspheres (often called the Tammes problem) is also well studied <ref type="bibr" target="#b43">(Tammes, 1930)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries on Unsupervised Contrastive Representation Learning</head><p>The popular unsupervised contrastive representation learning method (often referred to as contrastive learning in this paper) learns representations from unlabeled data. It assumes a way to sample positive pairs, representing similar samples that should have similar representations. Empirically, the positive pairs are often obtained by taking two independently randomly augmented versions of the same sample, e.g. two crops of the same image <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b23">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>.</p><p>Let p data (•) be the data distribution over R n and p pos (•, •) the distribution of positive pairs over R n × R n . Based on empirical practices, we assume the following property.</p><p>Assumption. Distributions p data and p pos should satisfy</p><p>• Symmetry: ∀x, y, p pos (x, y) = p pos (y, x).</p><p>• Matching marginal: ∀x, p pos (x, y) dy = p data (x).</p><p>We consider the following specific and widely popular form of contrastive loss for training an encoder f : R n → S m−1 , mapping data to 2 normalized feature vectors of dimension m. This loss has been shown effective by many recent representation learning methods <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018;</ref><ref type="bibr" target="#b52">Wu et al., 2018;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019;</ref><ref type="bibr" target="#b23">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>.</p><formula xml:id="formula_0">L contrastive (f ; τ, M ) E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data − log e f (x) T f (y)/τ e f (x) T f (y)/τ + i e f (x − i ) T f (y)/τ ,<label>(1)</label></formula><p>where τ &gt; 0 is a scalar temperature hyperparameter, and M ∈ Z + is a fixed number of negative samples.</p><p>The term contrastive loss has also been generally used to refer to various objectives based on positive and negative samples, e.g., in Siamese networks <ref type="bibr" target="#b9">(Chopra et al., 2005;</ref><ref type="bibr" target="#b17">Hadsell et al., 2006)</ref>. In this work, we focus on the specific form in Equation ( <ref type="formula" target="#formula_0">1</ref>) that is widely used in modern unsupervised contrastive representation learning literature.</p><p>Necessity of normalization. Without the norm constraint, the softmax distribution can be made arbitrarily sharp by simply scaling all the features. <ref type="bibr" target="#b49">Wang et al. (2017)</ref> provided an analysis on this effect and argued for the necessity of normalization when using feature vector dot products in a cross entropy loss, as is in Eqn. (1). Experimentally, Chen et al. ( <ref type="formula">2020</ref>) also showed that normalizing outputs leads to superior representations.</p><p>The InfoMax principle. Many empirical works are motivated by the InfoMax principle of maximizing I(f (x); f (y)) for (x, y) ∼ p pos <ref type="bibr" target="#b46">(Tian et al., 2019;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b51">Wu et al., 2020)</ref>. Usually they interpret L contrastive in Eqn.</p><p>(1) as a lower bound of I(f (x); f (y)) <ref type="bibr" target="#b36">(Oord et al., 2018;</ref><ref type="bibr" target="#b23">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b46">Tian et al., 2019)</ref>. However, this interpretation is known to have issues in practice, e.g., maximizing a tighter bound often leads to worse downstream task performance <ref type="bibr" target="#b48">(Tschannen et al., 2019)</ref>. Therefore, instead of viewing it as a bound, we investigate the exact behavior of directly optimizing L contrastive in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Feature Distribution on the Hypersphere</head><p>The contrastive loss encourages learned feature representation for positive pairs to be similar, while pushing features from the randomly sampled negative pairs apart. Conventional wisdom says that representations should extract the most shared information between positive pairs and remain invariant to other noise factors <ref type="bibr" target="#b31">(Linsker, 1988;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b51">Wu et al., 2020;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>. Therefore, the loss should prefer two following properties:</p><p>• Alignment: two samples forming a positive pair should be mapped to nearby features, and thus be (mostly) invariant to unneeded noise factors.</p><p>• Uniformity: feature vectors should be roughly uniformly distributed on the unit hypersphere S m−1 , preserving as much information of the data as possible.</p><p>To empirically verify this, we visualize <ref type="bibr">CIFAR-10 (Torralba et al., 2008;</ref><ref type="bibr" target="#b28">Krizhevsky et al., 2009)</ref> representations on S 1 (m = 2) obtained via three different methods:</p><p>• Random initialization.</p><p>• Supervised predictive learning: An encoder and a linear classifier are jointly trained from scratch with cross entropy loss on supervised labels.</p><p>• Unsupervised contrastive learning: An encoder is trained w.r.t. L contrastive with τ = 0.5 and M = 256.</p><p>All three encoders share the same AlexNet based architecture <ref type="bibr" target="#b29">(Krizhevsky et al., 2012)</ref>, modified to map input images to 2D vectors in S 1 . Both predictive and contrastive learning use standard data augmentations to augment the dataset and sample positive pairs.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> summarizes the resulting distributions of validation set features. Indeed, features from unsupervised contrastive learning (bottom in Figure <ref type="figure" target="#fig_2">3</ref>) exhibit the most uniform distribution, and are closely clustered for positive pairs.</p><p>The form of the contrastive loss in Eqn. (1) also suggests this. We present informal arguments below, followed by more formal treatment in Section 4.2. From the symmetry  of p, we can derive</p><formula xml:id="formula_1">L contrastive (f ; τ, M ) = E (x,y)∼ppos −f (x) T f (y)/τ + E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data log e f (x) T f (y)/τ + i e f (x − i ) T f (x)/τ</formula><p>.</p><p>Because the i e f (x − i ) T f (x)/τ term is always positive and bounded below, the loss favors smaller E −f (x) T f (y)/τ , i.e., having more aligned positive pair features. Suppose the encoder is perfectly aligned, i.e., P [f (x) = f (y)] = 1, then minimizing the loss is equivalent to optimizing</p><formula xml:id="formula_2">E x∼p data {x − i } M i=1 i.i.d. ∼ p data log e 1/τ + i e f (x − i ) T f (x)/τ</formula><p>, which is akin to maximizing pairwise distances with a LogSumExp transformation. Intuitively, pushing all features away from each other should indeed cause them to be roughly uniformly distributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantifying Alignment and Uniformity</head><p>For further analysis, we need a way to measure alignment and uniformity. We propose the following two metrics (losses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">ALIGNMENT</head><p>The alignment loss is straightforwardly defined with the expected distance between positive pairs: Figure <ref type="figure">4</ref>: Average pairwise G 2 potential as a measure of uniformity. Each plot shows 10000 points distributed on S 1 , obtained via either applying an encoder on CIFAR-10 validation set (same as those in Figure <ref type="figure" target="#fig_2">3</ref>) or sampling from a distribution on S 1 , as described in plot titles. We show the points with Gaussian KDE and the angles with vMF KDE.</p><formula xml:id="formula_3">L align (f ; α) − E (x,y)∼ppos [ f (x) − f (y)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">UNIFORMITY</head><p>We want the uniformity metric to be both asymptotically correct (i.e., the distribution optimizing this metric should converge to uniform distribution) and empirically reasonable with finite number of points. To this end, we consider the Gaussian potential kernel (also known as the Radial Basis Function (RBF) kernel) G t : S d × S d → R + <ref type="bibr" target="#b10">(Cohn &amp; Kumar, 2007;</ref><ref type="bibr" target="#b5">Borodachov et al., 2019)</ref>:</p><formula xml:id="formula_4">G t (u, v) e −t u−v 2 2 = e 2t•u T v−2t , t &gt; 0,</formula><p>and define the uniformity loss as the logarithm of the average pairwise Gaussian potential:</p><formula xml:id="formula_5">L uniform (f ; t) log E x,y i.i.d. ∼ p data [G t (u, v)] = log E x,y i.i.d. ∼ p data e −t f (x)−f (y) 2 2 , t &gt; 0,</formula><p>where t is a fixed parameter.</p><p>The average pairwise Gaussian potential is nicely tied with the uniform distribution on the unit hypersphere.</p><p>Definition (Uniform distribution on S d ). σ d denotes the normalized surface area measure on S d .</p><p>First, we show that the uniform distribution is the unique distribution that minimize the expected pairwise potential.</p><p>Proposition 1. For M(S d ) the set of Borel probability measures on S d , σ d is the unique solution of</p><formula xml:id="formula_6">min µ∈M(S d ) u v G t (u, v) dµ dµ.</formula><p>Proof. See appendix.</p><p>In addition, as number of points goes to infinity, distributions of points minimizing the average pairwise potential converge weak * to the uniform distribution. Recall the definition of the weak * convergence of measures.</p><p>Definition (Weak * convergence of measures). A sequence of Borel measures {µ n } ∞ n=1 in R p converges weak * to a Borel measure µ if for all continuous function f : R p → R, we have</p><formula xml:id="formula_7">lim n→∞ f (x) dµ n (x) = f (x) dµ(x).</formula><p>Proposition 2. For each N &gt; 0, the N point minimizer of the average pairwise potential is</p><formula xml:id="formula_8">u * N = arg min u1,u2,...,u N ∈S d 1≤i&lt;j≤N G t (u i , u j ).</formula><p>The normalized counting measures associated with the {u * N } ∞ N =1 sequence converge weak * to σ d .</p><p>Proof. See appendix.</p><p>Designing an objective minimized by the uniform distribution is in fact nontrivial. For instance, average pairwise dot products or Euclidean distances is simply optimized by any distribution that has zero mean. Among kernels that achieve uniformity at optima, the Gaussian kernel is special in that it is closely related to the universally optimal point configurations and can also be used to represent a general class of other kernels, including the Riesz s-potentials. We refer readers to <ref type="bibr" target="#b5">Borodachov et al. (2019)</ref> and <ref type="bibr" target="#b10">Cohn &amp; Kumar (2007)</ref> for in-depths discussion on these topics. Moreover, as we show below, L uniform , defined with the Gaussian kernel, has close connections with L contrastive .</p><p>Empirically, we evaluate the average pairwise potential of various finite point collections on S 1 in Figure <ref type="figure">4</ref>. The values nicely align with our intuitive understanding of uniformity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Limiting Behavior of Contrastive Learning</head><p>In this section, we formalize the intuition that contrastive learning optimizes alignment and uniformity, and characterize its asymptotic behavior. We consider optimization problems over all measurable encoder functions from the p data measure in R n to the Borel space S m−1 .</p><p>We first define the notion of optimal encoders for each of these two metrics. Definition (Perfect Alignment). We say an encoder f is perfectly aligned if f (x) = f (y) a.s. over (x, y) ∼ p pos . Definition (Perfect Uniformity). We say an encoder f is perfectly uniform if the distribution of f (x) for x ∼ p data is the uniform distribution σ m−1 on S m−1 .</p><p>Realizability of perfect uniformity. We note that it is not always possible to achieve perfect uniformity, e.g., when the data manifold in R n is lower dimensional than the feature space S m−1 . Moreover, in the case that p data and p pos are formed from sampling augmented samples from a finite dataset, there cannot be an encoder that is both perfectly aligned and perfectly uniform, because perfect alignment implies that all augmentations from a single element have the same feature vector. Nonetheless, perfectly uniform encoder functions do exist under the conditions that n ≥ m − 1 and p data has bounded density.</p><p>We first analyze the asymptotics with infinite negative samples. Existing empirical work has established that larger number of negative samples consistently leads to better downstream task performances <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>, and often uses very large values (e.g., M = 65536 in He et al. ( <ref type="formula">2019</ref>)). The following theorem nicely confirms that optimizing w.r.t. the limiting loss indeed requires both alignment and uniformity.</p><p>Theorem 1 (Asymptotics of L contrastive ). For fixed τ &gt; 0, as the number of negative samples M → ∞, the (normalized) contrastive loss converges to</p><formula xml:id="formula_9">lim M →∞ L contrastive (f ; τ, M ) − log M = − 1 τ E (x,y)∼ppos f (x) T f (y) + E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ .</formula><p>(2)</p><p>We have the following results:</p><p>1. The first term is minimized iff f is perfectly aligned.</p><p>2. If perfectly uniform encoders exist, they form the exact minimizers of the second term.</p><p>3. For the convergence in Equation (2), the absolute deviation from the limit decays in O(M −2/3 ).</p><p>Proof. See appendix.</p><p>Relation with L uniform . The proof of Theorem 1 in the appendix connects the asymptotic L contrastive form with minimizing average pairwise Gaussian potential, i.e., minimizing L uniform . Compared with the second term of Equation ( <ref type="formula" target="#formula_23">2</ref>), L uniform essentially pushes the log outside the outer expectation, without changing the minimizer (perfectly uniform encoders). However, due to its pairwise nature, L uniform is much simpler in form and avoids the computationally expensive softmax operation in L contrastive <ref type="bibr" target="#b12">(Goodman, 2001;</ref><ref type="bibr" target="#b2">Bengio et al.;</ref><ref type="bibr" target="#b16">Gutmann &amp; Hyvärinen, 2010;</ref><ref type="bibr" target="#b15">Grave et al., 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2018)</ref>.</p><p>Relation with feature distribution entropy estimation.</p><p>When p data is uniform over finite samples {x 1 , x 2 , . . . , x N } (e.g., a collected dataset), the second term in Equation ( <ref type="formula" target="#formula_23">2</ref>) can be alternatively viewed as a resubstitution entropy estimator of f (x) <ref type="bibr" target="#b0">(Ahmad &amp; Lin, 1976)</ref>, where x follows the underlying distribution p nature that generates {x i } N i=1 , via a von Mises-Fisher (vMF) kernel density estimation (KDE):</p><formula xml:id="formula_10">E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ = 1 N N i=1 log   1 N N j=1 e f (xi) T f (xj )/τ   = 1 N N i=1 log pvMF-KDE (f (x i )) + log Z vMF − Ĥ(f (x)) + log Z vMF , x ∼ p nature − Î(x; f (x)) + log Z vMF , x ∼ p nature , where • pvMF-KDE is the KDE based on samples {f (x j )} N j=1 using a vMF kernel with κ = τ −1 ,</formula><p>• Z vMF is the normalization constant for vMF distribution with κ = τ −1 ,</p><p>• Ĥ denotes the resubstitution entropy estimator,</p><p>• Î denotes the mutual information estimator based on Ĥ, since f is a deterministic function.</p><p>Relation with the InfoMax principle. Many empirical works are motivated by the InfoMax principle, i.e., maximizing I(f (x); f (y)) for (x, y) ∼ p pos . However, the interpretation of L contrastive as a lower bound of I(f (x); f (y)) is known to be inconsistent with the actual behavior in practice <ref type="bibr" target="#b48">(Tschannen et al., 2019)</ref>. Our results instead analyze the properties of L contrastive itself. In particular, both Theorem 1 and the above connection with maximizing an entropy estimator provide alternative interpretations and motivations for L contrastive : aligned and information-preserving encoders (since a perfectly uniform encoder is indeed the most entropic and achieves the largest possible I(x; f (x))).</p><p>Finally, even for the case where only a single negative sample is used (i.e., M = 1), we can still prove a weaker result, which we describe in details in the appendix.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we empirically verify the hypothesis that alignment and uniformity are desired properties for representations. Recall that our two metrics are</p><formula xml:id="formula_11">L align (f ; α) E (x,y)∼ppos [ f (x) − f (y) α 2 ] L uniform (f ; t) log E x,y i.i.d. ∼ p data e −t f (x)−f (y) 2 2 .</formula><p>We conduct extensive experiments with convolutional neural network (CNN) and recurrent neural network (RNN) based encoders on four popular representation learning benchmarks with distinct types of downstream tasks: For image datasets, we follow the standard practice and choose positive pairs as two independent augmentations of the same image. For BOOKCORPUS, positive pairs are chosen as neighboring sentences, following Quick-Thought Vectors <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref>.</p><p>We perform majority of our analysis on STL-10 and NYU-DEPTH-V2 encoders, where we calculate L contrastive with negatives being other samples within the minibatch following the standard practice <ref type="bibr" target="#b23">(Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>, and L uniform as the logarithm of average pairwise feature potentials also within the minibatch. Due to their simple forms, these two losses can be implemented in PyTorch <ref type="bibr" target="#b38">(Paszke et al., 2019)</ref> with less than 10 lines of code, as shown in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>To investigate alignment and uniformity properties on recent contrastive representation learning variants and larger datasets, we also analyze IMAGENET-100 encoders trained with Momentum Contrast (MoCo) <ref type="bibr" target="#b21">(He et al., 2019)</ref> and BOOKCORPUS encoders trained with Quick-Thought Vectors <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref>, with these methods modified to also allow L align and L uniform .</p><p>We  • (possibly zero) weights on the three losses,</p><formula xml:id="formula_12">• temperature τ for L contrastive , • α ∈ {1, 2} for L align , • t ∈ {1, 2, . . . , 8} for L uniform ,</formula><p>• number of training epochs,</p><p>• learning rate,</p><p>• batch size (affecting the number of (negative) pairs for L contrastive and L uniform ),</p><p>• embedding dimension,</p><p>• initialization (from scratch vs. a pretrained encoder).</p><p>See the appendix for more experiment details and the exact configurations used.</p><p>L align and L uniform strongly agree with downstream task performance. For each encoder, we measure the downstream task performance, and the L align , L uniform metrics on the validation set. Figure <ref type="figure" target="#fig_6">5</ref> visualizes the trends between both metrics and representation quality. We observe that the two metrics strongly agrees the representation quality overall. In particular, the best performing encoders are exactly the ones with low L align and L uniform , i.e., the lower left corners in Figure <ref type="figure" target="#fig_6">5</ref>.</p><p>Directly optimizing only L align and L uniform can lead to better representations. As shown in Tables <ref type="table" target="#tab_2">1 and 2</ref>, encoders trained with only L align and L uniform consistently outperform their L contrastive -trained counterparts, for both tasks. Theoretically, Theorem 1 showed that L contrastive optimizes alignment and uniformity asymptotically with infinite negative samples. This empirical performance gap suggests that directly optimizing these properties can be superior in practice, when we can only have finite negatives.</p><p>Both alignment and uniformity are necessary for a good representation. Figure <ref type="figure" target="#fig_8">7</ref> shows how the final encoder changes in response to optimizing differently weighted combinations of L align and L uniform on STL-10. The tradeoff between the L align and L uniform indicates that perfect alignment and perfect uniformity are likely hard to simultaneously achieve in practice. However, the inverted-Ushaped accuracy curve confirms that both properties are indeed necessary for a good encoder. When L align is weighted much higher than L uniform , degenerate solution occurs and all inputs are mapped to the same feature vector (exp L uniform = 1). However, as long as the ratio between two weights is not too large (e.g., &lt; 4), we observe that the representation quality remains relatively good and insensitive to the exact weight choices.    L align and L uniform causally affect downstream task performance. We take an encoder trained with L contrastive using a suboptimal temperature τ = 2.5, and finetune it according to L align and/or L uniform . Figure <ref type="figure" target="#fig_9">8</ref> visualizes the finetuning trajectories. When only one of alignment and uniformity is optimized, the corresponding metric improves, but both the other metric and performance degrade. However, when both properties are optimized, the representation quality steadily increases. These trends confirm the causal effect of alignment and uniformity on the representation quality, and suggest that directly optimizing them can be a reasonable choice.</p><p>Alignment and uniformity also matter in other contrastive representation learning variants. MoCo <ref type="bibr" target="#b21">(He et al., 2019)</ref> and Quick-Thought Vectors <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref> are contrastive representation learning variants that have nontrivial differences with directly optimizing L contrastive in Equation (1). MoCo introduces a memory queue and a momentum encoder. Quick-Thought Vectors uses two different encoders to encode each sentence in a positive pair, only normalizes encoder outputs during evaluation, and does not use random sampling to obtain minibatches. After modifying them to also allow L align and L uniform , we train these methods on IMAGENET-100 and BOOKCORPUS, respectively. Figure <ref type="figure" target="#fig_12">9</ref> shows that L align and L uniform metrics are still correlated with the downstream task performances. Tables <ref type="table" target="#tab_5">3 and 4</ref> show that directly optimizing them also leads to comparable or better representation quality. These results suggest that alignment and uniformity are indeed desirable properties for representations, for both image and text modalities, and are likely connected with general contrastive representation learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Alignment and uniformity are often alluded to as motivations for representation learning methods (see Figure <ref type="figure">1</ref>). However, a thorough understanding of these properties is lacking in the literature.   Are they in fact related to the representation learning methods? Do they actually agree with the representation quality (measured by downstream task performance)?</p><p>In this work, we have presented a detailed investigation on the relation between these properties and the popular paradigm of contrastive representation learning. Through theoretical analysis and extensive experiments, we are able to relate the contrastive loss with the alignment and uniformity properties, and confirm their strong connection with downstream task performances. Remarkably, we have revealed that directly optimizing our proposed metrics often leads to representations of better quality.</p><p>Below we summarize several suggestions for future work.</p><p>Niceness of the unit hypersphere. Our analysis was based on the empirical observation that representations are often 2 normalized. Existing works have motivated this choice from a manifold mapping perspective <ref type="bibr" target="#b32">(Liu et al., 2017;</ref><ref type="bibr" target="#b11">Davidson et al., 2018)</ref> and computation stability <ref type="bibr" target="#b53">(Xu &amp; Durrett, 2018;</ref><ref type="bibr" target="#b49">Wang et al., 2017)</ref>. However, to our best knowledge, the question of why the unit hypersphere is a nice feature space is not yet rigorously answered. One possible direction is to formalize the intuition that connected sets with smooth boundaries are nearly linearly separable in the hyperspherical geometry (see Figure <ref type="figure" target="#fig_0">2</ref>), since linear separability is one of the most widely used criteria for representation quality and is related to the notion of disentanglement <ref type="bibr" target="#b23">(Higgins et al., 2018)</ref>.</p><p>Beyond contrastive learning. Our analysis focused on the relationship between contrastive learning and the alignment and uniformity properties on the unit hypersphere. However, the ubiquitous presence of 2 normalization in the representation learning literature suggests that the connection may be more general. In fact, several existing empirical methods are directly related to uniformity on the hypersphere <ref type="bibr" target="#b4">(Bojanowski &amp; Joulin, 2017;</ref><ref type="bibr" target="#b11">Davidson et al., 2018;</ref><ref type="bibr" target="#b53">Xu &amp; Durrett, 2018)</ref>. We believe that relating a broader class of representations to uniformity and/or alignment on the hypersphere will provide novel insights and lead to better empirical algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs and Additional Theoretical Results</head><p>In this section, we present proofs for propositions and theorems in main paper Sections 4.1.2 and 4.2.</p><p>The propositions in Section 4.1.2 illustrate the deep relations between the Gaussian kernel G t : S d × S d → R and the uniform distribution on the unit hypersphere S d . As we will show below in Section A.1, these properties directly follow well-known results on strictly positive definite kernels.</p><p>In Section A.2, we present a proof for Theorem 1. Theorem 1 describes the asymptotic behavior of L contrastive as the number of negative samples M approaches infinity. The theorem is strongly related to empirical contrastive learning, given an error term (deviation from the limit) decaying in O(M −2/3 ) and that empirical practices often use a large number of negatives (e.g., M = 65536 in He et al. ( <ref type="formula">2019</ref>)) based on the observation that using more negatives consistently leads to better representation quality <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b21">He et al., 2019)</ref>. Our proof further reveals connections between L contrastive and L uniform which is defined via the Gaussian kernels.</p><p>Finally, also in Section A.2, we present a weaker result on the setting where only a single negative is used in L contrastive (i.e., M = 1).</p><p>A.1. Proofs for Section 4.1.2</p><p>To prove Propositions 1 and 2, we utilize the strict positive definiteness <ref type="bibr" target="#b3">(Bochner, 1992;</ref><ref type="bibr" target="#b42">Stewart, 1976)</ref> of the Gaussian kernel G t :</p><formula xml:id="formula_13">G t (u, v) e −t u−v 2 2 = e 2t•u T v−2t , t &gt; 0.</formula><p>From there, we apply a known result about such kernels, from which the two propositions directly follow.</p><p>Definition (Strict positive definiteness <ref type="bibr" target="#b3">(Bochner, 1992;</ref><ref type="bibr" target="#b42">Stewart, 1976)</ref>). A symmetric and lower semi-continuous kernel K on A × A (where A is infinite and compact) is called strictly positive definite if for every finite signed Borel measure µ supported on A whose energy</p><formula xml:id="formula_14">I K [µ] S d S d K(u, v) dµ(v) dµ(u)</formula><p>is well defined, we have I K [µ] ≥ 0, where equality holds only if µ ≡ 0 on the σ-algebra of Borel subsets of A.</p><p>Definition. Let M(S d ) be the set of Borel probability measures on S d .</p><p>We are now in the place to apply the following two well-known results, which we present by restating Proposition 4.4.1, Theorem 6.2.1 and Corollary 6.2.2 of <ref type="bibr" target="#b5">Borodachov et al. (2019)</ref> in weaker forms. We refer readers to <ref type="bibr" target="#b5">Borodachov et al. (2019)</ref> for their proofs.</p><p>Lemma 1 (Strict positive definiteness of G t ). For t &gt; 0, the Gaussian kernel</p><formula xml:id="formula_15">G t (u, v) e −t u−v 2 2 = e 2t•u T v−2t is strictly positive definite on S d × S d .</formula><p>Lemma 2 (Strictly positive definite kernels on S d ). Consider kernel</p><formula xml:id="formula_16">K f : S d × S d → (−∞, +∞] of the form, K f (u, v) f ( u − v 2 2 ).</formula><p>(3)</p><formula xml:id="formula_17">If K f is strictly positive definite on S d × S d and I K f [σ d ] is finite, then σ d is the unique measure (on Borel subsets of S d ) in the solution of min µ∈M(S d ) I K f [µ],</formula><p>and the normalized counting measures associated with any K f -energy minimizing sequence of N -point configurations on S d converges weak * to σ d .</p><p>In particular, this conclusion holds whenever f has the property that −f (t) is strictly completely monotone on (0, 4] and</p><formula xml:id="formula_18">I K f [σ d ] is finite.</formula><p>We now recall Propositions 1 and 2.</p><p>Proposition 1. σ d is the unique solution (on Borel subsets of S d ) of</p><formula xml:id="formula_19">min µ∈M(S d ) I Gt [µ] = min µ∈M(S d ) S d S d G t (u, v) dµ(v) dµ(u).<label>(4)</label></formula><p>Proof of Proposition 1. This is a direct consequence of Lemmas 1 and 2.</p><p>Proposition 2. For each N &gt; 0, the N point minimizer of the average pairwise potential is</p><formula xml:id="formula_20">u * N = arg min u1,u2,...,u N ∈S d 1≤i&lt;j≤N G t (u i , u j ).</formula><p>The normalized counting measures associated with the {u * N } ∞ N =1 sequence converge weak * to σ d .</p><p>Proof of Proposition 2. This is a direct consequence of Lemmas 1 and 2.</p><p>A.2. Proofs and Additional Results for Section 4.2</p><p>The following lemma directly follows Theorem 3.3 and Remarks 3.4 (b)(i) of <ref type="bibr" target="#b41">Serfozo (1982)</ref>. We refer readers to <ref type="bibr" target="#b41">Serfozo (1982)</ref> for its proof.</p><p>Lemma 3. Let A be a compact second countable Hausdorff space. Suppose 1. {µ n } ∞ n=1 is a sequence of finite and positive Borel measures supported on A that converges weak * to some finite and positive Borel measure µ (which is same as vague convergence since A is compact);</p><formula xml:id="formula_21">2. {f n } ∞</formula><p>n=1 is a sequence of Borel measurable functions that converges continuously to a Borel measurable f ; 3. {f n } n are uniformly bounded over A.</p><p>Then, we have the following convergence:</p><formula xml:id="formula_22">lim n→∞ x∈A f n (x) dµ n (x) = x∈A f (x) dµ(x).</formula><p>We now recall Theorem 1.</p><p>Theorem 1 (Asymptotics of L contrastive ). For fixed τ &gt; 0, as the number of negative samples M → ∞, the (normalized) contrastive loss converges to</p><formula xml:id="formula_23">lim M →∞ L contrastive (f ; τ, M ) − log M = lim M →∞ E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data − log e f (x) T f (y)/τ e f (x) T f (y)/τ + i e f (x − i ) T f (y)/τ − log M = − 1 τ E (x,y)∼ppos f (x) T f (y) + E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ .<label>(2)</label></formula><p>We have the following results:</p><p>1. The first term is minimized iff f is perfectly aligned.</p><p>2. If perfectly uniform encoders exist, they form the exact minimizers of the second term.</p><p>3. For the convergence in Equation (2), the absolute deviation from the limit (i.e., the error term) decays in O(M −2/3 ).</p><p>Proof of Theorem 1. We first show the convergence stated in Equation ( <ref type="formula" target="#formula_23">2</ref>) along with its speed (result 3), and then the relations between the two limiting terms and the alignment and uniformity properties (results 1 and 2).</p><p>• Proof of the convergence in Equation (2) and the O(M −2/3 ) decay rate of its error term (result 3).</p><p>Note that for any x, y ∈ R n and {x</p><formula xml:id="formula_24">− i } M i=1 i.i.d.</formula><p>∼ p data , we have</p><formula xml:id="formula_25">lim M →∞ log 1 M e f (x) T f (y)/τ + 1 M M i=1 e f (x − i ) T f (x)/τ = log E x − ∼p data e f (x − ) T f (x)/τ almost surely, (5)</formula><p>by the strong law of large numbers (SLLN) and the Continuous Mapping Theorem.</p><p>Then, we can derive</p><formula xml:id="formula_26">lim M →∞ L contrastive (f ; τ, M ) − log M = E (x,y)∼ppos −f (x) T f (y)/τ + lim M →∞ E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data log 1 M e f (x) T f (y)/τ + 1 M M i=1 e f (x − i ) T f (x)/τ = E (x,y)∼ppos −f (x) T f (y)/τ + E lim M →∞ log 1 M e f (x) T f (y)/τ + 1 M M i=1 e f (x − i ) T f (x)/τ = − 1 τ E (x,y)∼ppos f (x) T f (y) + E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ ,</formula><p>where we justify the switching of expectation and limit by the convergence stated in Equation ( <ref type="formula">5</ref>), the boundedness of e u T v/τ (where u, v ∈ S d , τ &gt; 0), and the Dominated Convergence Theorem (DCT).</p><p>For convergence speed, we consider both sides:</p><formula xml:id="formula_27">(L contrastive (f ; τ, M ) − log M ) − lim M →∞ L contrastive (f ; τ, M ) − log M = E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data log 1 M e f (x) T f (y)/τ + 1 M M i=1 e f (x − i ) T f (x)/τ − E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ ≤ E x∼p data {x − i } M i=1 i.i.d. ∼ p data log 1 M e 1/τ + 1 M M i=1 e f (x − i ) T f (x)/τ − E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ ≤ E x∼p data log E {x − i } M i=1 i.i.d. ∼ p data 1 M e 1/τ + 1 M M i=1 e f (x − i ) T f (x)/τ − log E x − ∼p data e f (x − ) T f (x)/τ = E x∼p data log E x − ∼p data 1 M e 1/τ + e f (x − ) T f (x)/τ − log E x − ∼p data e f (x − ) T f (x)/τ ≤ E x∼p data 1 M e 2/τ = 1 M e 2/τ ,<label>(6)</label></formula><p>where the last inequality follows the concavity of log, and</p><formula xml:id="formula_28">lim M →∞ L contrastive (f ; τ, M ) − log M − (L contrastive (f ; τ, M ) − log M ) = E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data log E x − ∼p data e f (x − ) T f (x)/τ − log 1 M e f (x) T f (y)/τ + 1 M M i=1 e f (x − i ) T f (x)/τ ≤ M M + 1 e 1/τ E (x,y)∼ppos {x − i } M i=1 i.i.d. ∼ p data E x − ∼p data e f (x − ) T f (x)/τ − 1 M e f (x) T f (y)/τ + 1 M M i=1 e f (x − i ) T f (x)/τ ≤ 1 M + 1 e 2/τ + M M + 1 e 1/τ E x,{x − i } M i=1 i.i.d. ∼ p data E x − ∼p data e f (x − ) T f (x)/τ − 1 M M i=1 e f (x − i ) T f (x)/τ ≤ 1 M + 1 e 2/τ + 5 4 M 1/3 M + 1 e 1/τ e 1/τ − e −1/τ ,<label>(7)</label></formula><p>where the first inequality follows the concavity of log and the last inequality follows the simple bound from Chebychev's inequality: denoting i.i.d. random variables e f (xi) T f (x)/τ for x i ∼ p data as Y i with supp(Y i ) ⊂ [e −1/τ , e 1/τ ], and their</p><formula xml:id="formula_29">mean as Ȳ E [Y i ], we have E 1 M M i=1 Y i − Ȳ ≤ P 1 M M i=1 Y i − Ȳ ≥ M −2/3 e 1/τ − e −1/τ e 1/τ − e −1/τ + 1 − P 1 M M i=1 Y i − Ȳ ≥ M −2/3 e 1/τ − e −1/τ M −2/3 e 1/τ − e −1/τ ≤ Var [Y i ] M 2 • M −4/3 (e 1/τ − e −1/τ ) 2 e 1/τ − e −1/τ + M −2/3 e 1/τ − e −1/τ ≤ 5 4 M −2/3 e 1/τ − e −1/τ ,</formula><p>where the last inequality is from Var [Y i ] ≤ 1 4 e 1/τ − e −1/τ 2 given its bounded support. Combining both sides, we can immediately see that the absolute deviation from the limit decays in O(M −2/3 ).</p><p>• Proof of result 1: The first term is minimized iff f is perfectly aligned.</p><formula xml:id="formula_30">Note that for u, v ∈ S d , u − v 2 2 = 2 − 2 • u T v.</formula><p>Then the result follows directly the definition of perfect alignment, and the existence of perfectly aligned encoders (e.g., an encoder that maps every input to the same output vector).</p><p>• Proof of result 2: If perfectly uniform encoders exist, they form the exact minimizers of the second term.</p><p>For simplicity, we define the following notation:</p><p>Definition. ∀µ ∈ M(S d ), u ∈ S d , we define the continuous and Borel measurable function</p><formula xml:id="formula_31">U µ (u) S d e u T v/τ dµ(v).<label>(8)</label></formula><p>with its range bounded in [e −1/τ , e 1/τ ].</p><p>Then the second term can be equivalently written as</p><formula xml:id="formula_32">E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ = E x∼p data log U p data •f −1 (f (x)) ,</formula><p>where</p><formula xml:id="formula_33">p data • f −1 ∈ M(S d</formula><p>) is the probability measure of features, i.e., the pushforward measure of p data via f . We now consider the following relaxed problem, where the minimization is taken over M(S d ), all possible Borel probability measures on the hypersphere S d :</p><formula xml:id="formula_34">min µ∈M(S d ) S d log U µ (u) dµ(u). (<label>9</label></formula><formula xml:id="formula_35">)</formula><p>Our strategy is to show that the unique minimizer of Equation ( <ref type="formula" target="#formula_34">9</ref>) is σ d , from which the result 2 directly follows. The rest of the proof is structured in three parts.</p><p>1. We show that minimizers of Equation (9) exist, i.e., the above infimum is attained for some µ ∈ M(S d ).</p><p>Let {µ m } ∞ m=1 be a sequence in M(S d ) such that the infimum of Equation ( <ref type="formula" target="#formula_34">9</ref>) is reached in the limit:</p><formula xml:id="formula_36">lim m→∞ S d log U µm (u) dµ m (u) = inf µ∈M(S d ) S d log U µ (u) dµ(u).</formula><p>From the Helly's Selection Theorem, let µ * denote some weak * cluster point of this sequence. Then µ m converges weak * to µ * along a subsequence m ∈ N ∈ N. For simplicity and with a slight abuse of notation, we denote this convergent (sub)sequence of measures by {µ n } ∞ n=1 .</p><p>We want to show that µ * attains the limit (and thus the infimum), i.e.,</p><formula xml:id="formula_37">S d log U µ * (u) dµ * (u) = lim n→∞ S d log U µn (u) dµ n (u). (<label>10</label></formula><formula xml:id="formula_38">)</formula><p>In view of Lemma 3, since S d is a compact second countable Hausdorff space and {log U µn } n is uniformly bounded over S d , it remains to prove that {log U µn } n is continuously convergent to log U µ * . Consider any convergent sequence of points {x n } ∞ n=1 ∈ R d+1 s.t. x n → x where x ∈ S d . Let δ n = x n − x. By simply expanding U µn and µ µ * , we have e − δn /τ U µn (x) ≤ U µn (x n ) ≤ e δn /τ U µn (x).</p><p>Since both the upper and the lower bound converge to U µ * (x) (by the weak * convergence of {µ n } n to µ * ), U µn (x n ) must as well. We have proved the continuous convergence of {log U µn } n to log U µ * . Therefore, the limit in Equation ( <ref type="formula" target="#formula_37">10</ref>) holds. The infimum is thus attained at µ * :</p><formula xml:id="formula_39">lim n→∞ u log U µn (u) dµ n = u log U µ * (u) dµ * .</formula><p>2. We show that U µ * is constant µ * -almost surely for any minimizer µ * of Equation (9). Let µ * be any solution of Equation ( <ref type="formula" target="#formula_34">9</ref>): Note that for any such T ∈ T , the mixture (1 − α)µ * + αµ * T is a valid probability distribution (i.e., in M(S d )) for α ∈ (−µ * (T ), 1), an open interval containing 0. By the first variation, we must have</p><formula xml:id="formula_40">µ * ∈ arg min µ∈M(S d ) u log U µ (u) dµ.</formula><formula xml:id="formula_41">0 = ∂ ∂α S d log U (1−α)µ * +αµ * T (u) d((1 − α)µ * + αµ * T )(u) α=0 = ∂ ∂α (1 − α) S d log U (1−α)µ * +αµ * T (u) dµ * (u) α=0 + ∂ ∂α α S d log U (1−α)µ * +αµ * T (u) dµ * T (u) α=0 = − S d log U (1−α)µ * +αµ * T (u) dµ * (u) α=0 + ∂ ∂α S d log U (1−α)µ * +αµ * T (u) dµ * (u) α=0 + S d log U (1−α)µ * +αµ * T (u) dµ * T (u) α=0 + 0 • ∂ ∂α S d log U (1−α)µ * +αµ * T (u) dµ * T (u) α=0 = − S d log U µ * (u) dµ * (u) + S d U µ * T (u) − U µ * (u) U µ * (u) dµ * (u) + S d log U µ * (u) dµ * T (u) + 0 • S d U µ * T (u) − U µ * (u) U µ * (u) dµ * T (u) = S d U µ * T (u) U µ * (u) dµ * (u) + S d log U µ * (u) d(µ * T − µ * )(u) − 1,<label>(11)</label></formula><p>where the Leibniz rule along with the boundedness of U µ * and U µ * Tn together justify the exchanges of integration and differentiation.</p><p>Let {T n } ∞ n=1 be a sequence of sets in T such that</p><formula xml:id="formula_42">lim n→∞ S d U µ * (u) dµ * Tn (u) = sup T ∈T S d U µ * (u) dµ * T (u) U * ,</formula><p>where the supremum must exist since U µ * is bounded above. Because U µ * is a continuous and Borel measurable function, we have {u :</p><formula xml:id="formula_43">U µ * (u) &gt; U * } ∈ B(S d ) and thus µ * ({u : U µ * (u) &gt; U * }) = 0, µ * Tn ({u : U µ * (u) &gt; U * }) = 0, ∀n = 1, 2, . . . , otherwise {u : U µ * (u) &gt; U * } ∈ T . Asymptotically, U µ * is constant µ * Tn -almost surely: S d U µ * (u) − S d U µ * (u ) dµ * Tn (u ) dµ * Tn (u) = 2 S d max 0, U µ * (u) − S d U µ * (u ) dµ * Tn (u ) dµ * Tn (u) ≤ 2(U * − S d U µ * (u) dµ * Tn (u)) → 0, as n → ∞,</formula><p>where the inequality follows the boundedness of U µ * and that µ * Tn ({u : U µ * (u) &gt; U * }) = 0. Therefore, given the continuity of log and the boundedness of U µ * , we have</p><formula xml:id="formula_44">lim n→∞ S d log U µ * (u) dµ * Tn = log U * .</formula><p>Equation ( <ref type="formula" target="#formula_41">11</ref>) gives that ∀n = 1, 2, . . . ,</p><formula xml:id="formula_45">1 = S d U µ * Tn (u) U µ * (u) dµ * + S d log U µ * (u) d(µ * Tn − µ * ) ≥ 1 U * S d U µ * Tn (u) dµ * (u) + S d log U µ * (u) dµ * Tn − S d log U µ * (u) dµ * = 1 U * S d U µ * (u) dµ * Tn (u) + S d log U µ * (u) dµ * Tn − S d log U µ * (u) dµ * ,</formula><p>where the inequality follows the boundedness of</p><formula xml:id="formula_46">U µ *</formula><p>Tn U µ * and that µ * ({u : U µ * (u) &gt; U * }) = 0. Taking the limit of n → ∞ on both sides, we have</p><formula xml:id="formula_47">1 = lim n→∞ 1 ≥ 1 U * lim n→∞ S d U µ * (u) dµ * Tn (u) + lim n→∞ S d log U µ * (u) dµ * Tn (u) − S d log U µ * (u) dµ * (u) = 1 + log U * − S d log U µ * (u) dµ * (u) ≥ 1 + log U * − log S d U µ * (u) dµ * (u) ≥ 1,</formula><p>where the last inequality holds because the supremum taken over T ⊃ {S d }.</p><p>Since 1 = 1, all inequalities must be equalities. In particular,</p><formula xml:id="formula_48">S d log U µ * (u) dµ * (u) = log S d U µ * (u) dµ * (u).</formula><p>That is, for any solution µ * of Equation ( <ref type="formula" target="#formula_34">9</ref>), U µ * must be constant µ * -almost surely.</p><p>3. We show that σ d is the unique minimizer of the relaxed problem in Equation (9).</p><p>Let S ⊂ M(S d ) be the set of measures where the above property holds:</p><formula xml:id="formula_49">S µ ∈ M(S d ) : U µ is constant µ-almost surely .</formula><p>The problem in Equation ( <ref type="formula" target="#formula_34">9</ref>) is thus equivalent to minimizing over S:</p><formula xml:id="formula_50">arg min µ∈M(S d ) S d log U µ (u) dµ(u) = arg min µ∈S S d log U µ (u) dµ(u) = arg min µ∈S log S d U µ (u) dµ(u) = arg min µ∈S log S d S d e u T v/τ dµ(v) dµ(u) = arg min µ∈S 1 τ + log S d S d e − 1 2τ u−v 2 dµ(v) dµ(u) = arg min µ∈S S d S d G 1 2τ (u, v) dµ(v) dµ(u).</formula><p>By Proposition 1 and τ &gt; 0, we know that the uniform distribution σ d is the unique solution to</p><formula xml:id="formula_51">arg min µ∈M(S d ) S d S d G 1 2τ (u, v) dµ(v) dµ(u).<label>(12)</label></formula><p>Since σ d ∈ S, it must also be the unique solution to Equation ( <ref type="formula" target="#formula_34">9</ref>).</p><p>Finally, if perfectly uniform encoders exist, σ d is realizable, and they are the exact encoders that realize it. Hence, in such cases, they are the exact minimizers of</p><formula xml:id="formula_52">min f E x∼p data log E x − ∼p data e f (x − ) T f (x)/τ .</formula><p>Relation between Theorem 1, L align and L uniform . The first term of Equation ( <ref type="formula" target="#formula_23">2</ref>) is equivalent with L align when α = 2, up to a constant and a scaling. In the above proof, we showed that the second term favors uniformity, via the feature distribution that minimizes the pairwise Gaussian kernel (see Equation ( <ref type="formula" target="#formula_51">12</ref>)):</p><formula xml:id="formula_53">arg min µ∈M(S d ) S d S d G 1 2τ (u, v) dµ(v) dµ(u),<label>(13)</label></formula><p>which can be alternatively viewed as the relaxed problem of optimizing for the uniformity loss L uniform :</p><formula xml:id="formula_54">arg min f L uniform (f ; 1 2τ ) = arg min f E x,y i.i.d. ∼ p data G 1 2τ (f (x), f (y)) .<label>(14)</label></formula><p>The relaxation comes from the observation that Equation ( <ref type="formula" target="#formula_53">13</ref>) minimizes over all feature distributions on S d , while Equation ( <ref type="formula" target="#formula_54">14</ref>) only considers the realizable ones.</p><p>Relation between Equation (9) and minimizing average pairwise Gaussian potential (i.e., minimizing L uniform ). In view of the Proposition 1 and the proof of Theorem 1, we know that the uniform distribution σ d is the unique minimizer of both of the following problems:</p><formula xml:id="formula_55">{σ d } = min µ∈M(S d ) log S d S d e u T v/τ dµ(v) dµ(u), {σ d } = min µ∈M(S d ) S d log S d e u T v/τ dµ(v) dµ(u).</formula><p>So pushing the log inside the outer integral doesn't change the solution. However, if we push the log all the way inside the inner integral, the problem becomes equivalent with minimizing the norm of the mean, i.e.,</p><formula xml:id="formula_56">min µ∈M(S d ) E U ∼µ [U ] T E U ∼µ [U ] ,</formula><p>which is minimized for any distribution with mean being the all-zeros vector 0, e.g., 1 2 δ u + 1 2 δ −u for any u ∈ S d (where δ u is the Dirac delta distribution at u s.t. δ u (S) = 1 S (u), ∀S ∈ B(S d )). Therefore, the location of the log is important.</p><p>Theorem 2 (Single negative sample). If perfectly aligned and uniform encoders exist, they form the exact minimizers of the contrastive loss L contrastive (f ; τ, M ) for fixed τ &gt; 0 and M = 1.</p><p>Proof of Theorem 2. Since M = 1, we have</p><formula xml:id="formula_57">L contrastive (f ; τ, 1) = E (x,y)∼ppos x − ∼p data − 1 τ f (x) T f (y) + log e f (x) T f (y)/τ + e f (x − ) T f (x)/τ ≥ E x∼p data x − ∼p data − 1 τ + log e 1/τ + e f (x − ) T f (x)/τ (15) ≥ − 1 τ + min µ∈M(S d ) S d S d log e 1/τ + e u T v/τ dµ(u) dµ(v)<label>(16)</label></formula><formula xml:id="formula_58">= − 1 τ + min µ∈M(S d ) S d S d log e 1/τ + e (2− u−v 2 2 )/(2τ ) dµ(u) dµ(v).</formula><p>By the definition of perfect alignment, the equality in Equation ( <ref type="formula">15</ref>) is satisfied iff f is perfectly aligned.</p><p>Consider the function f : (0, 4] → R + defined as</p><formula xml:id="formula_59">f (t) = log(e 1 τ + e 2−t 2τ</formula><p>).</p><p>It has the following properties:</p><formula xml:id="formula_60">• −f (t) = 1 2τ e − t 2τ 1+e − t 2τ = 1 2τ (1 − (1 + e − t 2τ ) −1</formula><p>) is strictly completely monotone on (0, +∞): ∀t ∈ (0, +∞),</p><formula xml:id="formula_61">1 2τ (1 − (1 + e − t 2τ ) −1 ) &gt; 0 (−1) n d n dt n 1 2τ (1 − (1 + e − t 2τ ) −1 ) = n! (2τ ) n+1 (1 + e − t 2τ ) −(n+1) &gt; 0, n = 1, 2, . . . . • f is bounded on (0, 4].</formula><p>In view of Lemma 2, we have that the equality in Equation ( <ref type="formula" target="#formula_57">16</ref>) is satisfied iff the feature distribution induced by f (i.e., the pushforward measure</p><formula xml:id="formula_62">p data • f −1 ) is σ d , that is, in other words, f is perfectly uniform. Therefore, L contrastive (f ; τ, 1) ≥ − 1 τ + S d S d log e 1/τ + e u T v/τ dσ d (u) dσ d (v) = constant independent of f ,</formula><p>where equality is satisfied iff f is perfectly aligned and uniform. This concludes the proof.</p><p>Difference between conditions of Theorems 1 and 2. We remark that the statement in Theorem 2 is weaker than the previous Theorem 1. Theorem 2 is conditioned on the existence perfectly aligned and uniform encoders. It only shows that L contrastive (f ; τ, M = 1) favors alignment under the condition that perfect uniformity is realizable, and vice versa. In Theorem 1, L contrastive decomposes into two terms, each favoring alignment and uniformity. Therefore, the decomposition in Theorem 1 is exempt from this constraint.  <ref type="bibr" target="#b25">(Ioffe &amp; Szegedy, 2015)</ref>. A sequence of such blocks computes a tensor of the correct spatial shape, from an input containing intermediate activations of a CNN encoder (which downsamples the input RGB image by a power of 2). A final convolution at the end computes the single-channel depth prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Details</head><p>All experiments are performed on 1-4 NVIDIA Titan Xp, Titan X PASCAL, Titan RTX, or 2080 Ti GPUs.</p><p>B.1. CIFAR-10, STL-10 and NYU-DEPTH-V2 Experiments</p><p>For CIFAR-10, STL-10 and NYU-DEPTH-V2 experiments, we use the following settings, unless otherwise stated in Tables <ref type="table">7 and 8</ref> below:</p><p>• Standard data augmentation procedures are used for generating positive pairs, including resizing, cropping, horizontal flipping, color jittering, and random grayscale conversion. This follows prior empirical work in contrastive representation learning <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b46">Tian et al., 2019;</ref><ref type="bibr" target="#b23">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>.</p><p>• Neural network architectures follow the corresponding experiments on these datasets in <ref type="bibr" target="#b46">Tian et al. (2019)</ref>. For NYU-DEPTH-V2 evaluation, the architecture of the depth prediction CNN is described in Table <ref type="table" target="#tab_6">5</ref>.</p><p>• We use minibatch stochastic gradient descent (SGD) with 0.9 momentum and 0.0001 weight decay.</p><p>• We use linearly scaled learning rate (0.12 per 256 batch size) <ref type="bibr" target="#b14">(Goyal et al., 2017)</ref>.</p><p>-CIFAR-10 and STL-10: Optimization is done over 200 epochs, with learning rate decayed by a factor of 0.1 at epochs 155, 170, and 185.</p><p>-NYU-DEPTH-V2: Optimization is done over 400 epochs, with learning rate decayed by a factor of 0.1 at epochs 310, 340, and 370.</p><p>• Encoders are optimized over the training split. For evaluation, we freeze the encoder, and train classifiers / depth predictors on the training set samples, and test on the validation split.</p><p>-CIFAR-10 and STL-10: We use standard train-val split. Linear classifiers are trained with Adam (Kingma &amp; Ba, 2014) over 100 epochs, with β 1 = 0.5, β 2 = 0.999, = 10 −8 , 128 batch size, and an initial learning rate of 0.001, decayed by a factor of 0.2 at epochs 60 and 80.</p><p>-NYU-DEPTH-V2: We use the train-val split on the 1449 labeled images from Nathan <ref type="bibr" target="#b36">Silberman &amp; Fergus (2012)</ref>. Depth predictors are trained with Adam (Kingma &amp; Ba, 2014) over 120 epochs, with β 1 = 0.5, β 2 = 0.999, = 10 −8 , 128 batch size, and an initial learning rate of 0.003, decayed by a factor of 0.2 at epochs 70, 90, 100, and 110.</p><p>At each SGD iteration, a minibatch of K positive pairs is sampled {(x i , y i )} K i=1 , and the three losses for this minibatch are calculated as following:</p><p>• L contrastive : For each x i , the sample contrastive loss is taken with the positive being y i , and the negatives being {y j } j =i .</p><p>For each y i , the sample loss is computed similarly. The minibatch loss is calculated by aggregating these 2K terms:</p><formula xml:id="formula_63">1 2K K i=1 log e f (xi) T f (yi)/τ K j=1 e f (xi) T f (yj )/τ + 1 2K K i=1 log e f (xi) T f (yi)/τ K j=1 e f (xj ) T f (yi)/τ</formula><p>.</p><p>IMAGENET-100 Classes n02869837 n01749939 n02488291 n02107142 n13037406 n02091831 n04517823 n04589890 n03062245 n01773797 n01735189 n07831146 n07753275 n03085013 n04485082 n02105505 n01983481 n02788148 n03530642 n04435653 n02086910 n02859443 n13040303 n03594734 n02085620 n02099849 n01558993 n04493381 n02109047 n04111531 n02877765 n04429376 n02009229 n01978455 n02106550 n01820546 n01692333 n07714571 n02974003 n02114855 n03785016 n03764736 n03775546 n02087046 n07836838 n04099969 n04592741 n03891251 n02701002 n03379051 n02259212 n07715103 n03947888 n04026417 n02326432 n03637318 n01980166 n02113799 n02086240 n03903868 n02483362 n04127249 n02089973 n03017168 n02093428 n02804414 n02396427 n04418357 n02172182 n01729322 n02113978 n03787032 n02089867 n02119022 n03777754 n04238763 n02231487 n03032252 n02138441 n02104029 n03837869 n03494278 n04136333 n03794056 n03492542 n02018207 n04067472 n03930630 n03584829 n02123045 n04229816 n02100583 n03642806 n04336792 n03259280 n02116738 n02108089 n03424325 n01855672 n02090622</p><p>Table <ref type="table">6</ref>: 100 randomly selected IMAGENET classes forming the IMAGENET-100 subset. These classes are the same as the ones used by <ref type="bibr" target="#b46">Tian et al. (2019)</ref>.</p><p>This calculation follows empirical practices and is similar to Oord et al. ( <ref type="formula">2018</ref>); <ref type="bibr" target="#b22">Hénaff et al. (2019)</ref>, and end-to-end in <ref type="bibr" target="#b21">He et al. (2019)</ref>.</p><p>• L align : The minibatch alignment loss is straightforwardly computed as</p><formula xml:id="formula_64">1 K K i=1 f (x i ) − f (y i ) α 2 .</formula><p>• L uniform : The minibatch uniform loss is calculated by considering each pair of {x i } i and {y i } i :</p><formula xml:id="formula_65">1 2 log 2 K(K − 1) i =j e −t f (xi)−f (xj ) 2 2 + 1 2 log 2 K(K − 1) i =j e −t f (yi)−f (yj ) 2 2 .</formula><p>Tables <ref type="table">7 and 8</ref> below describe the full specifications of all 306 STL-10 and 64 NYU-DEPTH-V2 encoders. These experiment results are visualized in main paper Figure <ref type="figure" target="#fig_6">5</ref>, showing a clear connection between representation quality and L align &amp; L uniform metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. IMAGENET-100 and Momentum Contrast (MoCo) Variants</head><p>IMAGENET-100 details. We use the same IMAGENET-100 sampled by <ref type="bibr" target="#b46">Tian et al. (2019)</ref>, containing the 100 randomly selected classes listed in Table <ref type="table">6</ref>.</p><p>MoCo with L align and L uniform . At each SGD iteration, let</p><p>• K be the minibatch size,</p><p>• {f (x i ) i } K i=1 be the batched query features encoded by the current up-to-date encoder f (i.e., q in Algorithm 1 of He et al. ( <ref type="formula">2019</ref>)),</p><p>• {f EMA (y i )} K i=1 be the batched key features encoded by the exponential moving average encoder f EMA (i.e., k in Algorithm 1 of <ref type="bibr" target="#b21">He et al. (2019)</ref>),</p><p>• {queue j } N j=1 be the feature queue, where N is the queue size. L align and L uniform for this minibatch are calculated as following:</p><p>• L align : The minibatch alignment loss is computed as disparity between features from the two encoders:</p><formula xml:id="formula_66">1 K K i=1 f (x i ) − f EMA (y i ) α 2 .</formula><p>• L uniform : We experiment with two forms of L uniform :</p><p>1. Only computing pairwise distance between {f (x i )} i and {queue j } j :</p><formula xml:id="formula_67">log 1 N K K i=1 N j=1 e −t f (xi)−queue j 2 2 . (<label>17</label></formula><formula xml:id="formula_68">)</formula><p>2. Also computing pairwise distance inside {f (x i )} i :</p><formula xml:id="formula_69">log 2 2N K + K(K − 1) K i=1 N j=1 e −t f (xi)−queue j 2 2 + 2 2N K + K(K − 1) i =j e −t f (xi)−f (xj ) 2 2 . (18)</formula><p>Our experiment settings below mostly follow <ref type="bibr" target="#b21">He et al. (2019)</ref> and the unofficial implementation by <ref type="bibr" target="#b45">Tian (2019)</ref>, because the official implementation was not released at the time of performing these analyses:</p><p>• Standard data augmentation procedures are used for generating positive pairs, including resizing, cropping, horizontal flipping, color jittering, and random grayscale conversion, following Tian (2019).</p><p>• Encoder architecture is ResNet50 <ref type="bibr" target="#b20">(He et al., 2016)</ref>.</p><p>• We use minibatch stochastic gradient descent (SGD) with 128 batch size, 0.03 initial learning rate, 0.9 momentum and 0.0001 weight decay.</p><p>• Optimization is done over 240 epochs, with learning rate decayed by a factor of 0.1 at epochs 120, 160, and 200.</p><p>• We use 0.999 exponential moving average factor, following <ref type="bibr" target="#b21">He et al. (2019)</ref>.</p><p>• For evaluation, we freeze the encoder, and train a linear classifier on the training set samples, and test on the validation split. Linear classifiers are trained with minibatch SGD over 60 epochs, with a learning rate of 10, decayed by a factor of 0.2 at epochs 30, 40, and 50. • {x i } K i=1 be the K consecutive sentences forming this minibatch, where K be the minibatch size, • f and g be the two RNN sentence encoders.</p><p>The original Quick-Thought Vectors <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref> does not l2-normalize on encoder outputs during training the encoder. Here we describe the calculation of L contrastive , L align , and L uniform for l2-normalized encoders, in our modified Quick-Thought Vectors method. Note that this does not affect evaluation since features are l2-normalized before using in downstream tasks, following the original Quick-Thought Vectors <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref>. For a minibatch, these losses are calculated as following:</p><p>• L contrastive with temperature:</p><formula xml:id="formula_70">1 K cross entropy(softmax({f (x 1 ) T g(x j )} j ), {0, 1, 0, . . . , 0}) + 1 K K−1 i=2 cross entropy(softmax({f (x i ) T g(x j )} j ), {0, . . . , 0 (i − 2) 0's , 1 2 , 0, 1 2 , 0, . . . , 0 (K − i − 1) 0's })+ + 1 K cross entropy(softmax({f (x K ) T g(x j )} j ), {0, . . . , 1, 0}).</formula><p>This is almost identical with the original contrastive loss used by Quick-Thought Vectors, except that this does not additionally manually masks out the entries f (x i ) T g(x i ) with zeros, which is unnecessary with l2-normalization.</p><p>• L align : The minibatch alignment loss is computed as disparity between features from the two encoders encoding neighboring sentences (assuming K &gt;= 2):</p><formula xml:id="formula_71">1 K f (x 1 ) − g(x 2 ) α 2 + 1 2K K−2 i=2 ( f (x i−1 ) − g(x i ) α 2 + f (x i ) − g(x i+1 ) α 2 ) + 1 K f (x K−1 ) − g(x K ) α 2 .</formula><p>• L uniform : We combine the uniformity losses for each of f and g by summing them (instead of averaging since f and g are two different encoders):</p><formula xml:id="formula_72">2 K(K − 1) i =j e −t f (xi)−f (xj ) 2 2 + 2 K(K − 1) i =j e −t g(xi)−g(xj ) 2 2 .</formula><p>Our experiment settings below mostly the official implementation by Logeswaran &amp; Lee (2018):</p><p>• Sentence encoder architecture is bi-directional Gated Recurrent Unit (GRU) <ref type="bibr" target="#b8">(Cho et al., 2014)</ref> with inputs from a 620-dimensional word embedding trained jointly from scratch.</p><p>• We use Adam (Kingma &amp; Ba, 2014) with β 1 = 0.9, β 2 = 0.999, = 10 −8 , 400 batch size, 0.0005 constant learning rate, and 0.5 gradient norm clipping.</p><p>• Optimization is done during 1 epoch over the training data.</p><p>• For evaluation on a binary classification task, we freeze the encoder, and fit a logistic classifier with l2 regularization on the encoder outputs. A 10-fold cross validation is performed to determine the regularization strength among {1, 2 −1 , . . . , 2 −8 }, following <ref type="bibr" target="#b26">Kiros et al. (2015)</ref> and <ref type="bibr" target="#b34">Logeswaran &amp; Lee (2018)</ref>. The classifier is finally tested on the validation split.</p><p>Table <ref type="table" target="#tab_1">10</ref> below describes the full specifications of all 108 BOOKCORPUS encoders along with 6 settings that lead to training instability (i.e., NaN occurring). These experiment results are visualized in main paper Figure <ref type="figure" target="#fig_12">9b</ref>, showing a clear connection between representation quality and L align &amp; L uniform metrics. For the unnormalized encoders, the features are normalized before calculated L align and L uniform metrics, since they are nonetheless still normalized before being used in downstream tasks <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref>.</p><p>Table <ref type="table">7</ref>: Experiment specifications for all 306 STL-10 encoders. We report the encoder representation quality measured by accuracy of linear and k-nearest neighbor (k-NN) with k = 5 classifiers on either encoder outputs or fc7 activations, via both a 5-fold cross validation of the training set and the held out validation set. For encoder initialization, "rand" refers to standard network initialization, and symbols denote finetuning from a pretrained encoder, obtained via the experiment row marked with the same symbol. Initial learning rates (LRs) are usually either fixed as 0.12 or computed via a linear scaling (0.12 per 256 batch size). Dimensionality (abbreviated as "Dim.") shows the ambient dimension of the output features, i.e., they live on the unit hypersphere of one less dimension. The last three rows show encoders that are used to initialize finetuning, but are not part of the 285 encoders plotted in main paper Figure <ref type="figure" target="#fig_2">3</ref>, due to their unusual batch size of 786. Their accuracy and L align &amp; L uniform metrics follow the same trend shown in Figure <ref type="figure" target="#fig_6">5a</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Hypersphere:Figure 2 :</head><label>2</label><figDesc>Figure 2: Hypersphere: When classes are well-clustered (forming spherical caps), they are linearly separable. The same does not hold for Euclidean spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Random Initialization. Linear classification validation accuracy: 12.71%. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2Supervised Predictive Learning. Linear classification validation accuracy: 57.19%. Unsupervised Contrastive Learning. Linear classification validation accuracy: 28.60%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Representations of CIFAR-10 validation set on S 1 . Alignment analysis: We show distribution of distance between features of positive pairs (two random augmentations). Uniformity analysis: We plot feature distributions with Gaussian kernel density estimation (KDE) in R 2 and von Mises-Fisher (vMF) KDE on angles (i.e., arctan2(y, x) for each point (x, y) ∈ S 1 ). Four rightmost plots visualize feature distributions of selected specific classes. Representation from contrastive learning is both aligned (having low positive pair feature distances) and uniform (evenly distributed on S 1 ).</figDesc><graphic url="image-45.png" coords="4,278.19,330.68,57.22,57.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>306 STL-10 encoders are evaluated with linear classification on output features and 5-nearest neighbor (5-NN) on fc7 activations. Higher accuracy (blue color) is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>64 NYU-DEPTH-V2 encoders are evaluated with CNN depth regressors on conv5 activations. Lower MSE (blue color) is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Metrics and performance of STL-10 and NYU-DEPTH-V2 experiments. Each point represents a trained encoder, with its xand y-coordinates showing L align and L uniform metrics and color showing the performance on validation set. Blue is better for both tasks. Encoders with low L align and L uniform are consistently the better performing ones (lower left corners).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: PyTorch implementation of L align and L uniform .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Effect of optimizing different weighted combinations of L align (α=2) and L uniform (t=2) for STL-10. For each encoder, we show the L align and L uniform metrics, and validation accuracy of a linear classifier trained on encoder outputs. L uniform is exponentiated for plotting purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Finetuning trajectories from a STL-10 encoder trained with L contrastive using a suboptimal temperature τ = 2.5. Finetuning objectives are weighted combinations of L align (α=2) and L uniform (t=2). For each intermediate checkpoint, we measure L align and L uniform metrics, as well as validation accuracy of a linear classifier trained from scratch on the encoder outputs. L uniform is exponentiated for plotting purpose. Left and middle: Performance degrades if only one of alignment and uniformity is optimized. Right: Performance improves when both are optimized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>45 IMAGENET-100 encoders are trained with MoCo-based methods, and evaluated with linear classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>108 BOOKCORPUS encoders are trained with Quick-Thought-Vectors-based methods, and evaluated with logistic binary classification on Movie Review Sentence Polarity and Customer Product Review Sentiment tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Metrics and performance of IMAGENET-100 and BOOKCORPUS experiments. Each point represents a trained encoder, with its xand y-coordinates showing L align and L uniform metrics and color showing the validation accuracy. Blue is better. Encoders with low L align and L uniform consistently perform well (lower left corners), even though the training methods (based on MoCo and Quick-Thought Vectors) are different from directly optimizing the contrastive loss in Equation (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Consider the Borel sets where µ * has positive measure: T {T ∈ B(S d ) : µ * (T ) &gt; 0}. For any T ∈ T , let µ * T denote the conditional distribution of µ * on T , i.e., ∀A ∈ B(S d ), µ * T (A) = µ * (A ∩ T ) µ * (T ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>STL-10 encoder evaluations. Numbers show linear and 5-nearest neighbor (5-NN) classification accuracies on the validation set. The best result is picked by encoder outputs linear classifier accuracy from a 5-fold training set cross validation, among all 150 encoders trained from scratch with 128-dimensional output and 768 batch size.</figDesc><table><row><cell></cell><cell cols="2">Loss Formula</cell><cell cols="4">Validation Set Accuracy ↑ Output + Linear Output + 5-NN fc7 + Linear fc7 + 5-NN</cell></row><row><cell>Best Lcontrastive only</cell><cell cols="2">Lcontrastive(τ =0.19)</cell><cell>80.46%</cell><cell>78.75%</cell><cell>83.89%</cell><cell>76.33%</cell></row><row><cell>Best Lalign and Luniform only</cell><cell cols="2">0.98 • Lalign(α=2) + 0.96 • Luniform(t=2)</cell><cell>81.15%</cell><cell>78.89%</cell><cell>84.43%</cell><cell>76.78%</cell></row><row><cell>Best among all encoders</cell><cell cols="2">Lcontrastive(τ =0.5) + Luniform(t=2)</cell><cell>81.06%</cell><cell>79.05%</cell><cell>84.14%</cell><cell>76.48%</cell></row><row><cell></cell><cell></cell><cell cols="2">Loss Formula</cell><cell cols="2">Validation Set MSE ↓ conv5 conv4</cell></row><row><cell cols="2">Best Lcontrastive only</cell><cell cols="2">0.5 • Lcontrastive(τ =0.1)</cell><cell>0.7024</cell><cell>0.7575</cell></row><row><cell cols="2">Best Lalign and Luniform only</cell><cell cols="2">0.75 • Lalign(α=2) + 0.5 • Luniform(t=2)</cell><cell>0.7014</cell><cell>0.7592</cell></row><row><cell cols="2">Best among all encoders</cell><cell cols="2">0.75 • Lalign(α=2) + 0.5 • Luniform(t=2)</cell><cell>0.7014</cell><cell>0.7592</cell></row></table><note>optimize a total of 306 STL-10 encoders, 64 NYU-DEPTH-V2 encoders, 45 IMAGENET-100 encoders, and 108 BOOKCORPUS encoders without supervision. The encoders are optimized w.r.t. weighted combinations of L contrastive , L align , and/or L uniform , with varying</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>NYU-DEPTH-V2 encoder evaluations. Numbers show depth prediction mean squared error (MSE) on the validation set. The best result is picked based on conv5 layer MSE from a 5-fold training set cross validation, among all 64 encoders trained from scratch with 128-dimensional output and 128 batch size.</figDesc><table><row><cell>1.2 1.4</cell><cell cols="5">Optimize (1 ) align + uniform uniform(t = 2) (exp) align( = 2) Val accuracy</cell><cell></cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>0.0 align only</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>uniform only 1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>IMAGENET-100 encoder evaluations. Numbers show validation set accuracies of linear classifiers trained on encoder penultimate layer activations. The encoders are trained using MoCo-based methods. The best result is picked based on top1 accuracy from a 3-fold training set cross validation, among all 45 encoders trained from scratch with 128-dimensional output and 128 batch size.</figDesc><table><row><cell></cell><cell>MR Classification</cell><cell></cell><cell>CR Classification</cell><cell></cell></row><row><cell></cell><cell>Loss Formula</cell><cell>Val. Set Accuracy ↑</cell><cell>Loss Formula</cell><cell>Val. Set Accuracy ↑</cell></row><row><cell>Best Lcontrastive only</cell><cell>Lcontrastive(τ =0.075)</cell><cell>77.51%</cell><cell>Lcontrastive(τ =0.05)</cell><cell>83.86%</cell></row><row><cell>Best Lalign and Luniform only</cell><cell>0.9 • Lalign(α=2) + 0.1 • Luniform(t=5)</cell><cell>73.76%</cell><cell>0.9 • Lalign(α=2) + 0.1 • Luniform(t=5)</cell><cell>80.95%</cell></row><row><cell>Best among all encoders</cell><cell>Lcontrastive(τ =0.075)</cell><cell>77.51%</cell><cell>Lcontrastive(τ =0.05)</cell><cell>83.86%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>BOOKCORPUS encoder evaluations. Numbers show Movie Review Sentence Polarity (MR) and Customer Product Sentiment (CR) validation set classification accuracies of logistic classifiers fit on encoder outputs. The encoders are trained using Quick-Thought-Vectors-based methods. The best result is picked based on accuracy from a 5-fold training set cross validation, individually for MR and CR, among all 108 encoders trained from scratch with 1200-dimensional output and 400 batch size.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>NYU-DEPTH-V2 CNN depth predictor architecture. Each Conv. Transpose+BN+ReLU block increases the spatial shape by a factor of 2, where BN denotes Batch Normalization</figDesc><table><row><cell>Operator</cell><cell>Input Spatial Shape</cell><cell>Input #Channel</cell><cell>Kernel Size</cell><cell cols="2">Stride Padding</cell><cell>Output Spatial Shape</cell><cell>Output #Channel</cell></row><row><cell>Input</cell><cell>[h in , w in ]</cell><cell>c in</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>[h in , w in ]</cell><cell>c in</cell></row><row><cell>Conv. Transpose + BN + ReLU</cell><cell>[h in , w in ]</cell><cell>c in</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>[2h in , 2w in ]</cell><cell>c in /2</cell></row><row><cell>Conv. Transpose + BN + ReLU</cell><cell>[2h in , 2w in ]</cell><cell>c in /2</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>[4h in , 4w in ]</cell><cell>c in /4</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">Conv. Transpose + BN + ReLU [hout/2, wout/2]</cell><cell>c in /2 n−1</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>[hout, wout]</cell><cell>c in /2 n</cell></row><row><cell>Conv.</cell><cell>[hout, wout]</cell><cell>c in /2 n</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>[hout, wout]</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table9below describes the full specifications of all 45 IMAGENET-100 encoders. These experiment results are visualized in main paper Figure9a, showing a clear connection between representation quality and L align &amp; L uniform metrics. Since the original BOOKCORPUS dataset(Zhu et al., 2015)  is not distributed anymore, we use the unofficial code by Kobayashi (2019) to recreate our copy. Our copy ended up containing 52,799,513 training sentences and 50,000 validation sentences, compared to the original copy used by Quick-Thought Vectors<ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref>, which contains 45,786,400 training sentences and 50,000 validation sentences.Quick-Thought Vectors with L align and L uniform . With Quick-Thought Vectors, the positive pairs are the neighboring sentences. At each optimization iteration, let</figDesc><table><row><cell>B.3. BOOKCORPUS and Quick-Thought Vectors Variants</cell></row><row><cell>BOOKCORPUS details.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>We thank Ching-Yao Chuang, Justin Solomon, Yonglong Tian, and Zhenyang Zhang for many helpful comments and suggestions. Tongzhou Wang was supported by the MIT EECS Merrill Lynch Graduate Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head> <ref type="bibr" target="#b21">(He et al., 2019)</ref><p>. We report the encoder representation quality measured by accuracy of a linear classifier on penultimate layer activations, via both a 3-fold cross validation of the training set and the held out validation set. All encoders in this table use standard network initialization (denoted as "rand"). Dimensionality (abbreviated as "Dim.") shows the ambient dimension of the output features, i.e., they live on the unit hypersphere of one less dimension. For L uniform , the "Intra-batch" column denotes whether L uniform calculation includes pairwise distances within batch in addition to distances w.r.t. to the queue (i.e., Equation ( <ref type="formula">18</ref>) vs. Equation ( <ref type="formula">17</ref>)).   <ref type="bibr" target="#b34">(Logeswaran &amp; Lee, 2018)</ref>. We report the encoder representation quality measured by accuracy of logistic classifiers on encoder outputs for the Movie Review Sentence Polarity (MR) and Customer Product Sentiment (CR) binary classification tasks, via both a 5-fold cross validation of the training set (of the downstream task) and the held out validation set (of the downstream task). All encoders in this table use standard network initialization (denoted as "rand"). Dimensionality (abbreviated as "Dim.") shows the ambient dimension of the output features, i.e., features from l2-normalized encoders live on the unit hypersphere of one less dimension. Regardless of whether the encoder is l2-normalized (indicated in "Normalization" column), the features are always normalized before being used for downstream tasks, following <ref type="bibr" target="#b34">Logeswaran &amp; Lee (2018)</ref>.</p><p>The only unnormalized encoder is obtained using the unmodified Quick-Thought Vectors algorithm. 6 configurations that suffer from training instability (i.e., NaN occurring) are also reported. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A nonparametric estimation of the entropy for absolutely continuous distributions (corresp.)</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="375" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Quick training of probabilistic neural nets by importance sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bochner</surname></persName>
		</author>
		<title level="m">Monotone funktionen, stieltjessche integrale und harmonische analyse. Collected Papers of Salomon Bochner</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discrete energy on rectifiable sets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Borodachov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Saff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to screen for fast softmax inference on large vocabulary neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
				<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universally optimal distribution of points on spheres</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="148" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<title level="m">Hyperspherical variational auto-encoders. 34th Conference on Uncertainty in Artificial Intelligence (UAI-18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Note on dextremal configurations for the sphere in r d+1</title>
		<author>
			<persName><forename type="first">M</forename><surname>Götz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Progress in Multivariate Approximation</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="159" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Minimal riesz energy point configurations for rectifiable d-dimensional manifolds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Mathematics</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="174" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">von mises-fisher mixture model-based deep learning: Application to face verification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gentric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04264</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02230</idno>
		<idno>arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards a definition of disentangled representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2015. 2014</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<title level="m">Skip-thought vectors</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere Kobayashi, S. Homemade bookcorpus</title>
		<ptr target="https://github.com/soskek/bookcorpus/tree/5fe0cec8d7fd83940e48c799739496dc68ab2798" />
	</analytic>
	<monogr>
		<title level="m">GitHub repository</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Foundations of modern potential theory</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Landkof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep hypersphere embedding for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Sphereface</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning towards minimum hyperspherical energy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hyperspherical prototype networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1485" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
				<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2012. 2018. 2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convergence of lebesgue integrals with varying measures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Serfozo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhyā: The Indian Journal of Statistics, Series A</title>
		<imprint>
			<biblScope unit="page" from="380" to="402" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Positive definite functions and generalizations, an historical survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Rocky Mountain Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="434" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On the origin of number and arrangement of the places of exit on the surface of pollen-grains</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M L</forename><surname>Tammes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1930">1930</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="84" />
		</imprint>
	</monogr>
	<note>Recueil des travaux botaniques néerlandais</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">on the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><surname>Xxiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="237" to="265" />
			<date type="published" when="1904">1904</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="https://github.com/HobbitLong/CMC/tree/58d06e9a82f7fea2e4af0a251726e9c6bf67c7c9" />
		<title level="m">Contrastive multiview coding. GitHub repository</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
				<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th annual meeting of the association for computational linguistics: Short papers</title>
				<meeting>the 50th annual meeting of the association for computational linguistics: Short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">On the importance of views in unsupervised representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1506.06724</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2018. 2015</date>
			<biblScope unit="page" from="4503" to="4513" />
		</imprint>
	</monogr>
	<note>Spherical latent spaces for stable variational autoencoders</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
