<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Path-Based Attention Neural Model for Fine-Grained Entity Typing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
							<email>zhangdenghui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
							<email>limanlingcs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengshan</forename><surname>Cai</surname></persName>
							<email>pengshancai@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yantao</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
							<email>wangyuanzhuo@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Path-Based Attention Neural Model for Fine-Grained Entity Typing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained entity typing aims to assign entity mentions in the free text with types arranged in a hierarchical structure. It suffers from the label noise in training data generated by distant supervision. Although recent studies use many features to prune wrong label ahead of training, they suffer from error propagation and bring much complexity. In this paper, we propose an end-to-end typing model, called the path-based attention neural model (PAN), to learn a noise-robust performance by leveraging the hierarchical structure of types. Experiments on two data sets demonstrate its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Fine-grained entity typing aims to assign types to entity mentions in the local context (a single sentence), and the type set constitutes a tree-structured hierarchy (i.e., type hierarchy). Recent years witness the boost of neural models in this task, e.g., <ref type="bibr" target="#b1">(Shimaoka et al. 2016</ref>) employs an attention based LSTM to attain sentence representations and achieves state-of-the-art performance. However, it still suffers from noise in training data, which is a main challenge in this task. The training data is generated by distant supervision, which assumes that if an entity has a type in knowledge bases (KBs), then all sentences containing this entity will express this type. This method inevitably introduces irrelevant types to the context. For example, the entity "Donald Trump" has types "person", "businessman" and "politician" in KBs, thus all three types are annotated for its mentions in the training corpora. But in sentence "Donald Trump announced his candidacy for President of US.", only "person" and "politician" are correct types, while "businessman" can not be deduced from the sentence, thus serves as noise. to select relevant sentences to each type, which can dynamically reduce the weights of wrong labeled sentences for each type during training. This idea is inspired by some successful attempts to reduce noise in relation extraction, e.g., <ref type="bibr" target="#b0">(Lin et al. 2016</ref>). However, these methods fail to consider type hierarchy, which is distinct in fine-grained entity typing. Specifically, if a sentence indicates a type, its parent type can be also deduced from the sentence. Like the example above, "politician" is the subtype of "person". Since the sentence indicates that "Donald Trump" is "politician", "person" should also be assigned. Thus, we build path-based attention for each type by utilizing its path to its coarsest parent type (e.g., person) in the type hierarchy. Compared to the simple attention in relation extraction, it enables parameter sharing for types in the same path. With the support of hierarchical information of types, it can reduce noise effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path-Based Attention Neural Model</head><p>The architecture of PAN is illustrated in Figure1. Supposing that there are n sentences containing entity e, i.e., S e = {s 1 , s 2 , ..., s n }, and T e is the automatically labeled types based on KBs. Firstly PAN employs LSTM to generate representations of sentences s i following <ref type="bibr" target="#b1">(Shimaoka et al. 2016)</ref>, where s i ∈ R d is the semantic representation of s i , i ∈ {1, 2, ..., n}. Afterwards, we build path-based attention α i,t over sentences s i for each type t ∈ T e , which is expected to focus on relevant sentences to type t. Then, the representation of sentence set S e for type t, denoted by s e,t ∈ R d , is calculated through weighted sum of vectors of sentences. Finally, we obtain predicted types through a classification layer. </p><formula xml:id="formula_0">s 1 LSTM s 1 s 2 s 3 s n α 1,t α 2,t α 3,t α n,t s e,t s 2 LSTM s 3 LSTM s n LSTM classification layer • • • • • • • • •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Conclusion</head><p>Experiments are carried on two widely used datasets OntoNotes and FIGER(GOLD), and the training dataset of OntoNotes is noisy compared to FIGER(GOLD) <ref type="bibr" target="#b1">(Shimaoka et al. 2016)</ref>. Evaluation measures are Strict Accuracy (Acc), Loose Macro F1 (MaF1), and Loose Micro F1 (MiF1) <ref type="bibr" target="#b1">(Shimaoka et al. 2016)</ref>. The baselines are chosen from two aspects: (1) Predicting types in a unified process using raw noisy data, i.e., TLSTM <ref type="bibr" target="#b1">(Shimaoka et al. 2016)</ref>, and other methods in Table1.</p><p>(2) Predicting types using clean data by denoising ahead, i.e., <ref type="bibr">H PLE and F PLE (Ren et al. 2016</ref>).</p><p>To prove the superiority of path-based attention, we also directly apply the attention mechanism in relation extraction <ref type="bibr" target="#b0">(Lin et al. 2016)</ref> without considering type hierarchy (AN). We can observed that: (1) Compared with HYENA, FIGER and TLSTM using the same raw noisy data, PAN performs best on both data sets, which proves the anti-noise ability of PAN. (2) Compared with H PLE and F PLE using denoised data, PAN using raw noisy data still achieves highest Ma-F1 and Mi-F1. It makes sense that F PLE has higher Acc on OntoNotes since the noise is reduced before training, but it needs to learn additional parameters about mentions, context and types, while PAN only needs to learn parameters of attention. Thus, PAN is more efficient to reduce noise. (3) PAN performs better than AN, since the path-based attention utilizes hierarchical structures to enable parameter sharing. (4) The improvements on OntoNotes are higher than FIGER(GOLD), because OntoNotes is more noisy, and the hierarchical structure in OntoNotes is more complex with more layers, which further demonstrates that path-based attention handles well with type hierarchy and noise. (5) PAN-A achieves better performance than PAN-M, which shows that addition operator can better capture type hierarchy.</p><p>In conclusion, PAN can reduce noise effectively through an end-to-end process, and achieves better typing performance on datasets with more noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>R d×d is a weighted diagonal matrix. p t ∈ R d is the representation of path p t for type t. Specifically, for each type, we define one path as a sequence of types starting from its coarsest parent type, and ending with it. More formally, for type t l , p t l = t 1 →t 2 →...→t l , where t 1 is its coarsest parent type, and t i+1 is the subtype of t i . For example, for type t l = politician, its path is p t l = person→politician. We represent the path p t l as a semantic composition of all the types on the path, i.e., p t l = t 1 • t 2 • ... • t l , where t i ∈ R d is the representation of type t i , which is a parameter to learn. • is a composition operator. In this paper, we consider two types of operators: (1) Addition (PAN-A), where p t l equals the sum of type vectors. (2) Multiplication (PAN-M), where p t l equals the cumulative product of type vectors. In this way, path-based attention enables the model to share parameters between types in the same path. For example, the attention learned for "person" could assist the learning of the attention for "politician". It makes learning easier especially for infrequent subtypes, which suffer from dearth of training data, since the attentions for these subtypes can get support from the attention for parent type.Then, the representation of sentence set S e for type t, i.e., s e,t , is calculated through weighted sum of sentence vectors,</figDesc><table><row><cell></cell><cell></cell><cell>The Thirty-Second AAAI Conference</cell></row><row><cell></cell><cell></cell><cell>on Artificial Intelligence (AAAI-18)</cell></row><row><cell></cell><cell>n</cell></row><row><cell cols="2">s e,t =</cell><cell>α i,t s i .</cell></row><row><cell></cell><cell>i=1</cell></row><row><cell cols="3">Since one mention can have multiple types, we employ a</cell></row><row><cell cols="3">classification layer consisting of N logistic classifiers, where</cell></row><row><cell cols="3">N is the total number of types. Each classifier outputs the</cell></row><row><cell cols="3">probability of respective type, i.e.,</cell></row><row><cell>P (t|s e,t ) =</cell><cell cols="2">exp(w T t s e,t + b t ) 1 + exp(w T</cell></row></table><note>Figure 1: The architecture of PAN for given entity e, type t More precisely, given e, an attention α i,t is learned to score how well sentence s i matches type t, i.e., α i,t = exp(s i Ap t ) n j=1 exp(s j Ap t ) , where A ∈ t s e,t + b t ) , where w t , b t ∈ R d are the logistic regression parameters. To optimize the model, a multi-type loss is defined according to the cross entropy as follows, J = − e t [I t ln P (t|s e,t ) + (1 − I t ) ln(1 − P (t|s e,t ))], where I t is indicator function to indicate whether t is the annotated type of entity e, i.e., t ∈ T e .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance on FIGER(GOLD) and OntoNotes</figDesc><table><row><cell>Metric</cell><cell cols="4">OntoNotes Acc MaF1 MiF1 Acc MaF1 MiF1 FIGER(GOLD)</cell></row><row><cell cols="2">HYENA 24.9 49.7</cell><cell>44.6</cell><cell>28.8 52.8</cell><cell>50.6</cell></row><row><cell>FIGER</cell><cell>36.9 57.8</cell><cell>51.6</cell><cell>47.4 69.2</cell><cell>65.5</cell></row><row><cell cols="2">TLSTM 50.8 70.1</cell><cell>64.9</cell><cell>59.7 79.0</cell><cell>75.4</cell></row><row><cell>AN</cell><cell>52.3 71.7</cell><cell>65.2</cell><cell>60.0 79.5</cell><cell>75.9</cell></row><row><cell>PAN-A</cell><cell>54.9 72.8</cell><cell>66.5</cell><cell>60.2 79.9</cell><cell>76.2</cell></row><row><cell cols="2">PAN-M 53.0 71.9</cell><cell>65.3</cell><cell>60.0 79.4</cell><cell>76.0</cell></row><row><cell>H PLE</cell><cell>54.6 69.2</cell><cell>62.5</cell><cell>54.3 69.5</cell><cell>68.1</cell></row><row><cell>F PLE</cell><cell>57.2 71.5</cell><cell>66.1</cell><cell>59.9 76.3</cell><cell>74.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was funded by National Natural Science Foundation of China (No. 61572469, 61402442, 91646120,  61572473, 61402022), the National Key R&amp;D Program of China (No. 2016QY02D0405, 2016YFB1000902), National Grand Fundamental Research 973 Program of China (No. 2013CB329602, 2014CB340401), and 2016 Key Program of National Network Space Security.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<meeting><address><addrLine>Ren, X</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural architectures for fine-grained entity type classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
