<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">N ATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>&lt;boqinggo@outlook.com&gt;.</email>
							<affiliation key="aff2">
								<orgName type="department">Google</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Boqing Gong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">N ATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an "optimal" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN's internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper is concerned with the robustness of deep neural networks (DNNs). We aim at providing a strong adversarial attack method that can universally defeat a variety of DNNs and associated defense techniques. Our experiments mainly focus on attacking the recently developed defense methods, following <ref type="bibr">(Athalye et al., 2018)</ref>. Unlike their work, however, we do not need to tailor our algorithm to various forms for tackling different defenses. Hence, it may generalize better to new defense methods in the future. Progress on powerful adversarial attack algorithms will significantly facilitate the research toward more robust DNNs that are deployed in uncertain or even adversarial environments. <ref type="bibr" target="#b39">Szegedy et al. (2013)</ref> found that DNNs are vulnerable to adversarial examples whose changes from the benign ones are imperceptible and yet can mislead DNNs to make wrong predictions. A rich line of work furthering their finding reveals more worrisome results. Notably, adversarial examples are transferable, meaning that one can design adversarial examples for one DNN and then use them to fail others <ref type="bibr" target="#b31">(Papernot et al., 2016a;</ref><ref type="bibr" target="#b39">Szegedy et al., 2013;</ref><ref type="bibr" target="#b42">Tramèr et al., 2017b)</ref>. Moreover, adversarial perturbation could be universal in the sense that a single perturbation pattern may convert many images to adversarial ones <ref type="bibr" target="#b28">(Moosavi-Dezfooli et al., 2017)</ref>.</p><p>The adversarial examples raise a serious security issue as DNNs become increasingly popular <ref type="bibr" target="#b38">(Silver et al., 2016;</ref><ref type="bibr" target="#b18">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b15">Hinton et al., 2012;</ref><ref type="bibr" target="#b20">Li et al., 2018;</ref><ref type="bibr" target="#b10">Gan et al., 2017)</ref>. Unfortunately, the cause of the adversarial examples remains unclear. <ref type="bibr" target="#b12">Goodfellow et al. (2014b)</ref> conjectured that DNNs behave linearly in the high dimensional input space, amplifying small perturbations when their signs follow the DNNs' intrinsic linear weights. <ref type="bibr" target="#b9">Fawzi et al. (2018)</ref> experimentally studied the topology and geometry of adversarial examples and <ref type="bibr" target="#b46">Xu et al. (2019)</ref> provide the image-level interpretability of adversarial examples. <ref type="bibr" target="#b25">Ma et al. (2018)</ref> characterized the subspace of adversarial examples. Nonetheless, defense methods <ref type="bibr" target="#b30">(Papernot et al., 2015;</ref><ref type="bibr" target="#b41">Tramèr et al., 2017a;</ref><ref type="bibr" target="#b35">Rozsa et al., 2016;</ref><ref type="bibr" target="#b26">Madry et al., 2018)</ref> motivated by them were broken in a short amount of time <ref type="bibr" target="#b14">(He et al., 2017;</ref><ref type="bibr">Athalye et al., 2018;</ref><ref type="bibr" target="#b47">Xu et al., 2017;</ref><ref type="bibr">Sharma &amp; Chen, 2017)</ref>, indicating that better defense techniques are yet to be developed, and there may be unknown alternative factors that play a role in the DNNs' sensitivity.</p><p>Powerful adversarial attack methods are key to better understanding of the adversarial examples and for thorough testing of defense techniques.</p><p>In this paper, we propose a black-box adversarial attack algorithm that can generate adversarial examples to defeat both vanilla DNNs and those recently defended by various techniques. Given an arbitrary input to a DNN, our algorithm finds a probability density over a small region centered around the input such that a sample drawn from this density distribution is likely an adversarial example, without the need of accessing the DNN's internal layers or weightsthus, our method falls into the realm of black-box adversarial attack <ref type="bibr" target="#b33">(Papernot et al., 2017;</ref><ref type="bibr" target="#b2">Brendel et al., 2017;</ref><ref type="bibr" target="#b5">Chen et al., 2017;</ref><ref type="bibr" target="#b16">Ilyas et al., 2018)</ref>.</p><p>Our approach is strong; tested against two vanilla DNNs and 13 defended ones, it outperforms state-of-the-art blackbox or white-box attack methods for most cases, and it is on par with them for the remaining cases. It is also universal as it attacks various DNNs by a single algorithm. We hope it can effectively benchmark new defense methods in the future -code is available at https: //github.com/Cold-Winter/Nattack. Additionally, our study reveals that adversarial training remains one of the best defenses <ref type="bibr" target="#b26">(Madry et al., 2018)</ref>, and the adversarial examples are not as transferable across defended DNNs as them across vanilla ones. The latter somehow weakens the practical significance of white-box methods which otherwise could fail a black-box DNN by attacking a substitute.</p><p>Our optimization criterion is motivated by the natural evolution strategy (NES) <ref type="bibr" target="#b44">(Wierstra et al., 2008)</ref>. NES has been previously employed by <ref type="bibr" target="#b16">Ilyas et al. (2018)</ref> to estimate the gradients in the projected gradient search for adversarial examples. However, their algorithm leads to inferior performance to what we proposed (cf. Table <ref type="table" target="#tab_0">1</ref>). This is probably because, in their approach, the gradients have to be estimated relatively accurately for the projected gradient method to be effective. However, some of the neural networks F (x) are not smooth, so that the NES estimation of the gradient ∇F (x) is not reliable enough.</p><p>In this paper, we opt for a different methodology using a constrained NES formulation as the objective function instead of using NES to estimate gradients as in <ref type="bibr" target="#b16">Ilyas et al. (2018)</ref>. The main idea is to smooth the loss function by a probability density distribution defined over the p -ball centered around a benign input to the neural network. All adversarial examples of this input belong to this ball<ref type="foot" target="#foot_0">1</ref> . In this frame, assuming that we can find a distribution such that the loss is small, then a sample drawn from the distribution is likely adversarial. Notably, this formulation does not depend on estimating the gradient ∇F (x) any more, so it is not impeded by the non-smoothness of DNNs.</p><p>We adopt parametric distributions in this work. The initialization to the distribution parameters plays a key role in the run time of our algorithm. In order to swiftly find a good initial distribution to start from, we train a regression neural network such that it takes as input the input to the target DNN to be attacked and its output parameterizes a probability density as the initialization to our main algorithm.</p><p>Our approach is advantageous over existing ones in multiple folds. First, we can designate the distribution in a lowdimensional parameter space while the adversarial examples are often high-dimensional. Second, instead of questing an "optimal" adversarial example, we can virtually draw an infinite number of adversarial examples from the distribution. Finally, the distribution may speed up the adversarial training for improving DNNs' robustness because it is more efficient to sample many adversarial examples from a distribution than to find them using gradient based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>Consider a DNN classifier C(x) = arg max i F (x) i , where x ∈ [0, 1] dim(x) is an input to the neural network F (•). We assume softmax is employed for the output layer of the network and let F (•) i denote the i-th dimension of the softmax output. When this DNN correctly classifies the input, i.e., C(x) = y, where y is the groundtruth label of the input x, our objective is to find an adversarial example x adv for x such that they are imperceptibly close and yet the DNN classifier labels them distinctly; in other words, C(x adv ) = y. We exclude the inputs for which the DNN classifier predicts wrong labels in this work, following the convention of previous work <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017)</ref>.</p><p>We bound the p distance between an input x and its adversarial counterparts: x adv ∈ S p (x) := {x : x − x p ≤ τ p }, p = 2 or ∞. We omit from S p (x) the argument (x) and the subscript p when it does not cause ambiguity. Let proj S (x ) denote the projection of x ∈ R dim(x) onto S.</p><p>We first review the NES based black-box adversarial attack method <ref type="bibr" target="#b16">(Ilyas et al., 2018)</ref>. We show that its performance is impeded by unstable estimation of the gradients of certain DNNs, followed by our approach which does not depend at all on the gradients of the DNNs. <ref type="bibr">et al. (2018)</ref> proposed to search for an optimal adversarial example in the following sense,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A Black-box Adversarial Attack by NES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ilyas</head><formula xml:id="formula_0">x adv ← arg min x ∈S f (x ),<label>(1)</label></formula><p>given a benign input x and its label y correctly predicted by the neural network F (•), where S is a small region containing x defined above, and f (x ) is a loss function defined as f (x ) := −F (x ) y . In <ref type="bibr" target="#b16">(Ilyas et al., 2018)</ref>, this loss is minimized by the projected gradient method,</p><formula xml:id="formula_1">x t+1 ← proj S (x t − η sign(∇f (x t ))),<label>(2)</label></formula><p>where sign(•) is a sign function. The main challenge here is how to estimate the gradient ∇f (x t ) with derivativefree methods, as the network's internal architecture and weights are unknown in the black-box adversarial attack. One technique for doing so is by NES <ref type="bibr" target="#b44">(Wierstra et al., 2008)</ref>:</p><formula xml:id="formula_2">∇f (x t ) ≈ ∇ xt E N (z|xt,σ 2 ) f (z) (3) = E N (z|xt,σ 2 ) f (z)∇ xt log N (z|x t , σ 2 ),<label>(4)</label></formula><p>where N (z|x t , σ 2 ) is an isometric normal distribution with mean x t and variance σ 2 . Therefore, the stochastic gradient descent (SGD) version of eq. ( <ref type="formula" target="#formula_1">2</ref>) becomes:</p><formula xml:id="formula_3">x t+1 ← proj S x t − η sign 1 b b i=1 f (z i )∇ log N (z i |x t , σ 2 ) ,</formula><p>where b is the size of a mini-batch and z i is sampled from the normal distribution. The performance of this approach hinges on the quality of the estimated gradient. Our experiments show that its performance varies on attacking different DNNs probably because non-smooth DNNs lead to unstable NES estimation of the gradients (cf. eq. ( <ref type="formula">3</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">N ATTACK</head><p>We propose a different formulation albeit still motivated by NES. Given an input x and a small region S that contains x (i.e., S = S p (x) defined earlier), the key idea is to consider a smoothed objective as our optimization criterion:</p><formula xml:id="formula_4">min θ J(θ) := f (x )π S (x |θ)dx<label>(5)</label></formula><p>where π S (x |θ) is a probability density with support defined on S. Compared with problem (1), this frame assumes that we can find a distribution over S such that the loss f (x ) is small in expectation. Hence, a sample drawn from this distribution is likely adversarial. Furthermore, with appropriate π S (•|θ), the objective J(θ) is a smooth function of θ, and the optimization process of this formulation does not depend on any estimation of the gradient ∇f (x t ). Therefore, it is not impeded by the non-smoothness of neural networks. Finally, the distribution over S can be parameterized in a much lower dimensional space (dim(θ) dim(x)), giving rise to more efficient algorithms than eq. ( <ref type="formula" target="#formula_1">2</ref>) which directly works in the high-dimensional input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">THE DISTRIBUTION ON S</head><p>In order to define a distribution π S (x |θ) on S, we take the following transformation of variable approach:</p><formula xml:id="formula_5">x = proj S (g(z)), z ∼ N (z|µ, σ 2 ) (6)</formula><p>where N (z|µ, σ 2 ) is an isometric normal distribution whose mean µ and variance σ 2 are to be learned and the function g : R dim(µ) → R dim(x) maps a normal instance to the space of the neural network input. We leave it to future work to explore the other types of distributions.</p><p>In this work, we implement the transformation of the normal variable by the following steps:</p><formula xml:id="formula_6">1. draw z ∼ N (µ, σ 2 ), compute g(z) as g(z) = 1/2(tanh(g 0 (z)) + 1), 2. clip δ = clip p (g(z) − x), p = 2 or ∞,<label>and</label></formula><formula xml:id="formula_7">3. return proj S (g(z)) as x = x + δ</formula><p>Step 1 draws a "seed" z and then maps it by g 0 (z) to the space of the same dimension as the input x. In our experiments, we let z lie in the space of the CIFAR10 images <ref type="bibr" target="#b17">(Krizhevsky &amp; Hinton, 2009)</ref> (i.e., R 32×32×3 ), so the function g 0 (•) is an identity mapping for the experiments on CIFAR10 and a bilinear interpolation for the ImageNet images <ref type="bibr" target="#b7">(Deng et al., 2009)</ref>. We further transform g 0 (z) to the same range as the input by g x) and then compute the offset δ = g(z) − x between the transformed vector and the input. Steps 2 and 3 detail how to project g(z) onto the set S, where the clip functions are respectively</p><formula xml:id="formula_8">(z) = 1 2 (tanh (g 0 (z)) + 1) ∈ [0, 1] dim(</formula><formula xml:id="formula_9">clip 2 (δ) = δτ 2 / δ 2 if δ 2 &gt; τ 2 δ else (7) clip ∞ (δ) = min(δ, τ ∞ ) (8)</formula><p>with the thresholds τ 2 and τ ∞ given by users.</p><p>Thus far, we have fully specified our problem formulation (eq. ( <ref type="formula" target="#formula_4">5</ref>)). Before discussing how to solve this problem, we recall that the set S is the p -ball centered at x: S = S p (x).</p><p>Since problem ( <ref type="formula" target="#formula_4">5</ref>) is formulated for a particular input to the targeted DNN, the input x also determines the distribution π S (z|θ) via the dependency of S on x. In other words, we will learn personalized distributions for different inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">OPTIMIZATION</head><p>Let proj S (g(z)) be steps 1-3 in the above variable transformation procedure. We can rewrite the objective function J(θ) in problem (5) as</p><formula xml:id="formula_10">J(θ) = E N (z|µ,σ) f (proj(g(z))),</formula><p>where θ = (µ, σ 2 ) are the unknowns. We use grid search to find a proper bandwidth σ for the normal distribution and NES to find its mean µ:</p><formula xml:id="formula_11">µ t+1 ← µ t − η∇ µ J(θ)| µt ,<label>(9)</label></formula><p>whose SGD version over a mini-batch of size b is</p><formula xml:id="formula_12">µ t+1 ← µ t − η b b i=1 f (proj S (g(z i )))∇ µ log N (z i |µ t , σ 2 ).</formula><p>In practice, we sample i from a standard normal distribution and then use a linear transformation z i = µ + i σ to make it follow the distribution N (z|µ, σ 2 ). With this notion, we can simplify ∇ µ log N (z i |µ t , σ 2 ) ∝ σ −1 i . Algorithm 1 summarizes the full algorithm, called N ATTACK, for optimizing our smoothed formulation in eq. ( <ref type="formula" target="#formula_4">5</ref>). In line 6 of Algorithm 1, the z-score operation is to subtract from each loss quantity f i the mean of the losses f 1 , • • • , f b and divide it by the standard deviation of all the loss quantities. We find it stablizes N ATTACK; the algorithm converges well with a constant learning rate η. Otherwise, one would have to schedule more sophisticated learning rates as reported in <ref type="bibr" target="#b16">(Ilyas et al., 2018)</ref>. Regarding the loss function in line 5, we employ the C&amp;W loss <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017)</ref> in the experiments:</p><formula xml:id="formula_13">f (x ) := max 0, log F (x ) y − max c =y log F (x ) c .</formula><p>In order to generate an adversarial example for an input x to the neural network classifier C(•), we use the N ATTACK algorithm to find a probability density distribution over S p (x) and then sample from this distribution until arriving at an adversarial instance x such that C(x ) = C(x).</p><p>Note that our method differs from that of <ref type="bibr" target="#b16">Ilyas et al. (2018)</ref> in that we allow an arbitrary data transformation g(•) which is more flexible than directly seeking the adversarial example in the input space, and we absorb the computation of proj S (•) into the function evaluation before the update of µ (line 7 of Algorithm 1). On the contrary, the projection of <ref type="bibr" target="#b16">Ilyas et al. (2018)</ref> is after the computation of the estimated gradient (which is similar to line 7 in Algorithm 1) because it is an estimation of the projected gradient. The difference in the computational order of projection is conceptually important because, in our case, the projection is treated as part of the function evaluation, which is more stable than treating it as an estimation of the projected gradient. Practically, this also makes a major difference, which can be seen from our experimental comparisons of the two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Initializing N ATTACK by Regression</head><p>The initialization to the mean µ 0 in Algorithm 1 plays a key role in terms of run time. When a good initialization is given, we often successfully find adversarial examples in less than 100 iterations. Hence, we propose to boost the N ATTACK algorithm by using a regression neural network. It takes a benign example x as the input and outputs µ 0 to initialize N ATTACK. In order to train this regressor, we generate many (input, adversarial example) pairs {(x, x adv )} by running N ATTACK on the training set of benchmark Algorithm 1 Black-box adversarial N ATTACK Input: DNN F (•), input x and its label y, initial mean µ 0 , standard deviation σ, learning rate η, sample size b, and the maximum number of iterations T Output: µ T , mean of the normal distribution 1: for t = 0, 1, ..., T − 1 do</p><formula xml:id="formula_14">2: Sample 1 ,..., b ∼ N (0, I) 3: Compute g i = g(µ t + i σ) by Step 1 ∀i ∈ {1, • • • , b} 4:</formula><p>Obtain proj(g i ) by steps 2-3, ∀i 5:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute losses f</head><formula xml:id="formula_15">i := f (proj(g i )), ∀i 6: Z-score f i = (f i − mean(f ))/std(f ), ∀i 7: Set µ t+1 ← µ t − η bσ b i=1 f i i 8: end for</formula><p>datasets. The regression network's weights are then set by minimizing the 2 loss between the network's output and g −1 0 (arctan(2x adv − 1)) − g −1 0 (arctan(2x − 1)); in other words, we regress for the offset between the adversarial example x adv and the input x in the space R dim(µ) of the distribution parameters. The supplementary materials present more details about this regression network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We use the proposed N ATTACK to attack 13 defense methods for DNNs published in 2018 or 2019 and two representative vanilla DNNs. For each defense method, we run experiments using the same protocol as reported in the original paper, including the datasets and p distance (along with the threshold) to bound the differences between adversarial examples and inputs -this experiment protocol favors the defense method. In particular, CIFAR10 <ref type="bibr" target="#b17">(Krizhevsky &amp; Hinton, 2009)</ref> is employed in the attack on nine defense methods and ImageNet <ref type="bibr" target="#b7">(Deng et al., 2009)</ref> is used for the remaining four. We examine all the test images of CIFAR10 and randomly choose 1,000 images from the test set of ImageNet. 12 of the defenses concern the ∞ distance between the adversarial examples and the benign ones and one works with the 2 distance. We threshold the l ∞ distance in the normalized [0, 1] dim(x) input space. The l 2 distance is normalized by the number of pixels.</p><p>In addition to the main comparison results, we also investigate the defense methods' robustness versus the varying strengths of N ATTACK (cf. Section 3.2). Specifically, we plot the attack success rate versus the attack iteration. The curves provide a complementary metric to the overall attack success rate, uncovering the dynamic traits of the competition between a defense and an attack.</p><p>Finally, we examine the adversarial examples' transferabilities between some of the defended neural networks (cf. Section 3.3). Results show that, unlike the finding that many adversarial examples are transferable across different vanilla neural networks, a majority of the adversarial examples that fail one defended DNN cannot defeat the others. In some sense, this weakens the practical significance of whitebox attack methods which are often thought applicable to unknown DNN classifiers by attacking a substitute neural network instead <ref type="bibr" target="#b33">(Papernot et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attacking 13 Most Recent Defense Techniques</head><p>We consider 13 defenses recently developed: adversarial training (ADV-TRAIN) <ref type="bibr" target="#b26">(Madry et al., 2018)</ref>, adversarial training of Bayesian DNNs (ADV-BNN) <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>, Thermometer encoding (THERM) <ref type="bibr" target="#b3">(Buckman et al., 2018)</ref>, THERM-ADV <ref type="bibr">(Athalye et al., 2018;</ref><ref type="bibr" target="#b26">Madry et al., 2018)</ref>, ADV-GAN <ref type="bibr" target="#b43">(Wang &amp; Yu, 2019)</ref>, local intrinsic dimensionality (LID) <ref type="bibr" target="#b25">(Ma et al., 2018)</ref>, stochastic activation pruning (SAP) <ref type="bibr" target="#b8">(Dhillon et al., 2018)</ref>, random self-ensemble (RSE) <ref type="bibr" target="#b22">(Liu et al., 2018)</ref>, cascade adversarial training (CAS-ADV) <ref type="bibr" target="#b29">(Na et al., 2018)</ref>, randomization <ref type="bibr" target="#b45">(Xie et al., 2018)</ref>, input transformation (INPUT-TRANS) <ref type="bibr" target="#b13">(Guo et al., 2018)</ref>, pixel deflection <ref type="bibr" target="#b34">(Prakash et al., 2018)</ref>, and guided denoiser <ref type="bibr" target="#b21">(Liao et al., 2018)</ref>. We describe them in detail in the supplementary materials. Additionally, we also include Wide Resnet-32 (WRESNET-32) <ref type="bibr" target="#b48">(Zagoruyko &amp; Komodakis, 2016)</ref> and INCEPTION V3 <ref type="bibr" target="#b40">(Szegedy et al., 2016)</ref>, two vanilla neural networks for CIFAR10 and ImageNet, respectively. Implementation Details. In our experiments, the defended DNNs of SAP, LID, RANDOMIZATION, INPUT-TRANS, THERM, and THERM-DAV come from <ref type="bibr">(Athalye et al., 2018)</ref>, the defended models of GUIDED DENOISER and PIXEL DEFLECTION are based on <ref type="bibr">(Athalye &amp; Carlini, 2018)</ref>, and the models defended by RSE, CAS-ADV, ADV-TRAIN, and ADV-GAN are respectively from the original papers. For ADV-BNN, we attack an ensemble of ten BNN models. In all our experiments, we set T = 600 as the maximum number of optimization iterations, b = 300 for the sample size, variance of the isotropic Gaussian σ 2 = 0.01, and learning rate η = 0.008. N ATTACK is able to defeat most of the defenses under this setting and about 90% inputs for other cases. We then fine-tune the learning rate η and sample size b for the hard leftovers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">ATTACK SUCCESS RATES</head><p>We report in Table <ref type="table" target="#tab_0">1</ref> the main comparison results evaluated by the attack success rate, the higher the better. Our N ATTACK achieves 100% success on six out of the 13 defenses and more than 90% on five of the rest. As a single black-box adversarial algorithm, N ATTACK is better than or on par with the set of powerful white-box attack methods of various forms <ref type="bibr">(Athalye et al., 2018)</ref>, especially on the defended DNNs. It also significantly outperforms three state-of-the-art black-box attack methods: ZOO <ref type="bibr" target="#b5">(Chen et al., 2017)</ref>, which adopts the zero-th order gradients to find adversarial examples; QL <ref type="bibr" target="#b16">(Ilyas et al., 2018)</ref>, a query-limited attack based on an evolution strategy; and a decision-based (D-based) attack method <ref type="bibr" target="#b2">(Brendel et al., 2017)</ref> mainly generating 2 -bounded adversarial examples.</p><p>Notably, ADV-TRAIN is still among the best defense methods, so is its extension to the Bayesian DNNs (i.e., ADV-BNN). However, along with CAS-ADV and THERM-ADV which are also equipped with the adversarial training, their strengths come at the price that they give worse classification performances than the others on the clean inputs (cf. the third column of Table <ref type="table" target="#tab_0">1</ref>). Moreover, ADV-TRAIN incurs extremely high computation cost. When the image resolutions are high, <ref type="bibr" target="#b19">Kurakin et al. (2016)</ref> found that it is difficult to run the adversarial training at the ImageNet scale. Since our N ATTACK enables efficient generation of adversarial examples once we learn the distribution, we can potentially scale up the adversarial training with N ATTACK and will explore it in the future work.</p><p>We have tuned the main free parameters of the competing methods (e.g., batch size and bandwidth in QL). ZOO runs extremely slow with high-resolution images, so we instead use the hierarchical trick the authors described <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> for the experiments on ImageNet. In particular, we run ZOO starting from the attack space of 32 × 32 × 3, lift the resolution to 64 × 64 × 3 after 2,000 iterations and then to 128 × 128 × 3 after 10,000 iterations, and finally up-sample the result to the same size as the DNN input with bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">ABLATION STUDY AND RUN-TIME COMPARISON</head><p>N ATTACK vs. QL. We have discussed the conceptual differences between N ATTACK and QL <ref type="bibr" target="#b16">(Ilyas et al., 2018)</ref> in Section 2 (e.g., N ATTACK formulates a smooth optimization criterion and offers a probability density on the p -ball of an input). Moreover, the comparison results in Table <ref type="table" target="#tab_0">1</ref> verify the advantage of N ATTACK over QL in terms of the overall attack strengths. Additionally, we here conduct an ablation study to investigate two major algorithmic differences between them: N ATTACK absorbs the projection (proj S ) into the objective function and allows an arbitrary change of variable transformation g(•). Our study concerns THERM-ADV and SAP, two defended DNNs on which QL respectively reaches 42.3% and 96.2% attack success rates. After we instead absorb the projection in QL into the objective, the results are improved to 54.7% and 97.7%, respectively. If we further apply g(•), the change of variable procedure (cf. Steps 1-3), the success rates become 83.3% and 98.9%, respectively. Finally, with the z-score operation (line 6 of Algorithm 1), the results are boosted to 90.9%/100%, approaching N ATTACK's 91.2%/100%. Therefore, we say that N ATTACK boosts QL's performance, thanks to both the smoothed objective and the transformation g(•).</p><p>N ATTACK vs. the White-Box BPDA Attack. While BPDA achieves high attack success rates by different vari-  <ref type="bibr">(Athalye et al., 2018)</ref>. For all the other numbers, we obtain them by running the code released by the authors or implemented ourselves with the help of the authors. For D-based and ADV-TRAIN, we respectively report the results on 100 and 1000 images only because they incur expensive computation costs.) ants for handling the diverse defense techniques, N ATTACK gives rise to better or comparable results by a single universal algorithm. Additionally, we compare them in terms of the run time in the supplementary materials; the main observations are the following. On CIFAR10, BPDA and N ATTACK can both find an adversarial example in about 30s. To defeat an ImageNet image, it takes N ATTACK about 71s without the regression network and 48s when it is equipped with the regression net; in contrast, BPDA only needs 4s. It is surprising to see that BPDA is almost 7 times faster at attacking a DNN for ImageNet than a DNN for CIFAR10. It is probably because the gradients of the former are not "obfuscated" as well as the latter due to the higher resolution of the ImageNet input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attack Success Rate vs. Attack Iteration</head><p>The N ATTACK algorithm has an appealing property as follows. In expectation, the loss (eq. ( <ref type="formula" target="#formula_4">5</ref>)) decreases at every iteration and hence a sample drawn from the distribution π S (x|θ) is adversarial with higher chance. Though there could be oscillations, we find that the attack strengths do grow monotonically with respect to the evolution iterations in our experiments. Hence, we propose a new curve shown in Figure <ref type="figure" target="#fig_0">1a</ref> featuring the attack success rate versus number of evolution iterations -strength of attack. For the experiment here, the Gaussian mean µ 0 is initialized by µ 0 ∼ N (g −1 0 (arctan(2x − 1)), σ 2 ) for any input x to maintain about the same starting points for all the curves. by the success rate) is different from the ordering on the left half of Figure <ref type="figure" target="#fig_0">1a</ref>, signifying the attack success rate and the curve mutually complement. The curve reveals more characteristics of the defense methods especially when there are constraints on the computation time or number of queries to the DNN classifier.</p><p>Figure <ref type="figure" target="#fig_0">1b</ref> shows N ATTACK (solid lines) is more query efficient than the QL attack <ref type="bibr" target="#b16">(Ilyas et al., 2018</ref>) (dashed lines) on 6 defenses under most attack success rates and the difference is even amplified for higher success rates. For SAP, N ATTACK performs better when the desired attack success rate is bigger than 80%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transferability</head><p>We also study the transferability of adversarial examples across different defended DNNs. This study differs from the earlier ones on vanilla DNNs <ref type="bibr" target="#b39">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b24">Liu et al., 2016)</ref>. We investigate both the white-box attack BPDA and our black-box N ATTACK.</p><p>Following the experiment setup in <ref type="bibr" target="#b19">(Kurakin et al., 2016)</ref>, we randomly select 1000 images for each targeted DNN such that they are classified correctly, and yet the adversarial images of them are classified incorrectly. We then use the adversarial examples of the 1000 images to attack the other DNNs. In addition to the defended DNNs, we also include two vanilla DNNs for reference: VANILLA-1 and VANILLA-2. VANILLA-1 is a light-weight DNN classifier built by <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017)</ref> with 80% accuracy on CI-FAR10. VANILLA-2 is the Wide-ResNet-28 <ref type="bibr" target="#b48">(Zagoruyko &amp; Komodakis, 2016)</ref> which gives rise to 92.3% classification accuracy on CIFAR10. For fair comparison, we change the threshold τ ∞ to 0.031 for CAS-ADV. We exclude RSE and CAS-ADV from BPDA's confusion table because it is not obviously clear how to attack RSE using BPDA and the re-leased BPDA code lacks the piece for attacking CAS-ADV.</p><p>The confusion tables of BPDA and N ATTACK are shown in Figure <ref type="figure" target="#fig_1">2</ref>, respectively, where each entry indicates the success rate of using the adversarial examples originally targeting the row-wise defense model to attack the columnwise defense. Both confusion tables are asymmetric; it is easier to transfer from defended models to the vanilla DNNs than vice versa. Besides, the overall transferrability is lower than that across the DNNs without any defenses <ref type="bibr" target="#b24">(Liu et al., 2016)</ref>. We highlight some additional observations below.</p><p>Firstly, the transferability of our black-box N ATTACK is not as good as the black-box BPDA attack. This is probably because BPDA is able to explore the intrinsically common part of the DNN classifiers -it has the privilege of accessing the true or estimated gradients that observe the DNNs' architectures and weights.</p><p>Secondly, both the network architecture and defense methods can influence the transferability. VANILLA-2 is the underlying classifier of SAP, THERM-ADV, and THERM.</p><p>The adversarial examples originally attacking VANILLA-2 do transfer better to SAP and THERM than to the others probably because they share the same DNN architecture, but the examples achieve very low success rate on THERM-ADV due to the defense technique.</p><p>Finally, the transfer success rates are low no matter from THERM-ADV to the other defenses or vice versa, and ADV-TRAIN and ADV-BNN lead to fairly good results of transfer attacks on the other defenses and yet themselves are robust against the adversarial examples of the other defended DNNs. The unique result of THERM-ADV probably attributes to its use of double defense techniques, i.e., Thermometer encoding and adversarial training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>There is a vast literature of adversarial attacks on and defenses for DNNs. We focus on the most related works in this section rather than a thorough survey.</p><p>White-Box Attacks. The adversary has full access to the target DNN in the white-box attack. <ref type="bibr" target="#b39">Szegedy et al. (2013)</ref> first find that DNNs are fragile to the adversarial examples by using box-constrained L-BFGS. <ref type="bibr" target="#b11">Goodfellow et al. (2014a)</ref> propose a fast gradient sign (FGS) method, which is featured by efficiency and high performance for generating the ∞ bounded adversarial examples. <ref type="bibr" target="#b32">Papernot et al. (2016b)</ref> and <ref type="bibr" target="#b27">Moosavi-Dezfooli et al. (2016)</ref> instead formulate the problems with the l 0 and 2 metrics, respectively. <ref type="bibr" target="#b4">(Carlini &amp; Wagner, 2017)</ref> have proposed a powerful iterative optimization based attack. Similarly, a projected gradient descent has been shown strong in attacking DNNs <ref type="bibr" target="#b26">(Madry et al., 2018)</ref>. Most the white-box attacks rely on the gradients of the DNNs. When the gradients are "obfuscated" (e.g., by randomization), <ref type="bibr">(Athalye et al., 2018)</ref> derive various methods to approximate the gradients, while we use a single algorithm to attack a variety of defended DNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we present a black-box adversarial attack method which learns a probability density on the p -ball of a clean input to the targeted neural network. One of the major advantages of our approach is that it allows an arbitrary transformation of variable g(•), converting the adversarial attack to a space of much lower dimensional than the input space. Experiments show that our algorithm defeats 13 defended DNNs, better than or on par with state-of-the-art white-box attack methods. Additionally, our experiments on the transferability of the adversarial examples across the defended DNNs show different results reported in the literature: unlike the high transferability across vanilla DNNs, it is difficult to transfer the attacks on the defended DNNs.</p><p>Some existing works try to characterize the adversarial examples by their geometric properties. In contrast to this macro view, we model the adversarial population of each single input from a micro view by a probabilistic density.</p><p>There are still a lot to explore along this avenue. What is a good family of distributions to model the adversarial examples? How to conduct adversarial training by efficiently sampling from the distribution? These questions are worth further investigation in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FigureFigure 1 .</head><label>1</label><figDesc>Figure1aplots eight defense methods on CIFAR10 along with a vanilla DNN. It is clear that ADV-TRAIN, ADV-BNN, THERM-ADV, and CAS-ADV, which all employ the adversarial training strategy, are more difficult to attack than the others. What's more interesting is with the other five DNNs. Although N ATTACK completely defeats them all by the end, the curve of the vanilla DNN is the steepest while the SAP curve rises much slower. If there are constraints on the computation time or the number of queries to the DNN classifiers, SAP is advantageous over the vanilla DNN, RSE, THERM, and LID.Note that the ranking of the defenses in Table1(evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Transferabilities of BPDA (Athalye et al., 2018) (left) and N ATTACK (right). Each entry shows the attack success rate of attacking the column-wise defense by the adversarial examples that are originally generated for the row-wise DNN.</figDesc><graphic url="image-1.png" coords="8,87.46,69.31,178.09,177.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Black-Box Attacks. As the name suggests, some parts of the DNNs are treated as black boxes in the black-box attack. Thanks to the adversarial examples' transferabilities<ref type="bibr" target="#b39">(Szegedy et al., 2013)</ref>,<ref type="bibr" target="#b33">Papernot et al. (2017)</ref> train a substitute DNN to imitate the target black-box DNN, produce adversarial examples of the substitute model, and then use them to attack the target DNN. Chen et al. (2017) instead use the zero-th order optimization to find adversarial examples. Ilyas et al. (2018) use the evolution strategy (Salimans et al., 2017) to approximate the gradients. Brendel et al. (2017) introduce a decision-based attack by reading the hard labels predicted by a DNN, rather than the soft probabilistic output. Similarly, Cheng et al. (2019) also provide a formulation to explore the hard labels. Most of the existing black-box methods are tested against vanilla DNNs. In this work, we test them on defended ones along with our N ATTACK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Adversarial attack on 13 recently published defense methods. (* the number reported in</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">It is straightforward to extend our method to other constraints bounding the offsets between inputs and adversarial examples.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was supported in part by NSF-1836881, NSF-1741431, and ONR-N00014-18-1-2121.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03286</idno>
		<title level="m">On the robustness of the cvpr 2018 white-box adversarial example defenses</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04248</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Zoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Query-efficient hard-label black-box attack: An optimization-based approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJlk6iRqKX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<title level="m">Stochastic activation pruning for robust adversarial defense</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical study of the topology and geometry of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Vqs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyJ7ClWCb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04701</idno>
		<title level="m">Adversarial example defenses: Ensembles of weak defenses are not strong</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>.-R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08598</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How local is the local diversity? reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1778" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random self-ensemble</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adv-BNN: Improved adversarial defense through robust bayesian neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rk4Qso0cKm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02613</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJzIBfZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cascade adversarial machine learning regularized with a unified embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyRVBzap-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04508</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to blackbox attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016b</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
	<note>Security and Privacy</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rozsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00138</idno>
		<title level="m">Towards robust deep neural networks with bang</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Breaking the madry defense model with l 1-based adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mc-Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<title level="m">The space of transferable adversarial examples</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A direct approach to robust deep learning using adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1lIMn05F7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Natural evolution strategies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation</title>
		<title level="s">IEEE World Congress on Computational Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. CEC 2008. 2008</date>
			<biblScope unit="page" from="3381" to="3387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sk9yuql0Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Interpreting adversarial examples by activation promotion and suppression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1904.02057</idno>
		<ptr target="http://arxiv.org/abs/1904.02057" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
