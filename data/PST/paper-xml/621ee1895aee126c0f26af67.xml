<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PaSca: a Graph Neural Architecture Search System under the Scalable Paradigm</title>
				<funder ref="#_X2yWsrs #_vBvCPed">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_UrddHhY">
					<orgName type="full">PKU-Baidu Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-01">1 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
							<email>wentao.zhang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
							<email>shenyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheyu</forename><surname>Lin</surname></persName>
							<email>linzheyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
							<email>gdpouyang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
							<email>yangzhi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
							<email>bin.cui@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computational Social Science</orgName>
								<orgName type="institution">Peking University (Qingdao)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PaSca: a Graph Neural Architecture Search System under the Scalable Paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-01">1 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3511986</idno>
					<idno type="arXiv">arXiv:2203.00638v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Scalable Graph Learning</term>
					<term>Design Space</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based tasks. However, as mainstream GNNs are designed based on the neural message passing mechanism, they do not scale well to data size and message passing steps. Although there has been an emerging interest in the design of scalable GNNs, current researches focus on specific GNN design, rather than the general design space, limiting the discovery of potential scalable GNN models. This paper proposes PaSca, a new paradigm and system that offers a principled approach to systemically construct and explore the design space for scalable GNNs, rather than studying individual designs. Through deconstructing the message passing mechanism, PaSca presents a novel Scalable Graph Neural Architecture Paradigm (SGAP), together with a general architecture design space consisting of 150k different designs. Following the paradigm, we implement an auto-search engine that can automatically search well-performing and scalable GNN architectures to balance the trade-off between multiple criteria (e.g., accuracy and efficiency) via multi-objective optimization. Empirical studies on ten benchmark datasets demonstrate that the representative instances (i.e., PaSca-V1, V2, and V3) discovered by our system achieve consistent performance among competitive baselines. Concretely, PaSca-V3 outperforms the state-of-the-art GNN method JK-Net by 0.4% in terms of predictive accuracy on our large industry dataset while achieving up to 28.3? training speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Mathematics of computing ? Graph algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b55">[56]</ref> have become the state-of-theart methods in many graph representation learning scenarios such as node classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b60">61]</ref>, link prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref>, recommendation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58]</ref>, and knowledge graphs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>. Most GNN pipelines can be described in terms of the neural message passing (NMP) framework <ref type="bibr" target="#b14">[15]</ref>, which is based on the core idea of recursive neighborhood aggregation and transformation. Specifically, during each iteration, the representation of each node is updated (with neural networks) based on messages received from its neighbors. Since they typically need to perform a recursive neighborhood expansion to gather neural messages repeatedly, this process leads to an expensive neighborhood expansion, which grows exponentially with layers. The exponential growth of neighborhood size corresponds to an exponential IO overhead, which is the major challenge of large-scale GNN computation.</p><p>To scale up GNNs to web-scale graphs, recent work focuses on designing training frameworks with sampling approaches (e.g., DistDGL <ref type="bibr" target="#b65">[66]</ref>, NextDoor <ref type="bibr" target="#b18">[19]</ref>, SeaStar <ref type="bibr" target="#b54">[55]</ref>, FlexGraph <ref type="bibr" target="#b45">[46]</ref>, Dorylus <ref type="bibr" target="#b42">[43]</ref>, GNNAdvisor <ref type="bibr" target="#b49">[50]</ref>, etc.). Although distributed training is applied in these frameworks, they still suffer from high communication costs due to the recursive neighborhood aggregation during the training process. To demonstrate this issue, we utilize distributed training functions provided by DGL <ref type="bibr" target="#b1">[2]</ref> to execute the train pipeline of GraphSAGE <ref type="bibr" target="#b14">[15]</ref>. We partition the Reddit dataset across multiple machines and treat each GPU as a worker, and then calculate the speedup relative to the runtime of two workers. Figure <ref type="figure" target="#fig_2">1</ref> illustrates the training speedup along with the number of workers and the bottleneck in distributed settings. In particular, Figure <ref type="figure" target="#fig_2">1(a)</ref> shows that the scalability of GraphSAGE is limited even when the mini-batch training and graph sampling method are adopted. Figure <ref type="figure" target="#fig_2">1(b</ref>) further shows that the scalability is mainly bottlenecked by the aggregation procedure in which high data loading cost is incorporated to gather neighborhood information.</p><p>Different from the recently developed GNN systems <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50]</ref>, we address the scalability challenges from an orthogonal perspective: re-designing the GNN pipeline to make the computing naturally scalable. To ensure scalability, we consider a different GNN training pipeline from most existing work: treating data aggregation over the graph as pre/post-processing stages that are separate from training. While there has been an emerging interest in specific architectural designs with decoupled pipeline <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b67">68]</ref>, current researches   only focus on specific GNN instances, rather than the general design space, which limits the discovery of potential scalable GNN variants.</p><p>In addition, new architecture search systems are required to perform extensive exploration over the design space for scalable GNNs, which is also a major motivation to our work.</p><p>To the best of our knowledge, we propose the first paradigm and system -PaSca to explore the designs of scalable GNN, which makes the following contributions: Scalable Paradigm. We introduce the Scalable Graph Neural Architecture Paradigm (SGAP) with three operation abstractions: <ref type="bibr" target="#b0">(1)</ref> graph_aggregator captures the structural information via graph aggregation operations, (2) message_aggregator combines different levels of structural information, and (3) message_updater generates the prediction based on the multi-scale features. Compared with the recently published scalable GNN systems, the SGAP interfaces in PaSca are motivated and implemented differently: (1) The APIs of GNN systems are used to express existing GNNs, whereas we propose a novel GNN pipeline abstraction to define the general design space for scalable GNN architectures. (2) The existing system contains two stages -sampling and training, where sampling is not a decoupled pre-processing stage and needs to be performed for each training iteration. By contrast, the SGAP paradigm considers propagation as pre/post-processing and does not require the expensive neighborhood expansion during training.</p><p>Design Space. Based on the proposed SGAP paradigm, we further propose a general design space consisting of 6 design dimensions, resulting in 150k possible designs of scalable GNN. We find that recently emerging scalable GNN models, such as SGC <ref type="bibr" target="#b51">[52]</ref>, SIGN <ref type="bibr" target="#b11">[12]</ref>, S 2 GC <ref type="bibr" target="#b67">[68]</ref> and GBP <ref type="bibr" target="#b5">[6]</ref> are special instances in our design space. Instead of simply generalizing existing specific GNN designs, we propose a design space with adaptive aggregation and a complementary post-processing stage beyond what is typically considered in the literature. The extension is motivated by the observation that previous GNNs (e.g., GCN <ref type="bibr" target="#b19">[20]</ref> and SGC) suffer from model scalability issue, as shown in Figure <ref type="figure" target="#fig_3">2</ref>. Here we use model scalability to describe its capability to cope with the large-scale neighborhood with increased aggregation step ?. We find the underlying reason is that their aggregation processes are restricted to a fixed-hop neighborhood and are insensitive to the actual demands of different nodes, which may lead to two limitations preventing them from unleashing their full potential: (1) long-range dependencies cannot be fully leveraged due to limited hops/layers, and (2) local information are lost due to the introduction of irrelevant nodes and unnecessary messages when the number of hops increases (i.e., over-smoothing issue <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b61">62]</ref>). Through extending design space with adaptive Auto-search Engines. We design and implement a search system that automates the search procedure for well-performing scalable GNN architectures to explore the proposed design space instead of the manual design. Our search system contains the following two engines: (1) Suggestion engine that implements a multi-objective search algorithm, which aims to find Pareto-optimal GNN instances given multiple criteria (e.g., predictive performance, inference time, resource consumption), allowing for a designer to select the best Pareto-optimal solution based on specific requirements; (2) Evaluation engine that evaluates the GNN instances from the search engine in a distributed manner. Due to the repetitive expansion in the training stage of GNNs, it is hard for existing GNN systems to scale to increasing workers. Based on the SGAP paradigm, the evaluation engine in PaSca involves the expensive neighborhood expansion only once in the pre/post-processing stages, and thus ensuring the scalability upon the number of training workers. To support the new pipeline, we implement two components: (1) the distributed graph data aggregator to pre/post-process data over graph structure, and (2) the distributed trainer where workers only need to exchange neural parameters. Based on our auto-search system PaSca, we discover new scalable GNN instances from the proposed design space for different accuracy-efficiency requirements. Extensive experiments on ten graph datasets demonstrate the superior training scalability/efficiency and performance of searched representatives given by PaSca among competitive baselines. Concretely, the representatives (i.e., PaSca-V2 and PaSca-V3) outperform the state-of-the-art JK-Net by 0.2% and 0.4% in predictive accuracy on our industry dataset, while achieving up to 56.6? and 28.3? training speedups, respectively.</p><p>Relevance to Web. GNNs have recently been applied to a broad spectrum of web research such as social influence prediction <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, network role discovery <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>, recommendation system <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58]</ref>, and fraud/spam detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>. However, scalability is a major challenge that precludes GNN-based methods in practical web-scale graphs. Moreover, manually designing the well-behaved GNNs for web tasks requires immense human expertise. To bridge this gap, we highlight the relevance of the proposed PaSca platform to GNN-based web research. First, PaSca provides easier support for experts in solving their web problems via scalable GNN paradigm. Domain experts only need to provide properly formatted datasets, and PaSca can automatically search suitable and scalable GNN designs to the web-scale graphs. So PaSca permits a transition from particular GNN instances to GNN design space, which offers exciting opportunities for scalable GNN architecture innovation. Second, PaSca provides the dis-aggregated execution pipeline for efficiently training and evaluating searched GNN models, without resorting to any approximation technique (e.g., graph sampling). Given these advancements in scalability and automaticity, our PaSca system enables practical and scalable GNN-based implementation for web-scale tasks, and thus significantly reducing the barrier when applying GNN models in web research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>GNN Pipelines. Considering a graph G = (V, E) with nodes V, edges E and features for all nodes x ? ? R ? , ?? ? V, Most GNNs can be formulated using the neural message passing framework, like GCN <ref type="bibr" target="#b19">[20]</ref>, GraphSAGE <ref type="bibr" target="#b14">[15]</ref>, GAT <ref type="bibr" target="#b43">[44]</ref>, and GraphSAINT <ref type="bibr" target="#b59">[60]</ref>, where each layer adopts one aggregation and an update function. At time step ?, a message vector m ? ? is computed with the representations of its neighbors N ? using an aggregate function, and m ? ? is then updated by a neural-network based update function:</p><formula xml:id="formula_0">m ? ? ? aggregate h ? -1 ? |? ? N ? , h ? ? ? update(m ? ? ).<label>(1)</label></formula><p>In this way, messages are passed for ? time steps in a ?-layer GNN so that the steps of message passing correspond to the GNN depth.</p><p>Taking the vanilla GCN <ref type="bibr" target="#b19">[20]</ref> as an example, we have:</p><formula xml:id="formula_1">GCN-aggregate h ? -1 ? |? ? N ? = ?? ? ?N ? h ? -1 ? / ?? d? d? , GCN-update(m ? ? ) = ? (? m ? ? )</formula><p>, where d? is the degree of node ? obtained from the adjacency matrix with self-connections ? = ? +?. Recently, some GNN variants adopt the decoupled neural message passing (DNMP) for better graph learning. More details can be found in Appendix A.2.</p><p>Scalable GNN Instances. Following SGC <ref type="bibr" target="#b51">[52]</ref>, a recent direction for scalable GNN is to remove the non-linearity between each layer in the forward aggregation, and models in this direction have achieved state-of-the-art performance in leaderboards of Open Graph Benchmark <ref type="bibr" target="#b17">[18]</ref>. Concretely, SIGN <ref type="bibr" target="#b11">[12]</ref> proposes to concatenate different iterations of aggregated feature messages, while S 2 GC <ref type="bibr" target="#b67">[68]</ref> proposes a simple spectral graph convolution to average them. In addition, GBP <ref type="bibr" target="#b5">[6]</ref> applies constants to weight aggregated feature messages of different layers. As current researches focus on studying specific architectural designs, we systematically study the architectural design space for scalable GNNs.</p><p>Graph Neural Architecture Search. As a popular direction of AutoML <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>, neural architecture search <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b64">65]</ref> has been proposed to solve the labor-intensive problem of neural architecture design. Auto-GNN <ref type="bibr" target="#b66">[67]</ref> and GraphNAS <ref type="bibr" target="#b12">[13]</ref> are early approaches that apply reinforcement learning with RNN controllers on a fixed search space. You et al. <ref type="bibr" target="#b58">[59]</ref> define a similarity metric and search for the best transferable architectures across tasks via random search. Based on DARTS <ref type="bibr" target="#b28">[29]</ref>, GNAS <ref type="bibr" target="#b3">[4]</ref> proposes the differentiable searching strategy to search for GNNs with optimal message-passing step. DSS <ref type="bibr" target="#b26">[27]</ref> also adopts the differentiable strategy but optimizes over a dynamically updated search space. PaSca differs from these works in two aspects: <ref type="bibr" target="#b0">(1)</ref> To pursue efficiency and scalability on large graphs, PaSca searches for scalable architectures under the novel Algorithm 1 Scalable graph neural architecture paradigm.</p><p>Input: Graph G = (V, E), maximum aggregation steps for preprocessing and post-processing ? ??? , ? ???? , feature x ? .</p><p>Output: Prediction message m</p><formula xml:id="formula_2">? ???? ? , ?? ? V. 1: Initialize message set M ? = {x ? }, ?? ? V; 2: // Stage 1: Pre-processing 3: Initialize feature message m 0 ? = x ? , ?? ? V; 4: for 1 ? ? ? ? ??? do 5: for ? ? V do 6: m ? ? ? graph_aggregator(m ? -1 N ? );</formula><p>7: </p><formula xml:id="formula_3">M ? = M ? ? {m</formula><formula xml:id="formula_4">for ? ? V do 19: m ? ? ? graph_aggregator(m ? -1 N ? ); 20:</formula><p>end for 21: end for 22: return m</p><formula xml:id="formula_5">? ???? ? , ?? ? V;</formula><p>SGAP paradigm instead of classic architectures under the message passing framework; (2) Rather than optimizing the predictive performance alone, PaSca tackles the accuracy-efficiency trade-off through multi-objective optimization, and provides architectures to meet different needs of performance and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PASCA ABSTRACTION</head><p>To address data and model scalability issues mentioned in Section 1, we propose a novel abstraction under which more scalable GNNs can be derived. Then we define the general design space for scalable GNNs based on the proposed abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SGAP Paradigm</head><p>Our PaSca system introduces a Scalable Graph Neural Architecture Paradigm (SGAP) for designing scalable GNNs. As shown in Algorithm 1, it consists of the following three decoupled stages: Pre-processing. For each node ?, we range the step ? from 1 to ? ??? , where ? ??? is the maximum feature aggregation step. At each step ?, we use an operator, namely graph_aggregator, to aggregate the message vector collected from the neighbors N ? :</p><formula xml:id="formula_6">m ? ? ? graph_aggregator m ? -1 ? |? ? N ? ,<label>(2)</label></formula><p>where m 0 ? = x ? . The messages are passed for ? ??? steps in total during pre-processing, and m ? ? at step ? can gather the neighborhood information from nodes that are ?-hop away (lines 4-9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-training. The multi-hop messages</head><formula xml:id="formula_7">M ? = {m ? ? | 0 ? ? ? ? ??? }</formula><p>are then aggregated into a single combined message vector c ? for each node ? (line 12) as:</p><formula xml:id="formula_8">c ? ? message_aggregator(M ? ).<label>(3)</label></formula><p>Note that, if the message_aggregator is not applied, the combined message vector c ? is set to the message of the last step m ? ??? ? . We then use the message_updater to learn the class distribution of all nodes, i.e., the soft predictions (softmax outputs) predicted by the updater (line 13). Specifically, PaSca applies the Multi-layer Perceptron (MLP) as the updater, and we denote the depth of MLP as the transformation step ? ????? . It learns node embedding h ? from the combined message vector c ? :</p><formula xml:id="formula_9">h ? ? message_updater(c ? ).<label>(4)</label></formula><p>Post-processing. Motivated by Label Propagation <ref type="bibr" target="#b44">[45]</ref> which aggregates the node labels, we regard the soft predictions as new features (line 16). Then, we use the graph_aggregator again at each step to aggregate the adjacent node predictions and make the final prediction (lines 17-21) as:</p><formula xml:id="formula_10">m ? ? ? graph_aggregator m ? -1 ? |? ? N ? ,<label>(5)</label></formula><p>where m 0 ? = h ? is the original node prediction. We introduce SGAP to address both training and model scalability challenges. Specifically, it differs from the previous NMP and DNMP framework in terms of message type, message scale, and pipeline: (1) To perform the aggregate function for the next step, existing GNNs in NMP and DNMP update the hidden state h ? ? by applying the message vector m ? ? with neural networks. By contrast, SGAP allows passing node feature messages without applying graph_aggregator on the hidden states. As a result, this message passing procedure is independent of learnable model parameters and can be easily pre-computed, thus leading to high scalability and speedups. . SGAP assumes that the optimal neighborhood expansion size should be different for each node ?, and thus we retain all the messages {m ? ? |? ? [1, ? ??? ]} that a node ? receives over different steps (i.e., localities). The multi-scale messages are then aggregated per node into a single vector via message_aggregator, such that we could balance the preservation of information from both local and extended (multi-hop) neighborhoods for each node. (3) Besides feature aggregation, we propose a complementary post-processing stage to aggregate predictions (soft labels), which is not typically considered in the existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Design Space under SGAP</head><p>Following the SGAP paradigm, we propose a general design space for scalable GNNs, as shown in Table <ref type="table" target="#tab_1">1</ref>. The design space contains three integer and three categorical parameters, which are responsible for the choice of aggregators and the steps of aggregation and transformation. Each configuration sampled from the search space represents a unique scalable architecture, resulting in 150k possible designs in total. One can also include more aggregators in the current space with future state-of-the-arts. In the following, we first introduce the aggregators used in our design space, and then explore interesting GNN instances in our defined space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Graph Aggregators.</head><p>To capture the information of nodes that are several hops away, PaSca adopts a graph_aggregator to combine the nodes with their neighbors during each time step. Intuitively, it is unsuitable to use a fixed graph_aggregator for each task since the choice of graph aggregators depends on the graph structure and features. Thus PaSca provides three different graph aggregators to cope with different scenarios, and one could add more aggregators following the semantic of graph_aggregator.</p><p>Augmented normalized adjacency (Aug. NA) <ref type="bibr" target="#b19">[20]</ref>. It applies the random walk normalization on the augmented adjacency matrix ? = ? + ?, which is simple yet effective on a range of GNNs. The normalized graph_aggregator is:</p><formula xml:id="formula_11">m ? ? = ?? ??N? 1 d? m ? -1 ? .<label>(6)</label></formula><p>Personalized PageRank (PPR). It focuses on its local neighborhood using a restart probability ? ? (0, 1] and performs well on graphs with noisy connectivity. While the calculation of the fully personalized PageRank matrix is computationally expensive, we apply its approximate computation <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_12">m ? ? = ?m 0 ? + (1 -?) ?? ??N? 1 ?? d? d? m ? -1 ? ,<label>(7)</label></formula><p>where the restart probability ? allows to balance preserving locality (i.e., staying close to the root node to avoid over-smoothing) and leveraging the information from a large neighborhood.</p><p>Triangle-induced adjacency (Triangle. IA) <ref type="bibr" target="#b34">[35]</ref>. It accounts for the higher-order structures and helps distinguish strong and weak ties on complex graphs like social graphs. We assign each edge a weight representing the number of different triangles it belongs to, which forms a weight matrix ? ??? . We denote ? ??? ? as the degree of node ? from the weighted adjacency matrix ? ??? . The aggregator is then calculated by applying a row-wise normalization:</p><formula xml:id="formula_13">m ? ? = ?? ??N? 1 ? ??? ? m ? -1 ? .<label>(8)</label></formula><p>3.2.2 Message Aggregators. Before updating the hidden state of each node, PaSca proposes to apply a message_aggregator to combine messages obtained by graph_aggregator per node into a single vector, such that the subsequent model learns from the multiscale neighborhood of a given node. We summarize the different message aggregators PaSca as follows,</p><p>Non-adpative aggregator. This type of aggregator does not consider the correlation between messages and the center node. The messages are directly concatenated or summed up with weights to obtain the combined message vector as,</p><formula xml:id="formula_14">? ??? ? ? m ? ? ??? ? ? ? (m ? ? ),<label>(9)</label></formula><p>where ? is a function used to reduce the dimension of message vectors, and ? can be concatenating or pooling operators including average pooling or max pooling. Note that, for aggregator type "Mean", "Max" and "Concatenate", each weight ? ? is set to 1 for each message. For aggregator type "Weighted", we set the weight to constants following GBP <ref type="bibr" target="#b5">[6]</ref>. Compared with pooling operators, though the concatenating operator keeps all the input message  information, the dimension of its outputs increases as ? ??? grows, leading to additional computational cost in the downstream updater.</p><p>Adpative aggregators. The messages of different hops make different contributions to the final performance. As shown in Figure <ref type="figure" target="#fig_5">3</ref>, we apply GCN with different layers to conduct node classification on Citeseer. Note that the X-axis is the node id and Y-axis is the aggregation steps (number of layers in GCN). The color from white to blue represents the ratio of being predicted correctly in 50 different runs. We observe that most nodes are well classified with two steps, and as a result, most carefully designed GNN models are set with two layers (i.e., steps). In addition, the predictive accuracy on 13 of the 20 sampled nodes increases with a certain step larger than two. This motivates the design of node-adaptive aggregation functions, which determines the importance of a node's message at different ranges rather than fixing the same weights for all nodes.</p><p>To this end, we propose the gating aggregator, which generates retainment scores that indicate how much the corresponding messages should be retained in the final combined message.</p><formula xml:id="formula_15">c msg ? ?? m ? ? ??? ? ? m ? ? , ? ? = ? (sm ? ? ),<label>(10)</label></formula><p>where s is a trainable vector to generate gating scores, and ? is the sigmoid function. With the adaptive message_aggregator, the model can balance the messages from the multi-scale neighborhoods for each node at the expense of training extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SGAP Instances</head><p>Recent scalable GNN models, such as SGC <ref type="bibr" target="#b51">[52]</ref>, SIGN <ref type="bibr" target="#b11">[12]</ref>, S 2 GC <ref type="bibr" target="#b67">[68]</ref> and GBP <ref type="bibr" target="#b5">[6]</ref>, can be considered as specific instances in our defined design space, as shown in Table <ref type="table" target="#tab_2">2</ref>. We see that most current scalable GNNs ignore the post-processing stages, which can boost the model performance demonstrated by our experiments.</p><p>Besides, various scalable GNNs can be obtained by using different design choices under SGAP. For example, the current state-of-theart scalable model GBP sets the graph_aggregator as Aug.NA and uses the weighted strategy in the message_aggregator. We decouple MLP training and information propagation in APPNP <ref type="bibr" target="#b20">[21]</ref> into two individual processes and get a new scalable model PaSca-APPNP. To effectively explore the large design space, we implement an auto-search system engine below to automate the search procedure of scalable GNN architecture instead of manual design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PASCA ENGINES</head><p>Figure <ref type="figure" target="#fig_6">4</ref> shows the overview of our proposed auto-search system to explore GNN designs under PaSca abstraction. It consists of two engines: the search engine and the evaluation engine. The search engine includes the proposed designed search space for scalable GNNs and implements a suggestion server that is responsible for suggesting architectures to evaluate. The evaluation engine receives an instance and trains the corresponding architecture in a distributed fashion. An iteration of the searching process is as follows: 1) The suggestion server samples an architecture instance based on its built-in optimization algorithm and sends it to the evaluation engine; 2) The evaluation engine evaluates the architecture and updates the suggestion server with its observed performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Engine</head><p>While prior researches on scalable GNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> focus on optimizing the classification error, recent applications not only require high predictive performance but also low resource-consumption, e.g. model size or inference time. In addition, there is typically an implicit trade-off between predictive performance and resource consumption. To this end, the suggestion server implements a multi-objective search algorithm to tackle this trade-off. Concretely, we use the Bayesian optimization based on EHVI <ref type="bibr" target="#b8">[9]</ref>, a widely-used algorithm that maximizes the predicted improvement of hypervolume indicator of Pareto-optimal points relative to a given reference point. The suggestion server then optimizes over the search space following a typical Bayesian optimization as 1) Based on the observation history, the server trains multiple surrogates, namely the Gaussian Process, to model the relationships between each architecture instance and its objective values; 2) The server randomly samples a number of new instances, and suggests the best one which maximizes the EHVI based on the predicted outputs of trained surrogates; 3) The server receives the results of the suggested instance and updates its observation history. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Engine</head><p>Different from the sampling-training process of existing GNN systems (e.g., DistDGL <ref type="bibr" target="#b65">[66]</ref>, NextDoor <ref type="bibr" target="#b18">[19]</ref>, and FlexGraph <ref type="bibr" target="#b45">[46]</ref>), the process of PaSca evaluation engine is decoupled into pre-processing, training and post-processing as illustrated in Figure <ref type="figure" target="#fig_6">4</ref>: PaSca first pre-computes the feature messages for each node over the graph, and then it combines the messages and trains the model parameters with parameter sever. Finally, PaSca post-computes the prediction messages for each node over the graph. All the messages are partitioned and stored in a distributed storage system, and the stages can be implemented in a distributed fashion. Specifically, the engine consists of the following two components:</p><p>Graph Data Aggregator. This component handles pre-processing and post-processing stages on data aggregation over graph structure. The two stages share the same pipeline but take different messages as inputs (features for pre-processing and predictions for post-processing). We implement an efficient batch processing pipeline over distributed graph storage: The nodes are partitioned into batches, and the computation of each batch is implemented by workers in parallel with matrix multiplication. As shown in Figure <ref type="figure" target="#fig_6">4</ref>, for each node in a batch, we firstly pull all the ?-th step messages of its 1-hop neighbors from the message distributed storage and then compute the (? + 1)-th step messages of the batch in parallel.</p><p>Next, We push these aggregated messages back for reuse in the calculation of the (? + 2)-th step messages. In our implementation, we treat GPUs as workers for fast processing, and the graph data are partitioned and stored on host memory across machines. Given the parallel message computation, our implementation could scale to large graphs and significantly reduce the runtime.</p><p>Neural Architecture Trainer. This component handles the training of neural networks. We optimize the parameters of each architecture with distributed SGD. The model parameters are stored on a parameter server, and multiple workers (GPUs) process the data in parallel. We adopt asynchronous training to avoid the communication overhead between workers. Each worker fetches the most up-to-date parameters and computes the gradients for a mini-batch of data, independent of the other workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Settings</head><p>Datasets. We conduct the experiments on three citation networks (Citeseer, Cora, and PubMed) in <ref type="bibr" target="#b19">[20]</ref>, two social networks (Flickr and Reddit) in <ref type="bibr" target="#b59">[60]</ref>, four co-authorship graphs (Amazon and Coauthor) in <ref type="bibr" target="#b35">[36]</ref>, the co-purchasing network (ogbn-products) in <ref type="bibr" target="#b17">[18]</ref> and one short-form video recommendation graph (Industry) from our industrial cooperative enterprise. Table <ref type="table">6</ref> in Appendix A.1 provides the overview of the used graph datasets.</p><p>Parameters and Environment. To eliminate random factors, we run each method 20 times and report the mean and variance of the performance. More details for experimental setups and reproduction are provided in Appendix A.4 and A.5.</p><p>Baselines. In the transductive settings, we compare the searched scalable GNNs with GCN <ref type="bibr" target="#b19">[20]</ref>, GAT <ref type="bibr" target="#b43">[44]</ref>, JK-Net <ref type="bibr" target="#b56">[57]</ref>, Res-GCN <ref type="bibr" target="#b19">[20]</ref>, APPNP <ref type="bibr" target="#b20">[21]</ref>, AP-GCN <ref type="bibr" target="#b41">[42]</ref>, SGC <ref type="bibr" target="#b51">[52]</ref>, SIGN <ref type="bibr" target="#b11">[12]</ref>, S 2 GC <ref type="bibr" target="#b67">[68]</ref> and GBP <ref type="bibr" target="#b5">[6]</ref>, which are SOTA models of different message passing types. In the inductive settings, the compared baselines are Graph-SAGE <ref type="bibr" target="#b14">[15]</ref>, FastGCN <ref type="bibr" target="#b4">[5]</ref>, ClusterGCN <ref type="bibr" target="#b6">[7]</ref> and GraphSAINT <ref type="bibr" target="#b59">[60]</ref>. More descriptions about the baselines are provided in Appendix A.3.</p><p>In the following, we first analyze the superiority of representative instances searched by PaSca. Then we evaluate the transferability, training efficiency, and model scalability of PaSca representatives compared with competitive state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Searched Representatives</head><p>We apply the multi-objective optimization targeting at classification error and inference time on Cora. Figure <ref type="figure" target="#fig_9">6</ref> demonstrates the Pareto Front found by PaSca with a budget of 2000 evaluations, together with the results of several manually designed scalable GNNs. The inference time has been normalized based on instances with the minimum and maximum inference time in our design space. Interestingly, we observe that GBP and PaSca-APPNP, our extended variant of APPNP (see Table <ref type="table" target="#tab_2">2</ref>), falls on the Pareto Front, which indicates the superior design of the "Weighted" message_aggregator and "PPR" graph_aggregator. We also choose other three instances from the Pareto Front as PaSca-V1 to V3 with different accuracyefficiency requirements as searched representatives of SGAP for the following evaluations. The corresponding parameters of each architecture are shown in Table <ref type="table" target="#tab_3">3</ref>. Among the three architectures, PaSca-V1 Pareto-dominates the other baselines except GBP, and   PaSca-V3 is the architecture with the best predictive performance found by the search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Scalability</head><p>The main characteristic of our proposed design space is that the architectures sampled from the space, namely PaSca-SGAP, share high scalability upon workers. To examine the scalability of PaSca-SGAP, we choose PaSca-APPNP as a representative and compare it with GraphSAGE, a widely-used method in industry on two largescale datasets. We train GraphSAGE with DGL and PaSca-APPNP with the evaluation engine of PaSca, respectively. We train both methods in stand-alone and distributed scenarios and then measure their corresponding speedups. The batch size is 8192 for Reddit and 16384 for ogbn-product, and the speedup is calculated by runtime per epoch relative to that of one worker in the stand-alone scenario and two workers in the distributed scenario. Without considering extra cost, the speedup will increase linearly in an ideal condition.</p><p>The corresponding results are shown in Figure <ref type="figure" target="#fig_8">5</ref>. Since Graph-SAGE requires aggregating the neighborhood nodes during training, GraphSAGE trained with DGL meets the I/O bottleneck when transmitting a large number of required neural messages. Thus, the speedup of GraphSAGE training increases slowly as the number of workers grows, which is less than 2? even with four workers in the stand-alone scenario and eight workers in the distributed scenario. Recall that the only communication cost of PaSca-SGAP is to synchronize parameters among different workers, which is essential to all distributed training methods. As a result, PaSca-SGAP trained with the evaluation engine scales up close to the ideal circumstance in both scenarios, indicating the superiority of PaSca. T est Accuracy (%) </p><formula xml:id="formula_16">SGC 1x GCN 33x APPNP 78x 1x 2x GAT 372x AP-GCN 112x JK-Net 113x Res-GCN 132x SIGN 3x S2GC 1x GBP 1x PASCA-SGAP 4x PASCA-V PASCA-V3 PASCA-V2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance-Efficiency Analysis</head><p>To test the transferability and training efficiency of PaSca models, we further evaluate them on more datasets compared with competitive baselines. The results are summarized in Table <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref>.</p><p>We observe that PaSca models obtain quite competitive performance in both transductive and inductive settings. In transductive settings, our simplified variant PaSca-V1 also achieves the best performance among Non-SGAP baselines on most datasets, which shows the superiority of SGAP design. In addition, PaSca-V2 and V3 outperform the best baseline GBP by a margin of 0.1%?0.6% and 0.2%?1.3% on each dataset. We attribute this improvement to the application of the adaptive message_aggregator. In inductive settings, Table <ref type="table" target="#tab_5">5</ref> shows that PaSca-V3 outperforms the best baseline GraphSAINT by a margin of 1.1% on Flickr and 0.1% on Reddit.</p><p>We also evaluate the training efficiency of each method in the real production environment. Figure <ref type="figure" target="#fig_11">7</ref> illustrates the performance over training time on Industry. In particular, we pre-compute the feature messages of each scalable method, and the training time takes into account the pre-computation time. We observe that NMP architectures require at least a magnitude of training time than PaSca-SGAP. Among considered baselines, PaSca-V3 achieves the best performance with 4? training time compared with GBP and PaSca-V1. Note that, though PaSca-V1 requires the same training time as GBP, its inference time is less than GBP. Therefore, we recommend choosing PaSca-V1 to V3, along with GBP, according to different requirements of predictive performance, training efficiency, and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Model Scalability</head><p>We observe that both PaSca-V2 and V3 found by the search engine contain the "Adaptive" message_aggregator. In this subsection,  We plot the changes of model performance along with the message passing steps in the left subfigure of Figure <ref type="figure" target="#fig_13">8</ref>. For a fair comparison, we use PaSca-V2 which does not include post-processing. The vanilla GCN gets the best results with two aggregation steps, but its performance drops rapidly along with the increased steps due to the over-smoothing issue. Both Res-GCN and SGC show better performance than GCN with larger aggregation steps. Take Res-GCN as an example, it carries information from the previous step by introducing the residual connections and thus alleviates this problem. However, these two methods cannot benefit from deep GNN architecture since they are unable to balance the needs of preserving locality (i.e., staying close to the root node to avoid over-smoothing) and leveraging the information from a large neighborhood. In contrast, PaSca-V2 achieves consistent improvement and remains nondecreasing across steps, which indicates that PaSca can scales to large depth. The reason is that the adaptive message_aggregator in PaSca-V2 is able to adaptively and effectively combine multiscale neighborhood messages for each node.</p><p>To demonstrate this, the right subfigure of Figure <ref type="figure" target="#fig_13">8</ref> shows PaSca-V2's average gating weights of feature messages according to the number of steps and degrees of input nodes, where the maximum step is 6. In this experiment, we randomly select 20 nodes for each degree range (1-4, 5-8, 9-12) and plot the relative weight based on the maximum value. We get two observations from the heat map: 1) The 1-step and 2-step graph messages are always of great importance, which shows that the adaptive message_aggregator captures the local information as those widely used 2-layer GNNs do; 2) The weights of graph messages with larger steps drop faster as the degree grows, which indicates that the attention-based aggregator could prevent high-degree nodes from including excessive irrelevant nodes which lead to over-smoothing. From the two observations, we conclude that the adaptive message_aggregator can identify the different message-passing demands of nodes and explicitly weight each graph message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed PaSca, a new auto-search system that offers a principled approach to systemically construct and explore the design space for scalable GNNs, rather than studying individual designs. Experiments on ten real-world benchmarks demonstrate that the representative instances searched by PaSca outperform SOTA GNNs in terms of performance, efficiency, and scalability. PaSca can help researchers understand design choices when developing new scalable GNN models, and serve as a system to support extensive exploration over the design space for scalable GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Dataset description</head><p>Cora, Citeseer, and Pubmed<ref type="foot" target="#foot_0">1</ref> are three well-known citation network datasets, and we follow the same training/validation/test split as GCN <ref type="bibr" target="#b19">[20]</ref>.</p><p>Reddit is a social network modeling the community structure of Reddit posts. This dataset is often used for inductive training, and the training/validation/test split is coherent with that of Graph-SAGE <ref type="bibr" target="#b14">[15]</ref>.</p><p>Flickr originates from NUS-wide<ref type="foot" target="#foot_1">2</ref> and contains different types of images based on the descriptions and common properties of online images. We use a public version of Reddit and Flickr provided by GraphSAINT <ref type="foot" target="#foot_2">3</ref> .</p><p>Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph <ref type="bibr" target="#b39">[40]</ref>, where nodes represent goods, edges indicate that two goods are frequently bought together, node features are bag-of-words encoded product reviews, and class labels are given by the product category.</p><p>Coauthor CS and Coauthor Physics are co-authorship graph based on the Microsoft Academic Graph from the KDD Cup 2016 challenge <ref type="foot" target="#foot_3">4</ref> . Here, nodes are authors, that are connected by an edge if they co-authored a paper; node features represent paper keywords for each author's papers, and class labels indicate the most active fields of study for each author. We use a pre-divided version of these datasets through the Deep Graph Library (DGL) <ref type="foot" target="#foot_4">5</ref> .</p><p>ogbn-products is an unweighted graph representing an Amazon product co-purchase network. Each node represents a product sold on Amazon, and edges between two products indicate that the products are purchased together. We use the public data split for this dataset as in Open Graph Benchmark <ref type="foot" target="#foot_5">6</ref> .</p><p>Industry is a user-video graph collected from a real-world mobile application from our industry partner. We sampled 1,000,000 users and videos from the app, and treat these items as nodes. The edges in the generated bipartite graph represent that the user clicks the short videos. Each user has 64 features, and the target is to category these short videos into 253 different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Decoupled Neural Message Passing</head><p>Note that the aggregate and update operations are inherently intertwined in Equation (1), i.e., each aggregate operation requires a neural layer to update the node's hidden state in order to generate a new message for the next step. Recently, some researches show that such entanglement could compromise performance on a range of benchmark tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64]</ref>, and suggest separating GCN from the aggregation scheme. We reformulate these models into a single Decoupled Neural Message Passing (DNMP) framework: Neural prediction messages are first generated (with update function) for each node utilizing only that node's own features, and then aggregated using aggregate function.</p><formula xml:id="formula_17">h 0 ? ? update(x ? ), h ? ? ? aggregate h ? -1 ? |? ? N ? . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where ? ? is the input feature of node ?. Existing methods, such as PPNP <ref type="bibr" target="#b20">[21]</ref>, APPNP <ref type="bibr" target="#b20">[21]</ref>, AP-GCN <ref type="bibr" target="#b41">[42]</ref> and etc., follows this decoupled MP. Taking APPNP as an example: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More details about the compared baselines</head><p>The main characteristic of all baselines are listed as follows:</p><p>? GCN <ref type="bibr" target="#b19">[20]</ref> produces node embedding vectors by truncating the Chebyshev polynomial to the first-order neighborhoods.</p><p>? ResGCN <ref type="bibr" target="#b19">[20]</ref> adopts the residual connections between hidden layers to facilitate the training of deeper models by enabling the model to carry over information from the previous layer's input. ? JK-Net <ref type="bibr" target="#b56">[57]</ref> proposes a new aggregation scheme for node representation learning that can adapt neighborhood ranges to nodes individually. ? APPNP <ref type="bibr" target="#b20">[21]</ref> uses the relationship between GCN and PageRank to derive an improved propagation scheme based on personalized PageRank. ? AP-GCN <ref type="bibr" target="#b41">[42]</ref> is a variation of GCN wherein each node selects automatically the number of propagation steps performed across the graph. ? SGC <ref type="bibr" target="#b51">[52]</ref> reduces the excess complexity of GCN through successively removing non-linearities and collapsing weight matrices between consecutive layers. ? SIGN <ref type="bibr" target="#b11">[12]</ref> is a sampling-free Graph Neural Network model that is able to easily scale to gigantic graphs while retaining enough expressive power. ? GraphSAGE <ref type="bibr" target="#b14">[15]</ref> is an inductive framework that leverages node attribute information to efficiently generate representations on previously unseen data.</p><p>? GAT <ref type="bibr" target="#b27">[28]</ref> leverages masked self-attentional layers to address the shortcomings of prior GNNs based on graph convolutions or their approximations, and enables specifying different weights to different nodes in a neighborhood. ? S 2 GC [68]: S 2 GC uses a modified Markov Diffusion Kernel to derive a variant of GCN, and it can be used as a trade-off of low-pass and high-pass filter which captures the global and local contexts of each node. ? FastGCN <ref type="bibr" target="#b4">[5]</ref> interprets graph convolutions as integral transforms of embedding functions under probability measures, and enhances GCN with importance sampling. ? ClusterGCN <ref type="bibr" target="#b6">[7]</ref> designs the batches based on efficient graph clustering algorithms, and it proposes a stochastic multiclustering framework to improve the convergence. ? GBP <ref type="bibr" target="#b5">[6]</ref>: GBP utilizes a localized bidirectional propagation process to further improve SGC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The speedup and bottleneck of a two-layer Graph-SAGE along with the increased workers on Reddit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Test accuracy of different models along with the increased aggregation steps on PubMed dataset. aggregators, we could discover new GNNs to achieve both model scalability and training scalability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b1">(2)</ref> Most GNNs in NMP and DNMP only utilizes the last message vector m ? ??? ? to compute the final hidden state h ? ??? ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The influence of aggregation steps on 20 randomly sampled nodes on Citeseer dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The workflow of PaSca, which consists of the searching and evaluation engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Distributed on ogbn-product</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Scalability comparison on Reddit and ogbn-product datasets. The stand-alone scenario means the graph has only one partition stored on a multi-GPU server, whereas the distributed scenario means the graph is partitioned and stored on multi-servers. In the distributed scenario, we run two workers per machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pareto Front found on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Test accuracy over training time on Industry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>explain the advantage of adaptive message_aggregator in the perspective of model scalability on message passing steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Left: Test accuracy of different models along with the increased aggregation steps on PubMed. Right: PaSca-V2's average gating weights of graph messages of different steps on 60 randomly selected nodes from PubMed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>,</head><label></label><figDesc>APPNP-update(xv ) = ? (? x ? ), APPNP-aggregate h ? -1 ? |? ? N ? = ?h 0 ? + (1 -?)where aggregate function adopts personalized PageRank with the restart probability ? ? (0, 1] controlling the locality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The search space for scalable GNNs in our PaSca system.</figDesc><table><row><cell>Stages</cell><cell>Name</cell><cell>Range/Choices</cell><cell>Type</cell></row><row><cell>Pre-processing</cell><cell>Aggregation steps (? ??? ) Graph aggregators (?? ???</cell><cell>[0, 10]</cell><cell>Integer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Current scalable GNNs in our design space.</figDesc><table><row><cell></cell><cell>Pre-processing</cell><cell cols="2">Model training</cell><cell>Post-processing</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>?? ???</cell><cell>??</cell><cell>? ?????</cell><cell>?? ????</cell></row><row><cell>SGC</cell><cell>Aug.NA</cell><cell>None</cell><cell>1</cell><cell>/</cell></row><row><cell>SIGN</cell><cell>Optional</cell><cell>Concatenate</cell><cell>1</cell><cell>/</cell></row><row><cell>S 2 GC</cell><cell>PPR</cell><cell>Mean</cell><cell>1</cell><cell>/</cell></row><row><cell>GBP</cell><cell>Aug.NA</cell><cell>Weighted</cell><cell>? 2</cell><cell>/</cell></row><row><cell>PaSca-APPNP</cell><cell>/</cell><cell>/</cell><cell>? 2</cell><cell>PPR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Scalable GNNs found by PaSca.</figDesc><table><row><cell></cell><cell cols="2">Pre-processing</cell><cell></cell><cell>Model training</cell><cell cols="2">Post-processing</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>?????</cell><cell>??</cell><cell>????</cell><cell>??????</cell><cell>??????</cell><cell>?????</cell></row><row><cell cols="3">PaSca-V1 PPR(? = 0.1) Weighted</cell><cell>3</cell><cell>2</cell><cell>/</cell><cell>/</cell></row><row><cell>PaSca-V2</cell><cell>Aug.NA</cell><cell>Adaptive</cell><cell>6</cell><cell>2</cell><cell>/</cell><cell>/</cell></row><row><cell>PaSca-V3</cell><cell>Aug.NA</cell><cell>Adaptive</cell><cell>6</cell><cell>3</cell><cell>PPR (? = 0.3)</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (%) in transductive settings. "NMP" and "DNMP" refer to architectures following NMP and DNMP paradigm. "SGAP" refers to architectures following the scalable graph archtecture paradigm proposed in Section 3.1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Amazon</cell><cell>Amazon</cell><cell>Coauthor</cell><cell>Coauthor</cell></row><row><cell>Type</cell><cell>Models</cell><cell>Cora</cell><cell>Citeseer PubMed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Industry</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Computer</cell><cell>Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell></cell><cell>GCN</cell><cell cols="2">81.8?0.5 70.8?0.5 79.3?0.7</cell><cell>82.4?0.4</cell><cell>91.2?0.6</cell><cell>90.7?0.2</cell><cell>92.7?1.1</cell><cell>45.9?0.4</cell></row><row><cell>NMP</cell><cell>GAT JK-Net</cell><cell cols="2">83.0?0.7 72.5?0.7 79.0?0.3 81.8?0.5 70.7?0.7 78.8?0.7</cell><cell>80.1?0.6 82.0?0.6</cell><cell>90.8?1.0 91.9?0.7</cell><cell>87.4?0.2 89.5?0.6</cell><cell>90.2?1.4 92.5?0.4</cell><cell>46.8?0.7 47.2?0.3</cell></row><row><cell></cell><cell>ResGCN</cell><cell cols="2">82.2?0.6 70.8?0.7 78.3?0.6</cell><cell>81.1?0.7</cell><cell>91.3?0.9</cell><cell>87.9?0.6</cell><cell>92.2?1.5</cell><cell>46.8?0.5</cell></row><row><cell>DNMP</cell><cell cols="3">APPNP AP-GCN 83.4?0.3 71.3?0.5 79.7?0.3 83.3?0.5 71.8?0.5 80.1?0.2</cell><cell>81.7?0.3 83.7?0.6</cell><cell>91.4?0.3 92.1?0.3</cell><cell>92.1?0.4 91.6?0.7</cell><cell>92.8?0.9 93.1?0.9</cell><cell>46.7?0.6 46.9?0.7</cell></row><row><cell></cell><cell>SGC</cell><cell cols="2">81.0?0.2 71.3?0.5 78.9?0.5</cell><cell>82.2?0.9</cell><cell>91.6?0.7</cell><cell>90.3?0.5</cell><cell>91.7?1.1</cell><cell>45.2?0.3</cell></row><row><cell></cell><cell>SIGN</cell><cell cols="2">82.1?0.3 72.4?0.8 79.5?0.5</cell><cell>83.1?0.8</cell><cell>91.7?0.7</cell><cell>91.9?0.3</cell><cell>92.8?0.8</cell><cell>46.3?0.5</cell></row><row><cell></cell><cell>S 2 GC</cell><cell cols="2">82.7?0.3 73.0?0.2 79.9?0.3</cell><cell>83.1?0.7</cell><cell>91.6?0.6</cell><cell>91.6?0.6</cell><cell>93.1?0.8</cell><cell>45.9?0.4</cell></row><row><cell>SGAP</cell><cell>GBP</cell><cell cols="2">83.9?0.7 72.9?0.5 80.6?0.4</cell><cell>83.5?0.8</cell><cell>92.1?0.8</cell><cell>92.3?0.4</cell><cell>93.3?0.7</cell><cell>47.1?0.6</cell></row><row><cell></cell><cell cols="3">PaSca-V1 83.4?0.5 72.2?0.5 80.5?0.4</cell><cell>83.7?0.7</cell><cell>92.1?0.7</cell><cell>91.9?0.3</cell><cell>93.2?0.6</cell><cell>46.3?0.4</cell></row><row><cell></cell><cell cols="3">PaSca-V2 84.4?0.3 73.1?0.3 80.7?0.7</cell><cell>84.1?0.7</cell><cell>92.4?0.7</cell><cell>92.6?0.4</cell><cell>93.6?0.8</cell><cell>47.4?0.6</cell></row><row><cell></cell><cell cols="3">PaSca-V3 84.6?0.6 73.4?0.5 80.8?0.6</cell><cell>84.8?0.7</cell><cell cols="2">92.7?0.8 92.8?0.5</cell><cell>93.8?0.9</cell><cell>47.6?0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy (%) in inductive settings.</figDesc><table><row><cell>Models</cell><cell>Flickr</cell><cell>Reddit</cell></row><row><cell cols="2">GraphSAGE 50.1?1.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/tkipf/gcn/tree/master/gcn/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://lms.comp.nus.edu.sg/research/NUS-WIDE.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/GraphSAINT/GraphSAINT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://kddcup2016.azurewebsites.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://docs.dgl.ai/en/0.4.x/api/python/data.html#coauthor-dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/snap-stanford/ogb</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/pytorch</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by <rs type="funder">NSFC</rs> (No. <rs type="grantNumber">61832001</rs>, <rs type="grantNumber">61972004</rs>), <rs type="projectName">Beijing Academy of Artificial Intelligence (BAAI</rs>), <rs type="funder">PKU-Baidu Fund</rs> <rs type="grantNumber">2019BD006</rs>, and <rs type="institution">PKU-Tencent Joint Research Lab</rs>. <rs type="person">Zhi Yang</rs> and <rs type="person">Bin Cui</rs> are the corresponding authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_X2yWsrs">
					<idno type="grant-number">61832001</idno>
				</org>
				<org type="funded-project" xml:id="_vBvCPed">
					<idno type="grant-number">61972004</idno>
					<orgName type="project" subtype="full">Beijing Academy of Artificial Intelligence (BAAI</orgName>
				</org>
				<org type="funding" xml:id="_UrddHhY">
					<idno type="grant-number">2019BD006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Experiments setup</head><p>We use PyTorch 7 and DGL to implement the models, and we train them using Adam optimizer. To evaluate the scalability of Graph-SAGE, we implement GraphSAGE via DistDGL. Besides, we train each model 400 epochs and terminate the training process if the validation accuracy does not improve for 20 consecutive steps. Note that both GraphSAGE and JKNet have three aggregators, and we choose the concatenation and mean as their aggregator, respectively, since these two aggregators perform best in most datasets.</p><p>For GAT, the number of attention heads is fixed to 8. For Graph-SAGE, we use the results on Flickr and Reddit as reported in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b59">[60]</ref>. For ClusterGCN, we use the results on Reddit as reported in <ref type="bibr" target="#b6">[7]</ref> and run our own implementation on Flickr. The hyperparameters are selected from random search. The random search was performed over the following search space: hidden size ? {8, 16, 32, 64, 128, 256, 512}, learning rate ? {1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 2e-1}, dropout rate ? {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9]}, regularization strength ? {1e-4, 5e-4,1e-3, 5e-3, 1e-2, 5e-2, 1e-1}. Note that both Res-GCN and JK-Net will degrade into GCN if they have only two layers, so we set their aggregation steps ? <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref> in all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Experiment Environment and Reproduction Instructions</head><p>The experiments are implemented on 4 machines with 14 Intel(R) Xeon(R) CPUs (Gold 5120 @ 2.20GHz) and four NVIDIA TITAN RTX GPUs. The code is written in Python 3.6, and the multi-objective algorithm is implemented based on OpenBox <ref type="bibr" target="#b24">[25]</ref>. We use Pytorch 1.7.1 on CUDA 10.1 to train the model on GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">RECON: relation extraction using knowledge graph context in a graph neural network</title>
		<author>
			<persName><forename type="first">Anson</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaiah</forename><surname>Onando Mulang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeedeh</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1673" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-scale approach for graph link prediction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3308" to="3315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking graph neural architecture search from message-passing</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6657" to="6666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable graph neural networks via bidirectional propagation</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14556" to="14566" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the equivalence of decoupled graph convolution network and label propagation</title>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxian</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3651" to="3662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Single-and multi-objective evolutionary design optimization assisted by gaussian random field metamodels</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Emmerich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Dormund</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Identify High Betweenness Centrality Nodes from Scratch: A Novel Graph Neural Network Approach</title>
		<author>
			<persName><forename type="first">Changjun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Managemenat, CIKM 2019</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Managemenat, CIKM 2019<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. Nov 3-7, 2019</date>
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">EAT-NAS: Elastic architecture transfer for accelerating large-scale neural architecture search</title>
		<author>
			<persName><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Fabrizio Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, Christian Bessiere</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, Christian Bessiere</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Syntax-guided text generation via graph neural network</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AutoML: A survey of the stateof-the-art</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">106622</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RetaGNN: Relational temporal attentive graph neural networks for holistic sequential recommendation</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2968" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accelerating graph sampling for graph machine learning using GPUs</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jangda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Polisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Serafini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth European Conference on Computer Systems</title>
		<meeting>the Sixteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="311" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spam Review Detection with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Managemenat, CIKM 2019</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Managemenat, CIKM 2019<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. Nov 3-7, 2019</date>
			<biblScope unit="page" from="2703" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8491" to="8500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Openbox: A generalized black-box optimization service</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaijun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3209" to="3219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VolcanoML: speeding up end-to-end AutoML via scalable search space decomposition</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2167" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One-shot graph neural architecture search with dynamic search space</title>
		<author>
			<persName><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zean</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8510" to="8517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph Partition Neural Networks for Semi-Supervised Classification</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pick and choose: a GNN-based imbalanced learning approach for fraud detection</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3168" to="3177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving Graph Neural Networks with Structural Adaptive Receptive Fields</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2438" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Degnn: Improving graph neural networks with graph decomposition</title>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nezihe</forename><forename type="middle">Merve</forename><surname>G?rel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susie</forename><forename type="middle">Xi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture</title>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motifnet: a motifbased graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Science Workshop</title>
		<imprint>
			<biblScope unit="page" from="225" to="228" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting Customer Value with Social Relationships via Motif-based Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<idno>ACM / IW3C2</idno>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event</title>
		<editor>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marko</forename><surname>Grobelnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leila</forename><surname>Zia</surname></persName>
		</editor>
		<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			<biblScope unit="page" from="3146" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pathfinder Discovery Networks for Neural Message Passing</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Englert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2547" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cui</forename><surname>Bin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10423</idno>
		<title level="m">ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive propagation graph convolutional network</title>
		<author>
			<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4755" to="4760" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dorylus: Affordable, Scalable, and Accurate {GNN} Training with Distributed {CPU} Servers and Serverless Threads</title>
		<author>
			<persName><forename type="first">John</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Eyolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="495" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">FlexGraph: a flexible and efficient distributed framework for GNN training</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbang</forename><surname>Chao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth European Conference on Computer Systems</title>
		<meeting>the Sixteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph structure estimation neural networks</title>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanpeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="342" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mixed-Curvature Multi-Relational Graph Neural Network for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?cero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">O</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">F</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
		<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19">2021. April 19-23, 2021</date>
			<biblScope unit="page" from="1761" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs</title>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gushu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="515" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep reasoning with knowledge graph for social relationship understanding</title>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02260</idno>
		<title level="m">Graph neural networks in recommender systems: a survey</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hashing-accelerated graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2910" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Seastar: vertex-centric programming for graph neural networks</title>
		<author>
			<persName><forename type="first">Yidi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth European Conference on Computer Systems</title>
		<meeting>the Sixteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="359" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ROD: reception-aware online distillation for sparse graphs</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuezihan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2232" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuezihan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikuan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00955</idno>
		<title level="m">Evaluating deep graph neural networks</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Node Dependent Local Smoothing for Scalable Graph Learning</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10097</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Automated Machine Learning on Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4704" to="4712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Distdgl: distributed graph neural network training for billion-scale graphs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qidong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM 10th Workshop on Irregular Applications: Architectures and Algorithms</title>
		<imprint>
			<biblScope unit="issue">IA3</biblScope>
			<biblScope unit="page" from="36" to="44" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
