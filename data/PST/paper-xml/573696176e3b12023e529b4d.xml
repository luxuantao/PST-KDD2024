<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A REDUCED HESSIAN METHOD FOR LARGE-SCALE CONSTRAINED OPTIMIZATION*</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lorenz</forename><forename type="middle">T</forename><surname>Bieglert</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>AND</roleName><forename type="first">Jorge</forename><surname>Nocedal$</surname></persName>
							<email>nocedal@eecs.nwu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Claudia</forename><surname>Schmidt</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Chemical Engineering Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">National Science Foundation-sponsored Engineering Research Center at Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<postCode>060439</postCode>
									<settlement>Argonne</settlement>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A REDUCED HESSIAN METHOD FOR LARGE-SCALE CONSTRAINED OPTIMIZATION*</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">440767770BF4C6EFED04441E52A3607F</idno>
					<note type="submission">Received by the editors May 5, 1993; accepted for publication (in revised form) December 1, 1993.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a quasi-Newton algorithm for solving large optimization problems with nonlinear equality constraints. It is designed for problems with few degrees of freedom and is moti- vated by the need to use sparse matrix factorizations. The algorithm incorporates a correction vector that approximates the cross term zTWYpy in order to estimate the curvature in both the range and null spaces of the constraints. The algorithm can be considered to be, in some sense, a practical implementation of an algorithm of Coleman and Conn. We give conditions under which local and superlinear convergence is obtained. Key words, successive quadratic programming, reduced Hessian methods, constrained opti- mization, quasi-Newton method, large-scale optimization AMS subject classifications. 65, 49 1. Introduction. We consider the nonlinear optimization problem</p><p>(1) min f(x) xE R, subject to (x) 0, where f R n --R and c R n -, R m are smooth functions. We are particularly interested in the case when the number of variables n is large, and the algorithm we propose, which is a variation of the successive quadratic programming (SQP) method, is designed to be efficient in this case. We assume that the first derivatives of f and c are available, but our algorithm does not require second derivatives.</p><p>The SQP method for solving (1)-( <ref type="formula">2</ref>) generates, at an iterate xk, a search direction dk by solving 1 (3) min g(xk)Td + dTW(xk,)d dER -(4) subject to c(xk) + A(xk)Td O,</p><p>where g denotes the gradient of f, W denotes the Hessian of the Lagrangian function L(x, ,) f(x) + ATc(x), and A denotes the n m matrix of constraint gradients</p><p>(5) A(x) [Vcl (x),..., Vcm(x)].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new iterate is then computed as <ref type="bibr" target="#b5">(6)</ref> xk+l x. + akda, where ck is a steplength parameter chosen so as to reduce the value of the merit function. In this study we use the gl merit function .(x) f(x) + where # is a penalty parameter; see, for example, Conn [13], Han [24], or Fletcher <ref type="bibr" target="#b16">[17]</ref>.</p><p>We could have used other merit functions, but the essential points we wish to convey in this article are not dependent upon the particular choice of the merit function.</p><p>The solution of the quadratic program (3)-( <ref type="formula">4</ref>) can be written in a simple form if we choose a suitable basis of R n to represent the search direction dk. For this purpose, we introduce a nonsingular matrix of dimension n, which we write as <ref type="bibr" target="#b7">(8)</ref> where Yk E R nm and Zk Rn(n-m), and we assume that <ref type="bibr" target="#b8">(9)</ref> dZk O.   (From now on we abbreviate A(xk) as Ak, 9(x) as 9k, etc.) Thus Zk is a basis for the tangent space of the constraints. We can now express dk, the solution to (3)-( <ref type="formula">4</ref>), <ref type="bibr" target="#b9">(10)</ref> dk YkP, + ZkPz, for some vectors Pv e R m and pz Rn-too Due to <ref type="bibr" target="#b8">(9)</ref> the linear constraints (4) become T <ref type="bibr" target="#b10">(11)</ref> c + A k YPv O.   If we assume that Ak has full column rank, then the nonsingularity of [Y Zk] and (9)   imply that the matrix T A k Y is nonsingular, so that Pv is determined by (11)   (12)</p><p>p. --[AYk]-ck.</p><p>Substituting this in <ref type="bibr" target="#b9">(10)</ref>, we have <ref type="bibr" target="#b12">(13)</ref> dk -Yk[AzYk]-ck + Zkpz.   Note that <ref type="bibr" target="#b13">(14)</ref> Yk[AYk] -1 is a right inverse of A T and that the first term in (13) represents a particular solution of the linear equations (4).</p><p>We have thus reduced the size of the SQP subproblem, which can now be expressed exclusively in terms of the variables Pz. Indeed, substituting <ref type="bibr" target="#b9">(10)</ref> into <ref type="bibr" target="#b2">(3)</ref>, considering   Ypv as constant, and ignoring constant terms, we obtain the unconstrained quadratic problem <ref type="bibr" target="#b14">(15)</ref> rain 1_ Tt,TT (Z[gk + Z[WkYkpv)Tpz + -pz ,z, WkZk)pz.</p><p>One point in this derivation requires clarification. In the left-hand side of (19) we have Z[Wk+I, and not ZkTw1wk_t_l We could have used Zk+l in (19), avoiding an inconsistency of indices, but this is not necessary since we will show that using Zk instead of Zk+l in (20) results in algorithms with all the desirable properties. This fact will not be surprising to readers familiar with the analysis of SQP methods; see, for example, Coleman and Conn <ref type="bibr" target="#b10">[11]</ref> or Nocedal and Overton <ref type="bibr" target="#b25">[26]</ref>. In addition, using Zk allows updating of Sk+ and Bk+l prior to creating Z}+ at the new point.</p><p>Let us now consider how to approximate the reduced Hessian matrix ZWkZk Using <ref type="bibr" target="#b5">(6)</ref> and <ref type="bibr" target="#b9">(10)</ref>in <ref type="bibr" target="#b19">(20)</ref>, we obtain [S}+ Zk]pz -cS+ (Y}p) / zT [vxL(xk+ Ak+l) VxL(xk, Ak+l)].</p><p>Since Sk+l approximates ZWk, this suggests the following secant equation for.Bk+l, the quasi-Newton approximation to the reduced Hessian Z kTWkzk" <ref type="bibr" target="#b20">(21)</ref> B}+sk Yk, where Sk is defined by 8k Okpz and Yk by <ref type="bibr" target="#b21">(22)</ref> Yk Z[[VxL(xk+I,Ak+I) VxL(xk, Ak+)] k, with <ref type="bibr" target="#b22">(23)</ref> k Ck Sk+ (YkP). We will update Bk by the BFGS formula (cf. Fletcher <ref type="bibr" target="#b16">[17]</ref>) T T</p><p>Bksks k Bk YkYk <ref type="bibr" target="#b23">(24)</ref> Bk+l Bk- 8 kTBksk y[sk' provided T  Sk Yk is sufficiently positive.</p><p>We highlight a subtle but important point. We have defined two correction terms, Wk and }. Both are approximations to the cross term (zTWY)p. The first term, wk, which is needed to define the null-space step <ref type="bibr" target="#b17">(18)</ref> and thus the new iterate xk+ makes use of the matrix S. The second term, , which is used in <ref type="bibr" target="#b21">(22)</ref> to define the BFGS update of Bk, is computed by using the new Broyden matrix Sk+ and takes into account the steplength k. We see below that it is useful to incorporate the most recent information in k. Note that this requires the Broyden update to be applied before the vector Yk for the BFGS update can be calculated from <ref type="bibr" target="#b21">(22)</ref>.</p><p>The Lagrange multiplier estimates Ak needed in the definition (22) of Yk are defined by <ref type="bibr" target="#b24">(25)</ref> This formula is motivated by the fact that, at a solution x, of (1)-( <ref type="formula">2</ref>), we have [A, Y,] is a right inverse of A,T, -g, A,A, and since Y, T -1</p><p>A, -[y, T A,]-y, T g,.</p><p>Using the same right inverse <ref type="bibr" target="#b13">(14)</ref> in the definitions of p. and Ak allows us a convenient simplification in the formulae presented in the following sections. We stress, however, that other Lagrange multiplier estimates can be used and that the best choice in practice might be the one that involves the least computation or storage.</p><p>We can now outline the sequential quadratic programming method analyzed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM I</head><p>1. Choose constants r] E (0, 1/2) and , with 0 &lt; &lt; T &lt; 1. Set k 1, and choose a starting point xl and an (n m) (nm) symmetric and positive definite starting matrix B1. 2. Evaluate fk, gk, ck, and Ak, and compute Yk and Zk. 3. Compute p by solving the system <ref type="bibr" target="#b25">(26)</ref> T (range space step) (Ak Yk)Pv --ck 4. Compute an approximation wk to (Z[WkYk)p. <ref type="bibr" target="#b4">5</ref>. Choose the damping parameter Ck E (0, 1] and compute Pz from T <ref type="bibr" target="#b26">(27)</ref> Bkpz -[Zk gk -t-kwk].</p><p>(null space step)</p><p>Define the search direction by <ref type="bibr" target="#b27">(28)</ref> dk Ykp, + Zkpz.</p><p>6. Set ck 1, and choose the weight #k of the merit function <ref type="bibr" target="#b6">(7)</ref>. </p><p>(xk + ckdk) &lt;_ (xk) / ?kD, (Xk; dk), where D (xk;dk) is the directional derivative of the merit function in the direction dk. 8. If <ref type="bibr" target="#b28">(29)</ref> is not satisfied, choose a new k ITChy, 7'Ck] and go to (7); otherwise set <ref type="bibr" target="#b29">(30)</ref> x+ xk + (dk. 9. Evaluate fc+,gk+j.,ck+, and Ak+, and compute Y+I and Z+I. If the update criterion (discussed in 3.3) is satisfied, compute B+ by the BFGS formula <ref type="bibr" target="#b23">(24)</ref>; else set Bk+ Bk.  11. Set k := k + 1, and go to (3).</p><p>The algorithm has been left in a very general form, but in the next sections we discuss all its aspects in detail. In 2 we consider the choice of the basis matrices Yk and Zk. In 3 we describe the calculation of the correction terms wk and k, the conditions under which BFGS updating takes place, the choice of the damping parameter Ck, and the procedure for updating the weight ttk in the merit function. In 4 and 5 we .analyze of the local behavior of the algorithm and show that the rate of convergence is at least R-linear. In 6 we present a superlinear convergence result, and some final remarks in 7 conclude the paper.</p><p>We now make a few comments about our notation. Throughout the paper, the vectors p and Pz are computed at xk and could be denoted by p() and pz (), but we normally omit the superscript for simplicity. The symbol I1" I I denotes the 12 vector norm or the corresponding induced matrix norm. When using the 11 or l norms we indicate it explicitly by writing I1" II1 or I1" I1. A solution of problem (1) is denoted by x,, and we deride (34) e x x, and a max{llekll, lick+ill}.</p><p>Here, and for the rest of the paper, VL(x, A) indicates the gradient of the Lagrangian with respect to x only.</p><p>2. The basis matrices. As long as Zk spans the null space of Ak T, and [Yk Zk]   is nonsingular, the choice of Yk and Zk is arbitrary. However, from the viewpoint of numerical stability and robustness of the algorithm, it is desirable to define Yk and Zk to be orthonormal, that is, Z(</p><p>Y(x)Ty(x) I,, V(x) Z(x) =o.</p><p>One way of obtaining these matrices is by forming the QR factorization of A. For large problems, however, computing this QR factorization is often too expensive. Therefore many researchers, including Gabay <ref type="bibr" target="#b17">[18]</ref>, Gilbert [20], Fletcher [17], Murray and Prieto <ref type="bibr" target="#b24">[25]</ref>, and Xie <ref type="bibr" target="#b29">[30]</ref>, consider other, nonorthogonal choices of Y and Z. For example, if we partition x into m basic or dependent variables (which without loss of generality are assumed to be the first m variables) and n-m nonbasic or control variables, we induce the partition</p><formula xml:id="formula_2">(35) A(x) T [C(x) N(x)],</formula><p>where the m m basis matrix C(x) is assumed to be nonsingular. We now define Z(x) and Y(x) to be (36}</p><formula xml:id="formula_3">Z(x)= [ -C(x)-lN(x) ] Y(x)= I I ] I 0</formula><p>When A(x) is large and sparse, a sparse LU decomposition of C(x) can often be computed efficiently, and this approach will be considerably less expensive than the QR factorization of A. Note that from the assumed nonsingularity of C(x) both Y(x) and Z(x) vary smoothly with x, provided the same partition of the variables is maintained. In our implementation of the new algorithm (Biegler, Nocedal, and   Schmid <ref type="bibr" target="#b0">[1]</ref>) we choose Yk and Zk by (36)  There is a price to pay for using nonorthogonal bases. If the matrix C is ill condi- tioned (and this can be difficult to detect), the step computation may be inaccurate.</p><p>Moreover, even if the basis is well conditioned, the range space step Yt:pv can be large, and ignoring the cross term can cause serious difficulties. This phenomenon is illustrated in a two-dimensional example given by Biegler, Nocedal, and Schmid <ref type="bibr" target="#b0">[1]</ref>.</p><p>It is shown in that example that if the cross term ZWkYkpy is ignored, the ratio I[Xk + dk]l/[lXk[[ can be arbitrarily large, even close to the solution. It is also shown that these inefficiencies disappear if the cross term is approximated as suggested in the following sections.</p><p>In the rest of the paper we allow much freedom in the choice of the basis matrices.</p><p>They can be given by (36), can be orthonormal, or can be chosen in other ways. The only restrictions we impose are that AZk 0 is satisfied, that the n n matrix [Yk Zk] is nonsingular and well conditioned, and that this matrix varies smoothly in a neighborhood of the solution.</p><p>3. Further details of the algorithm. In this section we consider how to cal- culate approximations wk and k to T (Z k WkYt:)py to be used in the determination of the search direction pz and in updating B, respectively. We also discuss when to skip the BFGS update of the reduced Hessian approximation, as well as the selection of the damping factor Ck and the penalty parameter #.</p><p>To calculate approximations to (zTWY)p,, we propose two approaches. First, we consider a finite difference approximation to zTwk along the direction YkP,. While this approach requires additional evaluations of reduced gradients at each iteration, it gives rise to a very good step. The second, more economical approach defines wk and wk in terms of a Broyden approximation to T Z k Wt:, as discussed in 1, and requires no additional function or gradient evaluations. Our algorithm will normally use this second approach, but as we later see, it is sometimes necessary to use finite differences.</p><p>3.1. Calculating wk and through finite differences. We first calculate the range space step pv at xk through <ref type="bibr" target="#b25">(26)</ref>. Next we compute the reduced gradient of the Lagrangian at x + YkP, and define (37) wk Z'[[VL(xk -t-Yt:p,, )k) VL(xt:, Ak)].</p><p>After the step to the new iterate xk+l has been taken, we define (38) Z[[VL(x + okYp,,,k+)-VL(x, +)], which requires a new evaluation of gradients if a 1. Thus, up to three evaluations of the objective function gradient may be required at each iteration.</p><p>We note that this finite-difference approach is very similar to the algorithm of Coleman and Corm <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Starting at a point z, the Coleman-Corm algorithm (with steplength ak 1) is given by (39) Zpz -Z (zk)B-Z(zk)Tg(zk),  Let us now consider Algorithm I, and to better illustrate its similarity with the Cole- man and Conn method, let us assume that instead of (37), wk is defined by Wk Z(xk + Ykpy)Tg(xk + YkPv) Z(xk)Tg(xk), which differs from (37) by terms of order O([[pyII ). Then Algorithm I with ak 1 and Ck 1 is given by (42)</p><formula xml:id="formula_4">YkPY -Y(xk [A(x)TY(x ]-1c(xk), (<label>43</label></formula><formula xml:id="formula_5">) Z,p + -Z(x )B-l[Z(xk + Ykp )Tg(xk + YkPv)],</formula><p>(44)</p><formula xml:id="formula_6">Xk+l xk + YkPY + ZkPz.</formula><p>The similarity between the two approaches is apparent in Fig. <ref type="figure" target="#fig_3">1</ref>, especially if we consider the intermediate points in the Coleman-Conn iteration to be the starting and final points, respectively. In the Coleman-Conn algorithm, the approximation Bk to reduced Hessian Z[WkZ is obtained by moving along the null space direction ZkPz, and making a new evalua- tion of the function and constraint gradients. To be more precise, Coleman   --Xk] and apply a quasi-Newton formula to update Ba. Algorithm I, using finite differences, amounts essentially to the same thing. To see this, note that if Formula (38) is used in (33), then Yk Z[[VL(xk+I, Ak+I) VL(xk -t- which represents a difference in reduced gradients of the Lagrangian along the null space direction Zkpz. Byrd [4] and Gilbert [19] showed that the sequence {zk + Zkpz} (but not the se- quence {zk }) generated by the Coleman-Conn method converges 1-step Q-superlinearly.</p><p>If Algorithm I always computed the correction terms wk and k byfinite differences, its cost and convergence behavior would be similar to those of the Coleman-Conn method (except when Ck 1, which requires one extra gradient evaluation for Algo- rithm I). However, we are often able to avoid the use of finite differences and instead use the more economical approach discussed next.</p><p>3.2. Using Broyden's method to compute wk and k. We can approximate the rectangular mat'rix Z[Wk by a matrix Sk updated by Broyden's method, and then compute wk and by post-multiplying this matrix by YkP, or by a multiple of this vector. As discussed in 1, it is reasonable to impose the secant equation (20) on this Broyden approximation, which can therefore be updated by the formula (cf.  We now define <ref type="bibr">(48)</ref> wk SkYkP. and k oSk+ Ykp.. It should be noted that this approach requires the storage of the (nm) n matrix Sk, in addition to the reduced Hessian approximation, Bk. For problems where n-m is small, this expense is far less than the storage of a full Hessian approximation to Wk. On the other hand, if n-m is not very small, it may be preferable to use a limited-memory implementation of Broyden's method. Here the matrices S are represented implicitly, using, for example, the compact representation described in Byrd, Nocedal, and Schnabel <ref type="bibr" target="#b6">[7]</ref>. The advantage of the limited memory implementation is that it requires the storage of only a few n-vectors to represent S.</p><p>Since there is no guarantee that the Broyden approximations Sk will remain bounded, we need to safeguard them. At the beginning of the algorithm we choose a positive constant F and define The correction k will be safeguarded in a different way. We choose a sequence of positive numbers ('k} such that ]=l/k &lt; c, and we set if</p><formula xml:id="formula_7">I I &lt; (50) k :---- k llPYII otherwise.</formula><p>1111</p><p>As the iterates converge to the solution, pv --. 0, so that from (48) and from the boundedness of Yk we see that these safeguards allow the Broyden updates Sk to become unbounded, but in a controlled manner. We show in 4 and 5 that with the safeguards (49) and (50) Algorithm I is locally and R-linearly convergent and that this implies that the Broyden updates Sk do, in fact, remain bounded, so that the safeguards become inactive asymptotically.</p><p>Our Broyden approximation to the correction terms wk and k was motivated by recent work of Gurwitz <ref type="bibr" target="#b21">[22]</ref>. She approximates Z[WkZk by the BFGS formula with Z[[X +l</p><p>Yk Z[[VL(xk+I, )k+l) VL(xk, k+l)] and approximates Z[WY by a matrix Dk using Broyden's formula (45) with k Z[[VL(xt:+I, )t:+l) VL(xt:, At:+1)] Bapz.</p><p>Since the updates may not always be defined, Gurwitz proposes to sometimes skip the update of Bt: or Dt:. She shows 1-step Q-superlinear convergence if and only if one of the updates is taken at each iteration, but this cannot be guaranteed. The analysis of this paper shows that it is preferable to update an approximation to Z[Wt:, as in Algorithm I, instead of an approximation to Z[WkYt:, as proposed by Gurwitz, since our approach leads to 1-step superlinear convergence in all cases.</p><p>A related method was derived by Coleman and Fenyes <ref type="bibr" target="#b11">[12]</ref>. Their lower parti- tion BFGS formula (LPB) simultaneously updates approximations to Z[WkZt: and Z[Wt:Yt:, by means of a new variational problem. The resulting updating formula requires the solution of a cubic equation, and its roots can correspond to cases where updates should be avoided (e.g. T  st: yt: &lt;_ 0). The drawback of this approach is that choosing the correct root is not always easy.</p><p>An earlier proposal by Tagliaferro <ref type="bibr" target="#b27">[28]</ref> consists of approximating the matrices Z[Wt:Zt: and T Zt: WkYt: using the Powell-symmetric-Broyden (PSB) update formula and Broyden's method, respectively. One disadvantage of this approach is that the matrices generated by this updating procedure may become very ill conditioned.</p><p>3.3. Update criterion. It is well known that the BFGS update ( <ref type="formula">24</ref>) is well defined only if the curvature condition T st: yt: &gt; 0 is satisfied. This condition can always be enforced in the unconstrained case by performing an appropriate line search; see, for example, Fletcher <ref type="bibr" target="#b16">[17]</ref>. When constraints are present, however, the curvature condition T st: yt: &gt; 0 can be difficult to obtain, even near the solution. To show this, we first note from (33), (28), and (32) and from the mean value theorem that o =_ <ref type="bibr">(51)</ref> where we have defined f0 2</p><p>VxxL(xk + Takdk, k-l-1)dT" Thus (53) 8k Yk S 8k Near the solution, the first term on the right-hand side will be positive, since Z[ITVkZk can be assumed positive definite. Nevertheless, the last two terms are of T uncertain sign and can make s k Yk negative. Several reduced Hessian methods in the literature set k equal to zero for all k, and update Bk only if py is small enough compared with sk that the first term in the right-hand side of (53) dominates the second term (see Nocedal and Overton <ref type="bibr" target="#b25">[26]</ref>, Gurwitz and Overton <ref type="bibr" target="#b22">[23]</ref>, and Xie <ref type="bibr" target="#b28">[29]</ref>).</p><p>Skipping the BFGS update may appea to be a crude heuristic, but we argue that it gives rise to a sound algorithm. First of all, the last two terms in (53) normally converge to zero faster than the first term, so that the right-hand side of (53) will often be positive near the solution and BFGS updating will take place frequently.</p><p>Furthermore, if the right-hand side of (53) is negative, the range space step YkPY is relatively large, resulting in sufficient progress towards the solution. These arguments will be made more precise in 5.</p><p>We therefore opt for skipping the BFGS update, when necessary, and we now present a strategy for deciding when to do so. Recall that ak, defined by (34), con- verges to zero if the iterates converge to x.. Update Criterion I. Choose a constant d &gt; 0 and a sequence of positive numbers {/k} such that E=lk &lt; c (this is the same sequence {/k} that was used in (50)).</p><p>T If k is computed by Broyden's method, and if both s k Yk &gt; 0 and (54) IlPvlI-&lt; 7llPzll hold at iteration k, then update the matrix Bk by means of the BFGS formula (24)  with sk and yk given by (32) and (33). Otherwise, set If k is computed by finite differences, and if both syk &gt; 0 and (55) hold at iteration k, then update the matrix Bk by means of the BFGS formula <ref type="bibr" target="#b23">(24)</ref> with Sk and Yk given by (32) and (33). Otherwise, set Bk+l Bk.</p><p>Note that 6k requires knowledge of the solution vector x. and is therefore not computable. However, later we see that ak can be replaced by any quantity that is of the same order as the error ek, for example, the optimality conditions (llZ[gkll + Ilckll).</p><p>Nevertheless, for convenience we will leave ak in (55).</p><p>We now closely consider the properties of the BFGS matrices Bk when Update Criterion I is used..Let us define sBksk (56) cos 0k ilSk IIBksk I1' which, as we will see, is a measure of the goodness of the null space step Zkpz. We begin by restating a theorem from Byrd and Nocedal [5] regarding the behavior of cos Ok when the matrix Bk is updated by the BFGS formula.</p><p>THEOREM 3.1. Let {Bk} be generated by the BFGS formula (24) where, for all k &gt;_ l, sk O and T &gt; m &gt; 0, (58) T8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;_M. Yk k</head><p>Then, there exist constants 1, 2, 3 &gt; 0 such that, for any k &gt;_ 1, the relations (59) cos Oj 1, (60) /2 _&lt; IIBjsjll _ 3 hold for at least [1/2k values of j e [1, k]. This theorem refers to the iterates for which BFGS updating takes place; but since, for the other iterates, B+ B, the theorem characterizes the whole sequence of matrices (Bk}. Theorem 3.1 states that, if T st: Yk is always sufficiently positive, in the sense that conditions (57) and (58) are satisfied, then at least half of the iterates at which updating takes place are such that cos Oy is bounded away from zero and Bjsi O(llsill). Since it will be useful to refer easily to these iterates, we make the following definition. DEFINITION 3.2. We define J to be the set of iterates for which (59) and (60) hold. We call J the set of "good iterates" and define Jk J Note that if the matrices Bk are updated only a finite number of times, their condition number is bounded, and (59)-(60) are satisfied for all k. Thus in this case all iterates are good iterates.</p><p>We now study the case when BFGS updating takes place an infinite number of times. Let us assume that all functions under consideration are smooth and bounded.</p><p>If at a solution point x, the reduced Hessian zT,w,z, is positive definite, then for all xk in a neighborhood of x, the smallest eigenvalue of ZITVkZk is bounded away from zero (l is defined in (52)). We now show that in such a neighborhood Update Criterion I implies (57)-(58).</p><p>Let us first consider the case when k is computed by Broyden's method. Using (53), (54), and (50), and since "Yk converges to zero, we have T (61) &gt;_ mll 2,   for some positive constants C, m. Also, from (51), (54), and (50) we hve that Ily ll &lt; O(ll ll) + o(7 1ts 11) + (62) -&lt; O(llkll).</p><p>We thus see from (61)-(62) that there is a constant M such that for all k for which updating takes place, &lt;_M, T 8 Yk k which together with (61) shows that (57)-(58) hold when Broyden's method is used.</p><p>If k is computed by the finite-difference formula (38), we see from (33) and the mean value theorem that there is a matrix lfidk such that Yk Z[[VL(xk+,)+)-VL(x + =_ z[w Z s .</p><p>Reasoning as before we see that (61) and (62) also hold in this case, and that (57)-( <ref type="formula">58</ref>)</p><p>are satisfied in the case when finite differences are used. We have therefore established the following result.</p><p>LEMMA 3.3. In a neighborhood of a solution point x., and whenever BFGS T updating takes plac'e as stipulated by Update Criterion I, s k Yk is sufficiently positive in the sense that (57)-(58) hold. 3.4. Choosing #k and Ck. We will now see that by appropriately choosing the penalty parameter #k and the damping parameter Ck for Wk, the search direction gen- erated by Algorithm I is always a descent direction for the merit function. Moreover, for the good iterates J, it is a direction of strong descent.</p><p>Since dk satisfies the linearized constraint (11), it is easy to show (see Eq. (2.24) of Byrd and Nocedal <ref type="bibr" target="#b5">[6]</ref>) that the directional derivative of the/71 merit function in the direction dk is given by (63)</p><p>The fact that the same right inverse of A is used in (26) and (31) implies that (64) T g Yp c.</p><p>Recalling the decomposition (28) and using (64), we obtain D (xk da T (65) T (Zk gk + Ckwk)Tpz Ck T WkPz --,kllckll + Ck. cos0 IIZ/a + 1 Ilpzll" T Recalling the inequality Ak ck llAklllckl, and using (67)in (65), we obtain, for all k, (68) D,(xk;dk) T W r z a + 11 llpzll cos0 z (, I111)1111.</p><p>Note also from (66) and (32) that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I111</head><p>Ilpzll</p><p>We now concentrate on the good iterates J, as given in Definition 3.2. If j J, we have from (69) and (60) that Using this and (59) in (68), we obtain, for j e J, 1 zj gy + Cjwjll cos0y ; j p (y -IIyll)llyll Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where we have dropped the nonpositive term -y cos 0j Ilwy 2/3. Since we can assume that 3 &gt; 1 (it is defined as an upper bound in (60)), we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. (xj dj</head><p>This means that if (71) and (72) hold, then for the good iterates j E J, the search direction dj is a strong direction of descent for the gl merit function in the sense that the first-order reduction is proportional to the Karush-Kuhn-Tucker (KKT) error.</p><p>We will choose Ck so that (71) holds for all iterations. To show how to do this, we note from (27) that T Wk, Pz -BIzk gk kB so that, for j k, (71) can be written as 74)</p><p>[2cose Ig 'z l-l-w[B T Zk g + kwB-[Wk] &lt;_ plIckl [1.   Clearly this condition is satisfied for a sufficiently small and positive value of k. Specifically, at the beginning of the algorithm we choose a constant p &gt; 0 and, at every iteration k, define</p><p>k min{ 1, k }, where k is the largest value that satisfies (74) as an equality.</p><p>The penalty prameter #k must satisfy (72), so we define it at every iteration of the algorithm by (76</p><formula xml:id="formula_9">) #k []Ak[[ + 3p otherwise.</formula><p>The damping factor Ck and the updating formula for the penalty parameter # have been defined so as to give strong descent for the good iterates J. We now show that they ensure that the search direction is also a direction of descent (but not necessarily of strong descent) for the other iterates, k J. Since (71) holds for all iterations by our choice of (k, we have in particular The directional derivative is thus nonpositive. Furthermore, since w 0 whenever ck 0 (regardless of whether wk is obtained by finite differences or through Broyden's method), it is easy to show that this directional derivative can be zero only at a stationary point of problem (1)-( <ref type="formula">2</ref>). Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 3.5. The algorithm. We can now give a complete description of the algorithm that incorporates all the ideas discussed so far and that specifies the only remaining question, namely, when to apply finite differences and when to use Broyden's method to approximate the cross term. The idea is to consider the relative sizes of p. and pz.</p><p>Update Criterion I generates the three regions R1, R2, and R3 illustrated in Fig. <ref type="figure" target="#fig_9">2</ref>.</p><p>The algorithm starts by computing wk through Broyden's method and by calculating p and pz. If the search direction is in R1 or R3, we proceed. Otherwise we recompute wk by finite differences, use this value to recompute pz, and proceed. The reason for applying finite differences in this fashion is that in the middle region R2 Broyden's method is not good enough, nor is the convergence sufficiently tangential, to give a superlinear step. Therefore we must resort to finite differences to obtain a good estimate of wk. The motivation behind this strategy will become clearer when we study the rate of convergence of the algorithm in 6. Note from Updating Criterion I that the BFGS update of Bk is skipped if the search direction is in R3. A precise description of the algorithm follows.</p><p>ALGORITHM II 1. Choose constants v] E (0, 1/2), p &gt; 0 and T,T' with 0 &lt; T &lt; T' &lt; 1, and positive constants F and fd for conditions (49) and (55), respectively. For conditions (50) and (54), select a summable sequence of positive numbers {}. Set k :-1, and choose a starting point xl, an initial value #1 for the penalty parameter, an (nm) (nm) symmetric and positive definite starting matrix B, and an (n-m) n starting matrix S. 2. Evaluate fk, gk, Ck, and Ak, and compute Yk and Z. (range space step) 4. Calculate wk using Broyden's method, from (48) and (49). 5. Choose the damping parameter Ck from (74) and (75), and compute pz from (null space step) Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 6. If (55) is satisfied and (54) is not satisfied, set findiff true and recompute wk from (37). (In practice we replace ak by IIZ[gkll-t-IIcll in (55).)</p><p>7. If findiff true, use this new value of w} to choose the damping parameter Ck from (74) and (75), and recompute pz from (79).  k+ -[YlA+1-1YkT+lgk+l, and update #k so as to satisfy (76). 13. Update Sk+ using (45) to (47). If findiff false, calculate k by Broyden's method through (48) and (50); otherwise calculate k by (38).</p><p>14. If (s'yk _ 0) or if (findiff--true and (55) is not satisfied) or if (findiff--false and (54) is not satisfied), set B+ B. Else, compute (84) s kPz, (85</p><formula xml:id="formula_10">) Yk Z[[VL(x+,A+i) VL(xk,+)] k,</formula><p>and compute Bk+l by the BFGS formula (24).</p><p>15. Set k := k + 1, and go to 3.</p><p>We mentioned in 3.1 that, when using finite differences, there are various ways of defining w and k, but for concreteness we now assume in steps 6 and 13 that they are computed by (37) and (38), respectively. We should also point out that the curves in Fig. <ref type="figure" target="#fig_9">2</ref> may intersect, creating a fourth region, and in practice we should stipulate a new set of conditions in this region. We discuss these conditions in another paper that considers the implementation of the algorithm (Biegler, Nocedal, and Schmid <ref type="bibr" target="#b0">[1]</ref>)o</p><p>In the next sections we present several convergence results for Algorithm II. The analysis, which does not assume that the BFGS matrices Bk or the Broyden matrices Sk are bounded, is based on the results of Byrd and Nocedal [6], who have studied the convergence of the Coleman-Conn updating algorithm. We also make use of some results of Xie <ref type="bibr" target="#b28">[29]</ref>, who has analyzed the algorithm proposed by Nocedal and Overton [26] using nonorthogonal bases Y and Z. The main difference between this paper and that of Xie stems from our use of the correction terms Wk and k, which are not employed in his method.</p><p>4. Semilocal behavior of the algorithm. We first show that the merit func- tion decreases significantly at the good iterates J and that this gives the algorithm a weak convergence property. To establish the results of this section, we make the following assumptions. Assumption 4.1. The sequence {xk} generated by Algorithm II is contained in a convex set D with the following properties:</p><p>(I) The functions f R n --+ R and c R n --R m and their first and second derivatives are uniformly bounded in norm over D.</p><p>(II) The matrix A(x) has full column rank for all x E D, and there exist constants o and flo such that (III) For all k _&gt; 1 for which Bk is updated, (57) and (58) hold. (IV) The correction term wk is chosen so that there is a constant n &gt; 0 such that for all k, (87) I1,,11 &lt;_ ,11akll 1/2.</p><p>Note that Condition (I) is rather strong, since it would often be satisfied only if D is bounded, and it is far from certain that the iterates will remain in a bounded set. Nevertheless, the convergence result of this section can be combined with the local analysis of 5 to give a satisfactory semiglobal result. Condition (II) requires that the basis matrices Y and Z be chosen carefully, and is important to obtain good behavior in practice. Note that (86) and (78) imply that (88) IIY,p,,,II Condition (III) is justified by Lemma 3.1. Condition (III) and Theorem 3.1 ensure that at least half of the iterates at which BFGS updating takes place are good iterates.</p><p>We have left some freedom in the choice of w since (87) suffices for the analysis of this section. Relation (87) holds for the finite-difference approach, since (37) implies that wk O(YkPv) and since Condition (I) ensures that {llcll} is uniformly bounded (see (121)). Furthermore, the safeguard (49) and (88)immediately imply that (87)is satisfied when the Broyden approximation is used.</p><p>The following result concerns the good iterates J, as given in Definition 3.2.</p><p>LEMMA 4.1. If. Assumptions 4.1 hold and if #j # is constant for all sufficiently large j, then there is a positive constant " such that for all large j J, where b2 min(fll/fl3, p). Note that the line search enforces the Armijo condition (81), (91) ,j (xj) ,j (xj+l) &gt;_ -ajD,(xj; dy).</p><p>It is then clear from (90) that (89) holds, provided the aj, j E J, can be bounded from below. Suppose that aj &lt; 1, which means that (91) failed for a steplength &amp;:</p><p>(92)</p><p>Cm (xj + ad) Cm (xj) &gt; vaD m (xj; dj), where</p><p>(see step 10 of Algorithm II). On the other hand, expanding to second order, we have (94)</p><p>Cm(xj + &amp;dj) Cm (xj) &lt; &amp;DCm(xj;dj) + &amp;2bllldjll2</p><p>where bl depends on #j. Combining (92) and (94), we have (95) 07-1)&amp;DCm (xj; dj) &lt; Next we show that, for j E J,</p><p>for some constant b3. To do this, we make repeated use of the following elementary result"</p><p>a,b &gt; O = a 2 + 2ab + b2&lt;_3a2+3b2.</p><p>Using (80), ( <ref type="formula" target="#formula_13">97</ref> Also by (70), (97), and (87) and noting that I1" &lt; I1" IIx ,we have that for j e J since j &lt;_ This relation and (93) imply that the steplengths cj are bounded away from zero for all j E J. Since by assumption #j # for all large j, we conclude that (89) holds with / rib2 min{1, (1 vl)Tb2/(blb3)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D</head><p>It is now easy to show that the penalty parameter settles down and that the set of iterates is not bounded away from stationary points of the problem. Proof. First note that by Assumptiops 4.1 (I)-(II) and (83) that {1111} is bounded. Therefore, since the procedure (76) increases #k by at least p whenever it changes the penalty parameter, it follows that there are an index k0 and a value # such that for all k &gt; k0, #k # &gt;_ IIkll + 2p.</p><p>If BFGS updating is performed an infinite number of times, by Assumption 4.1 (III) and Theorem 3.1 there is an infinite set J of good iterates, and by Lemma 4.1 and the fact that the Armijo condition (81) forces Ct,(Xk) to decrease at each iterate, we have that for k &gt; k0, By Assumption 4.1(I) (x) is bounded below for all x D, so the lt sum is finite, and thus the term i.nside the square brackets converges to zero. Therefore (100) lim (11 T z gll + IIcll)= 0. j If BFGS updating is performed a finite number of times, then, as discussed after Definition 3.1, all iterates are good iterates, and in this ce we obtain the stronger result lim ([i T k--cx 5. Local convergence. In this section we show that if x, is a local minimizer that satisfies the second-order optimality conditions, and if the penalty parameter #k is chosen large enough, then x, is a point of attraction for the sequence of iterates (Xk} generated by Algorithm II. To prove this result, we make the following assumptions.</p><p>In what follows, G denotes the reduced Hessian of the Lagrangian function, namely,</p><p>G Z T VL(x,)Z.  The functions f R n R and c: R n R m are twice continuously differ- entiable in a neighborhood of x,, and their Hessians are Lipschitz continuous in a neighborhood of x,.</p><p>2. The matrix A(x,) has full column rank. This implies that there exists a vector</p><p>,, E pm such that VL(x,, A,) g(x,) + A(x,)A, 0. 3. For all q In-m, q 7 O, we have qTG,q &gt; O.</p><p>4. There exist constants '70,/30, and '7c such that, for all x in a neighborhood of IIY(x)[A(x)Ty(x)]-II o, IIZ(x)ll o, lilY(x) Z(x)]-ll &lt; %. 5. Z(x) and A(x) are Lipschitz continuous in a neighborhood of x,. That is, there exist constants '7z and '7 such that IIX(x)-X(z)ll ,llx-zll, IIZ(x)-z(z)ll zllx-zll, for all x, z near x,.</p><p>Note that conditions 1, 3, and 5 imply that for all (x,/k) sufficiently near (x,, A,), and for all q Rn-m (106) mllqll 2 _&lt; qTG(x,,)q &lt;_ MIIqll , for some positive constants m, M. We also note that Assumptions 5.1 ensure that the conditions (57)-(58) required by Theorem 3.1 hold whenever BFGS updating takes place in a neighborhood of x,, as shown in Lemma 5.1. Therefore Theorem 3.1 can be applied in the convergence analysis.</p><p>The following two lemmas are proved by Xie <ref type="bibr" target="#b28">[29]</ref> for very general choices of Y and Z. His result generalizes Lemmas 4.1 and 4.2 of Byrd and Nocedal <ref type="bibr" target="#b5">[6]</ref>; see also Powell <ref type="bibr" target="#b26">[27]</ref>.</p><p>LEMMA 5.1. If Assumptions 5.1 hold, then for all x sufficiently near x, (lO7) O'lllX-x, II(x)ll + IIZ(x)Tg(x)ll llx-x.ll, for some positive constants "71, "72. This result states that, near x,, the quantities c(x) and Z(x)Tg(x) may be re- garded as a measure of the error at x. The next lemma states that, for a large enough weight, the merit function may also be regarded as a measure of the error.</p><p>LEMMA 5.2. Suppose that Assumptions 5.1 hold at x,. Then for any # &gt; II;,11o there exist constants "73 &gt; 0 and "74 &gt; O, such that for all x sufficiently near x, (108</p><formula xml:id="formula_15">) 3IIx-x,[[ 2 &lt;_ ,(x)-O,(x,) 4 [[[Z(x)Tg(x)l] 2 + [la(x)[[1].</formula><p>Note that the left inequality in (108) implies that, for a sufficiently large value of the penalty parameter, the merit function will have a strong local minimizer at x,. We L. BEIGLER, J. NOCEDAL, AND C. SCHMID now use the descent property of Algorithm II to show convergence of the algorithm.</p><p>However, because of the nonconvexity of the problem, the line search could generate a step that decreases the merit function but that takes us away from the neighborhood of x,. To rule this out, we make the following assumption. Assumption 5.2. The line search has the property that, for all large k, ((1 0)xk + 0Xk+l) &lt;_ ,(Xk) for all E [0, 1]. In other words, Xk+l is in the connected component of the level set {x: ,(x) _&lt; ,(xk)} that contains xk.</p><p>There is no practical line search algorithm that can guarantee this condition, but it is likely to hold close to x,. Assumption 5.2 is made by Byrd, Nocedal, and Yuan <ref type="bibr" target="#b7">[8]</ref> when analyzing the convergence of variable metric methods for unconstrained problems, as well as by Byrd and Nocedal <ref type="bibr" target="#b5">[6]</ref> in the analysis of Coleman-Conn updates   for equality constrained optimization.</p><p>LEMMA 5.3. Suppose that the iterates generated by Algorithm II (with a line search satisfying Assumption 5.2) are contained in a convex region D satisfying As- sumptions 4.1. If an iterate Xko is suJficiently close to a solution point x, that satisfies Assumptions 5.1, and if the weight #ko is large enough, then the sequence of iterates converges to x,.</p><p>Proof. By Assumptions 4.1 (I)-(II) and (83) we know that {IIAII} is bounded.</p><p>Therefore the procedure (76) ensures that the weights #k are constant, say # # for all large k. Moreover, if an iterate gets sufficiently close to x,, we know by <ref type="bibr">(76)</ref> and by the continuity of A that # &gt; IIA, II. For such value of #, Lemma 5.2 implies that the merit function has a strict local minimizer at x,. Now suppose that once the penalty parameter has settled, and for a given e &gt; 0, there is an iterate Xko such that .40</p><p>where "0 is such that I1" II1 -&lt; 011" II. Assumption 5.2 shows that for any k &gt;_ k0, xk is in the connected component of the level set of Xko that contains Xko, and we can assume that e is small enough that Lemmas 5.1 and 5.2 hold in this level set. Thus since dp(xk) &lt;_ ,(xko) for k &gt;_ k0, and since we can assume that IIZkTogoll _&lt; 1, we have from Lemmas 5.1 and 5.2, for any k _&gt; k0 &lt; % (, (x 0) , (x,))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[llz + I1 ] Z og oll</head><p>This implies that the whole sequence of iterates remains in a neighborhood of radius e of x,. If e is small enough, we conclude by (108), by the monotonicity of and by Theorem 4.2 that the iterates converge to x,.</p><p>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>The assumptions of this lemma, which is modeled after a result in Xie <ref type="bibr" target="#b28">[29]</ref>, are restrictive especially the assumption on the penalty parameter. One can relax these assumptions and obtain a stronger result, such as Theorem 4.3 in Byrd and Nocedal <ref type="bibr" target="#b5">[6]</ref>, but the proof would be more complex and is not particularly relevant to Algorithm II since it is based only on the properties of the merit function. Therefore, instead of further analyzing the local convergence properties of the new algorithm, we will study its rate of convergence.</p><p>5.1. R-linear convergence. For the rest of the paper we assume that the line search strategy satisfies Assumption 5.2. We also assume that the iterates generated by Algorithm II converge to a point x, at which Assumptions 5.1 hold, which implies that for all large k, #k P &gt; , . The analysis that follows depends on how often BFGS updating is applied. To make this concept precise, we define U to be the set of iterates at which BFGS updating takes place, (</p><p>U {k" Bk+ BFGS(Bk, Sk, Yk)}, and let <ref type="bibr">(110)</ref> Uk U {1,2,...,k}.</p><p>The number of elemems in Uk will be denoted by Uk.</p><p>THEOREM 5.4. Suppose that the iterates {x} generated by Algorithm II converge to a point x, that atisfies Assumptions 5.1. Then for any k U and any j k for some constants C &gt; 0 and 0 r &lt; 1.</p><p>Proof. Using (89) and (108), we have for e J, (112) Let us define r (1 -7,/74)/4. Then for J (113) We know that the merit function decreases at each step, and by (108) we have, for jkandkU, We continue in this fhion, bounding the right-hand side by terms involving earlier iterates, but using now (113) for all good iterates. Since by Theorem 3.1 at let half we of the iterates at which updating tkes place are good iterates (i.e., ]Jk[ have _1 _1 N [2 5 (,(Xl) .(x,))lr TM This result implies that if {IUkl/k} is bounded away from zero, then Algorithm II is R-linearly convergent. However, BFGS updating could take place only a finite number of times, in which case this ratio would converge to zero. It is also possible for BFGS updating to take place an infinite number of times, but every time less often, in such a way that IUkl/k --O. We therefore need to examine the iteration more closely.</p><p>We make use of the matrix function defined by (114) (B) tr(B) -ln(det(B)),</p><p>where tr denotes the trace, and det the determinant. It can be shown that (115)</p><p>In cond(B)</p><p>for any positive definite matrix B (Byrd and Nocedal <ref type="bibr" target="#b4">[5]</ref>). We also make use of the weighted quantities</p><formula xml:id="formula_17">y-l/2 (116) k GI/2yk, Sk .. Sk,<label>(117)</label></formula><p>/k G{I/2BkG{1/2, ( </p><p>This expression characterizes the behavior of the BFGS matrices Bk and is crucial to the analysis of this section. Before we can make use of this relation, however, we need to consider the accuracy of the correction terms. We begin by showing that when finite differences are used to estimate Wk and k, these are accurate to second order.</p><p>LEMMA 5.5. If at the iterate xk, the corrections Wk and k are computed by the finite-difference formulae (37)-(38), and if xk is sufficiently close to a solution point x. that satisfies Assumptions 5.1, then ( Proof. Recalling that VL(x, A) g(x) + A(x)A, we have from (37) that Let us sume that z is in the neighborhood of z, where (102)-(10g) hold. Then I ,1 o(llell) 0(), where k is defined by (a4). Therefore the lt term in (124) is O(IIPII), which proves (121). Also, a simple computation shows that Using these facts in (124) yields the desired result (122). To prove (12a), we note only that N 1 and re,on in the same manner. Next we show that the condition number of the matrices Bk is bounded and that, in the limit, at the iterates U at which BGS updating takes place, the matrices Bk are accurate approximations of the reduced Hessian of the Lagrangian. TOaM g.6. Sppose that the iterates {zk} 9eeerated b Algorithm II converge to sol,floe point z, that sti4es Assumptions .1. Thee bounded, d for all k U Pro@ We only consider iterates k for which BFGS updating of Bk takes place. We have from (8g), (82), (80), (g2), and (84) z2[(z+, +1) (z, +1)1 z. w.)gp + ( z. .gp ).</p><p>Since Nk can be computed by Broyden's method or by finite differences, we need consider these two cases separately.</p><p>Part I. Let us first sume that Nk is determined by Broyden's method. A simple computation shows that IIz zw, o(), and from (g0) we have that O(llpll/). Using this and Assumptions g.1 in (127), we have T Recalling (116) and noting that -T- T 8</p><p>Yk st: Yk k we have Tk k sTk (z[ikZk G,)sk + ]]k] 2 + (ak + 1 + 1/7)O(a]]pv]])]]k, since ]]gk] and ]Sk]] are of the same order. Therefore </p><formula xml:id="formula_21">+ (ak + 1 + 1/k)20 (]akPv]]2)</formula><p>At this point we invoke the update criterion and note from (54) that, if BFGS updating of Bk takes place at iteration k, then ]akPv]] 7]]sk], where {Tk} is summable. Using this, the sumption that ak converges to zero, and (129), we see that for large k Yk 8k <ref type="bibr" target="#b10">(11)</ref> ]]</p><p>+ o( + z), and using (130) Therefore + o( + ,).</p><p>(132)</p><formula xml:id="formula_22">11112 IIkl12 I]kll2 1 + O(ak + 3'k).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~T ~T</head><p>We now consider (+) given by (120). A simple expansion shows that for large k, ln(1 + O(ak + k)) O(ak + k). Using this, (131), and (132), we have (133) (k+) (k) + O(ak + k) + lncos2 0k + [ l . Ok +ln 0.]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COS2 k Cos2</head><p>Note that for x 0 the function 1 x + In x is nonpositive, implying that the term in square brackets is nonpositive and that In cos 2 Ok is also nonpositive. We can therefore delete these terms to obtain (134)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(+) _&lt; (h)+ o(o +).</head><p>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Before proceeding further we show that a similar expression holds when finite differ- ences are used.</p><p>Part II. Let us now consider the iterates k for which updating takes place and for which k is computed by finite differences. In this case (55) holds. Again we begin by considering (127), z[z + (z-Z, W,)Ykpv + (akZW,Ykpv --).</p><p>Using (123) the last term is of order ak(ak]]Pvl), and so is the second term. Thus</p><formula xml:id="formula_23">z[z + o() () (z[Wz a,) + a, + o().</formula><p>Noting once more that [k y[sk and recalling the definition (116), we have --(z[z a,) + + o(,,i). B takes place at iteration k then IlPvll &lt; /fdllPzll,ak Using this, (136), and the fact that ak converges to zero, we see that for large k We now consider (Bk+l) given by (120). Noting that ln(1 + O(a k )) t(a k for all large k, we see t'hat if updating takes place at iteration k We now combine the results of Parts I and II of this proof. Let us subdivide the set of iterates U for which BFGS updating takes place into two subsets: U' corresponds to the iterates in which is computed by Broyden's method, and U" to the iterates in which finite differences are used. We also define U U {1,2,...,k} and v" Summing over the set of iterates in Uk, using (134) and (140), and noting that Bj+ Bj for j Uk, we have ( <ref type="formula">14</ref>)</p><formula xml:id="formula_24">&lt; jeU jeU' U'</formula><p>for some constants C, C2, Ca. Since 0 r 1 and U'] Uy we have, from (111) and since {/_} is summable, we conclude from (141) that {(/)k)} is bounded above.</p><p>By (114) (Bk) }"il(li --ln/i), where li are the eigenvalues of/)k, and it is easy to see that this implies that both ]]Bk ]1 and IIB -1 I I are bounded.</p><p>To prove (126), we sum relations (133) and (139), recalling that ak, 7k and a/2 ~2 k + 1. cos 0</p><p>It is clear from (142) that the last term converges to 0 for k E U, which implies that (126) holds.</p><p>[] This result immediately implies that the iterates are R-linearly convergent, re- gardless of how often updating takes place. THEOREM 5.7. Suppose that the iterates {xk} generated by Algorithm II converge to a solution point x, that satisfies Assumptions 5.1 and the fact that IUI -oc. Then the rate of convergence is at least R-linear.</p><p>Proof. Theorem 5.6 implies that the condition number of the matrices {Bk} is bounded. Therefore, all the iterates are good iterates. Reasoning as in the proof of Theorem 5.4, we conclude that for all j for some constants C &gt; 0 and 0 &lt; r &lt; 1.</p><p>[] Prior to considering the convergence rate, we show that the Broyden matrices Sk are bounded.</p><p>LEMMA 5.8. Suppose that the iterates {xk} generated by Algorithm II converge R-linearly to a solution point x, that satisfies Assumptions 5.1. Then the Broyden matrices Sk are bounded and the safeguards (49) and (50) become inactive for all large k.</p><p>Proof. We make use of the well-known bounded deterioration property for Broy- den's method (cf. Lemma 8.2.1 in Dennis and Schnabel <ref type="bibr" target="#b14">[15]</ref>), which states that under Assumptions 5.1 IIS+ z,w, _&lt; IIs z,w, + c, for some constant C &gt; 0. As a result of the R-linear convergence of {x}, we obtain and it is clear that safeguards (49) and (50) become inactive for all large k. D Therefore, the algorithm will not modify the information supplied by Broyden's method, asymptotically. This is an important point in establishing superlinear con- vergence. 6. Superlinear convergence. Without the correction terms wk and k, and with appropriate update criteria, Algorithm II is 2-step Q-superlinearly convergent.</p><p>This was proved by Nocedal and Overton <ref type="bibr" target="#b25">[26]</ref> assuming that Yk and Zk are orthogonal bases and assuming that a good starting matrix B1 is used. This result has been extended by Xie [291 for more general bases and for any starting matrix B1 &gt; 0. In this section we show that if the correction terms are used in Algorithm II, the rate of convergence is 1-step Q-superlinear. This result is possible by Update Criterion I and by the selected application of finite-difference approximations, which allow BFGS updating to occur more frequently.</p><p>To establish superlinear convergence, we need to ensure that the steplengths ck have the value 1 for all large k. When a smooth merit function, such as Fletcher's differentiable function <ref type="bibr">(Fletcher [17]</ref>) is used, it is not difficult to show that, near the solution, unit steplengths give a sufficient reduction in the merit function and will be accepted. However, the nondifferentiable el merit function (7) used in this paper may reject steplengths of one, even very close to the solution. This so-called Maratos effect requires that the algorithm be modified to allow unit steplengths and to achieve a fast rate of convergence. We do not consider this modification here, so as not to complicate our already lengthy analysis and since it does not affect the main structure of the algorithm or its essential properties. In the companion paper (Biegler, Nocedal,</p><p>and Schmid <ref type="bibr" target="#b0">[1]</ref>), which is devoted to a numerical investigation of Algorithm II, we describe how to incorporate the nonmonotone line search (or watchdog technique) of Chamberlain et al. <ref type="bibr" target="#b8">[9]</ref> that allows unit steplengths to be accepted for all large k. The analysis of the modified algorithm would be similar to that presented in 5.5 of Byrd and Nocedal <ref type="bibr" target="#b5">[6]</ref>.</p><p>In the remainder of this section we assume that the iterates generated by Algo- rithm II converge R-linearly to a solution and that unit steplengths are taken for all large k. In the presentation of the results that follow we do not restate the assump- tions under which R-linear convergence was proved in 5, but simply assume that R-linear convergence occurs. We begin by showing that the damping parameter k, used in (79) to ensure that descent directions are always generated, has the value of 1 for all large k. We have shown in Theorem 5.6 that I[B-II is bounded above. Also, (121), (102), and (78) show that, when finite differences are used, wk O(llp, ll) O(llckll), and by (143) we see that this is also the case when Broyden's method is used. Using these facts, and noting that I1" -&lt; I1" II1, we see that there is a constant C such that the left-hand side of (74) can be bounded by cos + r Zk gk / w B;Xwk] &lt;_ [CkC(ll kll / ll ll)]llckllx, since d[Zk O(llell), .As the iterates converge to the solution, and since ff &lt;_ 1, the term inside the square brackets is less than the constant p given in (74), showing that Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Ck 1 for all large k. This and the remarks made at the end of 5 show that all the safeguards included in Algorithm II become inactive asymptotically.</p><p>We can now show that the Broyden matrices satisfy the condition of Dennis and</p><p>Mor <ref type="bibr" target="#b13">[14]</ref> for superlinear convergence. Note from Algorithm II that a Broyden update of Sk is always performed, regardless of whether a BFGS update of Bk takes place or not. The following result is a straightforward modification of a well-known property for Broyden's method. LEMMA 6.1. Suppose that the iterates generated by Algorithm II converge Rlinearly to a point x, that satisfies Assumptions Proof. The proof is essentially given in Griewank [21] and is also very similar to the analysis in Dennis and Schnabel [15, pp. 183-184], but we give it here for the sake of completeness. Using the Broyden formula (45), we have Defining Ek Sk T Z, W,, applying Lemma 8.2.5 of Dennis and Schnabel <ref type="bibr" target="#b14">[15]</ref>, recalling (46)-(47), and using the mean value theorem, we obtain IIE/IIF llEk(I $k'/$''Sk)llF + O(ak)  llEk kll 2   Rearranging this expression yields (145) llEk kll 2 211E IIF [IIEklIF -IIE+IIF + By Lemma 5.8, we know that the matrices S remain bounded, therefore there exists some A such that for all k &gt;_ k, IIEkll &lt;_ A/2 and _&lt; A[IIE II + Since {a } converges R-linearly, the last term is summable, which implies that Noting that $, Ozkdk gives the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O</head><p>This lemma shows that in the limit Sk is an accurate approximation to z,Tw, along dk, and Theorem 5.6 shows that, when updating takes place, Bk is an accurate</p><p>If k E R3 and the correction term wk is computed by Broyden's method as wk SkYkP (see (48)), we have &lt; II(S -T Z, W,)de]l + II(Se zT, w,)ZePzII.</p><p>Using (144), (155), and the boundedness of Sk, we see that the right-hand side is of order o(lldkll), so that (154) holds. On the other hand, if wk is computed by finite differences, we have directly from (122) that (154) holds. In addition, (155) and the boundedness of Bk show that (153) holds for all k E R3, regardless of whether finite differences or Broyden's method are used.</p><p>7. Final remarks. We have presented a new reduced Hessian algorithm for large-scale equality-constrained optimization. The motivation for this work has been practical: our earlier reduced Hessian code, designed for large problems, was of- ten subject to instabilities, and we have aimed to develop a more robust algorithm that resembles the full-space SQP method but is less expensive to implement. In a forthcoming paper (Biegler, Nocedal, and Schmid [1]), we discuss our computational experience with the new method. That paper describes how to handle inequality con- straints and discusses numerous important details of implementation not considered here. These include, the choices of all constants and tolerances, the strategy for coping with the case when the basis matrix C in (35) changes, and the procedure for com- puting the damping parameter , which was only outlined in (75). We also discuss in that paper how to apply the updating criterion away from the solution. We believe that the new algorithm can be very useful for solving large problems, especially those with few degrees of freedom.</p><p>We have focused only on convergence results that helped us in the design of the algorithm and that revealed its main properties. The analysis was complicated by two factors. We did not assume that the BFGS matrices Bk or the Broyden matrices Sk were bounded, which required careful consideration of their behavior. This analysis paid off by suggesting safeguards that are useful in practice and ensure a superlinear rate of convergence. The other complicating factor was the fact that the frequency of BFGS updating can vary drastically: it can take place at every iteration, never, or in various patterns. As was found earlier by Xie <ref type="bibr" target="#b28">[29]</ref>, it is necessary to develop the theory in sufficient generality to cover all of these cases, and this significantly increased the complexity of some of the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 .</head><label>7</label><figDesc>Test the line search condition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>]xL(xk+l, Ak.+) V,L(x,,k+)] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(40) YkP, --Y(zk)[A(zk)Ty(zk)]-c(zk + ZkPz), (41) Zk+l Zk + ZkPz -YkP,.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 1 .</head><label>1</label><figDesc>FIG. 1. Comparison of Coleman-Conn method and Algorithm I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fletcher   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>R2FIG. 2 .</head><label>2</label><figDesc>FIG.2. Three regions generated by Update Criterion I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 .</head><label>3</label><figDesc>Set findiff false and compute pv by solving the system (78) (ATkyk)p, --ck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Bkpz -[Z gk + wk].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>xk + ad}) &lt;_ dpt, (xk) + aDdpt, (xk; dk).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>)[A(x)Ty(x)]-II &lt;_ "o, IIZ(x)ll &lt;_ o, for all x E D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>xj; d) &lt;_ b2 [[I T zj gj + Ilcj II1],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>THEOREM 4 . 2 .</head><label>42</label><figDesc>If Assumptions 4.1 hold, then the weights {ttk} are constant for all sufficiently large k and liminf( T -oo IIZ gll / IIcll) 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Assumptions 5 . 1 .</head><label>51</label><figDesc>The point x, is a local minimizer for problem (1)-(2), at which the following conditions hold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>wk Z[[VL(xk + YkP., Ak) VL(xk, Z[[VL(x + Ykpv, A.) VL(xk, A.)] +Z[[(A(x + Ykpv) Ak)(Ak A.)] o +z2[(( + gpl l(a (4 z2gp. + z2[(( + gp )(a a,ll.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>1 + O(ak) + (ak + 1 + 1/k)(1 + ak)O (130)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>cos 2 k and the term inside the square brackets are nonpositive, we can delete (B)+ O(ak ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>-Z, W, I I &lt;_ IIS1 -Z,W, + C k i=1 which shows that the matrices Sk remain bounded. We then see from (48) that the Broyden corrections wk and k satisfy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>zT* W*)dkll O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>10. If (81) is not satisfied, choose a new ak E [TOlk,TtOlk] and go to step 9; . Evaluate fk+i,gk+,ck+l,Ak+l, and compute Yk+i and Zk+.</figDesc><table><row><cell>otherwise set</cell><cell></cell></row><row><cell>(82)</cell><cell>xk+l Xk + ckdk.</cell></row><row><cell cols="2">12. Compute the Lagrange multiplier estimate</cell></row><row><cell>(83)</cell><cell></cell></row></table><note><p>11</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>One can show (see Eq. (3.22) of Byrd and Nocedal [51) that if Bk is updated by</figDesc><table /><note><p>the BFGS formula, then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>We now invoke Update Criterion I and note from (55) that, if BFGS updating of</figDesc><table><row><cell>YkSk</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">since ][k{ and [lsk]] are of the same order. Therefore</cell></row><row><cell>Yk k IIk</cell><cell cols="2">(z[z ,) 1 + sk IIk</cell><cell>+ 0</cell><cell>IIk</cell></row><row><cell>(laa)</cell><cell>1 + 0() + 0</cell><cell cols="2">gl</cell></row><row><cell cols="2">Similarly from (135) and (116) we have</cell><cell></cell><cell></cell></row><row><cell>and thus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(137)</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>/ 1/2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>and</cell><cell></cell></row><row><cell cols="3">lim[1 k U -cos 2 0 t-In 0. CO82 k Now, for x &gt;_ 0 the function 1 x + In x is concave and has its unique maximizer at</cell></row><row><cell cols="3">x 1. Therefore the relations above imply that</cell></row><row><cell>(142)</cell><cell cols="2">lim cos0k lim k 1. k--*oo</cell></row><row><cell></cell><cell>kEU</cell><cell>kEU</cell></row><row><cell>Now from (118)-(119)</cell><cell></cell></row><row><cell cols="2">IIGj1/2(Bk G,)pzll 2</cell><cell>I1( I)11</cell></row><row><cell></cell><cell>1/2 ll. pzll</cell><cell>I111</cell></row><row><cell cols="3">are summable, to obtain b(k+l) &lt;_ C + E (lncos2 Ok _ j uk</cell><cell>[1 Ok cos2 k</cell></row></table><note><p>for some constant C. Since (/k+l) &gt; 0, and since both In cos 2 Ok and the term inside the square brackets are nonpositive, we see that lim In cos2/}k 0 k6U</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1"><p>k 8k Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>+ O(a/2). Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank R. Byrd for many interesting discussions on the subject of this paper. We are also thankful to a referee who made very useful sugges- tions on how to improve the presentation of the results.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported by National Science Foundation grants CCR-9101359 and ASC-9213149, U.S. Department of Energy grant DE-FG02-87ER25047-A004, and Office of Scientific Computing, U.S. Department of Energy contract W-31-109-Eng-38. 314</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>approximation to zT.w.z, along pv. We use these two facts and the following lemma, which is an application of the well-known result of Boggs, Tolle, and Wang <ref type="bibr" target="#b1">[2]</ref>. LEMMA 6.2. Suppose that the iterates generated by Algorithm II converge Rlinearly to a point x. that satisfies Assumptions 5.1, and suppose that ak 1 for all large k. If, in addition, (146) lim IIBpz + w zT, W,dII O, -' IIdll then the rate of convergence is 1-step Q-superlinear.</p><p>Proof. Nocedal and Overton [26, Thm. 3.2] show that if an algorithm of the form (aT) Let us write wk Tkp for some matrix Tk. Then, recalling that } 1 for all large k, we have from (79</p><p>Thus we can define convergence is [Tk Bk][Yk Zk] -1 and the condition (148) for superlinear Z, W,)dkll O.</p><p>lim II([Tk Bk][Yk Zk] -T However, using (149) and wk Tkpv, we have that [Tk Bk][Y Zk]-ldk Tkpv + Bpz wk + Bpz, giving the desired result.</p><p>We can now prove the final result of this section. The analysis is complicated by the fact that BFGS updating may not always take place and by the fact that the correction terms are sometimes computed by finite differences and sometimes by Broyden's method. We therefore consider the following three sets of iterates, based on Update Criterion I and illustrated in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R3 {Jl IIp &gt; Ilp(z )</head><p>Downloaded 01/02/13 to 152.3.102.242. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php A REDUCED HESSIAN METHOD 345 and note that both "Yk and ak are summable. THEOREM 6.3. Suppose that the iterates generated by Algorithm II converge Rlinearly to a point x, that satisfies Assumptions 5.1, and suppose that Ck 1 for all large k. Then the rate of convergence is 1-step Q-superlinear.</p><p>Proof. Since dk YkPY + ZkPz, we have PY ] [Y Zk dk. for this together with (146) will give the desired result. We consider the three regions R1, R2, and R3 separately. Algorithm II is designed so that, in R2, wk must be computed by finite differences. On the other hand, since Pz is recomputed in step 7, after which we can be in any of the three regions, we see that in R1 and R3 Wk may be computed by finite differences or by Broyden.</p><p>If k E Ri, we have that IlPvll o(llPzll) o(lldkll). We also know from (143)   that wk O(llpvll) when the correction is computed by Broyden's method, and by (121) this relation also holds when wk is computed by finite differences. Therefore,</p><p>Z, W, YkPII o(lldkll).</p><p>Furthermore, since updating always takes place in R1, (126) holds:</p><p>(153)</p><p>IIBkpz zT, w,Z,pzll o(lldkll ).</p><p>We have thus established (151) for all k</p><p>Let us now suppose that k R2, in which case wk is computed by finite differences.</p><p>Using (122), we have that (154)</p><p>where the last step follows from (150). Since updating always takes place in R2, (153) also holds in this case, and we conclude that (151) holds for all k R2.</p><p>Finally we consider the case when k R3. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">'</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Hmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the local convergence of a quasi-Newton method for constrained optimization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An example of irregular convergence in some constrained optimization methods that use the projected Hessian</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="232" to="237" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the conveyence of constrained optimization methods with accurate Hessian information on a subspace</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="141" to="153" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A tool for the analysis of quasi..Newton methods with application to unconstrained minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="727" to="739" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of reduced Hessian methods for constrained optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="285" to="323" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representations of quasi-Newton matrices and their application to limited memory methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="129" to="156" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global convergence of a class of quasi-Newton methods on convex problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1171" to="1190" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The watchdog technique for forcing convergence in algorithms for constrained optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lemarechal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Pederson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming Studies</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear programming via an exact penalty function: global analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Conn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="137" to="161" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the local convergence of a quasi-Newton method for the nonlinear programming problem</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="755" to="769" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Partitioned quasi-Newton methods for nonlinear equality constrained optimization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Fenyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="17" to="44" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constrained optimization using a nondifferentiable penalty function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Conn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A characterization of superlinear convergence and its application to quasi-Newton methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="549" to="560" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Numerical Methods for Unconstrained Optimization and Nonlinear Equations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Schnabel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An exact penalty for nonlinear programming with inequalities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m">Practical Methods of Optimization, 2nd ed</title>
		<meeting><address><addrLine>Chichester</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reduced quasi-Newton methods with feasibility improvement for nonlinearly constrained optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gabay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming Studies</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="18" to="44" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the local and global convergence of a reduced quasi-Newton method, Optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="421" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maintaining the positive definiteness of the matrices in reduced Hessian methods for equality constrained optimization</title>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The &quot;global&quot; convergence of Broyden-like methods with a suitable line search</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">(</forename><surname>Riewank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Austral. Math. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="75" to="92" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local convergence of a two-piece update of a projected Hessian matrix</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Gurwitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="461" to="485" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Gurwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SQP methods based on approximating a projected Hessian matrix</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="631" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A globally convergent method for nonlinear programming</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="297" to="309" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A sequential quadratic programming algorithm using an incomplete solution of the subproblem</title>
		<author>
			<persName><forename type="first">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Prieto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Operations Research, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Projected Hessian updating algorithms for nonlinearly constrained optimization</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="821" to="850" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The convergence of variable metric methods for nonlinearly constrained optimization calculations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
		<editor>Nonlinear Programming 3, O. Mangasarian, R. Meyer, and S. Robinson</editor>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="27" to="63" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On a quasi-Newton update and its application to equality constrained optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tagliaferro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universita di Trieste</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reduced Hessian algorithms for solving large-scale equality constrained optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Boulder</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Colorado</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An only 2-step Q-superlinear convergence example for some algorithms that use reduced Hessian approximations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="224" to="231" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
