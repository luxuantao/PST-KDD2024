<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D motion retrieval with motion index tree</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="laboratory">Microsoft Visual Perception Laboratory of Zhejiang University</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@cs.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="laboratory">Microsoft Visual Perception Laboratory of Zhejiang University</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="laboratory">Microsoft Visual Perception Laboratory of Zhejiang University</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunhe</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="laboratory">Microsoft Visual Perception Laboratory of Zhejiang University</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D motion retrieval with motion index tree</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CE844687624659CC0999D721FEB36027</idno>
					<idno type="DOI">10.1016/j.cviu.2003.06.001</idno>
					<note type="submission">Received 1 September 2002; accepted 1 June 2003</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Motion capture</term>
					<term>Motion retrieval</term>
					<term>Hierarchical motion description</term>
					<term>Motion index tree</term>
					<term>Dynamic clustering</term>
					<term>Key-frame extraction</term>
					<term>Elastic match</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the development of Motion capture techniques, more and more 3D motion libraries become available. In this paper, we present a novel content-based 3D motion retrieval algorithm. We partition the motion library and construct a motion index tree based on a hierarchical motion description. The motion index tree serves as a classifier to determine the sub-library that contains the promising similar motions to the query sample. The Nearest Neighbor rule-based dynamic clustering algorithm is adopted to partition the library and construct the motion index tree. The similarity between the sample and the motion in the sub-library is calculated through elastic match. To improve the efficiency of the similarity calculation, an adaptive clustering-based key-frame extraction algorithm is adopted. The experiment demonstrates the effectiveness of this algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion capture techniques have been widely used in computer animation, film making, game, etc. Optical, mechanical, or magnetic sensors are attached to the joints of a human performer to record his movement, which can be used to drive animated characters <ref type="bibr" target="#b0">[1]</ref>. These techniques enable animators to produce realistic Computer Vision and Image Understanding 92 (2003) 265-284 www.elsevier.com/locate/cviu animations efficiently. With the popularity of motion capture systems, a large number of 3D motion libraries have been built.</p><p>However, problems occur when these 3D motion libraries are put into use, mainly on the following two aspects: 1. How to make full use of existing motions. Although motion capture systems can accurately record the motion of a performer, the captured motion does not necessarily meet the requirements of animators in practice. Animators can hardly predict exactly what motion they need, and even they can, they often change their intentions. Moreover, because animated characters often interact with the environment and other characters, existing motions need to be modified accordingly before applied to animated characters. In addition, the captured motions need to be adapted to the special configuration of different characters before retargeted to them. 2. How to get desired motions from the library. It is difficult and tedious for users to obtain required motions by browsing the library. The new animation technologies, such as <ref type="bibr" target="#b1">[2]</ref>, in which new motions are produced by extracting and synthesizing motions (fragments) similar to motion samples from the library, also demand the technique for retrieving motion data automatically. In these years, fruitful work has been proposed to make full use of the captured 3D motion. A popular method is to adapt existing motions to new requirements through interactive control, such as constrained-based methods <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, motion retargeting <ref type="bibr" target="#b5">[6]</ref>, etc. Meanwhile, an attractive way is to apply techniques from signal and image processing fields to motion adaptation and new motion generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Recently, a more fantastic way is proposed to synthesize new motions from example <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. That is to generate motions through selecting and piecing existing motions along a specified path or according to statistical distributions.</p><p>To the best of our knowledge, however, no available algorithms have been proposed to retrieve 3D motions based on their content. The most relevant work proposed <ref type="bibr" target="#b1">[2]</ref> is to choose appropriate motion fragments from an existing motion library to synthesize required motions. This algorithm, however, is unsuitable for the full-body motion retrieval, as motion match is executed by comparing partial scripted key-frame motions with partial captured motions.</p><p>Inspired by content-based retrieval algorithms for retrieving multimedia data based on the features automatically extracted from the content <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>, we propose a content-based motion retrieval algorithm. Particularly, desired motions are obtained by submitting a similar sample in the form of a captured motion or a scripted one. To achieve this goal, the following problems have to be addressed:</p><p>1. An effective motion representation is needed. Each motion is a frame sequence, with each frame defining a posture. Because each posture is a configuration made up of all the body joints and each motion is a harmonic combination of sub-motions of all these joints, an efficient description of the posture is required. The description should also address the different effects of each joint on determining the posture. 2. An efficient match algorithm is demanded to calculate the similarity between two motions. Each posture in a motion is defined by a frame with tens of parameters depicting the character configuration, and the temporal order among the posture/ frame sequence is essential, so an efficient and effective match algorithm is demanded. 3. A key-frame extraction algorithm is required. Because each motion consists of a large number of frames, it is time-consuming to calculate the similarity or distance between two motions with these original data. To reduce the computational overhead, key-posture sequences need to be extracted to calculate the similarity. Aiming at the above challenges, we present a content-based 3D motion retrieval algorithm. Specifically, we describe two main stages in this paper, first building a motion index tree to structure the motion library, and then retrieving motions using the motion index tree. Particularly, we adopt a hierarchical motion description to represent a posture, based on which we partition the motion library hierarchically and construct a motion index tree to facilitate motion retrieval. Nearest Neighbor rule-based dynamic clustering algorithm is employed to partition the motion library and construct the motion index tree. The similarity between two motions is calculated by elastic match. To improve the efficiency during the similarity calculation, we adopt an adaptive clustering-based key-frame extraction algorithm to extract keyposture/-frame sequences, which are used in elastic match. During the retrieving stage, the motion index tree serves as a hierarchical classifier to determine the sublibrary that contains the promising similar motions to the query sample. Next, keyframe/-posture sequences are extracted from the sample and the motion from the sub-library respectively, and the similarity between them is calculated using elastic match.</p><p>The remainder of this paper is organized as follows. In the next section, we give a review on previous work. In Section 3, we give a brief description on the hierarchical motion representation. In Section 4, we describe the construction of the motion index tree in detail. In Section 5, we describe the procedure of motion retrieval. We discuss the experiment in Section 6 and conclude the paper in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>In this section, we first make a brief review on previous work on motion processing, namely motion editing, motion analysis, and synthesis. Then we summarize related work with key-posture/key-frame extraction used in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motion editing, analysis, and synthesis</head><p>Early research on motion processing aimed to provide convenient tools for interactive motion editing. A popular way is constraint-based method. Gleicher <ref type="bibr" target="#b2">[3]</ref> and Gleicher and Litwinowicz <ref type="bibr" target="#b3">[4]</ref> proposed a space-time constraint-based method for editing a pre-existing motion such that it meets various requirements yet preserves its original quality as much as possible. However, it is time-consuming to solve the constraint optimization problem in this method. To improve the efficiency, Lee and Shin <ref type="bibr" target="#b4">[5]</ref> presented a hierarchical approach, which divides the constraint optimization problem into two sub-processes: (1) the configuration of an articulated figure is adjusted to meet the constraint on each frame by an inverse kinematics solver and (2) the motion displacement in each constrained frame is interpolated and thus smoothly propagated to the nearby frames using a fitting technique. Another attractive method is the so-called motion signal processing. Bruderlin and Williams <ref type="bibr" target="#b6">[7]</ref> applied techniques from image and signal-processing fields to designing, modifying, and adapting motions. Similarly, Liu et al. <ref type="bibr" target="#b7">[8]</ref> provided a series of tools for editing motion at a high level by introducing wavelet transformation into motion analysis and synthesis.</p><p>Recently, a fantastic technique, example-based motion synthesis, is proposed. Lee et al. <ref type="bibr" target="#b8">[9]</ref> developed a technique for controlling an animated character in real time using several possible interfaces, through which users can choose from a set of possible actions, sketch a path on the screen and create animations by searching through a motion database using a clustering algorithm. The approach of Li et al. <ref type="bibr" target="#b9">[10]</ref> combines some low-level noise driven motion generators with a high-level Markov process to generate new motions with variations in fine details. Pullen and Bregler <ref type="bibr" target="#b1">[2]</ref> presented a motion capture assisted animation, which allows animators to key-frame motions for a sub-set of Degree of Freedoms (DOFs) of a character and use motion capture data to synthesize motion for the missing DOFs and add texture to those key-framed. Kovar et al. <ref type="bibr" target="#b10">[11]</ref> build a motion graph that encapsulates connections among the motion library and synthesize motions by building walks on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Key-frame extraction</head><p>Because motion resembles video in their representation (i.e., both of them can be represented as posture/frame sequences), we explore key-frame extraction algorithms in video abstraction to find or adapt an appropriate one for extracting key-frames from motions. Below we give a review on key-frame extraction from video.</p><p>In the earlier work such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, key-frames are selected by sampling video frames randomly or uniformly at certain time intervals. Though this is a fast way to extract key-frames, it neglects the actual video content. Therefore, many representative frames are missing, especially from short segments, whereas redundant frames are extracted especially from long segments. To address this problem, shot-based key-frame extraction algorithms are proposed. A video clip is first segmented into shot sequences based on features such as color <ref type="bibr" target="#b14">[15]</ref>, motion <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and then a certain frame in each shot, e.g., the first or the last frame, is selected as a key-frame. Another way is sequential comparison-based method <ref type="bibr" target="#b19">[20]</ref>, in which the current frame is compared with the last extracted key-frame. If the difference is significant, then the current frame is selected as a new key-frame. To avoid successive selection in highly active frames, a minimal interval between key-frames is set <ref type="bibr" target="#b20">[21]</ref>. A special way is to utilize the visual content as well as such information as corresponding audio streams for key-frame detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>As addressed in Kim and Hwang <ref type="bibr" target="#b23">[24]</ref>, previous key-frame extraction algorithms relied mostly on low-level features and other readily available information instead of using semantic primitives. Recently, object-based video abstract techniques have been presented in <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. The representative work of object-based algorithms is reported in Kim and Hwang <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. The objects in each frame are first segmented and identified through a moving-edge detection algorithm <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, and the first frame in each shot is selected as the first key-frame. If the object number changes, the current frame is selected as a new key-frame. Otherwise the dissimilarity between the current frame and the recent extracted key-frame is calculated through measuring the dissimilarity between two object masks extracted from the two frames, respectively, and if it excesses a given threshold, a new key-frame is created. These object-based methods work especially well in video surveillance systems. However, they are not suitable for extracting key-frames from motion, as there is no object, even no background/foreground in motion at all.</p><p>Because a large number of motions are nearly periodical, a promising method is clustering-based extraction. Particularly, similar frames are clustered into the same cluster, and a representative frame from each cluster is selected as a key-frame. Ratakonda et al. <ref type="bibr" target="#b29">[30]</ref> proposed a hierarchical video summarization using a pair-wise K-means algorithm. However, this temporal constrained K-means clustering cannot merge similar but temporally apart frames. Doulamis et al. <ref type="bibr" target="#b30">[31]</ref> adopted a fuzzy classifier to cluster all features extracted through a recursive shortest spanning tree algorithm to predetermined classes. And a genetic algorithm is adopted to extract key-frames by minimizing a cross-correlation criterion. This method is highly time-consuming. Kim and Hwang <ref type="bibr" target="#b23">[24]</ref> also present an object-based video abstraction through Mean Shift Clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion representation</head><p>Motion can be described as a frame sequence, with each frame depicting a posture at a given time. An intuitive description of a posture is a set of bones, each represented as a rigid 3D model with a position and orientation, together with a set of constraints imposed on joints that prevent them from separating or rotating in illegal ways. Within this model, the DOFs of a character are the positions and orientations of bones in the body.</p><p>A more concise representation imposes a hierarchical relationship on the bones, in which the position and orientation of each bone are specified with regard to its ''parent'' bone. A boneÕs configuration at a specific time can be specified by a fixed translation (thus not involving any DOFs) and a rotation with regard to its parent bone, which is in turn defined in terms of its parent. The configuration is recursively defined all the way up to the root joint, which has full translational and rotational DOFs <ref type="bibr" target="#b31">[32]</ref>. In the case of a human body, the posture can be depicted as a tree, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Within this hierarchical posture description, the DOFs of a character include the rotational parameters of non-root joints and the position and orientation of the root joint (which is Pelvis in Fig. <ref type="figure" target="#fig_0">1</ref>). Each motion can be represented as follows:</p><p>MðtÞ ¼ ðT root ðtÞ; Q root ðtÞ; R 1 ðtÞ; R 2 ðtÞ; . . . ; R n ðtÞÞ; ð1Þ</p><p>where T root ðtÞ and Q root ðtÞ are the position vector and orientation vector of root at time t, and R i ðtÞ is the rotation vector of joint i around its parent joint at time t. This hierarchical description implicitly holds the bones together. Moreover, it clearly demonstrates the different role played by each joint in determining the posture, with parent joints being more prominent than children joints. Furthermore, it reflects the effect of each jointÕs motion on the full-body motion, with the motions of parent joints being more significant than those of children joints. Motions represented in the intuitive way can be transformed into the hierarchical description using inverse kinematics <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Motion index tree</head><p>Within the hierarchical motion description, the motion of a parent joint may induce those of its children joints, whereas that of a child joint is unable to influence its parent joint. Obviously, the joints at high levels of the hierarchy are more significant than those at lower levels in terms of determining a motion. This hierarchy among the parameters of a posture can be well used to facilitate motion retrieval by building a corresponding motion index tree for a motion library.</p><p>Given the above human body as an example, all the joints can be divided into the following five levels according to their positions in the tree illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, as {Root}, {Lhip, Rhip, Chest}, {Lknee, Rknee, Neck, Lshoulder, Rshoulder}, {Lankle, Rankle, Head, Lelbow, Relbow}, and {Lwrist, Rwrist} from top to bottom. We construct a motion index tree based on this hierarchy to partition and structure the 3D motion library, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. This motion index tree serves as a hierarchical classifier to determine the sub-set that contains promising similar motions to a submitted example. For this sake, each non-leaf node in the motion index tree contains a sample set, built by selecting representative motions from its children nodes as described below. Each sample is labeled with the information of its source. The sample set is used in kNN rule (k Nearest Neighbor rule) to classify the submitted example in the process of retrieval described in Section 5. Each leaf node is associated with a sub-set, which contains promising similar motions to the example. All these sub-sets constitute the whole motion library. (For simplicity, the sub-motions of the joints at one or multiple levels are also called motions in the subsequent sections.)</p><p>The main steps to construct the motion index tree are outlined as follows: 1. Partition the 3D motion library ML into several sub-sets ML 2;k using dynamic clustering (to be described in Section 4.1) according to the motion of the first-level joint (viz. {Root}), and build a sample motion set from the sub-sets for the node Root. Particularly, motions closest to the centroid of each sub-set are selected as samples from this sub-set. Create a null node with an empty sample set, associate it with a sub-set ML 2;k , and take it as one of the children nodes of Root. 2. Partition each sub-set ML i;k , respectively, according to the motion of joints at the level i in the hierarchy and build a sample set for its corresponding node in the motion index tree in the similar way in Step <ref type="bibr" target="#b0">(1)</ref>. Again, create a null node for a newly created sub-set ML iþ1;j and take it as a child node of ML i;k . 3. Repeat Step (2) until all joints at the lowest level of the hierarchy are processed. 4. Take the last partitioned sub-sets as leaf nodes of the motion index tree.</p><p>In the following subsections, we will first describe the dynamic clustering algorithm for partitioning the motion library, and then elaborate the key-frame extraction algorithm and the elastic motion match algorithm, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Nearest Neighbor rule-based dynamic clustering</head><p>Partitioning a motion library into sub-sets is a well-defined clustering problem. As described in Section 3, motion is a high-dimensional signal and no function is available to describe the probability density up to now. Due to the lack of available probability density functions, we partition the motion library based on the similarity between sample motions. Therefore, Nearest Neighbor rule-based dynamic clustering <ref type="bibr" target="#b32">[33]</ref> is adopted to partition and structure the library.</p><p>Given two motion samples M i and M j , if M j is the Ith Nearest Neighbor (NN) of M i , the ''NN coefficient'' of M j to M i is defined as I. Likewise, if M i is the Kth Nearest Neighbor of M j , the NN coefficient of M i to M j is defined as K. Let a ij be the ''NN value'' between M i and M j , a ij ¼ I þ K À 2. If M i and M j are classified into the same cluster, the connection cost is defined as a ij ; otherwise, the cost is 0. To eliminate clusters with only one sample, the connection cost of the self-connection is defined as 2N M (N M is the number of motions in the library).</p><p>The objective function J NN is defined as the sum of the total inner-cost L IA and total inter-cost L IR :</p><formula xml:id="formula_0">J NN ¼ L IA þ L IR :</formula><p>The total inner-cost L IA can be defined as the sum of all the connection values between every pair of samples in the whole library, given that the connection cost between samples from different clusters is 0.</p><p>To calculate the inter-cost, the minimal NN value between cluster x i and x j , c ij , is computed first. And c i , the minimal NN value between cluster x i and all the other clusters can be calculated as follows:</p><formula xml:id="formula_1">c i ¼ min j6 ¼i c ij :</formula><p>Let a i max and a k max be the maximal connection cost between samples in the cluster x i and x k each, c be the number of clusters, b i , the inter-cost of cluster x i to the other clusters, is defined as follows:</p><formula xml:id="formula_2">b i ¼ À½ðc i À a i max Þ þ ðc i À a k max Þ ðc i &gt; a i max ; c i &gt; a k max Þ; c i þ a i max ðc i 6 a i max ; c i &gt; a k max Þ; c i þ a k max ðc i &gt; a i max ; c i 6 a k max Þ; c i þ a k max þ a i max ðc i 6 a i max ; c i 6 a k max Þ; 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :</formula><p>k ¼ arg min c ij j6 ¼i;j2h;h¼f1;2;...;cg</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>:</head><p>The total inter-cost can be defined as the sum of all the inter-costs.</p><p>The optimal clustering should result in the minimal objective value J NN . The following iterative method is proposed to solve this problem: 1. Calculate the distance matrix D, with each element D ij ¼ DðM i ; M j Þ, where DðM i ; M j Þ is the distance between motion M i and M j . The distance calculation is described in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Construct NN coefficient matrix M based on D, with each element</head><formula xml:id="formula_3">M ij being the NN coefficient of M i to M j . 3. Build NN value matrix L based on the NN coefficient matrix M L ij ¼ M ij þ M ji À 2 ðj 6 ¼ iÞ; 2N M ðj ¼ iÞ:</formula><p>4. Connect each element to the element, to which it has the minimal NN value in L, and form the initial clusters. 5. Calculate c i of each cluster and compare it with a i max and a k max . If c i is smaller than either of a i max and a k max , merge cluster x i and x k . 6. Repeat Step 5 until no clusters can be merged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Key-frame extraction</head><p>Because each motion is represented as a frame sequence, the similarity between two motions is defined as that between the two corresponding frame sequences. However, even a short motion of 4 s is composed of approximately 100 frames. It is time-consuming to calculate the similarity with these original data. To improve the computational efficiency, we extract key-frame sequences from two motions and calculate the similarity between them as that between the motions. Taking the motion of the second level joints as an example, we show the key-frame extraction algorithm in detail below.</p><p>Given a motion M with N frames, the motion at the second level can be represented as a (3 Â 3Â N ) Â 1 vector M 2 :</p><formula xml:id="formula_4">M 2 ¼ ½F 1 ; F 2 ; . . . ; F N ; F i ¼ ½r lxi ; r lyi ; r lzi ; r rxi ; r ryi ; r rzi ; r cxi ; r cyi ; r czi ;<label>ð2Þ</label></formula><p>where F i is the ith frame at the second level, r lxi , r lyi , and r lzi are the rotational parameters of joint Lhip, r rxi , r ryi , and r rzi are the rotational parameters of joint Rhip, and r cxi , r cyi , and r czi are the rotational parameters of joint Chest. As presented in Section 2.2, many methods have been proposed for key-frame extraction in the field of video abstraction. However, due to the periodicity of motions, most of these algorithms, including sampling-based, shot-based, and sequential comparison-based methods, will cause the redundancy in that similar frames in different cycles are extracted as key-frames. Methods, in which other information, such as audio stream, objects in each frame, are utilized for key-frame detection, are obviously not suitable for key-frame extraction from motions, as the only information available is the posture sequence in each motion.</p><p>A promising way is clustering-based extraction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Particularly, similar frames are clustered into the same cluster, and a representative frame from each cluster is selected as a key-frame. We adopt the adaptive clustering-based key-frame extraction technique proposed in <ref type="bibr" target="#b33">[34]</ref>, in which similar frames in different cycles can be clustered into the same cluster.</p><p>Define the similarity between two frames as some decreasing function f d about the weighted distance between them:</p><formula xml:id="formula_5">SimðF 1 ; F 2 Þ ¼ f d ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X k w k ðF 1k À F 2k Þ 2 r ! ;<label>ð3Þ</label></formula><p>where F 1k and F 2k are the motion parameters of frame F 1 and F 2 , respectively and w k is the weight indicating the significance of joint k. If these joints are from different levels in the hierarchy illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, we give higher weights to joints at higher levels whereas lower weights to those at lower levels empirically. Let d i be the ith cluster, the clustering algorithm can be summarized as follows:</p><p>1. Initialization:</p><formula xml:id="formula_6">F 1 ! d 1 , F 1 ! F c1 , the centroid of d 1 , 1 ! numCluster.</formula><p>2. Get the next frame F i . If the frame pool is empty, end.</p><p>3. Calculate the similarity between F i and the centroid of an existing cluster d k (k ¼ 1; 2; . . . numCluster) according to Eq. ( <ref type="formula" target="#formula_5">3</ref>). 4. Determine the cluster closest to F i through calculating Maxsim as follows:</p><formula xml:id="formula_7">Maxsim ¼ Max numCluster k¼1 simðF i ; F ck Þ:<label>ð4Þ</label></formula><p>If Maxsim is below a given threshold, it means F i is not close enough to be put into any cluster, goto Step 5; otherwise put F i into the cluster with Maxsim, and goto Step 6. 5. numCluster ¼ numCluster + 1. A new cluster is created: F i ! d numCluster . 6. Update the cluster centroid as follows:</p><formula xml:id="formula_8">F ck ¼ nF 0 ck þ F i n þ 1 ;<label>ð5Þ</label></formula><p>where F 0 ck and F ck are the centroids before and after update, respectively, and n is the size of the old cluster. Goto Step 2. According to Zhuang et al. <ref type="bibr" target="#b33">[34]</ref>, the frame that is closest to the centroid of a cluster is selected as a key-frame. However, as the order among frames is lost during clustering, the extracted key-frame sequence is not consistent with the original temporal sequence. To preserve this essential attribute, considering frames in the same cluster are extracted sequentially, the first frame in each cluster is selected as the key-frame. The effectiveness of this algorithm will be discussed in Section 6.1. This algorithm can also be applied to the motion with multiple level joints or to the full body motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Motion match</head><p>According to the hierarchical motion description in Section 3, the global position of a body is determined by T root ðtÞ in Eq. ( <ref type="formula">1</ref>). Because the initial position of a motion is inessential in comparing one motion with another, we replace the absolute position of a motion with its velocity to eliminate the disturbance of the initial position yet preserve the information of the global locomotion.</p><p>Most similarity measures for motions are defined based on their corresponding key-frame sequences. A simple method is the Nearest Center (NC) algorithm. Zhang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a Nearest Neighbor approach, in which the similarity is defined as the sum of the most similar pairs of key-frames. As discussed in Li et al. <ref type="bibr" target="#b34">[35]</ref>, these methods neglected the temporal order of frames. To address the problem, Li <ref type="bibr" target="#b34">[35]</ref> presented a Nearest Feature Line-based method. Though this method accommodated the temporal correlation between key-frames, it could only be applied to retrieving motions using a single frame as the query sample.</p><p>Elastic match performed with a dynamic time warping algorithm is a non-linear match method originally used in speech recognition and it has been successfully applied to online signature verification <ref type="bibr" target="#b35">[36]</ref>. Elastic match can be used in comparison of all kinds of continuous function about a continuous parameter, which is time typically.</p><p>Given two motions, M1 fF 1 1 ; F 1 2 ; . . . ; F 1 N g and M2 fF 2 1 ; F 2 2 ; . . . ; F 2 M g, the distance between them is defined as follows:</p><formula xml:id="formula_9">D ¼ 1 2 min fx1ðiÞg X N i¼1 dði; x1ðiÞÞ þ min fx2ðiÞg X M i¼1 dði;<label>x2ðiÞÞ</label></formula><formula xml:id="formula_10">! ;<label>ð6Þ</label></formula><p>where the first factor on the right part is the distance between M1 and the motion resulting from warping M2 according to the path defined by a time warping path x1ðiÞ, and the second factor on the right is the distance between M2 and the motion resulting from warping M1 according to the path defined by x2ðiÞ. dði; jÞ is the distance between the ith frame of M1 and the jth frame of M2, defined as:</p><formula xml:id="formula_11">dði; jÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X k w k ðF 1 ik À F 2 jk Þ 2 r ;<label>ð7Þ</label></formula><p>where F 1 ik and F 2 jk are the kth motion parameters in frame F 1 i and F 2 j , respectively, andw k is the weight, set in the same way described in Section 4.2.</p><p>The time warping path, for example, x1ðiÞ, is constrained by the following boundary and continuity conditions. The boundary conditions ensure that the first and last frame of M1 are matched to the frame b and frame e of M2: x1ð1Þ ¼ b, x1ðN Þ ¼ e, where b ¼ min i 6 M argðdð1; iÞ 6 thresholdÞ and e ¼ max i 6 M argðdðN ; iÞ 6 thresholdÞ.</p><p>The continuity conditions restrict the match of the intermediate frames, and x1ðiÞ is defined as a monotonically increasing function and thus the temporal order is preserved during match.</p><p>Let the left half part of D be D L , defined as D L ¼ min fx1ðiÞg P N i¼1 dði; x1ðiÞÞ, we solve it recursively by applying dynamic programming in the following way <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_12">D L ði; jÞ ¼ dði; jÞ þ minfD L ði À 1; jÞ; D L ði À 1; j À 1Þ; D L ði À 1; j À 2Þg; D L ð1; bÞ ¼ dð1; bÞ;<label>ð8Þ</label></formula><p>where D L ði À 1; j À 2Þ corresponds to skipping the ðj À 1Þth frame of M2 and D L ði À 1; jÞ means that at least two frames of M1 correspond to the jth frame of M2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Motion retrieval</head><p>By now, the motion index tree has been constructed. We describe motion retrieval using the motion index tree in this section.</p><p>We devise a two-stage process to retrieve similar motions to a query example M. Determine a sub-set that contains promising similar motions to M and calculate distances/similarities between M and motions in the sub-set. The process of motion retrieval is outlined as follows:</p><p>1. Fetch M 1 , the motion of the first-level joints, from the example M. 2. Extract key-frame sequences from M1 and each motion in the sample set of Root in the motion index tree using the key-frame extraction described in Section 4.2, and calculate the distances between them using elastic match described in Section 4.3. Then get the k Nearest Neighbors. Let k 1 ; k 2 ; . . . ; k c be the number of the Nearest Neighbors belonging to the children nodes of Root x 1 ; x 2 ; . . . ; x c , respectively, the decision-making function is defined as g i ðM 1 Þ ¼ k i ; i ¼ 1; 2; . . . ; c and the decision-making rule is defined as follows:</p><formula xml:id="formula_13">If g j ðM 1 Þ ¼ max i k i ; i ¼ 1; 2;</formula><p>. . . ; c, then M 1 belongs to x j , the jth child node of Root. 3. Continue classification until a leaf node of the motion index tree is reached.</p><p>Calculate the similarity between M and each motion stored in the sub-set in the leaf node, and select appropriate motions as the result according to the similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiment and discussion</head><p>To validate the effectiveness and efficiency of the proposed technique, we develop a prototype system of content-based motion retrieval (CBMR) and test it on a motion library consisting of about 450 different motions. The composition of the library is shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Key-frame extraction</head><p>Both the adaptive clustering-based key-frame extraction algorithm and the sequential comparison-based method, in which a frame is selected as a new key-frame when the difference between this frame and the last key-frame is significant, are implemented and tested. For convenience, these two algorithms are called ACE and CE, respectively. In both algorithms, the similarity between two frames is defined as the reciprocal of the weighted distance between them.</p><p>First, two non-periodical motions, jump-kick and dive, are experimented on. jumpkick consists of 70 frames and dive consists of 50 frames. The Maxsim value in ACE and the similarity in CE are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. For ACE, when the Maxsim value is below a given threshold, a new cluster is found and the current frame is selected as a new key-frame. From Fig. <ref type="figure" target="#fig_2">3</ref>, we can see that totally 11 key-frames, including the first frame and the other 10 new key-frames, are extracted from jump-kick, and totally 10 key-frames, including the first frame and the other 9 new key-frames, Eight students are engaged in partitioning the library. A motion can be classified into a certain class only when it gains ratifications from more than five of these students. are extracted from dive. For CE, the frame with its similarity below a given threshold is selected as a new key-frame. From Fig. <ref type="figure" target="#fig_2">3</ref>, we can see that totally 16 key-frames, including the first frame and the other 15 new key-frames, are extracted from jump-kick, and totally 14 key-frames, including the first frame and the other 13 new key-frames, are extracted from dive. The extracted key-frame sequences are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, more key-frames are extracted using ACE than using CE from both motions given the same threshold though the same similarity measure is adopted. Two reasons account for it. The Maxsim value in ACE is defined as the maximal similarity between the current frame and the centroid of each cluster, which means that the current frame is compared with the centroids of all the previously extracted clusters. In CE, the current frame is only compared with the most recent key-frame, which rules out the possibility for the current frame to match the most similar key-frame previously extracted. Another reason is that each motion is continuous, so the frames nearby are usually classified into the same cluster. Then the similarity between the current frame and the centroid decreases more slowly in ACE than that between the current frame and the last extracted key-frame in CE, which results in the fact that in ACE more frames after the last extracted key-frame have similarities over the threshold than in CE. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, the similar keyframe sequences can be extracted by ACE at a relatively high threshold and by CE at a relatively low threshold. These results show that the performance of CE and ACE are similar for non-periodical motions.</p><p>Second, two periodical motions, power-walk and hurdle, are tested on. power-walk consists of 120 frames, and hurdle consists of 195 frames. power-walk is composed of about 4 cycles of steps. From Fig. <ref type="figure" target="#fig_5">6a</ref>, we can see that using ACE, 5 clusters are formed in power-walk, including the initial one and the appended 4. The first frame of each cluster is extracted as a key-frame. All these key-frames are extracted from the first 30 frames, which compose the first cycle of power-walk. No frames in the successive cycles are extracted as key-frames. This shows the compactness of the extracted key-frame sequence. When it comes to CE, totally 29 key-frames, including the initial frame and the appended 28, are extracted from power-walk. From Fig. <ref type="figure" target="#fig_5">6a</ref>, we can see that the last 28 key-frames can nearly be divided into 4 cycles and the last 3 seven-frame sequences are almost the repetitions of the first seven-frame sequence. The experiment on hurdle shows the similar result. All these demonstrate that ACE is superior to CE when applying to periodical motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Motion match</head><p>To evaluate the performance of the elastic match algorithm, we implement this algorithm, as well as other methods, such as the NC method and the NN method, in our CBMR system. Some motions are selected from the library shown in Table <ref type="table" target="#tab_0">1</ref> as examples for queries to evaluate each algorithm. The distribution of these examples is stated in Table <ref type="table" target="#tab_1">2</ref>. Based on these queries, we plot the average precision-recall graph for each algorithm as illustrated in Fig. <ref type="figure" target="#fig_6">7</ref>, from which we can see that elastic match is superior to both NC and NN. The main reason is that both NC and NN neglect the temporal order among the frame sequence of a motion. For example, walk cannot be distinguished from run, and walk-back cannot be distinguished from walkforward either. Contrary to NC and NN, the continuity of the motion is preserved during the process of elastic match as a result of the constraint of continuity as addressed in Section 4.3. The result of one query is shown in Fig. <ref type="figure" target="#fig_7">8</ref>. Each row represents a motion. The top one is the example. The retrieved motions are sorted by similarity from top to bottom.</p><p>The calculation overhead of elastic match using dynamic programming is OðmnÞ, where m and n are the lengths of the two compared key-frame sequences, respectively. When retrieving a similar motion, the example is firstly classified into the promising sub-library. The calculation overhead for the classification is OðhsmnÞ, where h is the height of the index tree and s is the size of the sample set in each non-leaf node. After classification, only the similarities between the example and motions from the very sub-library are needed to be calculated and thus the efficiency is well improved.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, a content-based 3D motion retrieval algorithm is proposed. The main contribution is the motion index tree constructed based on the hierarchical motion description. This motion index tree features the hierarchical effect that each joint has on the full-body motion, and serves as a classifier to determine a sub-library that contains promising similar motions to the query example. Thus the efficiency can be well improved. We adopt Nearest Neighbor rule-based dynamic clustering algorithm to partition the motion library and construct the motion index tree. We employ a  novel elastic match algorithm to calculate the similarity between two motions. The elastic match algorithm combines the dynamic time warping and dynamic programming, and has excellent performance on comparing two sequences. To improve the efficiency, we adopt an adaptive clustering-based key-frame extraction algorithm, which is especially competent for key-frame extraction from periodical motions.</p><p>The presented work can be used to retrieve similar motions to an example from a motion library. It can also be used in various animation techniques, such as motion editing, analysis, and synthesis. In these promising techniques, the ability to find similar motions is demanded for synthesizing new motions. Moreover, a potential application is to find and provide appropriate motions for autonomous animated characters.</p><p>Contrary to such information as image, video, and audio, it is common for animators, the most possible users of 3D motion, to sketch a simple key-frame motion. Compared with the query with a real motion as the example, the query with a scripted motion is usually more relevant to the userÕs intent. In addition, it is often difficult to obtain an appropriate example. So such user interface as allowing users to script key-frame motions is needed and a corresponding algorithm to compare the scripted example with captured motion in the library is demanded. A promising method is to decompose the captured motion into different bands with wavelet analysis and compare the approximate content with the scripted one. In this paper, we determine the weights for the joints at each level empirically. A reasonable method is needed to calculate the weights automatically. We will address these two questions in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Hierarchical motion description.</figDesc><graphic coords="6,194.17,107.28,170.88,116.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Motion index tree.</figDesc><graphic coords="7,87.87,107.28,354.72,127.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Extract key-frame sequences from non-periodical motions using ACE and CE.</figDesc><graphic coords="13,96.38,107.28,332.64,341.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Key-frame sequences extracted from non-periodical motions. (a) Key-frame sequence extracted from jump-kick by ACE. (b) Key-frame sequence extracted from jump-kick by CE. (c) Key-frame sequence extracted from dive by ACE. (d) Key-frame sequence extracted from dive by CE.</figDesc><graphic coords="14,103.46,107.28,354.72,231.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Key-frame extraction using ACE and CE with different thresholds. The key-frame sequence extracted from dive by ACE with threshold, 0.033, is 0 4 6 8 9 10 18 20 27 28 29 43, and the key-frame sequence extracted by CE with threshold, 0.025, is 0 4 6 8 9 11 18 20 27 29 32 45.</figDesc><graphic coords="15,107.72,107.28,312.48,158.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Extract key-frame sequences from periodical motions using ACE and CE (a) Power-walk (b) Hurdle.</figDesc><graphic coords="16,109.13,107.28,340.32,352.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance of elastic match.</figDesc><graphic coords="17,102.05,107.28,322.56,145.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Retrieval result of walk-forward.</figDesc><graphic coords="17,102.05,297.85,320.64,190.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Composition of the motion library</figDesc><table><row><cell>Motion class</cell><cell>Size of class</cell><cell>Motion class</cell><cell>Size of class</cell></row><row><cell>Walk forward</cell><cell>70</cell><cell>Rotate</cell><cell>32</cell></row><row><cell>Run forward</cell><cell>62</cell><cell>Run backward</cell><cell>32</cell></row><row><cell>Walk backward</cell><cell>54</cell><cell>Climb</cell><cell>28</cell></row><row><cell>Jump</cell><cell>48</cell><cell>Box</cell><cell>18</cell></row><row><cell>Squat</cell><cell>45</cell><cell>Wave hand</cell><cell>12</cell></row><row><cell>Ballet dancing</cell><cell>36</cell><cell>Fall</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Distribution of examples for queriesThe number of examples from each class is nearly proportional to the size of this class.</figDesc><table><row><cell>Motion</cell><cell>The number</cell><cell>Motion</cell><cell>The number</cell></row><row><cell>class</cell><cell>of examples</cell><cell>class</cell><cell>of examples</cell></row><row><cell>Walk forward</cell><cell>6</cell><cell>Rotate</cell><cell>3</cell></row><row><cell>Run forward</cell><cell>5</cell><cell>Run backward</cell><cell>3</cell></row><row><cell>Walk backward</cell><cell>5</cell><cell>Climb</cell><cell>2</cell></row><row><cell>Jump</cell><cell>4</cell><cell>Box</cell><cell>2</cell></row><row><cell>Squat</cell><cell>4</cell><cell>Wave hand</cell><cell>1</cell></row><row><cell>Ballet dancing</cell><cell>3</cell><cell>Fall</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>F. Liu et al. / Computer Vision and Image Understanding 92 (2003) 265-284</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the CVIU editor and reviewers for their constructive comments, Jun Yang for his help to polish the presentation. This work is supported by 973 Program (No. 2002CB312101), the National Natural Science Foundation of China (No. 60272031), Zhejiang Provincial Natural Science Foundation of China (ZD0212), and Doctorate Research Foundation of the State Education Commission of China (No. 20010335049).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Animation from observation: motion capture and motion editing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page" from="51" to="55" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motion capture assisted animation: texturing and synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 2002</title>
		<meeting>SIGGRAPH 2002<address><addrLine>San Antonio, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Motion editing with spacetime constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, 1997 Symposium on Interactive 3D Graphics</title>
		<meeting>1997 Symposium on Interactive 3D Graphics<address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constraint-based motion adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Litwinowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual. Comput. Animat</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="94" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical approach to interactive motion editing for human-like figures</title>
		<author>
			<persName><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Yong</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 99</title>
		<meeting>SIGGRAPH 99<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retargeting motion to new characters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 98</title>
		<meeting>SIGGRAPH 98<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motion signal processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruderlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95<address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Yunhe Pan, A hybrid motion data manipulation: wavelet based motion processing and spacetime rectification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, IEEE Pacific Rim Conference on Multimedia</title>
		<meeting>IEEE Pacific Rim Conference on Multimedia<address><addrLine>Hsinchu, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="743" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><forename type="middle">S</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 2002</title>
		<meeting>SIGGRAPH 2002<address><addrLine>San Antonio, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Motion texture: a two-level statistical model for character synthesis</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 2002</title>
		<meeting>SIGGRAPH 2002<address><addrLine>San Antonio, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SIGGRAPH 2002</title>
		<meeting>SIGGRAPH 2002<address><addrLine>San Antonio, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Keyblock: an approach for content-based image retrieval</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aibing</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM Multimedia 2000</title>
		<meeting>ACM Multimedia 2000<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Content-based classification, search and retrieval of audio</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keislar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wheaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia Mag</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="27" to="36" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Query by image and video content: the QBIC system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gorkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An integrated system for content-based video retrieval and browsing, Pattern Recogn</title>
		<author>
			<persName><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="643" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A magnifier tool for video data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM Human Computer Interface</title>
		<meeting>ACM Human Computer Interface<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An intuitive and efficient access interface to real-time incoming video based on automatic indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tonomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM Multimedia</title>
		<meeting>ACM Multimedia<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Key frame selection by motion analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ICASSPÕ96</title>
		<meeting>ICASSPÕ96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1228" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic key-frame selection for content-based video indexing and access</title>
		<author>
			<persName><forename type="first">T</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shih-Ping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3972</biblScope>
			<biblScope unit="page" from="554" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video parsing, retrieval and browsing: an integrated and content-based solution</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jiang Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM Multimedia</title>
		<meeting>ACM Multimedia<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-based video abstraction for video surveillance systems</title>
		<author>
			<persName><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1128" to="1138" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stochastic framework for optimal key frame extraction from MPEG video databases</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="3" to="24" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evolving video skims into useful multimedia abstractions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Roy</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM CHIÕ98 Conference on Human Factors in Computing Systems</title>
		<meeting>ACM CHIÕ98 Conference on Human Factors in Computing Systems<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object-based video abstraction using cluster analysis</title>
		<author>
			<persName><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ICIP2001</title>
		<meeting>ICIP2001<address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="657" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object-based indexing of MPEG-4 compressed video</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gunsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, SPIE-3024</title>
		<meeting>SPIE-3024<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-02">February 1997</date>
			<biblScope unit="page" from="953" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic key video object plane selection using the shape information in the MPEG-4 compressed domain</title>
		<author>
			<persName><forename type="first">B</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kossentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="129" to="138" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An integrated scheme for object-based video abstraction</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Ick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM Multimedia 2000</title>
		<meeting>ACM Multimedia 2000<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="303" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and robust moving object segmentation in video sequences</title>
		<author>
			<persName><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ICIPÕ99, Kobe, Japan</title>
		<meeting>ICIPÕ99, Kobe, Japan</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="131" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and automatic video object segmentation and tracking for content-based applications</title>
		<author>
			<persName><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical video summarization</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Ratakonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sezan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crinon</forename><forename type="middle">J</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><surname>Regis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3653</biblScope>
			<biblScope unit="page" from="1531" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A fuzzy video content representation for video summarization and content-based retrieval, Signal Process</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kollias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1049" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Grassia</surname></persName>
		</author>
		<title level="m">Motion editing: mathematical foundations, in course: Motion Editing: Principles, Practice, and Promise, SIGGRAPH 2000</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pattern recognition</title>
		<author>
			<persName><forename type="first">Zhaoqi</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuegong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Tsinghua University Press</publisher>
			<biblScope unit="page" from="241" to="244" />
			<pubPlace>Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive key frame extraction using unsupervised clustering</title>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, IEEE ICIPÕ98</title>
		<meeting>IEEE ICIPÕ98<address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="886" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Key-frame extraction and shot retrieval using nearest feature line (NFL)</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, International Workshop on Multimedia Information Retrieval, in conjunction with ACM Multimedia Conference</title>
		<meeting>International Workshop on Multimedia Information Retrieval, in conjunction with ACM Multimedia Conference<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic programming optimization for online signature verification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Claesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, International Conference on Document Analysis and Recognition</title>
		<meeting>International Conference on Document Analysis and Recognition<address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive on-line handwriting recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Tappert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, International Conference on Pattern Recognition</title>
		<meeting>International Conference on Pattern Recognition<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="1004" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
