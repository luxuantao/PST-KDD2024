<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hand Pose Estimation from Local Surface Normals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chengde</forename><surname>Wan</surname></persName>
							<email>wanc@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angela</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>D-ITET</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">VISICS</orgName>
								<orgName type="institution" key="instit2">ESAT, K.U. Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hand Pose Estimation from Local Surface Normals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96E2C52AD54322D9AFC2F998A5DE709E</idno>
					<idno type="DOI">10.1007/978-3-319-46487-9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a hierarchical regression framework for estimating hand joint positions from single depth images based on local surface normals. The hierarchical regression follows the tree structured topology of hand from wrist to finger tips. We propose a conditional regression forest, i.e. the Frame Conditioned Regression Forest (FCRF) which uses a new normal difference feature. At each stage of the regression, the frame of reference is established from either the local surface normal or previously estimated hand joints. By making the regression with respect to the local frame, the pose estimation is more robust to rigid transformations. We also introduce a new efficient approximation to estimate surface normals. We verify the effectiveness of our method by conducting experiments on two challenging real-world datasets and show consistent improvements over previous discriminative pose estimation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the problem of 3D hand pose estimation from single depth images. Hand pose estimation has important applications in human-computer interaction (HCI) and augmented reality (AR). Estimating the freely moving hand has several challenges including large viewpoint variance, finger similarity and self occlusion and versatile and rapid finger articulation.</p><p>Methods for hand pose estimation from depth generally fall into two camps. The first is frame-to-frame model based tracking <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Model-based tracking approaches can be highly accurate if given enough computational resources for the optimization. The second camp, where our work also falls, is single frame discriminative pose estimation <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. These methods are less accurate than modelbased trackers but much faster and are targeted towards real-time performance without GPUs. Model-based tracking and discriminative pose estimation are complementary to each other and there have been notable hybrid methods <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> which try to maintain the advantages of both camps.</p><p>Earlier methods for discriminative hand pose estimation tried to estimate all joints directly <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> though such approaches tend to fail with dramatic viewpoint changes and extreme articulations. Following the lead of several notable methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">10]</ref>, we cast pose estimation as a hierarchical regression problem.</p><p>The idea is to start with easier parent parts such as the wrist or palm, and then tackle subsequent and more difficult children parts such as the fingers.</p><p>The assumption is that the children parts, once conditioned on the parents, will exhibit less variance and simplify the learning task. Furthermore, by constraining the underlying graphical model to follow the tree-structured topology of the hand, hierarchical regression implicitly captures the skeleton constraints and therefore shares some advantages of model-based tracking that are otherwise not present when directly estimating all joints independently. Our framework starts with estimating the surface normals of given point clouds. The normal direction establishes the local reference frames used in later conditional regression and serves as features. We then apply our Frame Conditioned Regression Forest (FCRF) to hierarchically regress hand joints down from the wrist to the finger tips. At each stage, the frame of reference is established based on previously estimated local surface normal or joint positions. The regression forest considers offsets between input points and joints of interest with respect to the local reference frame and also conditions the feature with respect to these local frames. Our use of conditioned features is inspired by <ref type="bibr" target="#b5">[6]</ref>, though we consider angular differences between local surface normals, which is far more robust to rigid transformations than the original depth difference feature.</p><p>Our proposed method has the following contributions:</p><p>1. We are the first to incorporate local surface normals for pose estimation.</p><p>Unlike previous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> based on global geometry, ours is based on local geometry. To this end, we propose an extremely efficient normal estimation method based on regression trees adapted to handle unit vector distributions, different from vector space properties. 2. We extend the commonly used depth difference feature <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> to an angular difference feature between two normal directions. Our normal difference feature is highly robust to 3D rigid transformation. In particular, the feature is invariant to in-plane rotations, which means we can dispense with data augmentation and have more efficient training and testing routines. 3. We propose a flexible conditional regression framework, encoding all previously estimated information as a part of the local reference frame. This includes local point properties such as the normal direction and global properties such as the estimated joint position.</p><p>We validate our method on two real-world challenging hand pose estimation datasets, ICVL <ref type="bibr" target="#b6">[7]</ref> and MSRA <ref type="bibr" target="#b5">[6]</ref>. On ICVL, we achieve the state-of-art performance against all previous discriminative based methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> with a large margin. On MSRA, our method is on-par with the state-of-art methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> at the threshold of 40 mm, and with some minor modifications outperforms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>We limit our discussion to the most relevant issues and works, and refer readers to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> for more comprehensive reviews on hand pose estimation in general.</p><p>Hierarchical Regression. Several methods have adopted some form of hierarchical treatment of the pose estimation problem. For example, in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>, the hand is first classified into several classes according to posture or viewpoint; further pose estimation is then conditioned on such initial class. Obviously, such an approach cannot generalize to unseen postures and viewpoints.</p><p>Other works <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> hierarchically follow the tree-structured hand topology. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, data points are recursively partitioned into subsets and only corresponding subsets of points are considered for subsequent joint estimation. In <ref type="bibr" target="#b9">[10]</ref>, estimated parent joints are used as inputs for regressing children joints; a final energy minimization is applied to refine the estimation. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, predictions are made based on previously estimated reference frames. Our work is similar in spirit to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, as we also make estimations based on reference frames. However, unlike <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, we utilize the normal direction to establish the reference frame and take local point properties into consideration. Further explanations on the differences between our work and <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> are given in Sects. 3.2 and 4.</p><p>Viewpoint Handling. The free moving hand can exhibit large viewpoint changes and a variety of techniques have been proposed to handle these. For example, <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> discretize viewpoints into multiple classes and estimate pose in the viewspecific classes. Unfortunately, these methods may introduce quantization errors and cannot generalize to unseen viewpoints. In <ref type="bibr" target="#b8">[9]</ref>, the regression for hand pose is conditioned on an estimated in-plane rotation angle. This is extended in <ref type="bibr" target="#b5">[6]</ref>, which regresses the pose residual iteratively, conditioned on the estimated 3D pose at each iteration. Such a method is highly sensitive to the pose initialization and may get trapped in local minima.</p><p>Point Cloud Features. Depth difference features are widely used together with random forests in body pose <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and hand pose <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> estimation. Depth differences, however, ignore many local geometric properties of the point cloud, e.g. local surface normals and curvatures, and are not robust to rigid transformations and sensor noise.</p><p>In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> geodesic extreme points such as finger tip candidates are used to guide later estimation. Rusu et al. <ref type="bibr" target="#b22">[23]</ref> proposed a histogram feature describing different local properties. Inspired by <ref type="bibr" target="#b22">[23]</ref>, we establish local Darboux frames and using angular differences as feature values, but unlike <ref type="bibr" target="#b22">[23]</ref>, our features are based on random offsets and retain the efficiency of <ref type="bibr" target="#b16">[17]</ref>. Most recently, convolutional neural networks (CNNs) have been used to automatically learn point cloud features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Due to the heavy computational burden, CNNs can still not be used in real-time without a GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Random Normal Difference Feature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random Difference Features</head><p>One of the most commonly used features in depth-based pose estimation frameworks, for both body pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and hand pose estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, is the random depth difference feature <ref type="bibr" target="#b16">[17]</ref>. Formally, the random difference feature f I for point p i ∈ R 3 from depth map I is defined as follows,</p><formula xml:id="formula_0">f I (p i , δ 1 , δ 2 ) = Δ(φ I (r(p i , δ 1 )), φ I (r(p i , δ 2 ))),<label>(1)</label></formula><p>where δ j ∈ R 3 , j = {1, 2} is a random offset, r(p i , δ j ) ∈ R 3 calculates a random position given point p i and offset δ j . φ I (q) is the local feature map for position q ∈ R 3 on the point cloud and Δ(•, •) returns the local feature difference. In the case of random depth difference features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref>, φ I is the recorded depth, though the same formalism applies for other features. Random difference features are well suited for random forest frameworks; the many possible combinations of offsets perfectly utilize their feature selection and generalization power. In addition, every dimension of the feature is calculated independently, which gives rise to parallelization schemes and allows for both temporal and spatial efficiency in training and testing. One of the main drawbacks of the depth-difference feature, however, is its inability to cope with transformations. Since random offsets in r(p i , δ 1 ) are determined either w.r.t. the camera frame <ref type="bibr" target="#b16">[17]</ref> or to a globally estimated frame <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, the depth difference for the same offset can vary widely under out of plane rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pose Conditioned Random Normal Difference Feature</head><p>Surface normals are an important local feature for many point-cloud based applications such as registration <ref type="bibr" target="#b22">[23]</ref> and object detection <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. Surface normals would seem a good cue for hand pose estimation too, since the direction of the surface helps to establish the local reference frame, as will be described in Sect. 4. For two given points, the angular difference between their normal directions remains unchanged after rigid transformations. Hence, we propose a pose-conditioned normal difference feature which is highly robust towards 3D rigid transformations.</p><p>To make random features invariant to 3D rigid transformations i.e.,</p><formula xml:id="formula_1">f I (p i , δ 1 , δ 2 ) = f I (p i , δ 1 , δ 2 ),<label>(2)</label></formula><p>where I and p i ∈ R 3 are the depth map and point position after transformation, it is necessary to satisfy the following two conditions:</p><formula xml:id="formula_2">i The random offset generator r(•, •) should be invariant to rigid transformations, i.e. T (r(p i , δ j )) = r(T (p i ), T (δ j )),<label>(3)</label></formula><p>where T (q) = R•q+t is the rigid transformation with R ∈ SO(3)<ref type="foot" target="#foot_0">1</ref> and t as its rotation and translation respectively. This condition is equivalent to guaranteeing that the relative position between p i and r(p i , δ j ) remains unchanged after transformation, i.e., T</p><formula xml:id="formula_3">(p i -r(p i , δ j )) = T (p i ) -r(T (p i ), T (δ j )). ii The feature difference Δ(•, •) should be invariant to rigid transformation, i.e. Δ(φ I (q 1 ), φ I (q 2 )) = Δ(φ I (q 1 ), φ I (q 2 )),<label>(4)</label></formula><p>where q j = T (q j ), j ∈ {1, 2} is the transformed offset position.</p><p>To meet condition i, we extend the random position calculation r(p i , δ j ) as</p><formula xml:id="formula_4">r(p i , δ j , R i ) = p i + R i • δ j , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where R i ∈ SO(3) is a latent variable representing the pose of local reference frame Sect.4. For any rigid transformation T = R p 0 1 , Eq. 5 satisfies condition</p><formula xml:id="formula_6">i iff R i = RR i , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where R i and R i are the estimated latent variable before and after rigid transformation respectively. In comparison to <ref type="bibr" target="#b5">[6]</ref>, which also uses a latent variable R, the R is estimated globally and therefore can be sensitive to the initialization. For us, the local Darboux frame is established through the local surface normal direction (see Sect. 5) and has no such sensitivity.</p><p>To meet condition ii, given the random positions q 1 and q 2 , we use the direction of the normal vector as our local feature map. The feature difference is cast as the angle between two normals, i.e. Δ(φ I ( q 1 ), φ I ( q 2 )) = n( q 1 ) • n( q 2 ), <ref type="bibr" target="#b6">(7)</ref> where q denotes the 2D projection of the random position onto the image plane, since the input 2.5D point cloud is indexed by the 2D projection coordinates. n(•) ∈ R 3 denotes the corresponding normal vector. Since the angle between two normal vectors remains unchanged under a rigid transformation for any two given surface points, our feature also fulfills condition ii. In comparison, the depth difference feature, as used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref>, does not fulfill this condition.</p><p>Our proposed normal difference feature can be computed based on any surface normal estimate. We describe a conventional method based on eigenvalue decomposition in Sect. 3.3 and then propose an efficient approximation alternative in Sect. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Surface Normal Estimation Based on Eigenvalue Decomposition</head><p>For an input 2.5D point cloud, we distinguish between inner points that lie inside the point cloud and edge points on the silhouette of the point cloud. For edge points, normal estimation degenerates to 2D curve normal estimation since the normal direction is constrained to lie in the image plane.</p><p>For inner points, the local surface can be approximated by the kneighbourhood surface direction <ref type="bibr" target="#b25">[26]</ref>. The eigenvector corresponding to the smallest eigenvalue of the neighbourhood covariance matrix can be considered the normal direction. The sign of the normal direction is further constrained to be the same as the projection ray. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Surface Normal Regression with Random Forests</head><p>Estimating the normal at every inner point in the point cloud can become very computationally expensive, with an eigenvalue decomposition per point. Alternatively, we can take advantage of the efficiency of random forests and regress an approximate normal direction. Directly regressing the normal vectors in vector space does not maintain unit length so we parameterize the normal vector with spherical coordinates (θ, ϕ) where θ and ϕ are the polar and azimuth angles, resp. θ and ϕ are independent and can be regressed separately. We model the distribution of a set of angular values S = {θ 1 , • • • θ n } as a Von Mises Distribution, which is the circular analogue of the normal distribution. The distribution is expressed as</p><formula xml:id="formula_8">p V M (θ i |μ, κ) = e κcos(θi-μ) 2πI 0 (κ) , (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where μ is the mean of the angles, κ is inversely related to the variance of the approximated Gaussian and I 0 (κ) is the modified Bessel function of order 0. To estimate the mean and variance of the distribution, we first define</p><formula xml:id="formula_10">C = i cos(θ i ), S = i sin(θ i ), R = (C 2 + S 2 ) 1 2 . (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>Then the maximum likelihood estimates of μ and κ are</p><formula xml:id="formula_12">μ = atan2(S, C) and R = I 1 (κ) I 0 (κ) . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>During training, each split node is set by maximizing the information gain as</p><formula xml:id="formula_14">I = H(S) - i∈{L,R} |S i | |S| H(S i ),<label>(11)</label></formula><p>where the entropy of the Von Mises Distribution is defined as</p><formula xml:id="formula_15">H(S) = ln(2πI 0 (κ)) -κ I 1 (κ) I 0 (κ) . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>The training procedure for the random forest that estimates the normal is almost identical to <ref type="bibr" target="#b16">[17]</ref> with the exception that the random offsets are restricted to lie within the region of the same k-nearest neighbourhood that was used for the eigenvalue decomposition based normal estimation in Sect. 3.3. The mean of the angular values propagated to each leaf node is selected as the leaf node's prediction value. In practice, to make the normal regression even more efficient, we combine the estimation of θ and ϕ into one forest by regressing the θ in the first 10 layers and ϕ in the later 10 layers, rather than estimating them independently.</p><p>Since the random offset is limited to a small area, which restricts the randomness of the trees, we find that the average error between approximated and true normal directions only goes up from ∼12 • to ∼14 • when decreasing the number of trees from 10 to 1. As the normal difference feature is not sensitive to such minor errors, we use only 1 tree for all experiments in this paper. The proposed method is extremely efficient; normals for input point clouds can be estimated in ∼4 ms on average, compared to ∼14 ms based on eigenvalue decompositions on the same machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Frame Conditioned Regression Forest</head><p>We formulate hand joint estimation as a regression problem by regressing the 3D offsets between an input 3D point and a subset of hand joints. Directly regressing all joints of the hand at once, as has been done in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> is difficult, given the highly articulated nature of the hand and the many ambiguities due to occlusions and local self-similarities of the fingers. Instead, we prefer to solve for the joints in a hierarchical manner, as state-of-the-art results <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> have demonstrated the benefits of solving the pose progressively down the kinematic chain.</p><p>In this section, we propose a conditional regression forest, namely the Frame Conditioned Regression Forest (FCRF) which performs regression conditioned on information estimated in the previous stages. The hand joints are regressed hierarchically by following the kinematic chain from wrist down to the finger joints. At each stage, we first estimate the reference frame based on results of previous stages and then regress the hand joints relevant to that stage with the FCRF.</p><p>There are three main benefits to using the FCRF. First of all, offsets between input points and finger joints are transformed into the local reference frame. This reduces the variance of the offsets and simplifies the training. It also implicitly incorporates skeleton constraints provided by the training data. Secondly, the related normal difference feature, as described in Sect. 3, is conditioned on the estimated reference frame and makes the joint regression highly robust to 3D rigid transformations. Finally, FCRF is in-plane rotation-invariant, and does not need manually generated in-plane rotated training samples for training as in <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, so the training time and resulting tree size can be reduced significantly.</p><p>Specifically, given input point p i ∈ R 3 from the point cloud, the FCRF for the j th stage solves the following regression</p><formula xml:id="formula_17">O (i) j = r j (I, C (i) j ),<label>(13)</label></formula><p>where</p><formula xml:id="formula_18">O (i)</formula><p>j ∈ R 3×n is the offsets between input point p i and the n joints to be estimated in j th stage, I denotes the input depth map and C (i) j ∈ SE(3) is the corresponding local frame. We define the position of the input point p i as the origin of the local reference frame, i.e.</p><formula xml:id="formula_19">C (i) j = R (i) j p i 0 1 , (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where R (i) j = x, y, z ∈ SO(3) is a rotation matrix representing the frame pose, and x, y, z ∈ R 3 are the corresponding axis directions. Both R i and p i are defined with respect to the camera frame.</p><p>The regression r j (I, C (i) j ) is done by a random forest. During training, o ik ∈ R 3 , the offset between point p i and joint l k to be estimated, is first rotated to the local reference frame C (i) j as o ik , i.e.</p><formula xml:id="formula_21">o ik = (R (i) j ) T • o ik . (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>The distribution of offset samples are modeled as a uni-modal Gaussian as in <ref type="bibr" target="#b16">[17]</ref>. For each split node of the tree, the normal difference feature which results in the maximum information gain from a random subset of features is selected. For each leaf node, mean-shift searching <ref type="bibr" target="#b28">[29]</ref> is performed and the maximal density point is used as the leaf prediction value. During testing, given the estimated local frame C (i) j , the resulting offset o ik can be re-projected to the camera frame as</p><formula xml:id="formula_23">o ik = (R (i) j ) • o ik . (<label>16</label></formula><formula xml:id="formula_24">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hierarchical Hand Joint Regression</head><p>In this section, we detail the design of reference frames used by FCRFs in every stage, given the estimated local surface normal and the parent joint positions from previous stages. Free moving hand pose estimation faces two major challenges, i.e., large variations of viewpoints, and self-similarities of different fingers. We decompose hand pose estimation into two sub-problems that explicitly tackle these two challenges: first, we estimate the reference frame of the palm and second, we estimate the finger joints. In Sects. 5.1 and 5.2 the palm estimation is introduced by first estimating the wrist joint (palm position) followed by MCP joints (Fig. <ref type="figure" target="#fig_0">1(a)</ref>) for all 5 fingers (palm pose), in which the Darboux frame for every input point is established by taking the estimated wrist joint as reference point. In Sects. 5.3 and 5.4 the joints for each finger are estimated, progressively conditioned on the previously estimated joint position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Wrist Estimation</head><p>We consider only edge points on the hand silhouette as inputs for estimating the wrist joint. Our rationale is that we cannot find unique reference frames for non-edge points, since knowing only the direction of the normal, i.e. the z-axis, is insufficient to uniquely determine the x-and y-axis on the tangent plane. We assume orthographic projection for the point cloud, i.e. the tangent plane of edge point is orthogonal to the image plane, then the local reference frame of edge point p i can be defined uniquely as follows,</p><formula xml:id="formula_25">x (i) wrist = n, y (i) wrist = z (i) wrist × x (i) wrist , z (i) wrist = n i , (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>where n is the image plane normal direction, n i is the normal to the silhouette at point i. The resulting local reference frame is not only invariant to 2D rotations in the image plane but to some degree also robust to out-of-plane rotations, provided that the hand silhouette does not change too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metacarpophalangeal (MCP) Joint Estimation</head><p>Given the estimated wrist point position as a reference point, we assume its relevant position under the local frame</p><formula xml:id="formula_27">C (i)</formula><p>MCP is unchanged then the local reference frame for point p i is established as follows</p><formula xml:id="formula_28">x (i) MCP = y (i) MCP × z (i) MCP , y (i) MCP = n i × (p wrist -p i ) n i × (p wrist -p i ) 2 , z (i) MCP = n i , (<label>18</label></formula><formula xml:id="formula_29">)</formula><p>where the z-axis of the local reference frame is defined as the normal direction n i , and the y-axis is defined by taking the wrist location p wrist as a reference point. The MCP joints from all five fingers are then regressed simultaneously, i.e., O</p><p>MCP ∈ R 3×5 using our previously defined FCRF. The estimated MCP joints are then replaced by the transformed MCP position from a template palm to reduce the accumulated regression error. We first find a closed form solution of the palm pose using a variation of ICP <ref type="bibr" target="#b29">[30]</ref>. The palm pose matrix R palm 's y-axis is defined as the direction from the wrist to the MCP joint of the middle finger, the z-axis is defined as the palm normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Proximal Interphalangeal (PIP) Joint Estimation</head><p>In the estimation of the PIP joint for finger k, all input reference frames share the same pose as the rotated palm reference frame as follows,</p><formula xml:id="formula_31">C (i) P IP k = Rot k (R palm ) p i 0 1 , (<label>19</label></formula><formula xml:id="formula_32">)</formula><p>where Rot k (•) is an in-plane rotation to align the reference frame's y-axis to the k-th finger's empirical direction Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Given the local self-similarity between fingers, it can be easy to double-count evidence. To avoid this, we adopt two simple measures. First, we use points only from the neighbourhood of the parent MCP joint as input for regressing each PIP joint, since these points best describe the local surface distortion raised by the parent joint articulation <ref type="bibr" target="#b30">[31]</ref>. Secondly we limit the offset of the FCRF to lie along the direction of the finger to maintain robustness to noisy observations from nearby fingers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Distal Interphalangeal Joint (DIP) and Finger Tip (TIP) Estimation</head><p>The ways to estimate DIP and TIP joints are identical, since their parents are both 1-DoF joints. The local reference frame for each joint is defined as follows</p><formula xml:id="formula_33">x l = z palm × y l , y l = p(l) -g(l), z l = x l × y l , (<label>20</label></formula><formula xml:id="formula_34">)</formula><p>where z palm is the normal direction of palm, p(l) and g(l) ∈ R 3 denote the parent and grandparent joint of l respectively. To avoid double counting of local evidence, we adopt the same techniques as in Sect. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We apply our proposed hand estimation method to two publicly available realworld hand pose estimation datasets: ICVL <ref type="bibr" target="#b6">[7]</ref> and MSRA <ref type="bibr" target="#b5">[6]</ref>. The performance of our method is evaluated both quantitatively and qualitatively. For quantitative evaluation, two evaluation metrics, per-joint error (in mm) averaged over all frames and percentage of frames in which all joints are below a threshold <ref type="bibr" target="#b17">[18]</ref>, are used. We show qualitative results in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>All experiments are conducted on an Intel 3.40 GHz I7 machine and the average run time is 29.4 fps or 33.9 ms per image. The maximum depth of all the trees is set to 20. The number of trees for all joint regression forests are set to 5 and 1 for normal estimation (see Sect. 3.4).</p><p>To highlight the effectiveness of our proposed normal difference feature, we first apply our frame conditioned regression forests with the same hierarchical structure but based on the standard depth difference feature <ref type="bibr" target="#b16">[17]</ref>. We denote this variation using the depth difference feature as our baseline method. It should be noted that the baseline does depend on normal estimation for the establishment of the local wrist frame. We also compare to methods directly regressing the wrist and MCP joint positions without establishing the frame <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> or based on an initial guess and the subsequent, iterative regression of the error <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ICVL Hand Dataset</head><p>The ICVL hand dataset <ref type="bibr" target="#b6">[7]</ref> has 20 K images from 10 subjects and an additional 160K in-plane rotated images for training. Since our method is invariant to inplane rotation, we train with only the initial 20 K. The test set is composed of 2 sequences with continuous finger movement but little viewpoint change.</p><p>We compare our method (both the baseline and the version with the normal difference feature) against the state-of-art methods Latent Regression Forest (LRF) <ref type="bibr" target="#b6">[7]</ref>, Segmentation Index Points (SIP) <ref type="bibr" target="#b7">[8]</ref>, and Cascaded Regression (Cascaded) <ref type="bibr" target="#b5">[6]</ref>. Figure <ref type="figure" target="#fig_2">3(a)-(c)</ref> shows that both variations of our proposed method outperform LRF <ref type="bibr" target="#b6">[7]</ref> and SIP <ref type="bibr" target="#b7">[8]</ref> by a large margin on both test sequences. In comparison to the Cascaded method of <ref type="bibr" target="#b5">[6]</ref>, shown in Fig. <ref type="figure" target="#fig_2">3(c</ref>), our baseline is comparable or better at almost all allowed distances, while the variation with the normal difference feature boosts performance by another 5-10%. As shown in Fig. <ref type="figure" target="#fig_2">3(d)</ref>, our method significantly out-performs <ref type="bibr" target="#b6">[7]</ref>, and it outperforms <ref type="bibr" target="#b5">[6]</ref> by ∼2 mm in terms of the mean error. These results confirm that conditioning finger localization on the wrist pose, as we have done and as is done in <ref type="bibr" target="#b5">[6]</ref>, can significantly boost accuracy. Furthermore, our proposed normal difference feature is able to better handle 3D rigid transformations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MSRA Hand Dataset</head><p>The MSRA hand dataset <ref type="bibr" target="#b5">[6]</ref> contains 76.5 K images from 9 subjects with 17 hand gestures. We use a leave-one-subject-out training/testing split and average the results over the 9 subjects. This dataset is complementary to the ICVL dataset since it has much larger viewpoint changes but limited finger movements. The sparse gesture set does not come close to reflecting the range of hand gestures in real-world HCI applications and as such, is not suitable for evaluating how well a method can generalize towards unseen hand gestures. Yet, this dataset is very good for evaluating the robustness of pose estimation methods to 3D rigid transformations; for HCI applications, this offers flexibility for mounting the camera in different locations.</p><p>As is shown in Fig. <ref type="figure" target="#fig_3">4</ref>(a)-(b), using the normal difference exhibits less variance to viewpoint changes than using the depth difference. This is more prominent in the pitch angle due to the elongated hand shape. For a given pair of points, their depth difference exhibits larger variation w.r.t. pitch angle viewpoint changes. Nevertheless, the performance of the normal difference does decrease under large viewpoint changes. We attribute this to the errors in surface normal estimation due to point cloud noise and to the fact that a 2.5D point cloud only partially represents the full 3D surface. We compare our proposed method against the state-of-the-art Cascaded Regression (Cascaded) <ref type="bibr" target="#b5">[6]</ref> and the Collaborative Filtering (Filtering) <ref type="bibr" target="#b12">[13]</ref> approaches. Above an allowed distance of 40 mm to the ground truth, our approach is comparable to the others. Below the 40 mm threshold, our baseline and the normal difference feature version has around ∼14 % less frames than competing methods. We attribute the difference to the fact that both the Cascaded and the Filtering approach consider the finger as a whole, in the former case for regression, and in the latter as a nearest neighbour search from the training data. While our method generalizes well to unseen finger poses by regressing each finger joint progressively, it is unable to utilize the sparse (albeit similar to testing) set of finger poses in the training. Nevertheless, in an HCI scenario, a user is often asked to first make calibration poses which are important to improve accuracy. As such, we propose two minor modifications to make more comparable evaluations.</p><p>For the first modification, we first regress the palm pose, normalize the hand, and then classify the hand pose as a whole. Based on the classification, we assign a corresponding pose sampled from the training set, transformed accordingly to the palm pose. This modification, which we denoted as pose classification is similar to Filtering <ref type="bibr" target="#b12">[13]</ref> as both methods consider the hand as a whole. By classifying the 17 gesture classes as provided by the MSRA dataset we now outperform <ref type="bibr" target="#b12">[13]</ref> over a large interval of thresholds larger than 22 mm. We attribute the increased performance to our accurate estimate of the palm pose.</p><p>For the second modification, we regress each finger (i.e. the 3 finger joints PIP, DIP, TIP) as a whole given the estimated palm pose. This is similar in spirit to the regression strategy in <ref type="bibr" target="#b5">[6]</ref> which takes each finger as a whole. Our method outperforms <ref type="bibr" target="#b5">[6]</ref> by ∼5 % in the 25-30 mm threshold interval. We attribute this improvement to our palm pose estimation scheme which avoids sensitivity to initialization <ref type="bibr" target="#b5">[6]</ref>.</p><p>Despite our modifications, it should be noted that regressing the finger as a whole cannot generalize to unseen joint angle combinations for one finger, which is usually the case in real-world HCI scenarios, e.g. grasping a virtual object, where one finger may exhibit various joint angle combinations according to the shapes of different objects. However, the two strategies are complementary, i.e. regressing finger joints progressively can generalize to unseen finger poses while regressing the finger as a whole can capture finger joint correlations in training samples. Given enough computational resources, the two strategies can be performed in parallel, with the best estimation being selected according to an energy function as in model-based tracking. We leave this as our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented a hierarchical regression scheme conditioned on local reference frames. We utilize the local surface normal both as a feature map for regression and to establish the local reference frame. We also proposed an efficient surface normal estimation method based on random forests. Our system shows excellent results on two real-world, challenging datasets and is either comparable or outperforms state-of-the-art methods in hand pose estimation. The surface normal serves as an important local property of the point cloud. While random forests are an efficient way of estimating the normal, they are only one way and other methods could be developed to be more accurate. Given the success of using surface normals in our work, we expect that there will be benefits for model-based tracking as well.</p><p>In our current work, we follow a tree-structured model of the hand. Given the flexibility of our proposed conditioned regression forest, one can also perform hierarchical regressions with other underlying graphical models. With different models, one could take into account the correlations and dependencies between fingers, especially with respect to grasping objects. We leave this as future work in improving the current system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework. (a) Shows the hand skeleton model used in our work. (b) Sketches our hierarchical regression framework, with each successive stage denoted by a shaded box. We first estimate a reference frame for every input point encoding all information from previous stages and use that reference frame as input to estimate the location of children joints. The sub-figure around the depth map amplifies a local region from the initial depth map and shows the corresponding frame for a specific point. To save space, only thumb and index finger cases are shown and finger tip points (TIP) estimation is omitted as it is identical to that of DIP (best viewed in colour) (Color figure online)</figDesc><graphic coords="2,166.11,56.24,177.01,140.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Estimated surface normal. From (a) to (c) the x, y, z-axis coordinate of the normal vector, resp. The first row is the regressed surface normal by the random forest and the second row is estimated by PCA. (Best viewed in colour) (Color figure online)</figDesc><graphic coords="6,59.97,451.85,325.63,94.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Quantitative evaluation on ICVL dataset. From (a) to (c), success rates over different thresholds on sequence A, B and both respectively. (d) pre-joint average error on both sequences (R:root, T:tip)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Quantitative evaluation on MSRA dataset. (a) to (b): average joint error as a function of pitch and yaw angle of the palm pose with respect to camera frame; (c) success rates over different thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of successful and failed pose estimates on the ICVL [7] and the MSRA [6] dataset. Failures are due to extreme view point, wrongly estimated normal direction, etc. (best viewed in colour) (Color figure online)</figDesc><graphic coords="14,92.85,137.78,288.46,107.05" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Readers unfamiliar with Lie group matrix notations may refer to http://ethaneade. com/lie.pdf for more details. In short, SO(3) represents a 3D rotation while SE(3) represents a 3D rigid transformation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors gratefully acknowledge support by EU Framework Seven project ReMeDi (grant 610902) and Chinese Scholarship Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Efficient model-based 3D tracking of hand articulations using kinect</title>
		<author>
			<persName><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evolutionary quasi-random search for hand articulations tracking</title>
		<author>
			<persName><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Realtime and robust hand tracking from depth</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiaoou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-based hand pose estimation via spatial-temporal hand parsing and 3D fingertip localization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="837" to="848" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-based hand tracking using a hierarchical Bayesian filter</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thayananthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Latent regression forest: structured estimation of 3D articulated hand posture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D hand pose estimation using randomized decision forest with segmentation index points</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient hand pose estimation from a single depth image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opening the black box: hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">robust, and flexible realtime hand tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Leichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CHI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Accurate</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and robust hand tracking using detection-guided optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A collaborative filtering approach to real-time hand pose estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion capture of hands in action using discriminative salient points</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part VI</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="640" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hand pose estimation and hand shape classification using multi-layered randomized decision forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kıraç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part VI</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="852" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hybrid one-shot 3D hand pose estimation by exploiting uncertainties</title>
		<author>
			<persName><forename type="first">G</forename><surname>Poier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roditakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The vitruvian manifold: inferring dense correspondences for one-shot human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth-based hand pose estimation: data, methods, and challenges</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vision-based hand pose estimation: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time articulated hand pose estimation using semi-supervised transductive regression forests</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards 3D point cloud based object maps for household environments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dolha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TRAS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014, Part VII</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A generalisation of the ICP algorithm for articulated bodies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kovalsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<title level="m">Learning 3D articulation and deformation using 2D images</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
