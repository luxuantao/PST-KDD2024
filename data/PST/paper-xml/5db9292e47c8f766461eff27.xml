<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
							<email>songjiang@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01DAC001EEBF83F249E17D17D8DCEACF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) have recently received wide attentions, due to their successful applications in different graph tasks and different domains. Training GCNs for a large graph, however, is still a challenge. Original full-batch GCN training requires calculating the representation of all the nodes in the graph per GCN layer, which brings in high computation and memory costs. To alleviate this issue, several sampling-based methods have been proposed to train GCNs on a subset of nodes. Among them, the node-wise neighbor-sampling method recursively samples a fixed number of neighbor nodes, and thus its computation cost suffers from exponential growing neighbor size; while the layer-wise importance-sampling method discards the neighbor-dependent constraints, and thus the nodes sampled across layer suffer from sparse connection problem. To deal with the above two problems, we propose a new effective sampling algorithm called LAyer-Dependent ImportancE Sampling (LADIES) 2 . Based on the sampled nodes in the upper layer, LADIES selects their neighborhood nodes, constructs a bipartite subgraph and computes the importance probability accordingly. Then, it samples a fixed number of nodes by the calculated probability, and recursively conducts such procedure per layer to construct the whole computation graph. We prove theoretically and experimentally, that our proposed sampling algorithm outperforms the previous sampling methods in terms of both time and memory costs. Furthermore, LADIES is shown to have better generalization accuracy than original full-batch GCN, due to its stochastic nature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph convolutional networks (GCNs) recently proposed by Kipf et al. <ref type="bibr" target="#b11">[12]</ref> adopt the concept of convolution filter into graph domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. For a given node, a GCN layer aggregates the embeddings of its neighbors from the previous layer, followed by a non-linear transformation, to obtain an updated contextualized node representation. Similar to the convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref> in the computer vision domain, by stacking multiple GCN layers, each node representation can utilize a wide receptive field from both its immediate and distant neighbors, which intuitively increases the model capacity.</p><p>Despite the success of GCNs in many graph-related applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>, training a deep GCN for large-scale graphs remains a big challenge. Unlike tokens in a paragraph or pixels in an image, which normally have limited length or size, graph data in practice can be extremely large. For example, Facebook social network in 2019 contains 2.7 billion users 3 . Such a large-scale graph is impossible to be handled using full-batch GCN training, which takes all the nodes into one batch to update parameters. However, conducting mini-batch GCN training is non-trivial, as the nodes Black nodes denote the nodes in the upper layer, blue nodes in the dashed circle are their neighbors, and node with the red frame is the sampled nodes. As is shown in the figure, GraphSAGE will redundantly sample a neighboring node twice, denoted by the red triangle, while FastGCN will sample nodes outside of the neighborhood. Our proposed LADIES can avoid these two problems.</p><p>in a graph are closely coupled and correlated. In particular, in GCNs, the embedding of a given node depends recursively on all its neighbors' embeddings, and such computation dependency grows exponentially with respect to the number of layers. Therefore, training deep and large GCNs is still a challenge, which prevents their application to many large-scale graphs, such as social networks <ref type="bibr" target="#b11">[12]</ref>, recommender systems <ref type="bibr" target="#b16">[17]</ref>, and knowledge graphs <ref type="bibr" target="#b14">[15]</ref>.</p><p>To alleviate the previous issues, researchers have proposed sampling-based methods to train GCNs based on mini-batch of nodes, which only aggregate the embeddings of a sampled subset of neighbors of each node in the mini-batch. Among them, one direction is to use a node-wise neighbor-sampling method. For example, GraphSAGE <ref type="bibr" target="#b8">[9]</ref> calculates each node embedding by leveraging only a fixed number of uniformly sampled neighbors. Although this kind of approaches reduces the computation cost in each aggregation operation, the total cost can still be large. As is pointed out in <ref type="bibr" target="#b10">[11]</ref>, the recursive nature of node-wise sampling brings in redundancy for calculating embeddings. Even if two nodes share the same sampled neighbor, the embedding of this neighbor has to be calculated twice. Such redundant calculation will be exaggerated exponentially when the number of layers increases. Following this line of research as well as reducing the computation redundancy, a series of work was proposed to reduce the size of sampled neighbors. VR-GCN <ref type="bibr" target="#b2">[3]</ref> proposes to leverage variance reduction techniques to improve the sample complexity. Cluster-GCN <ref type="bibr" target="#b4">[5]</ref> considers restricting the sampled neighbors within some dense subgraphs, which are identified by a graph clustering algorithm before the training of GCN. However, these methods still cannot well address the issue of redundant computations, which may become worse when training very deep and large GCNs.</p><p>Another direction uses a layer-wise importance-sampling method. For example, FastGCN <ref type="bibr" target="#b3">[4]</ref> calculates a sampling probability based on the degree of each node, and samples a fixed number of nodes for each layer accordingly. Then, it only uses the sampled nodes to build a much smaller sampled adjacency matrix, and thus the computation cost is reduced. Ideally, the sampling probability is calculated to reduce the estimation variance in FastGCN <ref type="bibr" target="#b3">[4]</ref>, and guarantee fast and stable convergence. However, since the sampling probability is independent for each layer, the sampled nodes from two consecutive layers are not necessarily connected. Therefore, the sampled adjacency matrix can be extremely sparse, and may even have all-zero rows, meaning some nodes are disconnected. Such a sparse connection problem incurs an inaccurate computation graph and further deteriorates the training and generalization performance of FastGCN.</p><p>Based on the pros and cons of the aforementioned sampling approaches, we conclude that an ideal sampling method should have the following features: 1) layer-wise, thus the neighbor nodes can be taken into account together to calculate next layers' embeddings without redundancy; 2) neighbordependent, thus the sampled adjacency matrix is dense without losing much information for training; 3) the importance sampling method should be adopted to reduce the sampling variance and accelerate convergence. To this end, we propose a new efficient sampling algorithm called LAyer-Dependent ImportancE-Sampling (LADIES) <ref type="foot" target="#foot_0">4</ref> , which fulfills all the above features.</p><p>The procedure of LADIES is described as below: (1) For each current layer (l), based on the nodes sampled in the upper layer (l + 1), it picks all their neighbors in the graph, and constructs a bipartite graph among the nodes between the two layers.</p><p>(2) Then it calculates the sampling probability according to the degree of nodes in the current layer, with the purpose to reduce sampling variance.</p><p>(3) Next, it samples a fixed number of nodes based on this probability. (4) Finally, it constructs the sampled adjacency matrix between layers and conducts training and inference, where row-wise normalization is applied to all sampled adjacency matrices to stabilize training. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, our proposed sampling strategy can avoid two pitfalls faced by existing two sampling strategies: layer-wise structure avoids exponential expansion of receptive field; layer-dependent importance sampling guarantees the sampled adjacency matrix to be dense such that the connectivity between nodes in two adjacent layers can be well maintained.</p><p>We highlight our contributions as follows:</p><p>• We propose LAyer-Dependent ImportancE Sampling (LADIES) for training deep and large graph convolutional networks, which is built upon a novel layer-dependent sampling scheme to avoid exponential expansion of receptive field as well as guarantee the connectivity of the sampled adjacency matrix.</p><p>• We prove theoretically that the proposed algorithm achieves significantly better memory and time complexities compared with node-wise sampling methods including GraphSage <ref type="bibr" target="#b8">[9]</ref> and VR-GCN <ref type="bibr" target="#b2">[3]</ref>, and has a dramatically smaller variance compared with FastGCN <ref type="bibr" target="#b3">[4]</ref>.</p><p>• We evaluate the performance of the proposed LADIES algorithm on benchmark datasets and demonstrate its superior performance in terms of both running time and test accuracy. Moreover, we show that LADIES achieves high accuracy with an extremely small sample size (e.g., 256 for a graph with 233k nodes), which enables the use of LADIES for training very deep and large GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we review GCNs and several state-of-the-art sampling-based training algorithms, including full-batch, node-wise neighbor sampling methods, and layer-wise importance sampling methods. We summarize the notation used in this paper in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing GCN Training Algorithms</head><p>Full-batch GCN. When given an undirected graph G, with P defined in Table <ref type="table">1</ref>, the l-th GCN layer is defined as:</p><formula xml:id="formula_0">Z (l) = PH (l 1) W (l 1)</formula><p>, H (l 1) = (Z (l 1) ),</p><p>Considering a node-level downstream task, given training dataset {(x i , y i )} vi2Vs , the weight matrices W (l) will be learned by minimizing the loss function:</p><formula xml:id="formula_2">L = 1 |Vs| P vi2Vs `(y i , z<label>(L)</label></formula><p>i ), where `(•, •) is a specified loss function, z L i denotes the output of GCN corresponding to the vertex v i , and V S denotes the training node set. Gradient descent algorithm is utilized for the full-batch optimization, where the gradient is computed over all the nodes as</p><formula xml:id="formula_3">1 |Vs| P vi2V S r`(y i , z (L) i ).</formula><p>When the graph is large and dense, full-batch GCN's memory and time costs can be very expensive while computing such gradient, since during both backward and forward propagation process it requires to store and aggregate embeddings for all nodes across all layer. Also, since each epoch only updates parameters once, the convergence would be very slow.</p><p>One solution to address issues of full-batch training is to sample a a mini-batch of labeled nodes V B 2 V S , and compute the mini-batch stochastic gradient 1</p><formula xml:id="formula_4">|V B | P vi2V B r`(y i , z L i ).</formula><p>This can help reduce computation cost to some extent, but to calculate the representation of each output node, we still need to consider a large-receptive field, due to the dependency of the nodes in the graph.</p><p>Node-wise Neighbor Sampling Algorithms. GraphSAGE <ref type="bibr" target="#b8">[9]</ref> proposed to reduce receptive field size by neighbor sampling (NS). For each node in l-th GCN layer, NS randomly samples s node of its neighbors at the (l 1)-th GCN layer and formulate an unbiased estimator P(l 1)</p><p>H (l 1) to approximate PH (l 1) in graph convolution layer:</p><formula xml:id="formula_5">P(l 1) i,j = ⇢ |N (vi)| s node P i,j , v j 2 N (l 1) (v i ); 0, otherwise.<label>(2)</label></formula><p>Table <ref type="table">1</ref>: Summary of Notations</p><formula xml:id="formula_6">G = (V, A), kAk0</formula><p>G denotes a graph consist of a set of nodes V with node number |V|, A is the adjacency matrix, and kAk0 denotes the number of non-zero entries in A. Mi,⇤, M⇤,j Mi,j Mi,⇤ is the i-th row of matrix M, M⇤,j is the j-th column of matrix M, and Mi,j is the entry at the position (i, j) of matrix M. Ã, D, P Ã = A + I, D is a diagonal matrix satisfying Di,i = P j Ãi,j, and</p><formula xml:id="formula_7">P = D 1 2 Ã D 1 2</formula><p>is the normalized Laplacian matrix. l, (•), H (l) , Z (l) , W (l) l denotes the GCN layer index, (•) denotes the activation function, H (l) = (Z (l) ) denotes the embedding matrix at layer l,</p><formula xml:id="formula_8">H (0) = X, Z (l) = PH (l 1) W (l 1)</formula><p>is the intermediate embedding matrix, and W (l) denotes the trainable weight matrix at layer l. L, K L is the total number of layers in GCN, and K is the dimension of embedding vectors (for simplicity, assume it is the same across all layers). b, s node , s layer For batch-wise sampling, b denotes the batch size, s node is the number of sampled neighbors per node for node-wise sampling, and s layer is number of sampled nodes per layer for layer-wise sampling.</p><p>Table <ref type="table">2</ref>: Summary of Complexity and Variance<ref type="foot" target="#foot_1">6</ref> . Here denotes the upper bound of the `2 norm of embedding vector, denotes the bound of the norm of the difference between the embedding and its history, D denotes the average degree, b denotes the batch size, and V (b) denotes the average number of nodes which are connected to the nodes sampled in the training batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Complexity Time Complexity Variance</head><p>Full-Batch <ref type="bibr" target="#b11">[12]</ref> O(L|V|K</p><formula xml:id="formula_9">+ LK 2 ) O(LkAk0K + L|V|K 2 ) 0 GraphSage [9] O(bKs L 1 node + LK 2 ) O(bKs L node + bK 2 s L 1 node ) O D kPk 2 F /(|V|s node ) VR-GCN [3] O(L|V|K + LK 2 ) O(bDKs L 1 node + bK 2 s L 1 node ) O D kPk 2 F /(|V|s node ) FastGCN [4] O(LKs layer + LK 2 ) O(LKs 2 layer + LK 2 s layer ) O( kPk 2 F /s layer ) LADIES O(LKs layer + LK 2 ) O(LKs 2 layer + LK 2 s layer ) O kPk 2 F V (b)/(|V|s layer )</formula><p>where N (v i ) and N (l 1) (v i ) are the full and sampled neighbor sets of node v i for (l 1)-th GCN layer respectively.</p><p>VR-GCN <ref type="bibr" target="#b2">[3]</ref> is another neighbor sampling work. It proposed to utilize historical activations to reduce the variance of the estimator under the same sample strategy as GraphSAGE. Though successfully achieved comparable convergence rate, the memory complexity is higher and the time complexity remains the same. Though NS scheme alleviates the memory bottleneck of GCN, there exists redundant computation under NS since the embedding calculation for each node in the same layer is independent, thus the complexity grows exponentially when the number of layers increases.</p><p>Layer-wise Importance Sampling Algorithms. FastGCN <ref type="bibr" target="#b3">[4]</ref> proposed a more advanced layer-wise importance sampling (IS) scheme aiming to solve the scalability issue as well as reducing the variance. IS conducts sampling for each layer with a degree-based sampling probability. The approximation of the i-th row of (PH (l 1) ) with s layer samples v j1 , . . . , v js l per layer can be estimated as:</p><formula xml:id="formula_10">(PH (l 1) ) i,⇤ ' 1 s layer s layer X k=1,vj k ⇠q(v) P i,j k H (l 1) j k ,⇤ /q(v j k )<label>(3)</label></formula><p>where q(v) is the distribution over V, q(v j ) = kP ⇤,j k 2 2 /kPk 2 F is the probability assigned to node v j . Though IS outperforms uniform sampling and the layer-wise sampling successfully reduced both time and memory complexities, however, this sampling strategy has a major limitation: since the sampling operation is conducted independently at each layer, FastGCN cannot guarantee connectivity between sampled nodes at different layers, which incurs large variance of the approximate embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complexity and Variance Comparison</head><p>We compare each method's memory, time complexity, and variance with that of our proposed LADIES algorithm in Table <ref type="table">2</ref>.</p><p>Complexity. We now compare the complexity of the proposed LADIES algorithm and existing sampling-based GCN training algorithms. The complexity for all methods are summarized in Table <ref type="table">2</ref>, detailed calculation could be found in Appendix A. Compare with full-batch GCN, the time and memory complexities of LADIES do not depend on the total number of nodes and edges, thus our algorithm does not have scalability issue on large and dense graphs. Unlike NS based methods including GraphSAGE and VR-GCN, LADIES is not sensitive to the number of layers and will not suffer exponential growth in complexity, therefore it can perform well when the neural network goes deeper. Compared to layer-wise importance sampling proposed in FastGCN, it maintains the same complexity while obtaining a better convergence guarantee as analyzed in the next paragraph. In fact, in order to guarantee good performance, our method requires a much smaller sample size than that of FastGCN, thus the time and memory burden is much lighter. Therefore, our proposed LADIES algorithm can achieve the best time and memory complexities and is applicable to training very deep and large graph convolution networks.</p><p>Variance. We compare the variance of our algorithm with existing sampling-based algorithms. To simplify the analysis, when evaluating the variance we only consider two-layer GCN. The results are summarized in Table <ref type="table">2</ref>. We defer the detailed calculation to Appendix B. Compared with FastGCN, our variance result is strictly better since V (b)  |V|, where V (b) denotes the number of nodes which are connected to the nodes sampled in the training batch. Moreover, V (b) scales with b, which implies that our method can be even better when using a small batch size. Compared with node-wise sampling, consider the same sample size, i.e., s layer = bs node . Ignoring the same factors, the variance of LADIES is in the order of O( V (b)/b) and the variance of GraphSAGE is O(D), where D denotes the average degree. Based on the definition of V (b), we strictly have V (b)  O(bD) since there is no redundant node been calculated in V (b). Therefore our method is also strictly better than GraphSAGE especially when the graph is dense, i.e., many neighbors can be shared. The variance of VR-GCN resembles that of GraphSAGE but relies on the difference between the embedding and its history, which is not directly comparable to our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LADIES: LAyer-Dependent ImportancE Sampling</head><p>We present our method, LADIES, in this section. As illustrated in previous sections, for node-wise sampling methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref>, one has to sample a certain number of nodes in the neighbor set of all sampled nodes in the current layer, then the number of nodes that are selected to build the computation graph is exponentially large with the number of hidden layers, which further slows down the training process of GCNs. For the existing layer-wise sampling scheme <ref type="bibr" target="#b3">[4]</ref>, it is inefficient when the graph is sparse, since some nodes may have no neighbor been sampled during the sampling process, which results in meaningless zero activations <ref type="bibr" target="#b2">[3]</ref>.</p><p>To address the aforementioned drawbacks and weaknesses of existing sampling-based methods for GCN training, we propose our training algorithms that can achieve good convergence performance as well as reduce sampling complexity. In the following, we are going to illustrate our method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Independent Layer-wise Sampling</head><p>We first revisit the independent layer-wise sampling scheme for building the computation graph of GCN training. Recall in the forward process of GCN (1), the matrix production PH (l 1) can be regarded as the embedding aggregation process. Then, the layer-wise sampling methods aim to approximate the intermediate embedding matrix Z (l) by only sampling a subset of nodes at the (l 1)-th layer and aggregating their embeddings for approximately estimating the embeddings at the l-th layer. Mathematically, similar to (3), let S (l 1) with |S l 1 | = s l 1 be the set of sampled nodes at the (l 1)-th layer, we can approximate PH (l 1) as</p><formula xml:id="formula_11">PH (l 1) ' 1 s l 1 X k2S (l 1) 1 p (l 1) k P ⇤,k H (l 1) k,⇤ ,</formula><p>where we adopt non-uniformly sampling scheme by assigning probabilities p } k2S l 1 be the indices of sampled nodes at the l 1-th layer, the estimator of PH (l 1) can be formulated as</p><formula xml:id="formula_12">PH (l 1)</formula><p>' PS (l 1)</p><formula xml:id="formula_13">H (l 1) ,<label>(4)</label></formula><p>where S (l 1)</p><p>2 R |V|⇥|V| is a diagonal matrix with only nonzero diagnoal matrix, defined by</p><formula xml:id="formula_14">S (l 1)</formula><p>s,s = 8 &lt; :</p><formula xml:id="formula_15">1 s l 1 p (l 1) i (l 1) k , s = i (l 1) k ; 0, otherwise.<label>(5)</label></formula><p>It can be verified that kS (l 1) k 0 = s l 1 and E[S (l 1) ] = I. Assuming at the l-th and l 1-th layers the sets of sampled nodes are determined. Then let {i (l) k } k2S l and {i (l 1) k } k2S l 1 denote the indices of sampled nodes at these two layers, and define the row selection matrix Q (l)</p><p>2 R s l ⇥|V| as:</p><formula xml:id="formula_16">Q (l) k,s = ⇢ 1 (k, s) = (k, i (l) k ); 0, otherwise,<label>(6)</label></formula><p>the forward process of GCN with layer-wise sampling can be approximated by Z(l) = P(l 1) H(l 1) W (l 1) , H(l 1) = ( Z(l 1) ),</p><p>where Z(l) 2 R s l ⇥d denotes the approximated intermediate embeddings for sampled nodes at the l-th layer and P(l 1) = Q (l) PS (l 1) Q (l 1)&gt; 2 R s l ⇥s l 1 serves as a modified Laplacian matrix, and can be also regarded as a sampled bipartite graph after certain rescaling. Since typically we have s l , s l 1 ⌧ |V|, the sampled graph is dramatically smaller than the entire one, thus the computation cost can be significantly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer-dependent Importance Sampling</head><p>However, independently conducting layer-wise sampling at different layers is not efficient since the sampled bipartite graph may still be sparse and even have all-zero rows. This further results in very poor performance and require us to sample lots of nodes in order to guarantee convergence throughout the GCN training. To alleviate this issue, we propose to apply neighbor-dependent sampling that can leverage the dependency between layers which further leads to dense computation graph. Specifically, our layer-dependent sampling mechanism is designed in a top-down manner, i.e., the sampled nodes at the l-th layer are generated depending on the sampled nodes that have been generated in all upper layers. Note that for each node we only need to aggregate the embeddings from its neighbor nodes in the previous layer. Thus, at one particular layer, we only need to generate samples from the union of neighbors of the nodes we have sampled in the upper layer, which is defined by</p><formula xml:id="formula_18">V (l 1) = [ vi2S l N (v i )</formula><p>where S l is the set of nodes we have sampled at the l-th layer and N (v i ) denotes the neighbors set of node v i . Therefore during the sampling process, we only assign probability to nodes in V (l 1) , denoted by {p (l 1) i } vi2V (l 1) . Similar to FastGCN <ref type="bibr" target="#b3">[4]</ref>, we apply importance sampling to reduce the variance. However, we have no information regarding the activation matrix H (l 1) when characterizing the samples at the (l 1)-th layer. Therefore, we resort to a important sampling scheme which only relies on the matrices Q (l) and P. Specifically, we define the importance probabilities as:</p><formula xml:id="formula_19">p (l 1) i = kQ (l) P ⇤,i k 2 2 kQ (l) Pk 2 F .<label>(8)</label></formula><p>Evidently, if v i / 2 V (l 1) , we have kQ (l) P ⇤,i k 2 2 = 0, which implies that p</p><formula xml:id="formula_20">(l 1) i = 0. Then, let {i (l 1) k } v k 2S l 1</formula><p>be the indices of sampled nodes at the (l 1)-th layer based on the importance probabilities computed in <ref type="bibr" target="#b7">(8)</ref>, we can also define the random diagonal matrix S (l 1) according to <ref type="bibr" target="#b4">(5)</ref>, and formulate the same forward process of GCN as in <ref type="bibr" target="#b6">(7)</ref> but with a different modified Laplacian matrix P(l 1) = Q (l) PS (l 1) Q (l 1)&gt; 2 R s l ⇥s l 1 . The computation of P(l 1) can be very efficient since it only involves sparse matrix productions. Here the major difference between our sampling method and independent layer-wise sampling is the different constructions of matrix S (l 1) . In our sampling mechanism, we have E[S (l 1) ] = L (l 1) , where L (l 1) is a diagonal matrix with</p><formula xml:id="formula_21">L (l 1) s,s = ⇢ 1 s 2 V (l 1) 0, otherwise.<label>(9)</label></formula><p>Since kL (l 1) k 0 = |V (l 1) | ⌧ |V|, applying independent sampling method results in the fact that many nodes are in the set V/V (l 1) , which has no contribution to the construction of computation graph. In contrast, we only sample nodes from V (l 1) which can guarantee more connections between the sampled nodes at l-th and (l 1)-th layers, and further leads to a dense computation graph between these two layers. Get layer-dependent laplacian matrix Q (l) P. Calculate sampling probability for each node using p</p><formula xml:id="formula_22">(l 1) i kQ (l) P⇤,ik 2 2 kQ (l) Pk 2 F</formula><p>, and organize them into a random diagonal matrix S (l 1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Sample n nodes in l 1 layer using p (l 1) . The sampled nodes formulate Q (l 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Reconstruct sampled laplacian matrix between sampled nodes in layer l 1 and l by P(l 1) Q (l) PS (l 1) Q (l 1)&gt; , then normalize it by P(l) D 1 P(l) P(l) .</p><p>6: end for 7: return Modified Laplacian Matrices { P(l) } l=1,...,L and Sampled Node at Input Layer Q 0 ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Normalization</head><p>Note that for original GCN, the Laplacian matrix P is obtained by normalizing the matrix I + A. Such normalization operation is crucial since it is able to maintain the scale of embeddings in the forward process and avoid exploding/vanishing gradient. However, the modified Laplacian matrix { P(l) } l=1,...,L may not be able to achieve this, especially when L is large, because its maximum singular values can be very large without sufficient samples. Therefore, motivated by <ref type="bibr" target="#b11">[12]</ref>, we propose to normalize P(l) such that the sum of all rows are 1, i.e., we have</p><formula xml:id="formula_23">P(l) D 1 P(l) P(l) ,</formula><p>where D P(l) 2 R s l+1 ⇥s l+1 is a diagonal matrix with each diagonal entry to be the sum of the corresponding row in P(l) . Now, we can leverage the modified Laplacian matrices { P} l=1,...,L to build the whole computation graph. We formally summarize the proposed algorithm in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments to evaluate LADIES for training deep GCNs on different node classification datasets, including Cora, Citeseer, Pubmed <ref type="bibr" target="#b15">[16]</ref> and Reddit <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>We compare LADIES with the original GCN (full-batch) , GraphSage (neighborhood sampling) and FastGCN (important sampling). We modified the PyTorch implementation of GCN<ref type="foot" target="#foot_2">7</ref> to add our LADIES sampling mechanism. To make the fair comparison only on the sampling part, we also choose the online PyTorch implementation of all these baselines released by their authors and use the same training code for all the methods. By default, we train 5-layer GCNs with hidden state dimension as 256, using the four methods. We choose 5 neighbors to be sampled for GraphSage, 64 and 512 nodes to be sampled for both FastGCN and LADIES per layer. We update the model with a batch size of 512 and ADAM optimizer with a learning rate of 0.001.</p><p>For all the methods and datasets, we conduct training for 10 times and take the mean and variance of the evaluation results. Each time we stop training when the validation accuracy doesn't increase a threshold (0.01) for 200 batches, and choose the model with the highest validation accuracy as the convergence point. We use the following metrics to evaluate the effectiveness of sampling methods:</p><p>• Accuracy: The micro F1-score of the test data at the convergence point. We calculate it using the full-batch version to get the most accurate inference (only care about training).</p><p>• Total Running Time (s): The total training time (exclude validation) before convergence point.</p><p>• Memory (MB): Total memory costs of model parameters and all hidden representations of a batch.</p><p>• Batch Time and Num: Time cost to run a batch and the total batch number before convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>As is shown in Table <ref type="table">4</ref>, our proposed LADIES can achieve the highest accuracy score among all the methods, using a small sampling number. One surprising thing is that the sampling-based method can achieve higher accuracy than the Full-Batch version, and in some cases using a smaller sampling number can lead to better accuracy (though it may take longer batches to converge). This is probably because the graph data is incomplete and noisy, and the stochastic nature of the sampling method can bring in regularization for training a more robust GCN with better generalization accuracy <ref type="bibr" target="#b9">[10]</ref>.</p><p>Another observation is that no matter the total size of the graph, LADIES a small sample number (64) can still converge well, and sometimes even better than a larger sample number (512). This indicates that LADIES is scalable to training very large and deep GCN while maintaining high accuracy.</p><p>As a comparison, FastGCN with 64 and 512 sampled nodes can lead to similar accuracy for small graphs (Cora). But for bigger graphs as Citeseer, Pubmed, and Reddit, it cannot converge to a good point, partly because of the computation graph sparsity issue. For a fair comparison, we choose a higher sampling number for FastGCN on these big graphs. For example, in Reddit, we choose 8192 nodes to be sampled, and FastGCN in such cases can converge to a similar accurate result compared with LADIES, but obviously taking more memory and time cost. GraphSage with 5 nodes to be sampled takes far more memory and time cost because of the redundancy problem we've discussed, and its uniform sampling makes it fail to converge well and fast compared to importance sampling.</p><p>In addition to the above comparison, we show that our proposed LADIES can converge pretty well with a much fewer sampling number. As is shown in Figure <ref type="figure" target="#fig_3">2</ref>, when we choose the sampling number as small as 16, the algorithm can already converge to the best result, with low time and memory cost. This implies that although in Table <ref type="table">4</ref>, we choose sample number as 64 and 512 for a fair comparison, but actually, the performance of our method can be further enhanced with a smaller sampling number.</p><p>Furthermore, we show that the stochastic nature of LADIES can help to achieve better generalization accuracy than original full-batch GCN. We plot the F1-score of both full-batch GCN and LADIES  on the PubMed dataset for 300 epochs without early stop in Figure <ref type="figure" target="#fig_4">3</ref>. From Figure <ref type="figure" target="#fig_4">3</ref>(a), we can see that, full-batch GCN can achieve higher F1-Score than LADIES on the training set. Nevertheless, on the validation and test datasets, we can see from Figures <ref type="figure" target="#fig_4">3(b</ref>) and 3(c) that LADIES can achieve significantly higher F1-Score than full-batch GCN. This suggests that LADIES has better generalization performance than full-batch GCN. The reason is: real graphs are often noisy and incomplete. Full-batch GCN uses the entire graph in the training phase, which can cause overfitting to the noise. In sharp contrast, LADIES employs stochastic sampling to use partial information of the graph and therefore can mitigate the noise of the graph and avoid overfitting to the training data. At a high-level, the sampling scheme adopted in LADIES shares a similar spirit as bagging and bootstrap <ref type="bibr" target="#b0">[1]</ref>, which is known to improve the generalization performance of machine learning predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a new algorithm namely LADIES for training deep and large GCNs. The crucial ingredient of our algorithm is layer-dependent importance sampling, which can both ensure dense computation graph as well as avoid drastic expansion of receptive field. Theoretically, we show that LADIES enjoys significantly lower memory cost, time complexity and estimation variance, compared with existing GCN training methods including GraphSAGE and FastGCN. Experimental results demonstrate that LADIES can achieve the best test accuracy with much lower computational time and memory cost on benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An illustration of the sampling process of GraphSage, FastGCN, and our proposed LADIES. Black nodes denote the nodes in the upper layer, blue nodes in the dashed circle are their neighbors, and node with the red frame is the sampled nodes. As is shown in the figure, GraphSAGE will redundantly sample a neighboring node twice, denoted by the red triangle, while FastGCN will sample nodes outside of the neighborhood. Our proposed LADIES can avoid these two problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>in V. Then the corresponding discount weights are {1/(s l 1 p (l 1) i )} i=1,...,|V| . Then let {i (l 1) k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Sampling Procedure of LADIES Require: Normalized Laplacian Matrix P; Batch Size b, Sample Number n; 1: Randomly sample a batch of b output nodes as Q L 2: for l = L to 1 do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F1-score, total time and memory cost at convergence point for training PubMed, when we choose different sampling numbers of our method. Results show that LADIES can achieve good generalization accuracy (F1-score = 77.6) even with a small sampling number as 16, while FastGCN cannot converge (only reach F1-score = 39.3) with a large sampling number as 512.</figDesc><graphic coords="9,108.73,72.00,392.04,76.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experiments on the PubMed dataset. We plot the F1-score of both full-batch GCN and LADIES every epoch on (a) Training dataset (b) Validation dataset and (c) Testing dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 :</head><label>3</label><figDesc>Comparison of LADIES with original GCN (Full-Batch), GraphSage (Neighborhood Sampling) and FastGCN (Important Sampling), in terms of accuracy, time, memory and convergence. Training 5-layer GCNs on different node classification datasets (node number is below the dataset name). Results show that LADIES can achieve the best accuracy with lower time and memory cost.</figDesc><table><row><cell>Dataset</cell><cell>Sample Method</cell><cell cols="4">F1-Score(%) Total Time(s) Mem(MB) Batch Time(ms)</cell><cell>Batch Num</cell></row><row><cell>Cora (2708)</cell><cell>Full-Batch GraphSage (5) FastGCN (64) FastGCN (512) LADIES (64) LADIES (512)</cell><cell>76.5 ± 1.4 75.2 ± 1.5 25.1 ± 8.4 78.0 ± 2.1 77.6 ±1.4 78.3 ±1.6</cell><cell>1.19 ± 0.82 6.77 ± 4.94 0.55 ± 0.65 4.70 ± 1.35 4.19 ± 1.16 0.72 ± 0.39</cell><cell>30.72 471.39 3.13 7.33 3.13 7.35</cell><cell>15.75 ± 0.52 78.42 ± 0.87 9.22 ± 0.20 10.08 ± 0.29 9.68 ± 0.48 9.77 ± 0.28</cell><cell>80.8 ± 51.7 65.2 ± 52.1 63.2 ± 71.2 487 ± 147 436 ± 118.4 75.6 ± 37.0</cell></row><row><cell>Citeseer (3327)</cell><cell>Full-Batch GraphSage (5) FastGCN (64) FastGCN (512) FastGCN (1024) LADIES (64) LADIES (512)</cell><cell>62.3 ± 3.1 59.4 ± 0.9 19.2 ± 2.7 44.6 ± 10.8 63.5 ± 1.8 65.0 ± .1.4 64.3 ± 2.4</cell><cell>0.61 ± 0.70 4.51 ± 3.68 0.53 ± 0.48 4.34 ± 1.73 2.24 ± 1.01 2.17 ± 0.65 0.41 ± 0.22</cell><cell>68.13 595.71 5.89 13.97 23.24 5.89 13.92</cell><cell>15.77 ± 0.58 53.14 ± 1.90 8.88 ± 0.40 10.41 ± 0.51 10.54 ± 0.27 9.60 ± 0.39 10.32 ± 0.23</cell><cell>40.6 ± 22.8 57.2 ± 42.1 64.0 ± 57.0 386 ± 167 223 ± 98.6 232 ± 66.8 37.6 ± 11.9</cell></row><row><cell>Pubmed (19717)</cell><cell>Full-Batch GraphSage (5) FastGCN (64) FastGCN (512) FastGCN (8192) LADIES (64) LADIES (512)</cell><cell>71.9 ± 1.9 70.1 ± 1.4 38.5 ± 6.9 39.3 ± 9.2 74.4 ± 0.8 76.8 ± 0.8 75.9 ± 1.1</cell><cell>4.80 ± 1.53 5.53 ± 2.57 0.40 ± 0.69 0.44 ± 0.61 3.47 ± 1.16 2.57 ± 0.72 2.27 ± 1.17</cell><cell>137.93 453.58 1.92 4.53 49.41 1.92 4.39</cell><cell>44.69 ± 0.57 44.73 ± 0.30 7.42 ± 0.16 10.06 ± 0.41 17.84 ± 0.33 9.43 ± 0.47 10.43 ± 0.36</cell><cell>102 ± 33.4 74.8 ± 31.7 58.8 ± 94.8 44.8 ± 55.0 195 ± 56.9 277 ± 82.2 245 ± 84.5</cell></row><row><cell>Reddit (232965)</cell><cell>Full-Batch GraphSage (5) FastGCN (64) FastGCN (512) FastGCN (8192) LADIES (64) LADIES (512)</cell><cell>91.6 ± 1.6 92.1 ± 1.1 27.8 ± 12.6 17.5 ± 16.7 89.5 ± 1.2 83.5 ± 0.9 92.8 ± 1.6</cell><cell cols="2">474.3 ± 84.4 2370.48 13.12 ± 2.84 1234.63 2.06 ± 1.29 3.75 0.31 ± 0.41 6.91 5.63 ± 2.12 74.28 5.62 ± 1.58 3.75 6.87 ± 1.17 7.26</cell><cell>1564 ± 3.41 121.47 ± 0.72 7.85 ± 0.72 10.01 ± 0.31 16.57 ± 0.58 9.42 ± 0.48 10.87 ± 0.63</cell><cell>179 ± 75.5 81.5 ± 42.3 57.4 ± 43.7 32.1 ± 72.3 278 ± 51.2 453 ± 88.2 393 ± 74.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>We would like to clarify that the proposed layer-dependent importance sampling is different from "layered importance sampling" proposed in<ref type="bibr" target="#b13">[14]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>For simplicity, when evaluating the variance we only consider two-layer GCN.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>https://github.com/tkipf/pygcn</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their helpful comments. D. Zou and Q. Gu were partially supported by the NSF BIGDATA IIS-1855099, NSF CAREER Award IIS-1906169 and Salesforce Deep Learning Research Award. Z. Hu, Y. Wang, S. Jiang and Y. Sun were partially supported by NSF III-1705169, NSF 1937599, NSF CAREER Award 1741634, and Amazon Research Award. We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>⇤ equal contribution <ref type="bibr" target="#b1">2</ref> codes are avaiable at https://github.com/acbull/LADIES 3 https://zephoria.com/top-15-valuable-facebook-statistics/</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019. 2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wen-Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08">2018. 2018, 3-8 December 2018. 2018</date>
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Layered adaptive importance sampling</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Elvira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Corander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="623" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03">2018. June 3-7, 2018. 2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>ESWC</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-19">2018. August 19-23, 2018. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
