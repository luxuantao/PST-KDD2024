<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-25">25 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
							<email>dominique@invivoai.com</email>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
							<email>pietro.lio@cst.cam.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Petar</forename><surname>Veliƒçkoviƒá</surname></persName>
							<email>petarv@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-25">25 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.05718v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features-which occur regularly in real-world input domains and within the hidden layers of GNNs-and we demonstrate the requirement for multiple aggregation functions in this setting. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a benchmark containing multiple tasks taken from classical graph theory, which demonstrates the capacity of our model. * Equal contribution.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have been an active research field for the last ten years with great advancements in graph representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, it is difficult to understand the effectiveness of new GNNs due to the lack of standardized benchmarks <ref type="bibr" target="#b4">[5]</ref> and of theoretical frameworks for their expressive power.</p><p>In fact, most work in this domain has focused on improving the GNN architectures on a set of graph benchmarks, without evaluating the capacity of their network to properly characterize the graphs' structural properties. Only recently there have been significant studies on the expressive power of various GNN models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. However, these have mainly focused on the capacity of distinguishing different graph topologies, with little work done on understanding their capacity to capture and exploit the underlying features of the graph structure.</p><p>Alternatively, some work focuses on generalizing convolutional neural networks (CNN) to graphs using the spectral domain, as first proposed by Bruna et al. <ref type="bibr" target="#b11">[12]</ref>. To improve the efficiency of the spectral analysis and improve the performance of the models, Chebyshev polynomials were developed <ref type="bibr" target="#b12">[13]</ref> and later generalized into Cayley filters <ref type="bibr" target="#b13">[14]</ref> or replaced by wavelet transforms <ref type="bibr" target="#b14">[15]</ref>. In our work, we look at the capacity of different GNN models to understand certain aspects of the spectral decomposition, namely the graph Laplacian and the spectral radius, as they constitute fundamental aspects of the graphs' spectral properties. Although the spectral properties and filters are not explicitly encoded in our architecture, a powerful enough spatial GNN should still be able to learn them effectively.</p><p>Previous work on tasks taken from classical graph theory focuses on evaluating the performance of GNN models on a single task such as shortest paths <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, graph moments <ref type="bibr" target="#b18">[19]</ref> or trav-elling salesman problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>. Instead, we took a different approach by developing a multi-task benchmark containing problems both on the node level and the graph level. In particular, we look at the ability of each GNN to predict single-source shortest paths, eccentricity, laplacian features, connectivity, diameter and spectral radius. Many of these tasks are based on algorithms using dynamic programming and, therefore, are known to be well suited for GNNs <ref type="bibr" target="#b16">[17]</ref>. We believe this multi-task approach ensures that the GNNs are able to understand multiple properties simultaneously, which is fundamental for solving complex graph problems. Moreover, efficiently sharing parameters between the tasks suggests a deeper understanding of the structural features of the graphs. Furthermore, we explore the generalization ability of the networks by testing on graphs of larger sizes than those present in the training set.</p><p>We hypothesize that the aggregation layers of current GNNs are unable to extract enough information from the nodes' neighbourhoods in a single layer, which limits their expressive power and learning abilities. In fact, recent works show how different aggregators perform better on different tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>, that GNNs do not excel at learning nodes' clustering themselves as they don't properly characterize their neighbourhood <ref type="bibr" target="#b20">[21]</ref>, and that they are unable to reliably find substructures <ref type="bibr" target="#b21">[22]</ref>.</p><p>We first prove mathematically the need for multiple aggregators by proposing a solution for the uncountable multiset injectivity problem introduced by <ref type="bibr" target="#b5">[6]</ref>. Then, we propose the concept of degreescalers as a generalization to the sum aggregation, which allow the network to amplify or attenuate signals based on the degree of each node. Combining the above, we design the proposed Principal Neighbourhood Aggregation (PNA) network and demonstrate empirically that using multiple aggregation strategies concurrently improves the performance of the GNN on graph theory problems.</p><p>Dehmamy et al. <ref type="bibr" target="#b18">[19]</ref> have also found empirically that using multiple aggregators (mean, sum and normalized mean), which extract similar statistics from the input message, improves the performance of GNNs on the task of graph moments. In contrast, our work extends the theoretical framework by deriving the necessity to use complementary aggregators. Accordingly, we propose using different statistical aggregations to allow each node to understand the distribution of the messages it receives, and we generalize the mean aggregation as the first moment of a set of possible n-moment aggregations.</p><p>We present a consistently well-performing and parameter efficient encode-process-decode architecture <ref type="bibr" target="#b22">[23]</ref> for GNNs. This differs from traditional GNNs by allowing a variable number of convolutions, vanquishing the limitation of GNNs to distributed local algorithms <ref type="bibr" target="#b23">[24]</ref> described by <ref type="bibr" target="#b10">[11]</ref>.</p><p>Using this model, we compare the multi-task performances of some of the most diffused models in the literature (GCN <ref type="bibr" target="#b24">[25]</ref>, GAT <ref type="bibr" target="#b25">[26]</ref>, GIN <ref type="bibr" target="#b5">[6]</ref> and MPNN <ref type="bibr" target="#b26">[27]</ref>) with our PNA and a clear hierarchy arises. In particular, we observe that the proposed PNA, formed by the combination of various aggregators and scalers, significantly outperforms these baselines. The fact that this outperformance was consistent along all tasks with all the architectures experimented further supports our hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Principal Neighbourhood Aggregation</head><p>In this section, we first explain the motivation behind using multiple aggregators concurrently. Then, we present the idea of degree-based scalers, linking to the prior related work of GNN expressiveness. Finally, we detail the design of graph convolutional layers which leverage the proposed Principal Neighbourhood Aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proposed aggregators</head><p>One of the main concerns of this manuscript is the ability to understand a one-hop node neighbourhood using a single GNN layer. Doing so will reduce the effects of over-smoothing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> and allow the depth of the network to focus on understanding the interactions of far-away nodes and support more complex latent state.</p><p>Most work in the literature uses only a single aggregation method, with mean, sum and max aggregators being the most used in the state-of-the-art models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref>. In Figure <ref type="figure">1</ref>, we observe how different neighbourhood aggregators fail to discriminate between different messages when using a single GNN layer.</p><p>We formalize our observations in the theorem given below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VS</head><p>Aggregators that fail:</p><formula xml:id="formula_0">0 2 2 2 0 0 ùë¢ 2 ùë¢ 1 v ùë¢ 3 ùë¢ 2 ùë¢ 1 v ùë¢ 3</formula><p>Most state-of-the-art GNNs use only the Mean aggregator. Sum is similar to Mean when the degree is idenÔøΩcal. STD is not used in the GNN literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Min</head><p>Max STD</p><formula xml:id="formula_1">1 4 1 0 3 3 ùë¢ 2 ùë¢ 1 v ùë¢ 3 ùë¢ 2 ùë¢ 1 v ùë¢ 3 VS Mean Min Max STD 0 4 2 2 ùë¢ 2 ùë¢ 1 v ùë¢ 3 ùë¢ 4 VS Mean Min Max STD 4 0 4 0 ùë¢ 2 ùë¢ 1 v ùë¢ 3 ùë¢ 4</formula><p>Figure <ref type="figure">1</ref>: Examples where, for a single GNN layer and continuous input feature spaces, some aggregators fail to differentiate between neighbourhood messages. For these examples and using a single layer, we also observe that the aggregators are complementary: there is always at least one aggregator that can discriminate between the different neighbour messages.</p><p>Theorem 1 (Number of aggregators needed). In order to discriminate between multisets of size n whose underlying set is R, at least n aggregators are needed.</p><p>Proposition 1 (Moments of the multiset). The moments of a multiset (as defined in equation 4) constitute a valid example using n aggregators.</p><p>We prove Theorem 1 in Appendix A and Proposition 1 in Appendix B. Note that unlike Xu et al. <ref type="bibr" target="#b5">[6]</ref>, we consider a continuous input features space; this better represents many real-world tasks where the observed values have uncertainty, and better models the latent node features within a neural network's representations. Using continuous features makes the multiset uncountable, and voids the injectivity proof of the sum aggregation presented by Xu et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>To make further progress, we redefine the concepts of aggregators and scalers. Aggregators are a continuous function of a multiset which compute a statistic on the neighbouring nodes, such as mean, max or standard deviation. The continuity is important with continuous input spaces, as small variations in the input should result in small variations of the aggregators' output. Scalers are applied on the aggregated value and perform either an amplification or an attenuation of the incoming messages, which is dependent on the number of messages being aggregated (usually the node degree). In this framework, we may re-express the sum aggregator as a mean aggregator followed by a linear-degree scaling (see Section 2.2).</p><p>Theorem 1 proves that the number of independent aggregators used is a limiting factor in the expressiveness of GNNs. To empirically demonstrate this, here we leverage four aggregators, namely mean, maximum, minimum and standard deviation. Furthermore, we note that this concept can be generalized to the normalized moment aggregator, which allows for variable numbers of aggregators and extracting advanced distribution information whenever the degree of the graph is high.</p><p>The following subsections will detail the aggregators we leveraged in our architectures. We also provide descriptions of a few additional aggregation functions of interest in Appendix D.</p><p>Mean aggregation ¬µ(X l ) The most common message aggregator in the literature, wherein each node receives a weighted average or weighted sum of its incoming messages. Equation 1 presents, on the left, the general mean equation, and, on the right, the direct neighbour formulation, where X is any multiset, X l are the nodes' features at layer l, N (i) is the neighbourhood of node i</p><formula xml:id="formula_2">and d i = |N (i)|. For clarity we use E[f (X)] where X is a multiset of size d to be defined as E[f (X)] = 1 d x‚ààX f (x). ¬µ(X) = E[X] , ¬µ i (X l ) = 1 d i j‚ààN (i) X l j (1)</formula><p>Maximum and minimum aggregations max(X l ), min(X l ) Also often used in literature, they are very useful for discrete tasks, when extrapolating such tasks to unseen distributions of graphs <ref type="bibr" target="#b15">[16]</ref> and for domains where credit assignment is important. Alternatively, we present the softmax and softmin aggregators in Appendix D, which are differentiable and work for weighted graphs, but don't perform as well on our benchmarks.</p><formula xml:id="formula_3">max i (X l ) = max j‚ààN (i) X l j , min i (X l ) = min j‚ààN (i) X l j (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>Standard deviation aggregation œÉ(X l ) The standard deviation (STD or œÉ) is used to quantify the spread of neighbouring nodes features, such that a node can assess the diversity of the signals it receives. Equation 3 presents, on the left, the standard deviation formulation and, on the right, the STD of a graph-neighbourhood. ReLU is the rectified linear unit used to avoid negative values caused by numerical errors and is a small positive number to ensure œÉ is differentiable.</p><formula xml:id="formula_5">œÉ(X) = E[X 2 ] ‚àí E[X] 2 , œÉ i (X l ) = ReLU Ô£´ Ô£¨ Ô£≠ 1 d i j‚ààN (i) X l j 2 ‚àí Ô£´ Ô£≠ 1 d i j‚ààN (i) X l j Ô£∂ Ô£∏ 2 Ô£∂ Ô£∑ Ô£∏ +<label>(3)</label></formula><p>Normalized moments aggregation M n (X l ) The mean and variance being the first and second central moments of a signal (n = 1, n = 2), additional moments could be useful do better describe the signal, such as the skewness (n = 3) and the kurtosis (n = 4). This becomes more important when the degree of a node is high, because the four previous aggregators are then insufficient to describe the neighbourhood accurately. The central moments normalized by the standard deviation are presented in equation 4, for which we develop a corresponding node aggregator in Equation <ref type="formula">5</ref>, where M n is the desired moment of degree n. A ReLU can once again be used for all even values of n to avoid negative moments caused by numerical errors.</p><formula xml:id="formula_6">M n (X) = E [(X ‚àí ¬µ) n ] œÉ n (4) M n,i (X l ) = œÉ ‚àín i (X l ) n k=0 Ô£Æ Ô£Ø Ô£∞ n k (‚àí1) n‚àík Ô£´ Ô£≠ 1 d i j‚ààN (i) X l j k Ô£∂ Ô£∏ Ô£´ Ô£≠ 1 d i j‚ààN (i) X l j Ô£∂ Ô£∏ n‚àík Ô£π Ô£∫ Ô£ª , n ‚â• 3 (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Degree-based scalers</head><p>Xu et al. <ref type="bibr" target="#b5">[6]</ref> shows that the use of mean and max aggregators by themselves fails to distinguish between neighbourhoods with identical node features' distribution but differing cardinalities and the same applies to the other aggregators described above. They propose the sum aggregator to discriminate between such multisets. Redefining the sum aggregator as the composition of a mean aggregator and a scaling linear to the degree of each node S amp (d) = d, allows us to generalise this property: Theorem 2 (Injective functions on countable multisets). The mean aggregation composed with any scaling linear to an injective function on the neighbourhood size can generate injective functions on bounded multisets of countable elements.</p><p>We formalize and prove Theorem 2 in Appendix C. Thus, the results proven in <ref type="bibr" target="#b5">[6]</ref> about the sum aggregator become a particular case of this theorem, and we can use any kind of injective scaler to discriminate between multisets of various sizes.</p><p>Recent work shows that summation aggregation doesn't generalize well to unseen graphs <ref type="bibr" target="#b15">[16]</ref>, especially when they are of larger size. One reason is that a small change of the degree will cause the message and gradients to be amplified/attenuated exponentially (a linear amplification at each layer will cause an exponential amplification after multiple layers). Although there are different strategies to deal with this problem, we propose using a logarithmic amplification S ‚àù log(d + 1) to reduce this effect. Note that the logarithm is injective for positive values, and d is defined non-negative.</p><p>Another motivation for using logarithmic scalers is to better describe the neighbourhood influence of a given node. Let's suppose we have a social network where nodes A, B and C have respectively 5 million, 1 million and 100 followers. On a linear scale, nodes B and C appear more similar than nodes A and B, however, this does not accurately model their relative influence. Hence, the logarithmic scale discriminates better between messages received by influencer and follower nodes.</p><p>We propose the logarithmic scaler S amp presented in Equation <ref type="formula" target="#formula_7">6</ref>, where Œ¥ is the average amplification in the training set, and d is the degree of the node receiving the message.</p><formula xml:id="formula_7">S amp (d) = log(d + 1) Œ¥ , Œ¥ = 1 |train| i ‚àà train log(d i + 1)<label>(6)</label></formula><p>We may further generalize this scaler in Equation <ref type="formula" target="#formula_8">7</ref>, where Œ± is a variable parameter that is negative for attenuation, positive for amplification or zero for no scaling. Other definitions of S(d) can be used-such as a linear scaling-as long as the function is injective for d &gt; 0.</p><formula xml:id="formula_8">S(d, Œ±) = log(d + 1) Œ¥ Œ± , d &gt; 0, ‚àí1 ‚â§ Œ± ‚â§ 1<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combined aggregation</head><p>Combining the aggregators and scalers presented in previous sections, we now propose the Principal Neighbourhood Aggregation (PNA). The PNA performs a total of twelve operations: four neighbouraggregations with three scalers each, summarized in Equation <ref type="formula">8</ref>. The aggregators are defined in Equations 1-3, while the scalers are defined in Equation <ref type="formula" target="#formula_8">7</ref>, with ‚äó being the tensor product.</p><formula xml:id="formula_9">= I S(D, Œ± = 1) S(D, Œ± = ‚àí1) scalers ‚äó Ô£Æ Ô£Ø Ô£∞ ¬µ œÉ max min Ô£π Ô£∫ Ô£ª aggregators (8)</formula><p>As mentioned earlier, higher degree graphs such as social networks could benefit from further aggregators (e.g. using the moments proposed in Equation <ref type="formula">5</ref>). We insert the PNA operator within the framework of a message passing neural network <ref type="bibr" target="#b26">[27]</ref>, obtaining the following GNN layer:</p><formula xml:id="formula_10">X (t+1) i = U Ô£´ Ô£≠ X (t) i , (j,i)‚ààE M X (t) i , X (t) j Ô£∂ Ô£∏<label>(9)</label></formula><p>where M and U are neural networks (for our benchmarks, a linear layer was enough). Further, U reduces the size of the concatenated message (in space R 13F ) back to R F where F is the dimension of the hidden features in the network. As in the MPNN paper <ref type="bibr" target="#b26">[27]</ref>, we employ multiple towers to improve computational complexity and generalization performance.</p><p>Using twelve operations per kernel will require the usage of additional weights per input feature in the U function, which could seem to be just quantitatively-not qualitatively-more powerful than an ordinary MPNN with a single aggregator <ref type="bibr" target="#b26">[27]</ref>. However, the overall increase in parameters in the GNN model is modest and, as per our analysis, it is likely that usage of a single aggregation method is a potential limiting factor in GNNs.</p><p>This is comparable to convolutional neural networks (CNN) where a simple 3 √ó 3 convolutional kernel requires 9 weights per feature (1 weight per neighbour). Using a CNN with a single weight per 3 √ó 3 kernel will clearly reduce the computational capacity since the feedforward network won't be able to compute derivatives or the Laplacian operator. Hence, it is intuitive that the GNNs should also require multiple weights per node, as previously demonstrated in Theorem 1. We will demonstrate this observation empirically, by running experiments on baseline models with larger dimensions of the hidden features (and, therefore, more parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>We compare various GNN layers, including PNA, on common architectures formed by M such layers, followed by three fully-connected layers for node labels and a set2set (S2S) <ref type="bibr" target="#b30">[31]</ref> readout function for graph labels. For the following experiments <ref type="foot" target="#foot_0">2</ref> we used an architecture with a GRU after the aggregation function, and a variable number M‚àí1 of repeated middle convolutional layer. In particular, we want to highlight:</p><p>Gated Recurrent Units (GRU) <ref type="bibr" target="#b31">[32]</ref> applied after the update function of each layer, as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Their ability to retain information from previous layers proved effective when increasing M.</p><p>Weight sharing in all the GNN layers from the second to (M‚àí1)-th (i.e. all but the first), makes the architecture follow an encode-process-decode configuration <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. This is a strong prior which works well on all our experimental tasks, with a parameter-efficient architecture that allows the model to have a variable number of layers, M.</p><p>Variable depth, M, decided at inference time (based on the size of the input graph and/or other heuristics). This is important when using models on distributions of graphs with a variety of different sizes. In our experiments, we have only used heuristics dependant on the number of nodes N (M = f (N )). For the final architecture, we settled with M = N/2 , where is the floor operation and N the number of nodes in the graph. It would be interesting to test heuristics based on properties of the graph such as the diameter or an adaptive computation time heuristic <ref type="bibr" target="#b33">[34]</ref> based on, for example, the convergence of the nodes' features <ref type="bibr" target="#b15">[16]</ref>. We leave these analyses to future work. This architecture layout (represented in Figure <ref type="figure" target="#fig_0">2</ref>) was determined based on the combination of its downstream performance and parameter efficiency. We note that all attempted architectures have yielded similar comparative performance of GNN layers.</p><p>Skip connections after each GNN layer all feeding into the fully connected layers were also tried. They are known to improve learning in deep architectures <ref type="bibr" target="#b34">[35]</ref>, especially for GNNs where they reduce over-smoothing <ref type="bibr" target="#b7">[8]</ref>. However, in presence of GRUs, they did not give significant performance improvements on our benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Alternative Graph Convolutions</head><p>In this subsection, we present the details of the four graph convolution layers from existing models that we used to compare the performance of the PNA.</p><p>Graph Convolutional Networks (GCN) <ref type="bibr" target="#b24">[25]</ref> use a form of normalized mean aggregator followed by a linear transformation and an activation function, as defined in Equation <ref type="formula" target="#formula_11">10</ref>. Here, √É = A + I N is the adjacency matrix with self-connections, W is a trainable weight matrix and b a learnable bias.</p><formula xml:id="formula_11">X (t+1) = œÉ D‚àí 1 2 √É D‚àí 1 2 X (t) W + b<label>(10)</label></formula><p>Graph Attention Networks (GAT) <ref type="bibr" target="#b25">[26]</ref> perform a linear transformation of the input features followed by an aggregation of the neighbourhood as a weighted sum of the transformed features, where the weights are set by an attention mechanism a, defined in Equation <ref type="formula" target="#formula_12">11</ref>. Here, W is a trainable projection matrix. As in the original paper, we employ the use of multi-head attention.</p><formula xml:id="formula_12">X (t+1) i = œÉ Ô£´ Ô£≠ (j,i)‚ààE a X (t) i , X (t) j W X (t) j Ô£∂ Ô£∏<label>(11)</label></formula><p>Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b5">[6]</ref> perform a sum aggregator between all the neighbours, followed by an update function U consisting in a multi-layer perceptron, as defined in Equation <ref type="formula" target="#formula_13">12</ref>.</p><p>Here, is a learnable parameter. As in the original paper, we use a 2-layer MLP for U .</p><formula xml:id="formula_13">X (t+1) i = U 1 + X (t) i + j‚ààN (i) X (t) j (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>Message Passing Neural Networks (MPNN) <ref type="bibr" target="#b26">[27]</ref> perform a transformation before and after arbitrary aggregator defined in Equation <ref type="formula">13</ref>, where M and U are neural networks and is a single aggregator. In particular, we test models with sum and max aggregators, as they are the most used in literature. As with PNA layers, we found that linear transformations are sufficient for M and U and, as in the original paper <ref type="bibr" target="#b26">[27]</ref>, we employ multiple towers.</p><formula xml:id="formula_15">X (t+1) i = U X (t) i , (j,i)‚ààE M X (t) i , X (t) j (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Random graph generation</head><p>Following previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref>, the benchmark contains undirected unweighted graphs of a wide variety of types (we provide, in parentheses, the approximate proportion of the graphs in the overall benchmark). Letting N be the total number of nodes:</p><p>‚Ä¢ Erd≈ës-R√©nyi <ref type="bibr" target="#b36">[37]</ref>  ‚Ä¢ Tree (15%): generated with a power-law degree distribution with exponent 3</p><p>‚Ä¢ Ladder graphs (5%)</p><p>‚Ä¢ Line graphs (5%)</p><p>‚Ä¢ Star graphs (5%) Additional randomness was introduced to the generated graphs by randomly toggling arcs, without strongly impacting the average degree and main structure. If e is the number of edges and m the number of 'missing edges' (2e + 2m = N (N ‚àí 1)), the probabilities P e and P m of an existing and missing edge to be toggled are: </p><formula xml:id="formula_16">‚Ä¢</formula><formula xml:id="formula_17">P e = 0.</formula><p>After performing the random toggling, we discarded graphs containing singleton nodes, as these are in no way affected by the choice of aggregation. For the presented results we used graphs of small sizes (15 to 50 nodes) as they were already sufficient to demonstrate clear differences between the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-task graph properties</head><p>The graph property tasks consist of a range of individual properties for each node and global properties of the entire graph. In the multi-task benchmark, we consider three node labels and three graph labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node tasks</head><p>1. Single-source shortest-path lengths: length of the shortest path from a node to all the others, where the source node is specified via a one-hot vector. The labels of nodes outside the connected component of the source are set to 0. Note that, since the graph is unweighted, this task corresponds to performing a breadth-first search (BFS).</p><p>2. Eccentricity: for every node v, the longest shortest path from v to any other node within its connected component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Laplacian features:</head><p>LX where L = (D ‚àí A) is the Laplacian matrix of the graph and X are the input node feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph tasks</head><p>4. Connected: whether the graph is connected.</p><p>5. Diameter: the longest shortest path between any two nodes that share components.</p><p>6. Spectral radius: the largest absolute value of the eigenvalues of the adjacency matrix (always real since A is real and symmetric).</p><p>Input features As input features, the network is provided with two vectors of size N . The first represents a one-hot vector representing which node is the starting point for the single-source shortestpath tasks. The second is the feature vector X where each element is i.i.d. sampled as</p><formula xml:id="formula_19">X i ‚àº U[0, 1].</formula><p>Apart from taking part in the Laplacian features task, this random feature vector also provides a "unique identifier" for the nodes in other tasks. This allows for addressing some of the problems highlighted in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>; e.g. the task of whether a graph is connected could be performed by continually aggregating the maximum feature of the neighbourhood and then checking whether they are all equal in the readout. Similar strengthening via random features was also concurrently discovered by <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model training</head><p>While having clear differences, these tasks also share related subroutines, e.g. tasks (1, 2, 4, 5) can all be expressed via graph traversals, and the diameter is the maximum of all node eccentricities. While we do not give this sharing of subroutines as prior to the models as in <ref type="bibr" target="#b15">[16]</ref>, we expect models that have the capacity of understanding and exploiting the graph structure to pick up on these commonalities, efficiently share parameters and reinforce each other during the training.</p><p>Tasks are normalised by dividing each label by the maximum value of their label (among all nodes in node labels) in the training set; since all labels are non-negative, this results in all tasks having normalised labels between 0 and 1. This normalisation allows for a better equilibrium between the various tasks during the training and validation. The model's predictive power on the benchmark is calculated as the average of the mean squared errors (MSE) on the (normalised) tasks.</p><p>Table <ref type="table">1</ref>: Average score of different models using feature sizes of 16 and 20, compared to the PNA with 16. "# params" is the total number of parameters in each architecture. We observe that, even with fewer parameters, PNA performs consistently better and the performance of the other models is not boosted by an increased number of parameters.  Due to the aforementioned challenges, as expected, the performance of the models (as a proportion of the baseline performance) gradually worsens, with some of the models having feature explosions. However, the PNA model had once again consistently outperformed all the other models on all graph sizes. Our results also follow the findings in <ref type="bibr" target="#b15">[16]</ref>, i.e. that between single aggregators the max tends to perform best when extrapolating to larger graphs. For the PNA, we believe that it converges to a better aggregator combining the advantages of each operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have extended the theoretical framework in which GNNs are analysed to continuous features and proven the need for multiple aggregators in such circumstances. We also generalize the sum aggregation by presenting degree-scalers and propose the use of a logarithmic scaling. Taking all of the above into consideration, we have presented a method, Principal Neighbourhood Aggregation, composed of multiple aggregators and degree-scalers. With the goal of understanding the capacity of GNNs to capture graph structures, we have proposed a novel multi-task benchmark and an encodeprocess-decode architecture for solving it. We believe that our findings constitute a step towards establishing a hierarchy of models w.r.t. their expressive power and, in this sense, the PNA model appears to outperform the prior art in GNN layer design.</p><p>A Proof for Theorem 1 (Number of aggregators needed)</p><p>Proof. Let S be the n-dimensional subspace S of R n formed by all tuples (x 1 , x 2 , . . . , x n ) such that x 1 ‚â§ x 2 ‚â§ . . . ‚â§ x n , and notice how S is the collection of the aforementioned multisets. We defined an aggregator as a continuous function from multisets to reals, which corresponds to a continuous function g : S ‚Üí R.</p><p>Assume by contradiction that it is possible to discriminate between all the multisets of size n using only n ‚àí 1 aggregators, viz. g 1 , g 2 , . . . , g n‚àí1 .</p><p>Define f : S ‚Üí R n‚àí1 to be the function mapping each multiset X to its output vector (g 1 (X), g 2 (X), . . . , g n‚àí1 (X)). Since g 1 , g 2 , . . . , g n‚àí1 are continuous, so is f , and, since we assumed these aggregators are able to discriminate between all the multisets, f is injective.</p><p>As S is a n-dimensional Euclidean subspace, it is possible to define a (n ‚àí 1)-sphere C n‚àí1 entirely contained within it, i.e. C n‚àí1 ‚äÜ S. According to Borsuk-Ulam theorem <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, there are two distinct (in particular, non-zero and antipodal) points</p><formula xml:id="formula_20">x 1 , x 2 ‚àà C n‚àí1 satisfying f ( x 1 ) = f ( x 2 ),</formula><p>showing f not to be injective; hence the required contradiction.</p><p>Note: n aggregators are actually sufficient. A simple example is to use g 1 , g 2 , . . . , g n where g k (X) = the k-th smallest item in X. It's clear to see that the multiset whose elements are g 1 (X), g 2 (X), . . . , g n (X) is X, which can hence be uniquely determined by the aggregators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof for Proposition 1 (Moments of the multiset)</head><p>Proof. For n &lt; 3, we can trivially uniquely determine the original multiset, so assume n ‚â• 3, and hence knowledge of ¬µ, œÉ 2 . Let X = {x 1 , x 2 , . . . , x n } be the multiset to be found, and define</p><formula xml:id="formula_21">R = {r 1 = x 1 ‚àí ¬µ, r 2 = x 2 ‚àí ¬µ, . . . , r n = x n ‚àí ¬µ}.</formula><p>Notice how r i 1 = 0, and r i 2 = nœÉ 2 , and for 2 &lt; k ‚â§ n we have r i k = nœÉ k M k (X), i.e. all the symmetric power sums p k = r i k (k ‚â§ n) are uniquely determined by the moments.</p><p>Additionally, e k , the elementary symmetric sums of R, i.e. the sum of the products of all the sub-multisets of size k (1 ‚â§ k ‚â§ n), are determined as follow:</p><p>e 1 , the sum of all elements, is equal to p 1 ; e 2 , the sum of the products of all pairs in R, is (e 1 p 1 ‚àí p 2 ) /2; e 3 ,the sum of the products of all triplets, is (e 2 p 1 ‚àí e 1 p 2 + p 3 ) /3, and so on. Notice how e 1 , e 2 , . . . , e n can be computed using the following recursive formula <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_22">1‚â§i1&lt;i2&lt;‚Ä¢‚Ä¢‚Ä¢&lt;i k ‚â§n Ô£´ Ô£≠ k j=1 r ij Ô£∂ Ô£∏ = e k = 1 k k j=1 (‚àí1) j‚àí1 e k‚àíj p j , e 0 = 1</formula><p>Consider polynomial P (x) = Œ†(x ‚àí r i ), i.e. the unique polynomial of degree n with leading coefficient 1 whose roots are R. This defines A, the coefficients of P , i.e. the real numbers a 0 , a 1 , . . . , a n‚àí1 for which P (x) = x n + a n‚àí1 x n‚àí1 + . . . + a 1 x + a 0 . Using Vieta's formulas <ref type="bibr" target="#b43">[44]</ref>:</p><formula xml:id="formula_23">1‚â§i1&lt;i2&lt;‚Ä¢‚Ä¢‚Ä¢&lt;i k ‚â§n Ô£´ Ô£≠ k j=1 r ij Ô£∂ Ô£∏ = (‚àí1) k a n‚àík a n</formula><p>which applied to P yield that a n‚àík is equal to a n (equal to 1 in P ) divided by (‚àí1) k multiplied by e k . Hence A is uniquely determined, and so is P , being its coefficients a valid definition of it. By the fundamental theorem of algebra, P has n (possibly repeated) roots, which are the elements of R, hence uniquely determining the latter.</p><p>Finally, X can be easily determined adding ¬µ to each element of R.</p><p>Note: the proof above assume the knowledge of n. In the case that n is variable (as in GNNs) and so we have multisets of up to n elements an extra aggregator will be needed. An example of such aggregator is the mean multiplied by any injective scaler which would allow the degree of the node to be inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof for Theorem 2 (Injective functions on countable multisets)</head><p>Proof. Let œá be the countable input feature space from which the elements of the multisets are taken. Since œá is countable and the cardinality of multisets is bounded, let Z : œá ‚Üí N + be an injection from œá to natural numbers, and N ‚àà N such that |X| + 1 &lt; N for all X.</p><p>Let's define an injective function s, and without loss of generality, assume s(0), s(1), . . . , s(N ) &gt; 0 (otherwise for the rest of the proof consider s as s (i) = s(i) ‚àí min j‚àà[0,N ] s(j) + which is positive for all i ‚àà [0, N ]). s(|X|) can only take value in {s(0), s(1), . . . , s(N )}, therefore let us define Œ≥ = min s(i) s(j) | i, j ‚àà [0, N ], s(i) ‚â• s(j) . Since s is injective, s(i) = s(j) for i = j, which implies Œ≥ &gt; 1.</p><p>Let K &gt; 1 Œ≥‚àí1 be a positive real number and consider f (x) = N ‚àíZ(x) + K.</p><formula xml:id="formula_24">‚àÄx ‚àà œá, Z(x) ‚àà [1, N ] ‚áí N ‚àíZ(x) ‚àà [0, 1] ‚áí f (x) ‚àà [K, K +1] , so E x‚ààX [f (x)] ‚àà [K, K +1].</formula><p>We proceed to show that the cardinality of X can be uniquely determined, and X itself can be determined as well, by showing that exist an injection h over the multisets.</p><p>Let us h as a function that scales the mean of f by an injective function of the cardinality:</p><formula xml:id="formula_25">h(X) = s(|X|) E x‚ààX [f (x)]</formula><p>We want show that the value of |X| can be uniquely inferred from the value of h(X). </p><formula xml:id="formula_26">s(|X |)(K +1) ‚â• s(|X |) E x‚ààX [f (x)] = h(X ) = h(X ) = s(|X |) E x‚ààX [f (x)] ‚â• s(|X |) K =‚áí K ‚â§ 1 s(|X |) s(|X |) ‚àí 1 ‚â§ 1 Œ≥ ‚àí 1</formula><p>which is a contradiction. So it is impossible for the size of a multiset X to be ambiguous from the value of h(X).</p><p>Let us define d as the function mapping h(X) to |X|. Considering the Z(j)-th digit i after the decimal point in the base N representation of h (X), it can be inferred that X contains i elements j, and, so, all the elements in X can be determined; hence h is injective over the multisets in X.</p><p>Note: this proof is a generalization of the one by Xu et al. <ref type="bibr" target="#b5">[6]</ref> on the sum aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Alternative aggregators</head><p>Apart from the aggregators we described and used above, there are other aggregators that we have experimented with or that are used in literature, you can find some examples below. Domain-specific metrics could also be an effective choice.</p><p>Softmax and softmin aggregations As an alternative to max and min, softmax and softmin are differentiable and can be weighted in the case of weighted graphs or attention networks. They also allow an asymmetric message passing in the direction of the strongest signal. Equation 15 presents the direct neighbour formulation of the softmax and softmin operations, where X l are the nodes' features at layer l with respect to node i and N (i) is the neighbourhood of node i:</p><formula xml:id="formula_27">softmax i (X l ) = j‚ààN (i)</formula><p>X l j exp(X l j ))</p><p>k‚ààN (i) exp(X l k )</p><p>, softmin i (X l ) = ‚àísoftmax i (‚àíX l ) <ref type="bibr" target="#b14">(15)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Layout of the architecture used, with a S2S layer for the graph labels and a multi-layer perceptron (MLP) at each output. When comparing different graph convolutions on the same graphs, the difference between the various models only lies in the type of graph convolution layer to use in place of GC 1 and GC m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(20%): random probability of edge creation for each node ‚Ä¢ Barab√°si-Albert [38] (20%): the number of edges for a new node is taken randomly from {1, 2, ..., N ‚àí 1} ‚Ä¢ Grid (5%): m √ó k 2d grid graph with N = mk and m and k as close as possible ‚Ä¢ Caveman [39] (5%): with m cliques of size k, with m and k as close as possible</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Caterpillar graphs (10%): with a backbone of size b (drawn from U[1, N ) ), and N ‚àí b pendent vertices uniformly connected to the backbone ‚Ä¢ Lobster graphs (10%): with a backbone of size b (drawn from U[1, N ) ), and p (drawn from U[1, N ‚àí b ] ) pendent vertices uniformly connected to the backbone, and additional N ‚àí b ‚àí p pendent vertices uniformly connected to the previous pendent vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Multi-task log 10 of the ratio of the MSE for different GNN models and the variance of the tasks (MSE of the baseline). The training is done with the same architectures on graphs of 15-25 nodes and the validation on 25-30 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Assume by contradiction ‚àÉ X , X multisets of size at most N such that |X | = |X | but h(X ) = h(X ); since s is injective s(|X |) = s(|X |), without loss of generality let s(|X |) &gt; s(|X |), then:</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">The code for all the aggregators, scalers, models, architectures and dataset generation is available at https://github.com/lukecavabarrett/pna .</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We trained the models using the Adam optimizer for a maximum of 10,000 epochs, using early stopping with a patience of 1,000 epochs. Learning rates, weight decay, dropout and other hyperparameters such as the number of towers/attention heads were tuned on the validation set for each model. For each model, we run 10 training runs with different seeds and different hyper-parameters (but close to the tuned values) and report the five with least validation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>The multi-task results are presented in Figure <ref type="figure">3a</ref>, where we observe that the proposed PNA model consistently outperforms state-of-the-art models, and in Figure <ref type="figure">3b</ref>, where we note that the PNA performs consistently better on all tasks. The baseline represents the MSE from predicting the average of the training set for all tasks (or the variance of each task).</p><p>The trend of these multi-task results follows but amplifies differences in the average performances of the models when trained separately on the individual tasks, which suggests that the PNA model can better capture and exploit the common sub-units of these tasks. Further, PNA showed to perform the best on all architecture layouts that we attempted. We should note that the GIN architecture was the only one whose performance suffered when switching from an architecture without weight-sharing to the encode-process-decode architecture; in all other cases, the GIN model had performances in-between the MPNN and GAT models.</p><p>We note in Figure <ref type="figure">3b</ref> that GCN, GAT and GIN are unable to estimate the Laplacian transformation of the features, and perform very close to the baseline for other node tasks. This shows a strong limitation of the models' capacity and can be attributed to the over-smoothing effect. This limitation is slightly reduced using skip-connections, but their general performance remains close to the baseline.    In order to demonstrate that the performance improvements of the PNA model are not due to the (relatively small) number of additional parameters it has compared to the other models (about 15%), we ran tests on all the other models with latent size increased from 16 to 20 features. The results of these models, presented in Table <ref type="table">1</ref>, suggest that even when baseline models are given 30% more parameters than the PNA, they are qualitatively less capable of capturing the graphs' structure.</p><p>Finally, we explored the extrapolation of the models to larger graphs, in particular, we trained models on graphs of sizes between 15 and 25, validated them with graphs between 25 and 30 and evaluated their performances on graphs between 20 and 50. This task presents many challenges, two of the most significant are: firstly, unlike in <ref type="bibr" target="#b15">[16]</ref> the models are not given the step-wise supervision or trained on subroutines that can be extended. Secondly, the models have to cope with their architectures being extended to further hidden layers than trained on, which can sometimes cause problems with rapidly increasing feature scales.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07">Jul 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06157</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10943" to="10953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Ryan L Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02541</idno>
		<title level="m">Relational pooling for graph representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha√´l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CayleyNets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<title level="m">Graph wavelet neural network</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10593</idno>
		<title level="m">Neural execution of graph algorithms</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13211</idno>
		<title level="m">What can neural networks reason about?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">G√≥mez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri√†</forename><surname>Puigdom√®nech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName><forename type="first">Nima</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-L√°szl√≥</forename><surname>Barab√°si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15387" to="15397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An efficient graph convolutional network technique for the travelling salesman problem</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01227</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards interpretable sparse graph representation learning with laplacian pooling</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Noutahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Horwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S√©bastien</forename><surname>Gigu√®re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04025</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01203</idno>
		<title level="m">Relational inductive bias for physical construction in humans and machines</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local and global properties in networks of processors (extended abstract)</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Annual ACM Symposium on Theory of Computing, STOC &apos;80</title>
				<meeting>the Twelfth Annual ACM Symposium on Theory of Computing, STOC &apos;80<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03211</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11945</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri√´nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04817</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the evolution of random graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erd≈ës</surname></persName>
		</author>
		<author>
			<persName><surname>R√©nyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<biblScope unit="page" from="17" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R√©ka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-L√°szl√≥</forename><surname>Barab√°si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="97" />
			<date type="published" when="2002-01">Jan 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Networks, dynamics, and the small-world phenomenon</title>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="493" to="527" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An Introduction to Algebraic Topology</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Rotman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Graduate Texts in Mathematics</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Drei s√§tze √ºber die n-dimensionale euklidische sph√§re</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Borsuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta Mathematicae</title>
		<imprint>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cambridge Studies in Advanced Mathematics no. 62</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Enumerative Combinatorics Volume</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Encyclopaedia of mathematics</title>
		<author>
			<persName><surname>Hazewinkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STO-ZYG. Encyclopaedia of mathematics</title>
				<meeting><address><addrLine>Dordecht</addrLine></address></meeting>
		<imprint>
			<publisher>STO-ZYG. Kluwer Academic</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
