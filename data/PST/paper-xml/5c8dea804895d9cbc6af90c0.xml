<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">k-Space Deep Learning for Accelerated MRI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yoseob</forename><surname>Han</surname></persName>
							<email>hanyoseob@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Leonard</forename><surname>Sunwoo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Bio and Brain Engineering</orgName>
								<orgName type="department" key="dep2">Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country>Korea, Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">Seoul National University College of Medicine</orgName>
								<orgName type="institution" key="instit2">Seoul National University Bundang Hospital</orgName>
								<address>
									<settlement>Seongnam</settlement>
									<country>Republic of Korea. J</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">k-Space Deep Learning for Accelerated MRI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFEC342040840256BE81E28C7715D337</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Compressed sensing MRI</term>
					<term>Deep Learning</term>
					<term>Hankel structured low-rank completion</term>
					<term>Convolution framelets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one of the state-of-the-art compressed sensing approaches that directly interpolates the missing k-space data using low-rank Hankel matrix completion. The success of ALOHA is due to the concise signal representation in the k-space domain thanks to the duality between structured low-rankness in the k-space domain and the image domain sparsity. Inspired by the recent mathematical discovery that links convolutional neural networks to Hankel matrix decomposition using datadriven framelet basis, here we propose a fully data-driven deep learning algorithm for k-space interpolation. Our network can be also easily applied to non-Cartesian k-space trajectories by simply adding an additional regridding layer. Extensive numerical experiments show that the proposed deep learning method consistently outperforms the existing image-domain deep learning approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECENTLY, inspired by the tremendous success of deep learning <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, many researchers have investigated deep learning approaches for MR reconstruction problems and successfully demonstrated significant performance gain <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b9">[10]</ref>.</p><p>In particular, Wang et al <ref type="bibr" target="#b3">[4]</ref> used the deep learning reconstruction either as an initialization or a regularization term. Deep network architecture using unfolded iterative compressed sensing (CS) algorithm was also proposed to learn a set of regularizers and associated filters <ref type="bibr" target="#b4">[5]</ref>. These works were followed by novel extension using deep residual learning <ref type="bibr" target="#b5">[6]</ref>, domain adaptation <ref type="bibr" target="#b6">[7]</ref>, data consistency layers <ref type="bibr" target="#b8">[9]</ref>, etc. An extreme form of the neural network called Automated Transform by Manifold Approximation (AUTOMAP) <ref type="bibr" target="#b9">[10]</ref> even attempts to estimate the Fourier transform itself using fully connected layers. All these pioneering works have consistently demonstrated superior reconstruction performances over the compressed sensing approaches <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref> at significantly lower run-time computational complexity.</p><p>Although the end-to-end recovery approach like AU-TOMAP <ref type="bibr" target="#b9">[10]</ref> may directly recover the image without ever interpolating the missing k-space samples (see Fig. <ref type="figure" target="#fig_0">1(c</ref>)), it works only for the sufficiently small size images due to its  <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, (b) cascaded network <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, (c) AUTOMAP <ref type="bibr" target="#b9">[10]</ref>, and (d) the proposed k-space learning. IFT: Inverse Fourier transform. huge memory requirement for fully connected layers. Accordingly, most of the popular deep learning MR reconstruction algorithms are either in the form of image domain postprocessing as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, or iterative updates between the k-space and the image domain using a cascaded network as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>One of the main purposes of this paper is to reveal that the aforementioned approaches are not all the available options for MR deep learning, but there exists another effective deep learning approach. In fact, as illustrated in Fig. <ref type="figure" target="#fig_0">1(d)</ref>, the proposed deep learning approach directly interpolates the missing k-space data so that accurate reconstruction can be obtained by simply taking the Fourier transform of the interpolated k-space data. In contrast to AUTOMAP <ref type="bibr" target="#b9">[10]</ref>, our network is implemented in the form of convolutional neural network (CNN) without requiring fully connected layer, so the GPU memory requirement for the proposed k-space deep learning is minimal.</p><p>In fact, our k-space deep learning scheme is inspired by the success of structured low-rank Hankel matrix approaches <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, exploiting concise signal representation in the k-space domain thanks to the duality between the structured low-rankness in the k-space and the image domain sparsity. In addition, the recent theory of deep convolutional framelets <ref type="bibr" target="#b18">[19]</ref> showed that an encoder-decoder network can be regarded as a signal representation that emerges from Hankel matrix decomposition. Therefore, by synergistically combining these findings, we propose a novel k-space deep learning algorithms that perform better and generalize well. We further show that our deep learning approach for k-space interpolation can handle general k-space sampling patterns beyond the Cartesian trajectory, such as radial, spiral, etc. Moreover, our theory and empirical results also shows that multi-channel calibration-free k-space interpolation can be easily realized using the proposed framework by simply stacking multi-coil k-space data along the channel direction as an input to feed in the neural network.</p><p>We are aware of recent k-space neural network approaches called the scan-specific robust artificial neural networks for kspace interpolation (RAKI) <ref type="bibr" target="#b19">[20]</ref>. Unlike the proposed method, RAKI considers scan-specific learning without training data, so the neural network weights needs to be recalculated for each k-space input data. On the other hand, the proposed method exploits the generalization capability of the neural network. More specifically, even with the same trained weights, the trained neural network can generate diverse signal representation depending on the input thanks to the combinatorial nature of the ReLU nonlinearity. This makes the neural network expressive and generalized well to the unseen test data. The theoretical origin of the expressiveness will be also discussed in this paper.</p><p>After the original version of this work was available on Arxiv, there appear several deep learning algorithms exploiting k-space learning <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. These works are based on hybrid formulation (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>) and utilize the deep neural network as a regularization term for k-space denoising. On the other hand, the our method exploits the nature of deep neural network as a generalizable and expressive k-space representation to directly interpolate the missing k-space data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATHEMATICAL PRELIMINARIES A. Notations</head><p>In this paper, matrices are denoted by bold upper case letters, i.e. A, B, whereas the vectors are represented by bold lower cases letters, i.e. x, y. In addition, [A] ij refers to the (i, j)-th element of the matrix A, and x[i] denotes the i-th element of the vector x. The notation v ∈ R d for a vector v ∈ R d denotes its flipped version, i.e. the indices of v are time-reversed such that v</p><formula xml:id="formula_0">[n] = v[-n].</formula><p>The N × N identity matrix is denoted as I N , while 1 N is an N -dimensional vector with 1's. The superscript T and for a matrix or vector denote the transpose and Hermitian transpose, respectively. R and C denote the real and imaginary fields, respectively. R + refers to the nonnegative real numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Forward Model for Accelerated MRI</head><p>The spatial Fourier transform of an arbitrary smooth function x : R 2 → R is defined by</p><formula xml:id="formula_1">x(k) = F[x](k) := R d e -ιk•r x(r)dr, with spatial frequency k ∈ R 2 and ι = √ -1. Let {k n } N n=1</formula><p>, for some integer N ∈ N, be a collection of finite number of sampling points of the k-space confirming to the Nyquist sampling rate. Accordingly, the discretized k-space data x ∈ C N is introduced by</p><formula xml:id="formula_2">x = x[0] • • • x[N -1] , where x[i] = x(k i ). (1)</formula><p>For a given under-sampling pattern Λ for accelerated MR acquisition, let the downsampling operator P Λ : C N → C N be defined as</p><formula xml:id="formula_3">[P Λ [x]] i = x[i], i ∈ Λ 0, otherwise .<label>(2)</label></formula><p>Then, the under-sampled k-space data is given by</p><formula xml:id="formula_4">ŷ := P Λ [x]<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Low-Rank Hankel Matrix Approaches</head><p>From the undersampled data in (3), CS-MRI <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> attempts to find the feasible solution that has minimum nonzero support in some sparsifying transform domain. This can be achieved by finding a function z : R 2 → R such that</p><formula xml:id="formula_5">min z T z 1 subject to P Λ [x] = P Λ [ẑ]<label>(4)</label></formula><p>where T denotes the image domain sparsifying transform and</p><formula xml:id="formula_6">z = z(k 0 ) • • • z(k N -1 ) T .<label>(5)</label></formula><p>This optimization problem usually requires iterative update between the k-space and the image domain after the discretization of z(r) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. On the other hand, in recent structured low-rank matrix completion algorithms <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the compressed sensing problems was solved either by imposing the low-rank structured matrix penalty <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref> or by converting it a direct k-space interpolation problem using low-rank structure matrix completion <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. More specifically, let H d ( x) denote a Hankel matrix constructed from the k-space measurement x in <ref type="bibr" target="#b0">(1)</ref>, where d denotes the matrix pencil size (for more details on the construction of Hankel matrices and their relation to the convolution, see Section I in Supplementary Material). Then, if the underlying signal x(r) in the image domain is sparse and described as the signal with the finite rate of innovations (FRI) with rate s <ref type="bibr" target="#b22">[23]</ref>, the associated Hankel matrix H d (x) with d &gt; s is low-ranked <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Therefore, if some of k-space data are missing, we can construct an appropriate weighted Hankel matrix with missing elements such that the missing elements are recovered using low-rank Hankel matrix completion approaches <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_7">(P ) min z∈C N RANK H d ( z)<label>(6)</label></formula><p>subject to</p><formula xml:id="formula_8">P Λ [ x] = P Λ [ z] .</formula><p>While the low-rank Hankel matrix completion problem (P ) can be solved in various ways, one of the main technical huddles is its relatively large computational complexity for matrix factorization and large memory requirement for storing Hankel matrix. Although several new approaches have been proposed to solve these problems <ref type="bibr" target="#b24">[25]</ref>, the following section shows that a deep learning approach is a novel and efficient way to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN CONTRIBUTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ALOHA as a signal representation</head><p>Consider the following image regression problem under the low-rank Hankel matrix constraint:</p><formula xml:id="formula_9">min z∈C N x -F -1 [ z] 2 (7) subject to RANK H d ( z) = s, P Λ [ x] = P Λ [ z] ,<label>(8)</label></formula><p>where s denotes the estimated rank. In the above formulation, the cost in <ref type="bibr" target="#b6">(7)</ref> is defined in the image domain to minimize the errors in the image domain, whereas the low-rank Hankel matrix constraint in <ref type="bibr" target="#b7">(8)</ref> is imposed in the k-space after the k-space weighting. Now, we convert the complex-valued constraint in (8) to a real-valued constraint. The procedure is as follows. First, the operator R :</p><formula xml:id="formula_10">C N → R N ×2 is defined as R[ z] := Re(ẑ) Im(ẑ) , ∀ z ∈ C N<label>(9)</label></formula><p>where Re(•) and Im(•) denote the real and imaginary part of the argument. Similarly, we define its inverse operator R -1 :</p><formula xml:id="formula_11">R N ×2 → C N as R -1 [ Z] := ẑ1 + ιẑ 2 , ∀ Z := [z 1 z 2 ] ∈ R N ×2<label>(10)</label></formula><p>Then, as shown in Section II in Supplementary Material, we can approximately convert <ref type="bibr" target="#b7">(8)</ref> to an optimization problem with real-valued constraint:</p><formula xml:id="formula_12">(P A ) min z∈C N x -F -1 [ z] 2 (11) subject to RANKH d|2 (R[ z]) = Q ≤ 2s, P Λ [ x] = P Λ [ z] .</formula><p>In the recent theory of deep convolutional framelets <ref type="bibr" target="#b18">[19]</ref>, this low-rank constraint optimization problem was addressed using learning-based signal representation. More specifically, for any z ∈ C N , let the Hankel structured matrix</p><formula xml:id="formula_13">H d|2 (R[ z]) have the singular value decomposition UΣV , where U = [u 1 • • • u Q ] ∈ R N ×Q and V = [v 1 • • • v Q ] ∈ R 2d×Q denote</formula><p>the left and the right singular vector bases matrices, respectively; Σ = (σ ij ) ∈ R Q×Q is the diagonal matrix with singular values. Now, consider matrix pair Ψ, Ψ ∈ R 2d×Q</p><formula xml:id="formula_14">Ψ := ψ 1 1 • • • ψ 1 Q ψ 2 1 • • • ψ 2 Q and Ψ := ψ1 1 • • • ψ1 Q ψ2 1 • • • ψ2 Q (12)</formula><p>that satisfy the low-rank projection constraint:</p><formula xml:id="formula_15">Ψ Ψ = P R(V) ,<label>(13)</label></formula><p>where P R(V) denotes the projection matrix to the range space of V. We further introduce the generalized pooling and unpooling matrices Φ, Φ ∈ R N ×M <ref type="bibr" target="#b18">[19]</ref> that satisfies the condition</p><formula xml:id="formula_16">ΦΦ = I N ,<label>(14)</label></formula><p>Using Eqs. ( <ref type="formula" target="#formula_15">13</ref>) and ( <ref type="formula" target="#formula_16">14</ref>), we can obtain the following matrix equality:</p><formula xml:id="formula_17">H d|2 (R[ z]) = ΦΦ H d|2 (R[ z]) Ψ Ψ = ΦC Ψ ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_18">C := Φ H d|2 (R[ z]) Ψ ∈ R N ×Q<label>(16)</label></formula><p>By taking the generalized inverse of Hankel matrix, <ref type="bibr" target="#b14">(15)</ref> can be converted to the framelet basis representation <ref type="bibr" target="#b18">[19]</ref>. Moreover, one of the most important observations in <ref type="bibr" target="#b18">[19]</ref> is that the resulting framelet basis representation can be equivalently represented by single layer encoder-decoder convolution architecture:</p><formula xml:id="formula_19">R[ z] = ΦC g( Ψ), where C = Φ (R[ z] h(Ψ))<label>(17)</label></formula><p>and denotes the multi-channel input multi-channel output convolution. The second and the first part of ( <ref type="formula" target="#formula_19">17</ref>) correspond to the encoder and decoder layers with the corresponding convolution filters h(Ψ) ∈ R 2d×Q and g</p><formula xml:id="formula_20">Ψ () ∈ R dQ×2 : h(Ψ) := ψ 1 1 • • • ψ 1 Q ψ 2 1 • • • ψ 2 Q , g Ψ :=    ψ 1 1 ψ 2 1 . . . . . . ψ 1 Q ψ 2 Q    ,</formula><p>which are obtained by reordering the matrices Ψ and Ψ in <ref type="bibr" target="#b11">(12)</ref>. Specifically,</p><formula xml:id="formula_21">ψ 1 i ∈ R d (resp. ψ 2 i ∈ R d</formula><p>) denotes the d-tap encoder convolutional filter applied to the real (resp. imaginary) component of the k-space data to generate the ith channel output. In addition, g( Ψ) is a reordered version of Ψ so that ψ1 i ∈ R d (resp. ψ2 i ∈ R d ) corresponds to the d-tap decoder convolutional filter to generate the real (resp. imaginary) component of the k-space data by convolving with the i-th channel input. We can further use recursive application of encoder-decoder representation for the resulting framelet coefficients C in <ref type="bibr" target="#b16">(17)</ref>. In Corollary 4 of our companion paper <ref type="bibr" target="#b25">[26]</ref>, we showed that the recursive application of the encoderdecoder operations across the layers increases the net length of the convolutional filters.</p><p>Since ( <ref type="formula" target="#formula_19">17</ref>) is a general form of the signals that are associated with a Hankel structured matrix, we are interested in using it to estimate bases for k-space interpolation. Specifically, we consider a complex-valued signal space H determined by the filters Ψ and Ψ:</p><formula xml:id="formula_22">H(Ψ, Ψ) = z ∈ C N R[z] = Φ C g( Ψ) , C = ( ΦR[z]) h(Ψ) .<label>(18)</label></formula><p>Then, the ALOHA formulation P A can be equivalently represented by</p><formula xml:id="formula_23">(P A ) min z∈H(Ψ, Ψ) min Ψ, Ψ x -F -1 [ z] 2 subject to P Λ [ x] = P Λ [ z] ,</formula><p>In other words, ALOHA is to find the optimal filter Ψ, Ψ and the associated k-space signal z ∈ H(Ψ, Ψ) that satisfies the data consistency conditions. In contrast to the standard CS approaches in which signal presentation in the image domain is separately applied from the data-consistency constraint in the k-space, the success of ALOHA over CS can be contributed to more efficient signal representation in the k-space domain that simultaneously take care of the data consistency in the k-space domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization and Depth</head><p>To allow training for neural networks, the problem formulation in (P A ) should be decoupled into two steps: the learning phase to estimate Ψ, Ψ from the training data, and the inference phase to estimate the interpolate signal z for the given filter set Ψ, Ψ. Although we have revealed the relations between ALOHA and encoder-decoder architecture, the derivation is for specific input signal and it is not clear how the relations would translate when training is performed over multiple training data set, and the trained network can be generalized to the unseen test data. Given that the sparsity prior in dictionary learning enables the selection of appropriate basis functions from the dictionary for each given input, one may conjecture that there should be similar mechanisms in deep neural networks that enable adaptation to the specific input signals.</p><p>Fig. <ref type="figure">2</ref>: An example of R 2 input space partitioning for the case of two-channel three-layer ReLU neural network. Depending on input k-space data, a partition and its associated linear representation are selected.</p><p>In Section IV of Supplementary Material, we show that the ReLU nonlinearities indeed plays a critical role in the adaptation and generalization. In fact, in our companion paper <ref type="bibr" target="#b25">[26]</ref>, we have shown that ReLU offers combinatorial convolution frame basis selection depending on each input image. More specifically, thanks to ReLU, a trained filter set produce large number of partitions in the input space as shown in Fig. <ref type="figure">2</ref>, in which each region shares the same linear signal representation. Therefore, depending on each k-space input data, a particular region and its associated linear representation are selected. Moreover, we show that the number of input space partition and the associated linear representation increases exponentially with the depth, channel and the skipped connection. By synergistically exploiting the efficient signal representation in the k-space domain, this enormous expressivity from the same filter sets can make the k-space deep neural network more powerful than the conventional image domain learning.</p><p>For the more details on the theoretical aspect of deep neural networks, see Section IV of Supplementary Material or our companion paper <ref type="bibr" target="#b25">[26]</ref>. For the parallel imaging, the input and output are multi-coil k-space data, after stacking them along the channel direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extension to parallel imaging</head><p>In <ref type="bibr" target="#b14">[15]</ref>, we formally showed that when {x i } P i=1 denote the k-space measurements from P receiver coils, the following extended Hankel structured matrix is low-ranked:</p><formula xml:id="formula_24">H d|P ( X) = H d ( x 1 ) • • • H d ( x P )<label>(19)</label></formula><p>where</p><formula xml:id="formula_25">X = x 1 • • • x P ∈ C N ×P .</formula><p>Thus, similar to the single channel cases, the date-driven decomposition of the extended Hankel matrix in <ref type="bibr" target="#b18">(19)</ref> can be represented by stacking the each k-space data along the channel direction and applies the deep neural network for the given multi-channel data. Therefore, except the number of input and output channels, the network structure for parallel imaging data is identical to the single channel k-space learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sparsification</head><p>To further improve the performance of the structured matrix completion approach, in <ref type="bibr" target="#b17">[18]</ref>, we showed that even if the image x(r) may not be sparse, it can be often converted to a sparse signal.</p><p>For example, the outmost skipped connection for the residual learning is another way to make the signal sparse. Note that fully sampled k-space data x can be represented by</p><formula xml:id="formula_26">x = y + ∆ x,</formula><p>where y is the undersampled k-space measurement in (3), and ∆ x is the residual part of k-space data that should be estimated. In practice, some of the low-frequency part of k-space data including the DC component are acquired in the undersampled measurement so that the image component from the residual k-space data ∆ x are mostly high frequency signals, which are sparse. Therefore, ∆ x has low-rank Hankel matrix structure, which can be effectively processed using the deep neural network. This can be easily implemented using a skipped connection before the deep neural network as shown in Fig. <ref type="figure" target="#fig_1">3(a)</ref>. However, the skipped connection also works beyond Fig. <ref type="figure">4</ref>: A network backbone of the proposed method. The input and output are complex-valued. the sparsification. In our companion paper <ref type="bibr" target="#b25">[26]</ref> (which is also repeated in Section IV of Supplementary Material), we showed that the skipped connection at the inner layers makes the frame basis more expressive. Therefore, we conjecture that the skipped connections play dual roles in our k-space learning.</p><p>Second, we can convert a signal to an innovation signal using a shift-invariant transform represented by the whitening filter h such that the resulting innovation signal z = h * x becomes an FRI signal <ref type="bibr" target="#b22">[23]</ref>. For example, many MR images can be sparsified using finite difference or wavelet transform <ref type="bibr" target="#b14">[15]</ref>. This implies that the Hankel matrix from the weighted k-space data, ẑ(k) = ĥ(k)x(k) are low-ranked, where the weight ĥ(k) is determined from the finite difference or Haar wavelet transform <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Thus, the deep neural network is applied to the weighted k-space data to estimate the missing spectral data ĥ(x)x(k), after which the original k-space data is obtained by dividing with the same weight, i.e. x(k) = ẑ(k)/ ĥ(k). This can be easily implemented using a weighting and unweighting layer as shown in Fig. <ref type="figure" target="#fig_1">3(b)</ref>.</p><p>In this paper, we consider these two strategies to investigate which strategy is better for different sampling trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extension to General Sampling Patterns</head><p>Since the Hankel matrix formulation in ALOHA is based on the Cartesian coordinate, we add extra regridding layers to handle the non-Cartesian sampling trajectories. Specifically, for radial and spiral trajectories, the non-uniform fast Fourier transform (NUFFT) was used to perform the regridding to the Cartesian coordinate. For Cartesian sampling trajectory, the regridding layer using NUFFT is not necessary, and we instead perform a zero-filling in the unacquired k-space regions as an initialization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Backbone</head><p>The network backbone follows the U-Net <ref type="bibr" target="#b2">[3]</ref> which consists of convolution, batch normalization, rectified linear unit (ReLU), and contracting path connection with concatenation as shown in Fig. <ref type="figure">4</ref>. Here, the input and output are the complex-valued k-space data, while R[•] and R -1 [•] denote the operators in ( <ref type="formula" target="#formula_10">9</ref>) and ( <ref type="formula" target="#formula_11">10</ref>), respectively, that convert complex valued input to two-channel real value signals and vice versa. For parallel imaging, multi-coil k-space data are given as input and output after stacking them along channel direction. Specially, in our parallel imaging experiments, we use eight coils k-space data.</p><p>The yellow arrow is the basic operator that consists of 3 × 3 convolutions followed by a rectified linear unit (ReLU) and batch normalization. The same operation exists between the separate blocks at every stage, but the yellow arrows are omitted for visibility. A red and blue arrows are 2 × 2 average pooling and average unpooling operators, respectively, located between the stages. A violet arrow is the skip and concatenation operator. A green arrow is the simple 1 × 1 convolution operator generating interpolated k-space data from multichannel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Training</head><p>We use the l 2 loss in the image domain for training. For this, the Fourier transform operator is placed as the last layer to convert the interpolated k-space data to the complexvalued image domain so that the loss values are calculated for the reconstructed image. Stochastic gradient descent (SGD) optimizer was used to train the network. For the IFT layer, the adjoint operation from SGD is also Fourier transform. The size of mini batch is 4, and the number of epochs in single and multi coil networks is 1000 and 500, respectively. The initial learning rate is 10 -5 , which gradually dropped to 10 -6 until 300-th epochs. The regularization parameter was λ = 10 -4 .</p><p>The labels for the network were the images generated from direct Fourier inversion from fully sampled k-space data. The input data for the network was the regridded down-sampled kspace data from Cartesian, radial, and spiral trajectories. The details of the downsampling procedure will be discussed later. For each trajectory, we train the network separately.</p><p>The proposed network was implemented using MatConvNet toolbox in MATLAB R2015a environment <ref type="bibr" target="#b26">[27]</ref>. Processing units used in this research are Intel Core i7-7700 central processing unit and GTX 1080-Ti graphics processing unit. Training time lasted about 5 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MATERIAL AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Acquisition</head><p>The evaluations were performed on single coil and multi coils k-space data for various k-space trajectories such as Cartesian, radial, and spiral cases.</p><p>For the Cartesian trajectory, knee k-space dataset (http://mridata.org/) were used. The raw data were acquired from 3D fast-spin-echo (FSE) sequence with proton density weighting included fat saturation comparison by a 3.0T whole body MR system (Discovery MR 750, DV22.0, GE Healthcare, Milwaukee, WI, USA). The repetition time (TR) and echo time (TE) were 1550 ms and 25 ms, respectively. There were 320 slices in total, and the thickness of each slice was 0.6 mm. The field of view (FOV) defined 160 × 128 mm 2 and the size of acquisition matrix is 320 × 256. The voxel size was 0.5 mm. The number of coils is 8. Eight coils k-space data was used for multi-coil k-space deep learning. In addition, to evaluate the performance of the algorithm for the single coil experiment, coil compression (http://mrsrl.stanford.edu/ tao/software.html) was applied to obtain a single coil k-space data. For the Cartesian trajectory as shown in Fig. <ref type="figure" target="#fig_2">5</ref>(a), the input k-space was downsampled to a Gaussian pattern using x4 acceleration factor in addition to the 10% auto-calibration signal (ACS) line. Therefore, the net acceleration factor is about 3 (R = 3). Among the 20 cases of knee data, 18 cases were used for training, 1 case for validation, and the other for test.</p><p>For radial and spiral sampling patterns, a synthesized kspace data from Human Connectome Project (HCP) MR dataset (https://db.humanconnectome.org) were used. Specifically, the multi-coil radial and spiral k-space data are generated using MRI simulator (http://bigwww.epfl.ch/algorithms/mrireconstruction/). The T2 weighted brain images contained within the HCP were acquired Siemens 3T MR system using a 3D spin-echo sequence. The TR and TE were 3200 ms and 565 ms, respectively. The number of coils was 8, but the final reconstruction was obtained as the absolute of the sum. The FOV was 224 × 224 mm 2 , and the size of acquisition matrix was 320 × 320. The voxel size was 0.7 mm. The total of 199 subject datasets was used in this paper. Among the 199 subject, 180 were used for network training, 10 subject for validation, and the other subject for test. Fig. <ref type="figure" target="#fig_2">5</ref>(b) shows the downsampled k-space radial sampling patterns. The downsampled radial k-space consists of only 83 spokes, which corresponds to R = 6 acceleration factor compared to the 503 spokes for the fully sampled data that were used as the ground-truth. On the other hand, Fig. <ref type="figure" target="#fig_2">5(c</ref>) shows the down-sampled spiral sampling pattern, composed of 4 interleaves that corresponds to R = 4 acceleration compared to the he full spiral trajectory with 16 interleaves. The spiral k-space trajectory was obtained with a variable density factor (VDF) of 2.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation</head><p>For quantitative evaluation, the normalized mean square error (NMSE) value was used, which is defined as</p><formula xml:id="formula_27">N M SE = M i=1 N j=1 [x * (i, j) -x(i, j)] 2 M i=1 N j=1 [x * (i, j)] 2 , (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>where x and x * denote the reconstructed images and ground truth, respectively. M and N are the number of pixel for row and column. We also use the peak signal to noise ratio (PSNR), which is defined by</p><formula xml:id="formula_29">P SN R = 20 • log 10 N M x * ∞ x -x * 2 . (<label>21</label></formula><formula xml:id="formula_30">)</formula><p>We also use the structural similarity (SSIM) index <ref type="bibr" target="#b27">[28]</ref>, defined as</p><formula xml:id="formula_31">SSIM = (2µ x µ x * + c 1 )(2σ xx * + c 2 ) (µ 2 x + µ 2 x * + c 1 )(σ 2 x + σ 2 x * + c 2 ) ,<label>(22)</label></formula><p>where µ x is a average of x, σ 2</p><p>x is a variance of x and σ xx * is a covariance of x and x * . There are two variables to stabilize the division such as c 1 = (k 1 L) 2 and c 2 = (k 2 L) 2 . L is a dynamic range of the pixel intensities. k 1 and k 2 are constants by default k 1 = 0.01 and k 2 = 0.03.</p><p>For extensive comparative study, we also compared with the following algorithms: total variation (TV) penalized CS, ALOHA <ref type="bibr" target="#b14">[15]</ref>, and four types of CNN models including the variational model <ref type="bibr" target="#b4">[5]</ref>, a cascade model <ref type="bibr" target="#b8">[9]</ref>, the cross-domain model called KIKI network <ref type="bibr" target="#b28">[29]</ref>, and an image-domain model <ref type="bibr" target="#b6">[7]</ref>. In particular, <ref type="bibr" target="#b6">[7]</ref> is a representative example of Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Specifically, the image domain residual learning using the standard U-Net backbone in Fig. <ref type="figure">4</ref> was used. Unlike the proposed network, the input and output are an artifact corrupted image and artifact-only image, respectively <ref type="bibr" target="#b7">[8]</ref>. In addition, the variational model <ref type="bibr" target="#b4">[5]</ref> and the cascade model <ref type="bibr" target="#b8">[9]</ref> represent Fig. <ref type="figure" target="#fig_0">1(b)</ref>. The cross-domain model is formed by linking the k-space model in Fig. <ref type="figure" target="#fig_0">1(d</ref>) and the image-domain model in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Unfortunately, Fig. <ref type="figure" target="#fig_0">1(c</ref>) does not scale well due to the enormous memory requirement, so cannot be used in the comparative study. For fair comparison, the cascade <ref type="bibr" target="#b8">[9]</ref> and cross-domain <ref type="bibr" target="#b28">[29]</ref> networks were modified for parallel imaging. All the neural networks were trained using exactly the same data set. For ALOHA <ref type="bibr" target="#b14">[15]</ref> and the proposed method in Fig. <ref type="figure" target="#fig_1">3(b)</ref>, the total variation based k-space weighting was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>To evaluate the performance of sparsifications in the single coil, the proposed method was trained using the sparsifications as shown in as Figs. <ref type="figure" target="#fig_1">3(a)(b)</ref>. Fig. <ref type="figure" target="#fig_3">6</ref> shows the objective functions along the trajectories such as (a) Cartesian, (b) radial, and (c) spiral trajectory. In the Cartesian trajectory, the proposed network in Fig. <ref type="figure" target="#fig_1">3</ref>(b) produces the lowest curve in the validation phase (see red curves in Fig. <ref type="figure" target="#fig_3">6(a)</ref>). The proposed network in Fig <ref type="figure" target="#fig_1">3(a)</ref> shows the best convergence during the training, but the generalization at the test phase was not good (see green curves in Fig. <ref type="figure" target="#fig_3">6(a)</ref>). In the non-Cartesian trajectories, the best convergence appears using the proposed Fig. <ref type="figure">7</ref>: Reconstruction results from Cartesian trajectory at R = 3 in single coil. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.  network with only skipped connection in Fig. <ref type="figure" target="#fig_1">3</ref>(a) (see green curves in Fig. <ref type="figure" target="#fig_3">6</ref>(b)(c)). Based on these convergence behaviour and generalization, the proposed network was trained with different sparsification schemes. The network in Fig. <ref type="figure" target="#fig_1">3(b</ref>) was trained for the Cartesian trajectory and the network in Fig. <ref type="figure" target="#fig_1">3</ref>(a) was used for the non-Cartesian trajectories such as radial and spiral trajectories. Fig. <ref type="figure">7</ref> shows the results of single coil reconstruction from Cartesian trajectory using the architecture with skipped connection and weighting layer as shown in Fig. <ref type="figure" target="#fig_1">3(a)</ref>. While all the algorithms provide good reconstruction results, the pro- posed method most accurately recovered high frequency edges and textures as shown in the enlarged images and difference images of Fig. <ref type="figure">7</ref>. Fig. <ref type="figure" target="#fig_4">8</ref> shows the reformed images along the (i) coronal and (ii) sagittal directions. Again, the reformatted coronal and sagittal images by the proposed method preserved the most detailed structures of underlying images without any artifact along the slice direction. The quantitative comparison in Table I in terms of average PSNR, NMSE, and SSIM values also confirmed that the proposed k-space interpolation method produced the best quantitative values in all area. The computation time of the proposed method is slightly slower than the image-domain learning because of the weighting and Fourier transform operations, but it is still about 3.5 times faster than the total variation penalized compressed sensing (CS) approach.</p><p>Fig. <ref type="figure" target="#fig_5">9</ref> shows the parallel imaging results from eight coil   measurement. Because the ALOHA <ref type="bibr" target="#b14">[15]</ref> and the proposed method directly interpolate missing k-space, these methods clearly preserve textures detail structures as shown in Fig. <ref type="figure" target="#fig_5">9</ref>. However, ALOHA <ref type="bibr" target="#b14">[15]</ref> is more than 100 times slower than the k-space deep learning as shown in Table <ref type="table" target="#tab_3">II</ref>. All CNN methods except the imaging-domain model outperform than CS methods. Although the cascade model <ref type="bibr" target="#b8">[9]</ref> was performed with data consistency step, the method did not completely overcome the limitations of image-domain learning. In the cross-domain learning <ref type="bibr" target="#b28">[29]</ref>, they proposed to train k-space and image-domain sequentially. The cross-domain network consists of deeper layers (100 layers; 25 layers × 4 individual models) than the proposed method, but the performance was worse than our method. As shown Table <ref type="table" target="#tab_3">II</ref>, the proposed method shows best performance in terms of average PSNR, NMSE, and SSIM values.</p><p>Fig. <ref type="figure" target="#fig_6">10</ref> shows the reconstruction images from x6 accelerated radial sampling patterns using the architecture in Fig. <ref type="figure" target="#fig_1">3</ref>(a) for 8 coils parallel imaging. The results for single coil are shown in Fig. <ref type="figure" target="#fig_0">S1</ref> in Supplementary Material. The proposed k-space deep learning provided realistic image quality and preserves the detailed structures as well as the textures, but the image domain network failed to remove the noise signals and the total variation method did not preserve the realistic textures and sophisticated structures. Our method also provides much smaller NMSE values, as shown at the bottom of each Fig. <ref type="figure" target="#fig_6">10</ref> and Fig. <ref type="figure" target="#fig_0">S1</ref>     all slices and 9 subjects. The proposed k-space deep learning provided the best quantitative values. Fig. <ref type="figure" target="#fig_7">11</ref> shows the reconstruction images from x4 accelerated spiral trajectory for 8 coils parallel imaging and Fig. <ref type="figure">S2</ref> in Supplementary Material illustrate the single coil results, respectively. Similar to the radial sampling patterns, the proposed method provides significantly improved image reconstruction results, and the average PSNR, NMSE and SSIM values in Table <ref type="table" target="#tab_8">IV</ref> and Table <ref type="table">S2</ref> in Supplementary Material also confirm that the proposed method consistently outperform other method for all patients.</p><p>To evaluate the improvements of the proposed method, a   radiologist (with 9 years of experience) thoroughly reviewed the reconstructed images. For radial trajectory images, the pyramidal tract (arrows in Fig. <ref type="figure" target="#fig_1">S3</ref>(i) of the Supplementary Material), a bundle of nerve fibers conducting motor signal from the motor cortex to the brainstem or to the spinal cord, was evaluated. It was noted that high signal intensity of the pyramidal tract was exaggerated on the TV method, which could be misdiagnosed as an abnormal finding. In addition, the ability to discriminate a pair of internal cerebral veins were evaluated. While the TV and image-domain learning methods can not differentiate the two veins, the proposed method is able to show the two internal cerebral veins separately (arrows on Fig. <ref type="figure" target="#fig_1">S3</ref>(ii) in Supplementary Material). With regard to spiral trajectory images (Fig. <ref type="figure">S4</ref> in the Supplementary Material), the TV method shows bright dot-like artifacts along several slices (Fig. <ref type="figure">S4</ref>(i)). When the small T2 hyperintensity lesions in the left frontal lobe were evaluated, the TV method fails to demonstrate the lesions. The image domain learning preserves the lesions, but the margin of the lesions is blurry in the noisy background. The small lesions are clearly depicted on the proposed method (arrows in Fig. <ref type="figure">S4</ref>(ii)). Overall, the quality of image reconstruction of the proposed method was superior to that of other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>In order to improve the performance of ALOHA, the matrix pencil size should be significantly large, which is not possible in standard ALOHA formulation due to the large memory requirement and extensive computational burden. We believe that one of the important reasons that the proposed k-space deep learning provides better performance than ALOHA is that the cascaded convolution results in much longer filter length. Nevertheless, with the same set of trained filters, our network can be adapted to different input images due to the combinatorial nature of ReLU nonlinearity. We believe that this contributes to the benefits of k-space learning over ALOHA. Moreover, efficient signal representation in k-space domain, which is the key idea of ALOHA, can synergistically work with the expressivity of the neural network to enable better performance than image domain learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Inspired by a link between the ALOHA and deep learning, this paper showed that fully data-driven k-space interpolation is feasible by using k-space deep learning and the image domain loss function. The proposed k-space interpolation network outperformed the existing image domain deep learning for various sampling trajectory. As the proposed k-space interpolation framework is quite effective and also supported by novel theory, so we believe that this opens a new area of researches for many Fourier imaging problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Deep learning frameworks for accelerated MRI: (a) image domain learning [6], [7], (b) cascaded network [4], [5], [9], (c) AUTOMAP [10], and (d) the proposed k-space learning. IFT: Inverse Fourier transform.</figDesc><graphic coords="1,355.91,321.81,163.18,51.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Overall reconstruction flows of the proposed method with (a) skipped connection, and (b) skipped connection and weighting layer. IFT denotes the inverse Fourier transform.For the parallel imaging, the input and output are multi-coil k-space data, after stacking them along the channel direction.</figDesc><graphic coords="4,311.98,127.55,251.05,77.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Various under-sampling patterns: (a) Cartesian undersampling at R = 3, (b) radial undersampling at R = 6, and (c) spiral undersampling at R = 4. Magnified views are provided for radial and spiral trajectories.</figDesc><graphic coords="6,61.52,56.07,225.95,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Objective functions of a single coil for (a) Cartesian, (b) radial, and (c) spiral trajectories. Dashed and solid lines indicate an objective function of the train and validation phase, respectively.</figDesc><graphic coords="6,48.96,601.98,251.05,90.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: (i) Coronal and (ii) sagittal reformated reconstruction results from Cartesian trajectory at R = 3 in single coil.The results from first to last rows indicate ground-truth, downsampled, total variation, image-domain learning and the proposed method. Yellow and red boxes illustrate the enlarged and difference views, respectively. The difference images were amplified five times. The number written to the images is the NMSE value.</figDesc><graphic coords="7,324.53,56.07,225.95,372.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Reconstruction results from Cartesian trajectory at R = 3 in multi coils: (a) axial, (b-i) coronal and (b-ii) sagittal reconstruction results. Yellow and red boxes illustrate the enlarged and difference views, respectively. The difference images were amplified five times. The number written to the images is the NMSE value.</figDesc><graphic coords="8,74.67,236.04,462.65,160.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Reconstruction results from radial trajectory at R = 6 in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc><graphic coords="9,61.52,56.07,225.95,373.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Reconstruction results from spiral trajectory at R = 4 in 8 coils parallel imaging. The difference images were amplified five times. Yellow and red boxes illustrate the enlarged and difference views, respectively. The number written to the images is the NMSE value.</figDesc><graphic coords="9,324.53,56.07,225.95,372.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative comparison from Cartesian trajectory at R = 3 in single coil.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Quantitative comparison from Cartesian trajectory at R = 3 in 8 coils parallel imaging.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>in Supplementary Material. Average PSNR, NMSE and SSIM values are shown in Table III and Table S1 in Supplementary Material for multi coils and single coil cases, respectively. The average values were calculated across</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Quantitative comparison from radial undersampling at R = 6 in 8 coils parallel imaging.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative comparison from spiral undersampling at R = 4 in 8 coils parallel imaging.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating magnetic resonance imaging via deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="514" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a variational network for reconstruction of accelerated MRI data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klatzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3055" to="3071" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for accelerated MRI using magnitude and phase networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning with domain adaptation for accelerated projection reconstruction MR</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1002/mrm.27106</idno>
		<ptr target="https://doi.org/10.1002/mrm.27106" />
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep cascade of convolutional neural networks for dynamic mr image reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="491" to="503" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image reconstruction by domain-transform manifold learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="issue">7697</biblScope>
			<biblScope unit="page">487</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse MRI: The application of compressed sensing for rapid mr imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">k-t FOCUSS: A general compressed sensing framework for high resolution dynamic MRI</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="116" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Calibrationless parallel imaging reconstruction based on structured low-rank matrix completion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ohliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Vigneron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="959" to="970" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-rank modeling of local k-space neighborhoods (loraks) for constrained mri</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Haldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="668" to="681" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general framework for compressed sensing and parallel MRI using annihilating filter based low-rank Hankel matrix</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="495" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Off-the-grid recovery of piecewise constant images from few Fourier samples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1004" to="1041" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MoDL: Model-based deep learning architecture for inverse problems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="405" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compressive sampling using annihilating filter-based low-rank interpolation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="777" to="801" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional framelets: A general deep learning framework for inverse problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="991" to="1048" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scan-specific robust artificial-neural-networks for k-space interpolation (RAKI) reconstruction: Database-free deep learning for fast imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Akc ¸akaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weingärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ugurbil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="439" to="453" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Off-the-grid model based deep learning (O-MODL)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pramanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10747</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-shot sensitivityencoded diffusion MRI using model-based deep learning (MODL-MUSSELS)</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08115</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sampling signals with finite rate of innovation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1417" to="1428" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast algorithm for convolutional structured low-rank matrix recovery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="550" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding geometry of encoderdecoder CNNs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07647</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on International Conference on Machine Learning (ICML). also available as arXiv preprint</title>
		<meeting>the 2019 International Conference on International Conference on Machine Learning (ICML). also available as arXiv preprint</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for Matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kiki-net: crossdomain convolutional neural networks for reconstructing undersampled magnetic resonance images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
